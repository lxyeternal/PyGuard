"""
Collect malicious PyPI packages from OSV database and BigQuery.
"""
import os
import git
import json
import requests
from google.cloud import bigquery
from urllib.parse import urlparse


class MaliciousPackageCollector:

    OSV_REPO_URL = "https://github.com/ossf/malicious-packages.git"

    def __init__(self, config_path=None):
        self.base_dir = os.path.dirname(os.path.abspath(__file__))
        self.records_dir = os.path.join(os.path.dirname(self.base_dir), "Records")
        self.dataset_dir = os.path.join(self.base_dir, "pypi_dataset")
        self.repo_path = os.path.join(self.base_dir, "malicious-packages")

        os.makedirs(self.records_dir, exist_ok=True)
        os.makedirs(self.dataset_dir, exist_ok=True)

        # Load Google Cloud credentials from config file
        self.google_cloud_key_path = config_path or os.path.join(
            self.base_dir, "google_cloud_key.json"
        )

        self.collected_packages = self._load_collected_packages()
        self.processed_ids = self._load_processed_ids()
        self.new_osv_data = []


    def _load_collected_packages(self):
        collected = {'pip': set()}
        for source in ['osv', 'snyk']:
            filepath = os.path.join(self.records_dir, f"{source}_pip_packages.json")
            if os.path.exists(filepath):
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if isinstance(data, dict) and "packages" in data:
                            collected['pip'].update(data["packages"].keys())
                except Exception as e:
                    print(f"Failed to read {source} record: {e}")
        return collected


    def _load_processed_ids(self):
        processed = {'pip': set()}
        filepath = os.path.join(self.records_dir, "processed_osv_ids.json")
        if os.path.exists(filepath):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, dict):
                        processed['pip'] = set(data.get('pip', []))
            except Exception as e:
                print(f"Failed to read processed IDs: {e}")
        return processed


    def _save_processed_ids(self):
        filepath = os.path.join(self.records_dir, "processed_osv_ids.json")
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump({"pip": list(self.processed_ids['pip'])}, f, indent=2)
        except Exception as e:
            print(f"Failed to save processed IDs: {e}")


    def clone_or_pull_repo(self):
        if not os.path.exists(self.repo_path):
            print(f"Cloning repository to {self.repo_path}")
            git.Repo.clone_from(self.OSV_REPO_URL, self.repo_path)
        else:
            print("Pulling latest changes")
            repo = git.Repo(self.repo_path)
            repo.remotes.origin.pull()


    def filter_new_osv_files(self):
        osv_dir = os.path.join(self.repo_path, "osv", "malicious", "pypi")
        print(f"Scanning OSV files in {osv_dir}")

        for root, _, files in os.walk(osv_dir):
            for file in files:
                if not file.endswith('.json'):
                    continue
                json_path = os.path.join(root, file)
                self._process_osv_file(json_path)

        # Sort by OSV ID (newest first)
        self.new_osv_data.sort(key=lambda x: self._parse_osv_id(x['osv_id']), reverse=True)


    def _process_osv_file(self, json_path):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                osv_data = json.load(f)

            osv_id = osv_data.get("id", "")
            if osv_id in self.processed_ids['pip']:
                return

            for affected in osv_data.get("affected", []):
                if not isinstance(affected, dict):
                    continue
                package_info = affected.get("package", {})
                if not isinstance(package_info, dict):
                    continue

                pkg_name = package_info.get("name", "")
                versions = self._get_affected_versions(affected)
                self.new_osv_data.append({
                    "osv_id": osv_id,
                    "pkg_name": pkg_name,
                    "versions": versions,
                    "file_path": json_path
                })
        except Exception as e:
            print(f"Error processing {json_path}: {e}")


    def _parse_osv_id(self, osv_id):
        """Parse OSV ID for sorting. Example: MAL-2024-11592 -> (2024, 11592)"""
        if not osv_id or not osv_id.startswith('MAL-'):
            return (0, 0)
        parts = osv_id.split('-')
        if len(parts) != 3:
            return (0, 0)
        try:
            return (int(parts[1]), int(parts[2]))
        except ValueError:
            return (0, 0)


    def _get_affected_versions(self, affected):
        if "versions" in affected and isinstance(affected["versions"], list):
            return affected["versions"]

        versions = []
        for range_info in affected.get("ranges", []):
            if not isinstance(range_info, dict):
                continue
            for event in range_info.get("events", []):
                if isinstance(event, dict) and event.get("introduced") in ["0", 0]:
                    versions.append("0")
        return versions


    def collect_packages(self):
        for entry in self.new_osv_data:
            osv_id = entry["osv_id"]
            pkg_name = entry["pkg_name"]
            versions = entry["versions"]
            file_path = entry["file_path"]

            if pkg_name in self.collected_packages['pip']:
                print(f"Already collected: {pkg_name}")
                continue

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    osv_data = json.load(f)

                query_result = self.query_bigquery([pkg_name])
                if query_result:
                    self._download_packages(query_result, versions)
                    self._save_package_info(osv_data, pkg_name, versions)
                    self.collected_packages['pip'].add(pkg_name)

            except Exception as e:
                print(f"Failed to process {pkg_name}: {e}")

            self.processed_ids['pip'].add(osv_id)
            self._save_processed_ids()


    def query_bigquery(self, names):
        client = bigquery.Client.from_service_account_json(self.google_cloud_key_path)

        names_condition = ' OR '.join([f'name = "{name}"' for name in names])
        query = f"""
        SELECT name, version, path
        FROM `bigquery-public-data.pypi.distribution_metadata`
        WHERE {names_condition}
        """

        results = {}
        for row in client.query(query).result():
            pkg_name = row.name.lower()
            version = row.version

            if pkg_name not in results:
                results[pkg_name] = {'versions': {}}
            if version not in results[pkg_name]['versions']:
                results[pkg_name]['versions'][version] = {'paths': set()}
            if row.path:
                results[pkg_name]['versions'][version]['paths'].add(row.path)

        return results


    def _download_packages(self, query_results, target_versions):
        for pkg_name, pkg_info in query_results.items():
            for version, version_info in pkg_info['versions'].items():
                if "0" not in target_versions and version not in target_versions:
                    continue

                paths = version_info.get('paths', set())
                if not paths:
                    print(f"No download path for {pkg_name}@{version}")
                    continue

                # Prefer zip > tar.gz > whl
                sorted_paths = sorted(paths, key=self._get_file_priority)
                selected_path = sorted_paths[0]
                url = f"https://files.pythonhosted.org/packages/{selected_path}"

                save_dir = os.path.join(self.dataset_dir, pkg_name, version)
                os.makedirs(save_dir, exist_ok=True)
                file_name = os.path.basename(urlparse(url).path)
                save_path = os.path.join(save_dir, file_name)

                try:
                    response = requests.get(url)
                    if response.status_code == 200:
                        with open(save_path, 'wb') as f:
                            f.write(response.content)
                        print(f"Downloaded: {save_path}")
                    else:
                        print(f"Failed to download {url}: {response.status_code}")
                except Exception as e:
                    print(f"Error downloading {url}: {e}")


    def _get_file_priority(self, path):
        if path.endswith('.zip'):
            return 0
        elif path.endswith('.tar.gz'):
            return 1
        elif path.endswith('.whl'):
            return 2
        return 3


    def _save_package_info(self, osv_data, pkg_name, versions):
        pkg_info = {
            "name": pkg_name,
            "affected_version": versions[0] if versions else "",
            "published": osv_data.get("published", ""),
            "modified": osv_data.get("modified", ""),
            "summary": osv_data.get("summary", ""),
            "details": osv_data.get("details", ""),
        }

        filepath = os.path.join(self.records_dir, "osv_pip_packages.json")
        if os.path.exists(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
        else:
            data = {"packages": {}}

        data["packages"][pkg_name] = pkg_info

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)


    def start(self, max_packages=None):
        print("Starting malicious package collection...")
        self.clone_or_pull_repo()
        self.filter_new_osv_files()

        if max_packages and len(self.new_osv_data) > max_packages:
            print(f"Limiting to {max_packages} packages (found {len(self.new_osv_data)})")
            self.new_osv_data = self.new_osv_data[:max_packages]

        self.collect_packages()
        print("Collection completed!")


if __name__ == '__main__':
    collector = MaliciousPackageCollector()
    collector.start(max_packages=500)
