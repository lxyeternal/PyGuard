{
  "metadata": {
    "package_name": "pandas_gbq-0",
    "original_json_path": "/home2/blue/Documents/PyPIAgent/Codes/code_contexral/codesnippets/benign/pandas_gbq-0.28.0.json",
    "dataset_type": "benign"
  },
  "code_files": [
    {
      "pyfile": "gbq.py",
      "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/pandas_gbq-0.28.0/pandas_gbq-0.28.0/pandas_gbq/gbq.py",
      "line_number": "516",
      "type_description": "shady-links",
      "context_snippet": "    def _download_results(\n        self,\n        rows_iter,\n        max_results=None,\n        progress_bar_type=None,\n        user_dtypes=None,\n    ):\n        # No results are desired, so don't bother downloading anything.\n        if max_results == 0:\n            return None\n\n        if user_dtypes is None:\n            user_dtypes = {}\n\n        create_bqstorage_client = self.use_bqstorage_api\n        if max_results is not None:\n            create_bqstorage_client = False\n\n        # If we're downloading a large table, BigQuery DataFrames might be a\n        # better fit. Not all code paths will populate rows_iter._table, but\n        # if it's not populated that means we are working with a small result\n        # set.\n        if (table_ref := getattr(rows_iter, \"_table\", None)) is not None:\n            table = self.client.get_table(table_ref)\n            if (\n                isinstance((num_bytes := table.num_bytes), int)\n                and num_bytes > pandas_gbq.constants.BYTES_TO_RECOMMEND_BIGFRAMES\n            ):\n                num_gib = num_bytes / pandas_gbq.constants.BYTES_IN_GIB\n                warnings.warn(\n                    f\"Recommendation: Your results are {num_gib:.1f} GiB. \"\n                    \"Consider using BigQuery DataFrames (https://bit.ly/bigframes-intro)\"\n                    \"to process large results with pandas compatible APIs with transparent SQL \"\n                    \"pushdown to BigQuery engine. This provides an opportunity to save on costs \"\n                    \"and improve performance. \"\n                    \"Please reach out to bigframes-feedback@google.com with any \"\n                    \"questions or concerns. To disable this message, run \"\n                    \"warnings.simplefilter('ignore', category=pandas_gbq.exceptions.LargeResultsWarning)\",\n                    category=pandas_gbq.exceptions.LargeResultsWarning,\n                    # user's code\n                    # -> read_gbq\n                    # -> run_query\n                    # -> download_results\n                    stacklevel=4,\n                )\n\n        try:\n            schema_fields = [field.to_api_repr() for field in rows_iter.schema]\n            conversion_dtypes = _bqschema_to_nullsafe_dtypes(schema_fields)\n            conversion_dtypes.update(user_dtypes)\n            df = rows_iter.to_dataframe(\n                dtypes=conversion_dtypes,\n                progress_bar_type=progress_bar_type,\n                create_bqstorage_client=create_bqstorage_client,\n            )\n        except self.http_error as ex:\n            self.process_http_error(ex)\n\n        df = _finalize_dtypes(df, schema_fields)\n\n        logger.debug(\"Got {} rows.\\n\".format(rows_iter.total_rows))\n        return df",
      "hash_value": "15d352b510897ce494357bf79ade9792",
      "detection_index": 1,
      "code_snippets": [
        {
          "snippet": "    def _download_results(\n        self,\n        rows_iter,\n        max_results=None,\n        progress_bar_type=None,\n        user_dtypes=None,\n    ):\n        # No results are desired, so don't bother downloading anything.\n        if max_results == 0:\n            return None\n\n        if user_dtypes is None:\n            user_dtypes = {}\n\n        create_bqstorage_client = self.use_bqstorage_api\n        if max_results is not None:\n            create_bqstorage_client = False\n\n        # If we're downloading a large table, BigQuery DataFrames might be a\n        # better fit. Not all code paths will populate rows_iter._table, but\n        # if it's not populated that means we are working with a small result\n        # set.\n        if (table_ref := getattr(rows_iter, \"_table\", None)) is not None:\n            table = self.client.get_table(table_ref)\n            if (\n                isinstance((num_bytes := table.num_bytes), int)\n                and num_bytes > pandas_gbq.constants.BYTES_TO_RECOMMEND_BIGFRAMES\n            ):\n                num_gib = num_bytes / pandas_gbq.constants.BYTES_IN_GIB\n                warnings.warn(\n                    f\"Recommendation: Your results are {num_gib:.1f} GiB. \"\n                    \"Consider using BigQuery DataFrames (https://bit.ly/bigframes-intro)\"\n                    \"to process large results with pandas compatible APIs with transparent SQL \"\n                    \"pushdown to BigQuery engine. This provides an opportunity to save on costs \"\n                    \"and improve performance. \"\n                    \"Please reach out to bigframes-feedback@google.com with any \"\n                    \"questions or concerns. To disable this message, run \"\n                    \"warnings.simplefilter('ignore', category=pandas_gbq.exceptions.LargeResultsWarning)\",\n                    category=pandas_gbq.exceptions.LargeResultsWarning,\n                    # user's code\n                    # -> read_gbq\n                    # -> run_query\n                    # -> download_results\n                    stacklevel=4,\n                )\n\n        try:\n            schema_fields = [field.to_api_repr() for field in rows_iter.schema]\n            conversion_dtypes = _bqschema_to_nullsafe_dtypes(schema_fields)\n            conversion_dtypes.update(user_dtypes)\n            df = rows_iter.to_dataframe(\n                dtypes=conversion_dtypes,\n                progress_bar_type=progress_bar_type,\n                create_bqstorage_client=create_bqstorage_client,\n            )\n        except self.http_error as ex:\n            self.process_http_error(ex)\n\n        df = _finalize_dtypes(df, schema_fields)\n\n        logger.debug(\"Got {} rows.\\n\".format(rows_iter.total_rows))\n        return df",
          "triple_sequences": [
            {
              "action_api": "getattr()",
              "action_description": "Checks if specified path exists in filesystem",
              "action_id": "check_path_exists",
              "object": "rows_iter, \"_table\", None",
              "object_description": "Local file or directory name",
              "object_id": "local_file_or_directory_name",
              "intention_description": "Determine_directory_presence",
              "intention_id": "determine_directory_presence"
            },
            {
              "action_api": "self.client.get_table()",
              "action_description": "Retrieves value and data type for registry value",
              "action_id": "get_registry_value",
              "object": "table_ref",
              "object_description": "Registry value name",
              "object_id": "registry_value_name",
              "intention_description": "Access_attribute_value",
              "intention_id": "access_attribute_value"
            },
            {
              "action_api": "isinstance()",
              "action_description": "Checks if object is instance of specified type",
              "action_id": "check_instance_type",
              "object": "table.num_bytes, int",
              "object_description": "Character code array",
              "object_id": "character_code_array",
              "intention_description": "Prepare_data_processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "warnings.warn()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "f\"Recommendation: Your results are {num_gib:.1f} GiB. ...\", category=pandas_gbq.exceptions.LargeResultsWarning, stacklevel=4",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Access_attribute_value",
              "intention_id": "access_attribute_value"
            },
            {
              "action_api": "field.to_api_repr()",
              "action_description": "Retrieves value and data type for registry value",
              "action_id": "get_registry_value",
              "object": "field",
              "object_description": "Registry value",
              "object_id": "registry_value",
              "intention_description": "Access_attribute_value",
              "intention_id": "access_attribute_value"
            },
            {
              "action_api": "_bqschema_to_nullsafe_dtypes()",
              "action_description": "Retrieves value and data type for registry value",
              "action_id": "get_registry_value",
              "object": "schema_fields",
              "object_description": "Registry value",
              "object_id": "registry_value",
              "intention_description": "Access_attribute_value",
              "intention_id": "access_attribute_value"
            },
            {
              "action_api": "conversion_dtypes.update()",
              "action_description": "Sets value for registry key",
              "action_id": "set_registry_value",
              "object": "user_dtypes",
              "object_description": "Registry value",
              "object_id": "registry_value",
              "intention_description": "Access_attribute_value",
              "intention_id": "access_attribute_value"
            },
            {
              "action_api": "rows_iter.to_dataframe()",
              "action_description": "Deserializes JSON response body to Python object",
              "action_id": "deserialize_json_response",
              "object": "dtypes=conversion_dtypes, progress_bar_type=progress_bar_type, create_bqstorage_client=create_bqstorage_client",
              "object_description": "JSON string",
              "object_id": "json_string",
              "intention_description": "Parse_json_data",
              "intention_id": "parse_json_data"
            },
            {
              "action_api": "self.process_http_error()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "ex",
              "object_description": "Exception type name",
              "object_id": "exception_type",
              "intention_description": "Access_command_error_output",
              "intention_id": "access_command_error_output"
            },
            {
              "action_api": "_finalize_dtypes()",
              "action_description": "Sets value for registry key",
              "action_id": "set_registry_value",
              "object": "df, schema_fields",
              "object_description": "Registry value",
              "object_id": "registry_value",
              "intention_description": "Prepare_data_processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "logger.debug()",
              "action_description": "Retrieves value and data type for registry value",
              "action_id": "get_registry_value",
              "object": "\"Got {} rows.\\n\".format(rows_iter.total_rows)",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Access_attribute_value",
              "intention_id": "access_attribute_value"
            }
          ]
        }
      ]
    }
  ]
}