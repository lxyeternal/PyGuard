{
  "metadata": {
    "package_name": "semgrep-1",
    "original_json_path": "/home2/blue/Documents/PyPIAgent/Codes/code_contexral/codesnippets/benign_bandit4mal/semgrep-1.118.0.json",
    "dataset_type": "benign_bandit4mal"
  },
  "code_files": [
    {
      "pyfile": "scans.py",
      "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/semgrep-1.118.0/semgrep-1.118.0/src/semgrep/app/scans.py",
      "line_number": "579",
      "type_description": "B821:post",
      "context_snippet": "@tracing.trace()\ndef report_findings(\n    self,\n    *,\n    matches_by_rule: RuleMatchMap,\n    rules: List[Rule],\n    targets: Set[Path],\n    renamed_targets: Set[Path],\n    ignored_targets: FrozenSet[Path],\n    cli_suggested_exit_code: int,\n    parse_rate: ParsingData,\n    total_time: float,\n    commit_date: str,\n    lockfile_dependencies: Dict[str, List[out.FoundDependency]],\n    dependency_parser_errors: List[DependencyParserError],\n    all_subprojects: List[Union[out.UnresolvedSubproject, out.ResolvedSubproject]],\n    contributions: out.Contributions,\n    engine_requested: \"EngineType\",\n    progress_bar: \"Progress\",\n) -> out.CiScanCompleteResponse:\n    \"\"\"\n    commit_date here for legacy reasons. epoch time of latest commit\n\n    Returns (success, block_scan, block_reason)\n    \"\"\"\n    state = get_state()\n    rule_ids = [out.RuleId(r.id) for r in rules]\n    all_matches = [\n        match\n        for matches_of_rule in matches_by_rule.values()\n        for match in matches_of_rule\n    ]\n    # we want date stamps assigned by the app to be assigned such that the\n    # current sort by relevant_since results in findings within a given scan\n    # appear in an intuitive order.  this requires reversed ordering here.\n    all_matches.reverse()\n    sort_order = {  # used only to order rules by severity\n        out.Experiment(): 0,\n        out.Inventory(): 1,\n        out.Info(): 2,\n        out.Low(): 2,\n        out.Warning(): 3,\n        out.Medium(): 3,\n        out.Error(): 4,\n        out.High(): 4,\n        out.Critical(): 5,\n    }\n    # NB: sorted guarantees stable sort, so within a given severity level\n    # issues remain sorted as before\n    all_matches = sorted(\n        all_matches, key=lambda match: sort_order[match.severity.value]\n    )\n    new_ignored, new_matches = partition(\n        all_matches, lambda match: match.match.extra.is_ignored\n    )\n\n    # Autofix is currently the only toggle in the App that\n    # indicates we are going to store your code. Until we\n    # have a dedicated toggle that allows users to opt-in\n    # to us storing their code we ommit code unless autofix\n    # is set.\n\n    findings = [\n        match.to_app_finding_format(\n            commit_date,\n            remove_dataflow_content=not self.autofix,\n        )\n        for match in new_matches\n    ]\n    ignores = [\n        match.to_app_finding_format(\n            commit_date,\n            remove_dataflow_content=not self.autofix,\n        )\n        for match in new_ignored\n    ]\n    token = (\n        # GitHub (cloud)\n        os.getenv(\"GITHUB_TOKEN\")\n        # GitLab.com (cloud)\n        or os.getenv(\"GITLAB_TOKEN\")\n        # Bitbucket Cloud\n        or os.getenv(\"BITBUCKET_TOKEN\")\n    )\n\n    self.ci_scan_results = out.CiScanResults(\n        # send a backup token in case the app is not available\n        token=token,\n        findings=findings,\n        ignores=ignores,\n        searched_paths=[out.Fpath(str(t)) for t in sorted(targets)],\n        renamed_paths=[out.Fpath(str(rt)) for rt in sorted(renamed_targets)],\n        rule_ids=rule_ids,\n        contributions=contributions,\n    )\n    if self.dependency_query:\n        self.ci_scan_results.dependencies = out.CiScanDependencies(\n            lockfile_dependencies\n        )\n\n    findings_and_ignores = self.ci_scan_results.to_json()\n\n    if any(\n        isinstance(match.severity.value, out.Experiment) for match in new_ignored\n    ):\n        logger.info(\"Some experimental rules were run during execution.\")\n\n    ignored_ext_freqs = Counter(\n        [os.path.splitext(path)[1] for path in ignored_targets]\n    )\n    ignored_ext_freqs.pop(\"\", None)  # don't count files with no extension\n\n    dependency_counts = {k: len(v) for k, v in lockfile_dependencies.items()}\n\n    # NOTE: This mirrors the logic in metrics.py to show the number of\n    #  findings by product for SCP customers. See PA-3312\n    #  We should consider refactoring this logic into a shared function\n    #  in a future PR for metric and behavioral consistency.\n    #  An open question remains on whether we should be including the number\n    #  of ignored findings in this count.\n\n    findings_by_product: Dict[str, int] = Counter()\n    for r, f in matches_by_rule.items():\n        # NOTE: For parity with metrics.py, we are using the human-readable product name,\n        #  (i.e. code) and falling back to the internal json string (i.e. sast) if we\n        #  somehow drift out of sync with the product enum.\n        name = USER_FRIENDLY_PRODUCT_NAMES.get(r.product, r.product.to_json())\n        findings_by_product[f\"{name}\"] += len(f)\n\n    subproject_stats: List[out.SubprojectStats] = []\n    if all_subprojects:\n        for subproject in all_subprojects:\n            if isinstance(subproject, out.UnresolvedSubproject):\n                stats = subproject_to_stats(subproject.info)\n            else:\n                stats = resolved_subproject_to_stats(subproject)\n            subproject_stats.append(stats)\n\n    complete = out.CiScanComplete(\n        exit_code=cli_suggested_exit_code,\n        dependency_parser_errors=dependency_parser_errors,\n        stats=out.CiScanCompleteStats(\n            findings=len(\n                [match for match in new_matches if not match.from_transient_scan]\n            ),\n            # We do not report errors anymore since they are large and have\n            # caused issues in the past with overloading api endpoints\n            #\n            # Also, we now use opentelemetry to report these, so they're not\n            # useful to us as it stands\n            # TODO: Remove this from the interface file?\n            errors=[],\n            total_time=total_time,\n            unsupported_exts=dict(ignored_ext_freqs),\n            lockfile_scan_info=dependency_counts,\n            parse_rate={\n                lang: out.ParsingStats(\n                    targets_parsed=data.num_targets - data.targets_with_errors,\n                    num_targets=data.num_targets,\n                    bytes_parsed=data.num_bytes - data.error_bytes,\n                    num_bytes=data.num_bytes,\n                )\n                for (lang, data) in parse_rate.get_errors_by_lang().items()\n            },\n            engine_requested=engine_requested.name,\n            findings_by_product=findings_by_product,\n            supply_chain_stats=out.SupplyChainStats(subproject_stats),\n        ),\n    )\n\n    if self.partial_output:\n        self.partial_output.write_text(\n            out.PartialScanResult(\n                out.PartialScanOk((self.ci_scan_results, complete)),\n            ).to_json_string()\n        )\n\n    if self.dry_run:\n        logger.info(\n            f\"Would have sent findings and ignores blob: {json.dumps(findings_and_ignores, indent=4)}\"\n        )\n        logger.info(\n            f\"Would have sent complete blob: {json.dumps(complete.to_json(), indent=4)}\"\n        )\n        return out.CiScanCompleteResponse(success=True)\n\n    # old: was also logging {json.dumps(findings_and_ignores, indent=4)}\n    # alt: save it in ~/.semgrep/logs/findings_and_ignores.json?\n    logger.debug(f\"Sending findings and ignores blob\")\n\n    results_task = progress_bar.add_task(\"Uploading scan results\")\n    response = state.app_session.post(\n        f\"{state.env.semgrep_url}/api/agent/scans/{self.scan_id}/results\",\n        timeout=state.env.upload_findings_timeout,\n        json=findings_and_ignores,\n    )\n\n    try:\n        response.raise_for_status()\n\n        res = response.json()\n        resp_errors = res[\"errors\"]\n        for error in resp_errors:\n            message = error[\"message\"]\n            click.echo(f\"Server returned following warning: {message}\", err=True)\n\n        if \"task_id\" in res:\n            complete.task_id = res[\"task_id\"]\n\n        progress_bar.update(results_task, completed=100)\n\n    except requests.RequestException as exc:\n        raise Exception(f\"API server returned this error: {response.text}\") from exc\n\n    complete_task = progress_bar.add_task(\"Finalizing scan\")\n    # The largest scans we've seen so far can take up to 30\n    # minutes to wait for completion. Eventually, this wait may\n    # be configurable as we see larger scans and increased backend\n    # load.\n    now = datetime.now().replace(tzinfo=None)\n    try_until = now + timedelta(minutes=30)\n    slow_down_after = now + timedelta(minutes=2)\n\n    while True:\n        # old: was also logging {json.dumps(complete.to_json(), indent=4)}\n        # alt: save it in ~/.semgrep/logs/complete.json?\n        logger.debug(f\"Sending /complete\")\n\n        if datetime.now().replace(tzinfo=None) > try_until:\n            # let the backend know we won't be trying again\n            complete.final_attempt = True\n\n        # mark as complete\n        response = state.app_session.post(\n            f\"{state.env.semgrep_url}/api/agent/scans/{self.scan_id}/complete\",\n            timeout=state.env.upload_findings_timeout,\n            json=complete.to_json(),\n        )\n\n        try:\n            response.raise_for_status()\n        except requests.RequestException:\n            raise Exception(\n                f\"API server at {state.env.semgrep_url} returned this error: {response.text}\"\n            )\n\n        ret = out.CiScanCompleteResponse.from_json(response.json())\n        success = ret.success\n\n        if success or complete.final_attempt:\n            progress_bar.update(complete_task, completed=100)\n            return ret\n        progress_bar.advance(complete_task)\n        sleep(5 if datetime.now().replace(tzinfo=None) < slow_down_after else 30)\n",
      "hash_value": "6b403466471d1bc39165bcb36fc50053",
      "severity": "High",
      "confidence": "Medium",
      "code_snippets": [
        {
          "snippet": "@tracing.trace()\ndef report_findings(\n    self,\n    *,\n    matches_by_rule: RuleMatchMap,\n    rules: List[Rule],\n    targets: Set[Path],\n    renamed_targets: Set[Path],\n    ignored_targets: FrozenSet[Path],\n    cli_suggested_exit_code: int,\n    parse_rate: ParsingData,\n    total_time: float,\n    commit_date: str,\n    lockfile_dependencies: Dict[str, List[out.FoundDependency]],\n    dependency_parser_errors: List[DependencyParserError],\n    all_subprojects: List[Union[out.UnresolvedSubproject, out.ResolvedSubproject]],\n    contributions: out.Contributions,\n    engine_requested: \"EngineType\",\n    progress_bar: \"Progress\",\n) -> out.CiScanCompleteResponse:\n    \"\"\"\n    commit_date here for legacy reasons. epoch time of latest commit\n\n    Returns (success, block_scan, block_reason)\n    \"\"\"\n    state = get_state()\n    rule_ids = [out.RuleId(r.id) for r in rules]\n    all_matches = [\n        match\n        for matches_of_rule in matches_by_rule.values()\n        for match in matches_of_rule\n    ]\n    # we want date stamps assigned by the app to be assigned such that the\n    # current sort by relevant_since results in findings within a given scan\n    # appear in an intuitive order.  this requires reversed ordering here.\n    all_matches.reverse()\n    sort_order = {  # used only to order rules by severity\n        out.Experiment(): 0,\n        out.Inventory(): 1,\n        out.Info(): 2,\n        out.Low(): 2,\n        out.Warning(): 3,\n        out.Medium(): 3,\n        out.Error(): 4,\n        out.High(): 4,\n        out.Critical(): 5,\n    }\n    # NB: sorted guarantees stable sort, so within a given severity level\n    # issues remain sorted as before\n    all_matches = sorted(\n        all_matches, key=lambda match: sort_order[match.severity.value]\n    )\n    new_ignored, new_matches = partition(\n        all_matches, lambda match: match.match.extra.is_ignored\n    )\n\n    # Autofix is currently the only toggle in the App that\n    # indicates we are going to store your code. Until we\n    # have a dedicated toggle that allows users to opt-in\n    # to us storing their code we ommit code unless autofix\n    # is set.\n\n    findings = [\n        match.to_app_finding_format(\n            commit_date,\n            remove_dataflow_content=not self.autofix,\n        )\n        for match in new_matches\n    ]\n    ignores = [\n        match.to_app_finding_format(\n            commit_date,\n            remove_dataflow_content=not self.autofix,\n        )\n        for match in new_ignored\n    ]\n    token = (\n        # GitHub (cloud)\n        os.getenv(\"GITHUB_TOKEN\")\n        # GitLab.com (cloud)\n        or os.getenv(\"GITLAB_TOKEN\")\n        # Bitbucket Cloud\n        or os.getenv(\"BITBUCKET_TOKEN\")\n    )\n\n    self.ci_scan_results = out.CiScanResults(\n        # send a backup token in case the app is not available\n        token=token,\n        findings=findings,\n        ignores=ignores,\n        searched_paths=[out.Fpath(str(t)) for t in sorted(targets)],\n        renamed_paths=[out.Fpath(str(rt)) for rt in sorted(renamed_targets)],\n        rule_ids=rule_ids,\n        contributions=contributions,\n    )\n    if self.dependency_query:\n        self.ci_scan_results.dependencies = out.CiScanDependencies(\n            lockfile_dependencies\n        )\n\n    findings_and_ignores = self.ci_scan_results.to_json()\n\n    if any(\n        isinstance(match.severity.value, out.Experiment) for match in new_ignored\n    ):\n        logger.info(\"Some experimental rules were run during execution.\")\n\n    ignored_ext_freqs = Counter(\n        [os.path.splitext(path)[1] for path in ignored_targets]\n    )\n    ignored_ext_freqs.pop(\"\", None)  # don't count files with no extension\n\n    dependency_counts = {k: len(v) for k, v in lockfile_dependencies.items()}\n\n    # NOTE: This mirrors the logic in metrics.py to show the number of\n    #  findings by product for SCP customers. See PA-3312\n    #  We should consider refactoring this logic into a shared function\n    #  in a future PR for metric and behavioral consistency.\n    #  An open question remains on whether we should be including the number\n    #  of ignored findings in this count.\n\n    findings_by_product: Dict[str, int] = Counter()\n    for r, f in matches_by_rule.items():\n        # NOTE: For parity with metrics.py, we are using the human-readable product name,\n        #  (i.e. code) and falling back to the internal json string (i.e. sast) if we\n        #  somehow drift out of sync with the product enum.\n        name = USER_FRIENDLY_PRODUCT_NAMES.get(r.product, r.product.to_json())\n        findings_by_product[f\"{name}\"] += len(f)\n\n    subproject_stats: List[out.SubprojectStats] = []\n    if all_subprojects:\n        for subproject in all_subprojects:\n            if isinstance(subproject, out.UnresolvedSubproject):\n                stats = subproject_to_stats(subproject.info)\n            else:\n                stats = resolved_subproject_to_stats(subproject)\n            subproject_stats.append(stats)\n\n    complete = out.CiScanComplete(\n        exit_code=cli_suggested_exit_code,\n        dependency_parser_errors=dependency_parser_errors,\n        stats=out.CiScanCompleteStats(\n            findings=len(\n                [match for match in new_matches if not match.from_transient_scan]\n            ),\n            # We do not report errors anymore since they are large and have\n            # caused issues in the past with overloading api endpoints\n            #\n            # Also, we now use opentelemetry to report these, so they're not\n            # useful to us as it stands\n            # TODO: Remove this from the interface file?\n            errors=[],\n            total_time=total_time,\n            unsupported_exts=dict(ignored_ext_freqs),\n            lockfile_scan_info=dependency_counts,\n            parse_rate={\n                lang: out.ParsingStats(\n                    targets_parsed=data.num_targets - data.targets_with_errors,\n                    num_targets=data.num_targets,\n                    bytes_parsed=data.num_bytes - data.error_bytes,\n                    num_bytes=data.num_bytes,\n                )\n                for (lang, data) in parse_rate.get_errors_by_lang().items()\n            },\n            engine_requested=engine_requested.name,\n            findings_by_product=findings_by_product,\n            supply_chain_stats=out.SupplyChainStats(subproject_stats),\n        ),\n    )\n\n    if self.partial_output:\n        self.partial_output.write_text(\n            out.PartialScanResult(\n                out.PartialScanOk((self.ci_scan_results, complete)),\n            ).to_json_string()\n        )\n\n    if self.dry_run:\n        logger.info(\n            f\"Would have sent findings and ignores blob: {json.dumps(findings_and_ignores, indent=4)}\"\n        )\n        logger.info(\n            f\"Would have sent complete blob: {json.dumps(complete.to_json(), indent=4)}\"\n        )\n        return out.CiScanCompleteResponse(success=True)\n\n    # old: was also logging {json.dumps(findings_and_ignores, indent=4)}\n    # alt: save it in ~/.semgrep/logs/findings_and_ignores.json?\n    logger.debug(f\"Sending findings and ignores blob\")\n\n    results_task = progress_bar.add_task(\"Uploading scan results\")\n    response = state.app_session.post(\n        f\"{state.env.semgrep_url}/api/agent/scans/{self.scan_id}/results\",\n        timeout=state.env.upload_findings_timeout,\n        json=findings_and_ignores,\n    )\n\n    try:\n        response.raise_for_status()\n\n        res = response.json()\n        resp_errors = res[\"errors\"]\n        for error in resp_errors:\n            message = error[\"message\"]\n            click.echo(f\"Server returned following warning: {message}\", err=True)\n\n        if \"task_id\" in res:\n            complete.task_id = res[\"task_id\"]\n\n        progress_bar.update(results_task, completed=100)\n\n    except requests.RequestException as exc:\n        raise Exception(f\"API server returned this error: {response.text}\") from exc\n\n    complete_task = progress_bar.add_task(\"Finalizing scan\")\n    # The largest scans we've seen so far can take up to 30\n    # minutes to wait for completion. Eventually, this wait may\n    # be configurable as we see larger scans and increased backend\n    # load.\n    now = datetime.now().replace(tzinfo=None)\n    try_until = now + timedelta(minutes=30)\n    slow_down_after = now + timedelta(minutes=2)\n\n    while True:\n        # old: was also logging {json.dumps(complete.to_json(), indent=4)}\n        # alt: save it in ~/.semgrep/logs/complete.json?\n        logger.debug(f\"Sending /complete\")\n\n        if datetime.now().replace(tzinfo=None) > try_until:\n            # let the backend know we won't be trying again\n            complete.final_attempt = True\n\n        # mark as complete\n        response = state.app_session.post(\n            f\"{state.env.semgrep_url}/api/agent/scans/{self.scan_id}/complete\",\n            timeout=state.env.upload_findings_timeout,\n            json=complete.to_json(),\n        )\n\n        try:\n            response.raise_for_status()\n        except requests.RequestException:\n            raise Exception(\n                f\"API server at {state.env.semgrep_url} returned this error: {response.text}\"\n            )\n\n        ret = out.CiScanCompleteResponse.from_json(response.json())\n        success = ret.success\n\n        if success or complete.final_attempt:\n            progress_bar.update(complete_task, completed=100)\n            return ret\n        progress_bar.advance(complete_task)\n        sleep(5 if datetime.now().replace(tzinfo=None) < slow_down_after else 30)\n",
          "triple_sequences": [
            {
              "action_api": "os.getenv()",
              "action_description": "Retrieves value of environment variable",
              "action_id": "get_env_var",
              "object": "\"GITHUB_TOKEN\"",
              "object_description": "Environment variable",
              "object_id": "environment_variable",
              "intention_description": "Collect environment variable",
              "intention_id": "collect_environment_variable"
            },
            {
              "action_api": "os.getenv()",
              "action_description": "Retrieves value of environment variable",
              "action_id": "get_env_var",
              "object": "\"GITLAB_TOKEN\"",
              "object_description": "Environment variable",
              "object_id": "environment_variable",
              "intention_description": "Collect environment variable",
              "intention_id": "collect_environment_variable"
            },
            {
              "action_api": "os.getenv()",
              "action_description": "Retrieves value of environment variable",
              "action_id": "get_env_var",
              "object": "\"BITBUCKET_TOKEN\"",
              "object_description": "Environment variable",
              "object_id": "environment_variable",
              "intention_description": "Collect environment variable",
              "intention_id": "collect_environment_variable"
            },
            {
              "action_api": "os.path.splitext()",
              "action_description": "Basic path string operations (getting absolute path, base name, parent directory, splitting and joining paths)",
              "action_id": "path_string_operations",
              "object": "path",
              "object_description": "Local file or directory name",
              "object_id": "local_file_or_directory_name",
              "intention_description": "Prepare path for file operations",
              "intention_id": "prepare_path_file_operations"
            },
            {
              "action_api": "Counter()",
              "action_description": "List directory contents",
              "action_id": "list_directory_contents",
              "object": "[os.path.splitext(path)[1] for path in ignored_targets]",
              "object_description": "File extension",
              "object_id": "file_extension",
              "intention_description": "List files in directory",
              "intention_id": "list_directory_files"
            },
            {
              "action_api": "Counter()",
              "action_description": "List directory contents",
              "action_id": "list_directory_contents",
              "object": "findings_by_product",
              "object_description": "Product name string",
              "object_id": "",
              "intention_description": "List files in directory",
              "intention_id": "list_directory_files"
            },
            {
              "action_api": "json.dumps()",
              "action_description": "Serializes Python object to JSON string",
              "action_id": "serialize_to_json",
              "object": "findings_and_ignores",
              "object_description": "JSON string",
              "object_id": "json_string",
              "intention_description": "Serialize data",
              "intention_id": "serialize_data"
            },
            {
              "action_api": "json.dumps()",
              "action_description": "Serializes Python object to JSON string",
              "action_id": "serialize_to_json",
              "object": "complete.to_json()",
              "object_description": "JSON string",
              "object_id": "json_string",
              "intention_description": "Serialize data",
              "intention_id": "serialize_data"
            },
            {
              "action_api": "logger.info()",
              "action_description": "Process HTTP response content",
              "action_id": "process_http_response",
              "object": "f\"Would have sent findings and ignores blob: {json.dumps(findings_and_ignores, indent=4)}\"",
              "object_description": "Command output",
              "object_id": "command_output",
              "intention_description": "Collect command output",
              "intention_id": "collect_command_output"
            },
            {
              "action_api": "logger.info()",
              "action_description": "Process HTTP response content",
              "action_id": "process_http_response",
              "object": "f\"Would have sent complete blob: {json.dumps(complete.to_json(), indent=4)}\"",
              "object_description": "Command output",
              "object_id": "command_output",
              "intention_description": "Collect command output",
              "intention_id": "collect_command_output"
            },
            {
              "action_api": "logger.debug()",
              "action_description": "Process HTTP response content",
              "action_id": "process_http_response",
              "object": "f\"Sending findings and ignores blob\"",
              "object_description": "Command output",
              "object_id": "command_output",
              "intention_description": "Collect command output",
              "intention_id": "collect_command_output"
            },
            {
              "action_api": "progress_bar.add_task()",
              "action_description": "Creates asynchronous task",
              "action_id": "create_async_task",
              "object": "\"Uploading scan results\"",
              "object_description": "Thread target and arguments",
              "object_id": "thread_target_arguments",
              "intention_description": "Prepare thread pool for concurrent execution",
              "intention_id": "concurrent_execution_preparation"
            },
            {
              "action_api": "state.app_session.post()",
              "action_description": "Sends HTTP request",
              "action_id": "send_http_request",
              "object": "f\"{state.env.semgrep_url}/api/agent/scans/{self.scan_id}/results\"",
              "object_description": "API endpoint",
              "object_id": "api_endpoint",
              "intention_description": "Transmit encoded data via HTTP POST",
              "intention_id": "transmit_data_http_post"
            },
            {
              "action_api": "response.raise_for_status()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "response",
              "object_description": "HTTP response content from remote server",
              "object_id": "http_response_remote_content",
              "intention_description": "Download remote content",
              "intention_id": "download_remote_content"
            },
            {
              "action_api": "response.json()",
              "action_description": "Deserializes JSON response body to Python object",
              "action_id": "deserialize_json_response",
              "object": "",
              "object_description": "",
              "object_id": "",
              "intention_description": "Parse JSON data",
              "intention_id": "parse_json_data"
            },
            {
              "action_api": "click.echo()",
              "action_description": "Process HTTP response content",
              "action_id": "process_http_response",
              "object": "f\"Server returned following warning: {message}\"",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Collect command output",
              "intention_id": "collect_command_output"
            },
            {
              "action_api": "progress_bar.update()",
              "action_description": "Starts thread execution",
              "action_id": "start_thread",
              "object": "results_task, completed=100",
              "object_description": "Thread arguments",
              "object_id": "thread_arguments",
              "intention_description": "Wait for events",
              "intention_id": "wait_for_events"
            },
            {
              "action_api": "progress_bar.add_task()",
              "action_description": "Creates asynchronous task",
              "action_id": "create_async_task",
              "object": "\"Finalizing scan\"",
              "object_description": "Thread target and arguments",
              "object_id": "thread_target_arguments",
              "intention_description": "Prepare thread pool for concurrent execution",
              "intention_id": "concurrent_execution_preparation"
            },
            {
              "action_api": "datetime.now()",
              "action_description": "Returns current time",
              "action_id": "get_current_time",
              "object": "",
              "object_description": "",
              "object_id": "",
              "intention_description": "Get datetime epoch",
              "intention_id": "get_datetime_epoch"
            },
            {
              "action_api": "timedelta()",
              "action_description": "Suspends execution",
              "action_id": "suspend_execution",
              "object": "minutes=30",
              "object_description": "Delay duration in seconds",
              "object_id": "delay_duration",
              "intention_description": "Delay next operation",
              "intention_id": "delay_next_operation"
            },
            {
              "action_api": "timedelta()",
              "action_description": "Suspends execution",
              "action_id": "suspend_execution",
              "object": "minutes=2",
              "object_description": "Delay duration in seconds",
              "object_id": "delay_duration",
              "intention_description": "Delay next operation",
              "intention_id": "delay_next_operation"
            },
            {
              "action_api": "logger.debug()",
              "action_description": "Process HTTP response content",
              "action_id": "process_http_response",
              "object": "f\"Sending /complete\"",
              "object_description": "Command output",
              "object_id": "command_output",
              "intention_description": "Collect command output",
              "intention_id": "collect_command_output"
            },
            {
              "action_api": "state.app_session.post()",
              "action_description": "Sends HTTP request",
              "action_id": "send_http_request",
              "object": "f\"{state.env.semgrep_url}/api/agent/scans/{self.scan_id}/complete\"",
              "object_description": "API endpoint",
              "object_id": "api_endpoint",
              "intention_description": "Transmit encoded data via HTTP POST",
              "intention_id": "transmit_data_http_post"
            },
            {
              "action_api": "response.raise_for_status()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "response",
              "object_description": "HTTP response content from remote server",
              "object_id": "http_response_remote_content",
              "intention_description": "Download remote content",
              "intention_id": "download_remote_content"
            },
            {
              "action_api": "out.CiScanCompleteResponse.from_json()",
              "action_description": "Deserializes JSON response body to Python object",
              "action_id": "deserialize_json_response",
              "object": "response.json()",
              "object_description": "JSON string",
              "object_id": "json_string",
              "intention_description": "Parse JSON data",
              "intention_id": "parse_json_data"
            },
            {
              "action_api": "progress_bar.update()",
              "action_description": "Starts thread execution",
              "action_id": "start_thread",
              "object": "complete_task, completed=100",
              "object_description": "Thread arguments",
              "object_id": "thread_arguments",
              "intention_description": "Wait for events",
              "intention_id": "wait_for_events"
            },
            {
              "action_api": "progress_bar.advance()",
              "action_description": "Starts thread execution",
              "action_id": "start_thread",
              "object": "complete_task",
              "object_description": "Thread arguments",
              "object_id": "thread_arguments",
              "intention_description": "Wait for events",
              "intention_id": "wait_for_events"
            },
            {
              "action_api": "sleep()",
              "action_description": "Suspends execution",
              "action_id": "suspend_execution",
              "object": "5 or 30",
              "object_description": "Delay duration in seconds",
              "object_id": "delay_duration",
              "intention_description": "Delay next operation",
              "intention_id": "delay_next_operation"
            }
          ]
        }
      ]
    }
  ]
}