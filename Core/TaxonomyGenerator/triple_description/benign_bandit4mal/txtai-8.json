{
  "metadata": {
    "package_name": "txtai-8",
    "original_json_path": "/home2/blue/Documents/PyPIAgent/Codes/code_contexral/codesnippets/benign_bandit4mal/txtai-8.4.0.json",
    "dataset_type": "benign_bandit4mal"
  },
  "code_files": [
    {
      "pyfile": "microphone.py",
      "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/txtai-8.4.0/txtai-8.4.0/src/python/txtai/pipeline/audio/microphone.py",
      "line_number": "116",
      "type_description": "B814:read",
      "context_snippet": "def listen(self, device):\n    \"\"\"\n    Listens for speech. Detected speech is converted to 32-bit floats for compatibility with\n    automatic speech recognition (ASR) pipelines.\n\n    This method blocks until speech is detected.\n\n    Args:\n        device: input device\n\n    Returns:\n        audio\n    \"\"\"\n\n    # Record in 100ms chunks\n    chunksize = self.rate // 10\n\n    # Open input stream\n    stream = sd.RawInputStream(device=device, samplerate=self.rate, channels=1, blocksize=chunksize, dtype=np.int16)\n\n    # Start the input stream\n    stream.start()\n\n    record, speech, nospeech, chunks = True, 0, 0, []\n    while record:\n        # Read chunk\n        chunk, _ = stream.read(chunksize)\n\n        # Detect speech using WebRTC VAD for audio chunk\n        detect = self.detect(chunk)\n        speech = speech + 1 if detect else speech\n        nospeech = 0 if detect else nospeech + 1\n\n        # Save chunk, if this is an active stream\n        if speech:\n            chunks.append(chunk)\n\n            # Pause limit has been reached, check if this audio should be accepted\n            if nospeech >= self.pause:\n                logger.debug(\"Audio detected and being analyzed\")\n                if speech >= self.active and self.isspeech(chunks[:-nospeech]):\n                    # Disable recording\n                    record = False\n                else:\n                    # Reset parameters and keep recording\n                    logger.debug(\"Speech not detected\")\n                    speech, nospeech, chunks = 0, 0, []\n\n    # Stop the input stream\n    stream.stop()\n\n    # Convert to float32 and return\n    audio = np.frombuffer(b\"\".join(chunks), np.int16)\n    return Signal.float32(audio)",
      "hash_value": "c7fe64715342318f465d839a0a8afc4b",
      "severity": "High",
      "confidence": "Medium",
      "code_snippets": [
        {
          "snippet": "def listen(self, device):\n    \"\"\"\n    Listens for speech. Detected speech is converted to 32-bit floats for compatibility with\n    automatic speech recognition (ASR) pipelines.\n\n    This method blocks until speech is detected.\n\n    Args:\n        device: input device\n\n    Returns:\n        audio\n    \"\"\"\n\n    # Record in 100ms chunks\n    chunksize = self.rate // 10\n\n    # Open input stream\n    stream = sd.RawInputStream(device=device, samplerate=self.rate, channels=1, blocksize=chunksize, dtype=np.int16)\n\n    # Start the input stream\n    stream.start()\n\n    record, speech, nospeech, chunks = True, 0, 0, []\n    while record:\n        # Read chunk\n        chunk, _ = stream.read(chunksize)\n\n        # Detect speech using WebRTC VAD for audio chunk\n        detect = self.detect(chunk)\n        speech = speech + 1 if detect else speech\n        nospeech = 0 if detect else nospeech + 1\n\n        # Save chunk, if this is an active stream\n        if speech:\n            chunks.append(chunk)\n\n            # Pause limit has been reached, check if this audio should be accepted\n            if nospeech >= self.pause:\n                logger.debug(\"Audio detected and being analyzed\")\n                if speech >= self.active and self.isspeech(chunks[:-nospeech]):\n                    # Disable recording\n                    record = False\n                else:\n                    # Reset parameters and keep recording\n                    logger.debug(\"Speech not detected\")\n                    speech, nospeech, chunks = 0, 0, []\n\n    # Stop the input stream\n    stream.stop()\n\n    # Convert to float32 and return\n    audio = np.frombuffer(b\"\".join(chunks), np.int16)\n    return Signal.float32(audio)",
          "triple_sequences": [
            {
              "action_api": "sd.RawInputStream()",
              "action_description": "Creates video capture object",
              "action_id": "create_video_capture",
              "object": "device=device, samplerate=self.rate, channels=1, blocksize=chunksize, dtype=np.int16",
              "object_description": "Camera device index",
              "object_id": "camera_device_index",
              "intention_description": "Access system camera",
              "intention_id": "access_system_camera"
            },
            {
              "action_api": "stream.start()",
              "action_description": "Starts thread execution",
              "action_id": "start_thread",
              "object": "",
              "object_description": "",
              "object_id": "",
              "intention_description": "Access system camera",
              "intention_id": "access_system_camera"
            },
            {
              "action_api": "stream.read()",
              "action_description": "Reads frame from video capture device",
              "action_id": "read_video_frame",
              "object": "chunksize",
              "object_description": "Camera device index",
              "object_id": "camera_device_index",
              "intention_description": "Access system camera",
              "intention_id": "access_system_camera"
            },
            {
              "action_api": "self.detect()",
              "action_description": "Detect cryptocurrency address in clipboard",
              "action_id": "detect_crypto_address_clipboard",
              "object": "chunk",
              "object_description": "",
              "object_id": "",
              "intention_description": "Detect image file",
              "intention_id": "detect_image_file"
            },
            {
              "action_api": "self.isspeech()",
              "action_description": "Detect cryptocurrency address in clipboard",
              "action_id": "detect_crypto_address_clipboard",
              "object": "chunks[:-nospeech]",
              "object_description": "",
              "object_id": "",
              "intention_description": "Detect image file",
              "intention_id": "detect_image_file"
            },
            {
              "action_api": "stream.stop()",
              "action_description": "Releases video capture device",
              "action_id": "release_video_device",
              "object": "",
              "object_description": "",
              "object_id": "",
              "intention_description": "Release system resource",
              "intention_id": "release_system_resource"
            },
            {
              "action_api": "np.frombuffer()",
              "action_description": "Creates in-memory bytes buffer from encoded string",
              "action_id": "create_memory_bytes",
              "object": "b\"\".join(chunks), np.int16",
              "object_description": "Obfuscated byte string",
              "object_id": "obfuscated_byte_string",
              "intention_description": "Prepare data processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "Signal.float32()",
              "action_description": "Convert integers to characters",
              "action_id": "convert_integers_to_chars",
              "object": "audio",
              "object_description": "",
              "object_id": "",
              "intention_description": "Prepare data processing",
              "intention_id": "prepare_data_processing"
            }
          ]
        }
      ]
    }
  ]
}