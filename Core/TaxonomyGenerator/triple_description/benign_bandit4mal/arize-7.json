{
  "metadata": {
    "package_name": "arize-7",
    "original_json_path": "/home2/blue/Documents/PyPIAgent/Codes/code_contexral/codesnippets/benign_bandit4mal/arize-7.38.0.json",
    "dataset_type": "benign_bandit4mal"
  },
  "code_files": [
    {
      "pyfile": "api.py",
      "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/arize-7.38.0/arize-7.38.0/src/arize/api.py",
      "line_number": "628",
      "type_description": "B821:post",
      "context_snippet": "    def log(\n        self,\n        model_id: str,\n        model_type: ModelTypes,\n        environment: Environments,\n        model_version: Optional[str] = None,\n        prediction_id: Optional[Union[str, int, float]] = None,\n        prediction_timestamp: Optional[int] = None,\n        prediction_label: Optional[PredictionLabelTypes] = None,\n        actual_label: Optional[ActualLabelTypes] = None,\n        features: Optional[\n            Dict[str, Union[str, bool, float, int, List[str], TypedValue]]\n        ] = None,\n        embedding_features: Optional[Dict[str, Embedding]] = None,\n        shap_values: Optional[Dict[str, float]] = None,\n        tags: Optional[\n            Dict[str, Union[str, bool, float, int, TypedValue]]\n        ] = None,\n        batch_id: Optional[str] = None,\n        prompt: Optional[Union[str, Embedding]] = None,\n        response: Optional[Union[str, Embedding]] = None,\n        prompt_template: Optional[str] = None,\n        prompt_template_version: Optional[str] = None,\n        llm_model_name: Optional[str] = None,\n        llm_params: Optional[Dict[str, Union[str, bool, float, int]]] = None,\n        llm_run_metadata: Optional[LLMRunMetadata] = None,\n    ) -> cf.Future:\n        \"\"\"\n        Logs a record to Arize via a POST request.\n\n        Args:\n            model_id (str): A unique name to identify your model in the Arize platform.\n            model_type (ModelTypes): Declare your model type. Can check the supported model types\n                running `ModelTypes.list_types()`\n            environment (Environments): The environment that this dataframe is for (Production,\n                Training, Validation).\n            model_version (str, optional): Used to group together a subset of predictions\n                and actuals for a given model_id to track and compare changes. Defaults to None.\n            prediction_id (str, int, or float, optional): A unique string to identify a\n                prediction event. This value is used to match a prediction to delayed actuals in Arize. If\n                prediction id is not provided, Arize will, when possible, create a random prediction\n                id on the server side.\n            prediction_timestamp (int, optional): Unix timestamp in seconds. If None, prediction\n                uses current timestamp. Defaults to None.\n            prediction_label (bool, int, float, str, Tuple(str, float), ObjectDetectionLabel,\n                RankingPredictionLabel or MultiClassPredictionLabel; optional):\n                The predicted value for a given model input. Defaults to None.\n            actual_label (\n                bool, int, float, str, Tuple[str, float],\n                ObjectDetectionLabel, RankingActualLabel, MultiClassActualLabel,\n                optional\n            ):\n                The ground truth value for a given model input. This will be matched to the\n                prediction with the same prediction_id as the one in this call. Defaults to None.\n            features (Dict[str, Union[str, bool, float, int]], optional): Dictionary containing\n                human readable and debuggable model features. Defaults to None.\n            embedding_features (Dict[str, Embedding], optional): Dictionary containing model\n                embedding features. Keys must be strings. Values must be of type Embedding. Defaults to\n                None.\n            shap_values (Dict[str, float], optional): Dictionary containing human readable and\n                debuggable model features keys, along with SHAP feature importance values. Defaults to None.\n            tags (Dict[str, Union[str, bool, float, int]], optional): Dictionary containing human\n                readable and debuggable model tags. Defaults to None.\n            batch_id (str, optional): Used to distinguish different batch of data under the same\n                model_id and model_version. Required for VALIDATION environments. Defaults to None.\n            prompt (str or Embedding, optional): input text on which the GENERATIVE_LLM model acts. It\n                accepts a string or Embedding object (if sending embedding vectors is desired). Required\n                for GENERATIVE_LLM models. Defaults to None.\n            response (str or Embedding, optional): output text from GENERATIVE_LLM model. It accepts\n                a string or Embedding object (if sending embedding vectors is desired). Required for\n                GENERATIVE_LLM models. Defaults to None.\n            prompt_template (str, optional): template used to construct the prompt passed to a large language\n                model. It can include variable using the double braces notation. Example: 'Given the context\n                {{context}}, answer the following question {{user_question}}.\n            prompt_template_version (str, optional): version of the template used.\n            llm_model_name (str, optional): name of the llm used. Example: 'gpt-4'.\n            llm_params (str, optional): hyperparameters passed to the large language model.\n            llm_run_metadata (LLMRunMetadata, optional): run metadata for llm calls.\n        Returns:\n            `concurrent.futures.Future` object\n\n        \"\"\"\n        # This method requires the API key and either space ID or space key to be set\n        # api_key and one of space_id or space_key must be provided\n        if not self._api_key or not (self._space_id or self._space_key):\n            raise AuthError(\n                missing_space_id=not (self._space_id or self._space_key),\n                missing_api_key=not self._api_key,\n                method_name=\"log\",\n            )\n        # Validate model_id\n        if not isinstance(model_id, str):\n            raise InvalidValueType(\"model_id\", model_id, \"str\")\n        # Validate model_type\n        if not isinstance(model_type, ModelTypes):\n            raise InvalidValueType(\n                \"model_type\", model_type, \"arize.utils.ModelTypes\"\n            )\n        # Validate environment\n        if not isinstance(environment, Environments):\n            raise InvalidValueType(\n                \"environment\", environment, \"arize.utils.Environments\"\n            )\n        # Validate batch_id\n        if environment == Environments.VALIDATION and (\n            batch_id is None\n            or not isinstance(batch_id, str)\n            or len(batch_id.strip()) == 0\n        ):\n            raise ValueError(\n                \"Batch ID must be a nonempty string if logging to validation environment.\"\n            )\n\n        # Convert & Validate prediction_id\n        prediction_id = _validate_and_convert_prediction_id(\n            prediction_id,\n            environment,\n            prediction_label,\n            actual_label,\n            shap_values,\n        )\n\n        # Cast feature & tag values\n        features = cast_dictionary(features)\n        tags = cast_dictionary(tags)\n\n        # Validate feature types\n        if features:\n            if not isinstance(features, dict):\n                raise InvalidValueType(\"features\", features, \"dict\")\n            for feat_name, feat_value in features.items():\n                _validate_mapping_key(feat_name, \"features\")\n                if is_list_of(feat_value, str):\n                    continue\n                else:\n                    val = convert_element(feat_value)\n                    if val is not None and not isinstance(\n                        val, (str, bool, float, int)\n                    ):\n                        raise InvalidValueType(\n                            f\"feature '{feat_name}'\",\n                            feat_value,\n                            \"one of: bool, int, float, str\",\n                        )\n\n        # Validate embedding_features type\n        if embedding_features:\n            if not isinstance(embedding_features, dict):\n                raise InvalidValueType(\n                    \"embedding_features\", embedding_features, \"dict\"\n                )\n            if len(embedding_features) > MAX_NUMBER_OF_EMBEDDINGS:\n                raise InvalidNumberOfEmbeddings(len(embedding_features))\n            if (\n                model_type == ModelTypes.OBJECT_DETECTION\n                and len(embedding_features.keys()) > 1\n            ):\n                # Check that there is only 1 embedding feature for OD model types\n                raise ValueError(\n                    \"Object Detection models only support one embedding feature\"\n                )\n            if model_type == ModelTypes.GENERATIVE_LLM:\n                # Check reserved keys are not used\n                reserved_emb_feat_names = {\"prompt\", \"response\"}\n                if reserved_emb_feat_names & embedding_features.keys():\n                    raise KeyError(\n                        \"embedding features cannot use the reserved feature names ('prompt', 'response') \"\n                        \"for GENERATIVE_LLM models\"\n                    )\n            for emb_name, emb_obj in embedding_features.items():\n                _validate_mapping_key(emb_name, \"embedding features\")\n                # Must verify embedding type\n                if not isinstance(emb_obj, Embedding):\n                    raise InvalidValueType(\n                        f\"embedding feature '{emb_name}'\", emb_obj, \"Embedding\"\n                    )\n                emb_obj.validate(emb_name)\n\n        # Validate tag types\n        if tags:\n            if not isinstance(tags, dict):\n                raise InvalidValueType(\"tags\", tags, \"dict\")\n            wrong_tags = [\n                tag_name for tag_name in tags if tag_name in RESERVED_TAG_COLS\n            ]\n            if wrong_tags:\n                raise KeyError(\n                    f\"The following tag names are not allowed as they are reserved: {wrong_tags}\"\n                )\n            for tag_name, tag_value in tags.items():\n                _validate_mapping_key(tag_name, \"tags\")\n                val = convert_element(tag_value)\n                if val is not None and not isinstance(\n                    val, (str, bool, float, int)\n                ):\n                    raise InvalidValueType(\n                        f\"tag '{tag_name}'\",\n                        tag_value,\n                        \"one of: bool, int, float, str\",\n                    )\n                if isinstance(tag_name, str) and tag_name.endswith(\"_shap\"):\n                    raise ValueError(\n                        f\"tag {tag_name} must not be named with a `_shap` suffix\"\n                    )\n                if len(str(val)) > MAX_TAG_LENGTH:\n                    raise ValueError(\n                        f\"The number of characters for each tag must be less than or equal to \"\n                        f\"{MAX_TAG_LENGTH}. The tag {tag_name} with value {tag_value} has \"\n                        f\"{len(str(val))} characters.\"\n                    )\n                elif len(str(val)) > MAX_TAG_LENGTH_TRUNCATION:\n                    logger.warning(\n                        get_truncation_warning_message(\n                            \"tags\", MAX_TAG_LENGTH_TRUNCATION\n                        )\n                    )\n\n        # Check the timestamp present on the event\n        if prediction_timestamp is not None:\n            if not isinstance(prediction_timestamp, int):\n                raise InvalidValueType(\n                    \"prediction_timestamp\", prediction_timestamp, \"int\"\n                )\n            # Send warning if prediction is sent with future timestamp\n            now = int(time.time())\n            if prediction_timestamp > now:\n                logger.warning(\n                    \"Caution when sending a prediction with future timestamp.\"\n                    \"Arize only stores 2 years worth of data. For example, if you sent a prediction \"\n                    \"to Arize from 1.5 years ago, and now send a prediction with timestamp of a year in \"\n                    \"the future, the oldest 0.5 years will be dropped to maintain the 2 years worth of data \"\n                    \"requirement.\"\n                )\n            if not is_timestamp_in_range(now, prediction_timestamp):\n                raise ValueError(\n                    f\"prediction_timestamp: {prediction_timestamp} is out of range.\"\n                    f\"Prediction timestamps must be within {MAX_FUTURE_YEARS_FROM_CURRENT_TIME} year in the \"\n                    f\"future and {MAX_PAST_YEARS_FROM_CURRENT_TIME} years in the past from the current time.\"\n                )\n\n        # Validate GENERATIVE_LLM models requirements\n        if model_type == ModelTypes.GENERATIVE_LLM:\n            if prompt is not None:\n                if not isinstance(prompt, (str, Embedding)):\n                    raise TypeError(\n                        f\"prompt must be of type str or Embedding, but found {type(val)}\"\n                    )\n                if isinstance(prompt, str):\n                    prompt = _PromptOrResponseText(data=prompt)\n                # Validate content of prompt, type is either Embedding or _PromptOrResponseText\n                prompt.validate(\"prompt\")\n            if response is not None:\n                if not isinstance(response, (str, Embedding)):\n                    raise TypeError(\n                        f\"response must be of type str or Embedding, but found {type(val)}\"\n                    )\n                if isinstance(response, str):\n                    response = _PromptOrResponseText(data=response)\n                # Validate content of response, type is either Embedding or _PromptOrResponseText\n                response.validate(\"response\")\n\n            # Validate prompt templates workflow information\n            _validate_prompt_templates_and_llm_config(\n                prompt_template,\n                prompt_template_version,\n                llm_model_name,\n                llm_params,\n            )\n            # Validate llm run metadata\n            if llm_run_metadata is not None:\n                llm_run_metadata.validate()\n        else:\n            if prompt is not None or response is not None:\n                raise ValueError(\n                    \"The fields 'prompt' and 'response' must be None for model types other \"\n                    \"than GENERATIVE_LLM\"\n                )\n\n        # Construct the prediction\n        p = None\n        # Only set a default prediction label for generative LLM models if there is no explicit prediction\n        # label AND no actual label to ensure that generative LLM model prediction records will have a\n        # prediction label that can be used for metrics in the platform (as users will generally pass in\n        # actuals/user feedback only). We do not want to add the default prediction label in if actual labels\n        # are present so that latent actuals will still work.\n        if (\n            model_type == ModelTypes.GENERATIVE_LLM\n            and prediction_label is None\n            and actual_label is None\n        ):\n            prediction_label = \"1\"\n\n        if prediction_label is not None:\n            if model_version is not None and not isinstance(model_version, str):\n                raise InvalidValueType(\"model_version\", model_version, \"str\")\n            _validate_label(\n                prediction_or_actual=\"prediction\",\n                model_type=model_type,\n                label=convert_element(prediction_label),\n                embedding_features=embedding_features,\n            )\n            p = pb2.Prediction(\n                prediction_label=_get_label(\n                    prediction_or_actual=\"prediction\",\n                    value=prediction_label,\n                    model_type=model_type,\n                ),\n                model_version=model_version,\n            )\n            if features is not None:\n                converted_feats = convert_dictionary(features)\n                feats = pb2.Prediction(features=converted_feats)\n                p.MergeFrom(feats)\n\n            if embedding_features or prompt or response:\n                # NOTE: Deep copy is necessary to avoid side effects on the original input dictionary\n                combined_embedding_features = (\n                    {k: v for k, v in embedding_features.items()}\n                    if embedding_features\n                    else {}\n                )\n                # Map prompt as embedding features for generative models\n                if prompt is not None:\n                    combined_embedding_features.update({\"prompt\": prompt})\n                # Map response as embedding features for generative models\n                if response is not None:\n                    combined_embedding_features.update({\"response\": response})\n                converted_embedding_feats = convert_dictionary(\n                    combined_embedding_features\n                )\n                embedding_feats = pb2.Prediction(\n                    features=converted_embedding_feats\n                )\n                p.MergeFrom(embedding_feats)\n\n            if tags or llm_run_metadata:\n                joined_tags = copy.deepcopy(tags)\n                if llm_run_metadata:\n                    if llm_run_metadata.total_token_count is not None:\n                        joined_tags[\n                            LLM_RUN_METADATA_TOTAL_TOKEN_COUNT_TAG_NAME\n                        ] = llm_run_metadata.total_token_count\n                    if llm_run_metadata.prompt_token_count is not None:\n                        joined_tags[\n                            LLM_RUN_METADATA_PROMPT_TOKEN_COUNT_TAG_NAME\n                        ] = llm_run_metadata.prompt_token_count\n                    if llm_run_metadata.response_token_count is not None:\n                        joined_tags[\n                            LLM_RUN_METADATA_RESPONSE_TOKEN_COUNT_TAG_NAME\n                        ] = llm_run_metadata.response_token_count\n                    if llm_run_metadata.response_latency_ms is not None:\n                        joined_tags[\n                            LLM_RUN_METADATA_RESPONSE_LATENCY_MS_TAG_NAME\n                        ] = llm_run_metadata.response_latency_ms\n                converted_tags = convert_dictionary(joined_tags)\n                tgs = pb2.Prediction(tags=converted_tags)\n                p.MergeFrom(tgs)\n\n            if (\n                prompt_template\n                or prompt_template_version\n                or llm_model_name\n                or llm_params\n            ):\n                llm_fields = pb2.LLMFields(\n                    prompt_template=prompt_template or \"\",\n                    prompt_template_name=prompt_template_version or \"\",\n                    llm_model_name=llm_model_name or \"\",\n                    llm_params=convert_dictionary(llm_params),\n                )\n                p.MergeFrom(pb2.Prediction(llm_fields=llm_fields))\n\n            if prediction_timestamp is not None:\n                p.timestamp.MergeFrom(get_timestamp(prediction_timestamp))\n\n        # Validate and construct the optional actual\n        is_latent_tags = prediction_label is None and tags is not None\n        a = None\n        if actual_label or is_latent_tags:\n            a = pb2.Actual()\n            if actual_label is not None:\n                _validate_label(\n                    prediction_or_actual=\"actual\",\n                    model_type=model_type,\n                    label=convert_element(actual_label),\n                    embedding_features=embedding_features,\n                )\n                a.MergeFrom(\n                    pb2.Actual(\n                        actual_label=_get_label(\n                            prediction_or_actual=\"actual\",\n                            value=actual_label,\n                            model_type=model_type,\n                        )\n                    )\n                )\n            # Added to support delayed tags on actuals.\n            if tags is not None:\n                converted_tags = convert_dictionary(tags)\n                a.MergeFrom(pb2.Actual(tags=converted_tags))\n\n        # Validate and construct the optional feature importances\n        fi = None\n        if shap_values is not None and bool(shap_values):\n            for k, v in shap_values.items():\n                if not isinstance(convert_element(v), float):\n                    raise InvalidValueType(f\"feature '{k}'\", v, \"float\")\n                if isinstance(k, str) and k.endswith(\"_shap\"):\n                    raise ValueError(\n                        f\"feature {k} must not be named with a `_shap` suffix\"\n                    )\n            fi = pb2.FeatureImportances(feature_importances=shap_values)\n\n        if p is None and a is None and fi is None:\n            raise ValueError(\n                \"must provide at least one of prediction_label, actual_label, tags, or shap_values\"\n            )\n\n        env_params = None\n        if environment == Environments.TRAINING:\n            if p is None or a is None:\n                raise ValueError(\n                    \"Training records must have both Prediction and Actual\"\n                )\n            env_params = pb2.Record.EnvironmentParams(\n                training=pb2.Record.EnvironmentParams.Training()\n            )\n        elif environment == Environments.VALIDATION:\n            if p is None or a is None:\n                raise ValueError(\n                    \"Validation records must have both Prediction and Actual\"\n                )\n            env_params = pb2.Record.EnvironmentParams(\n                validation=pb2.Record.EnvironmentParams.Validation(\n                    batch_id=batch_id\n                )\n            )\n        elif environment == Environments.PRODUCTION:\n            env_params = pb2.Record.EnvironmentParams(\n                production=pb2.Record.EnvironmentParams.Production()\n            )\n\n        rec = pb2.Record(\n            space_key=self._space_key,\n            model_id=model_id,\n            prediction_id=prediction_id,\n            prediction=p,\n            actual=a,\n            feature_importances=fi,\n            environment_params=env_params,\n            is_generative_llm_record=BoolValue(\n                value=model_type == ModelTypes.GENERATIVE_LLM\n            ),\n        )\n        return self._post(record=rec, uri=self._uri, indexes=None)\n",
      "hash_value": "96ead77b2a090dbcd772916fbdbdb42f",
      "severity": "High",
      "confidence": "Medium",
      "code_snippets": [
        {
          "snippet": "    def log(\n        self,\n        model_id: str,\n        model_type: ModelTypes,\n        environment: Environments,\n        model_version: Optional[str] = None,\n        prediction_id: Optional[Union[str, int, float]] = None,\n        prediction_timestamp: Optional[int] = None,\n        prediction_label: Optional[PredictionLabelTypes] = None,\n        actual_label: Optional[ActualLabelTypes] = None,\n        features: Optional[\n            Dict[str, Union[str, bool, float, int, List[str], TypedValue]]\n        ] = None,\n        embedding_features: Optional[Dict[str, Embedding]] = None,\n        shap_values: Optional[Dict[str, float]] = None,\n        tags: Optional[\n            Dict[str, Union[str, bool, float, int, TypedValue]]\n        ] = None,\n        batch_id: Optional[str] = None,\n        prompt: Optional[Union[str, Embedding]] = None,\n        response: Optional[Union[str, Embedding]] = None,\n        prompt_template: Optional[str] = None,\n        prompt_template_version: Optional[str] = None,\n        llm_model_name: Optional[str] = None,\n        llm_params: Optional[Dict[str, Union[str, bool, float, int]]] = None,\n        llm_run_metadata: Optional[LLMRunMetadata] = None,\n    ) -> cf.Future:\n        \"\"\"\n        Logs a record to Arize via a POST request.\n\n        Args:\n            model_id (str): A unique name to identify your model in the Arize platform.\n            model_type (ModelTypes): Declare your model type. Can check the supported model types\n                running `ModelTypes.list_types()`\n            environment (Environments): The environment that this dataframe is for (Production,\n                Training, Validation).\n            model_version (str, optional): Used to group together a subset of predictions\n                and actuals for a given model_id to track and compare changes. Defaults to None.\n            prediction_id (str, int, or float, optional): A unique string to identify a\n                prediction event. This value is used to match a prediction to delayed actuals in Arize. If\n                prediction id is not provided, Arize will, when possible, create a random prediction\n                id on the server side.\n            prediction_timestamp (int, optional): Unix timestamp in seconds. If None, prediction\n                uses current timestamp. Defaults to None.\n            prediction_label (bool, int, float, str, Tuple(str, float), ObjectDetectionLabel,\n                RankingPredictionLabel or MultiClassPredictionLabel; optional):\n                The predicted value for a given model input. Defaults to None.\n            actual_label (\n                bool, int, float, str, Tuple[str, float],\n                ObjectDetectionLabel, RankingActualLabel, MultiClassActualLabel,\n                optional\n            ):\n                The ground truth value for a given model input. This will be matched to the\n                prediction with the same prediction_id as the one in this call. Defaults to None.\n            features (Dict[str, Union[str, bool, float, int]], optional): Dictionary containing\n                human readable and debuggable model features. Defaults to None.\n            embedding_features (Dict[str, Embedding], optional): Dictionary containing model\n                embedding features. Keys must be strings. Values must be of type Embedding. Defaults to\n                None.\n            shap_values (Dict[str, float], optional): Dictionary containing human readable and\n                debuggable model features keys, along with SHAP feature importance values. Defaults to None.\n            tags (Dict[str, Union[str, bool, float, int]], optional): Dictionary containing human\n                readable and debuggable model tags. Defaults to None.\n            batch_id (str, optional): Used to distinguish different batch of data under the same\n                model_id and model_version. Required for VALIDATION environments. Defaults to None.\n            prompt (str or Embedding, optional): input text on which the GENERATIVE_LLM model acts. It\n                accepts a string or Embedding object (if sending embedding vectors is desired). Required\n                for GENERATIVE_LLM models. Defaults to None.\n            response (str or Embedding, optional): output text from GENERATIVE_LLM model. It accepts\n                a string or Embedding object (if sending embedding vectors is desired). Required for\n                GENERATIVE_LLM models. Defaults to None.\n            prompt_template (str, optional): template used to construct the prompt passed to a large language\n                model. It can include variable using the double braces notation. Example: 'Given the context\n                {{context}}, answer the following question {{user_question}}.\n            prompt_template_version (str, optional): version of the template used.\n            llm_model_name (str, optional): name of the llm used. Example: 'gpt-4'.\n            llm_params (str, optional): hyperparameters passed to the large language model.\n            llm_run_metadata (LLMRunMetadata, optional): run metadata for llm calls.\n        Returns:\n            `concurrent.futures.Future` object\n\n        \"\"\"\n        # This method requires the API key and either space ID or space key to be set\n        # api_key and one of space_id or space_key must be provided\n        if not self._api_key or not (self._space_id or self._space_key):\n            raise AuthError(\n                missing_space_id=not (self._space_id or self._space_key),\n                missing_api_key=not self._api_key,\n                method_name=\"log\",\n            )\n        # Validate model_id\n        if not isinstance(model_id, str):\n            raise InvalidValueType(\"model_id\", model_id, \"str\")\n        # Validate model_type\n        if not isinstance(model_type, ModelTypes):\n            raise InvalidValueType(\n                \"model_type\", model_type, \"arize.utils.ModelTypes\"\n            )\n        # Validate environment\n        if not isinstance(environment, Environments):\n            raise InvalidValueType(\n                \"environment\", environment, \"arize.utils.Environments\"\n            )\n        # Validate batch_id\n        if environment == Environments.VALIDATION and (\n            batch_id is None\n            or not isinstance(batch_id, str)\n            or len(batch_id.strip()) == 0\n        ):\n            raise ValueError(\n                \"Batch ID must be a nonempty string if logging to validation environment.\"\n            )\n\n        # Convert & Validate prediction_id\n        prediction_id = _validate_and_convert_prediction_id(\n            prediction_id,\n            environment,\n            prediction_label,\n            actual_label,\n            shap_values,\n        )\n\n        # Cast feature & tag values\n        features = cast_dictionary(features)\n        tags = cast_dictionary(tags)\n\n        # Validate feature types\n        if features:\n            if not isinstance(features, dict):\n                raise InvalidValueType(\"features\", features, \"dict\")\n            for feat_name, feat_value in features.items():\n                _validate_mapping_key(feat_name, \"features\")\n                if is_list_of(feat_value, str):\n                    continue\n                else:\n                    val = convert_element(feat_value)\n                    if val is not None and not isinstance(\n                        val, (str, bool, float, int)\n                    ):\n                        raise InvalidValueType(\n                            f\"feature '{feat_name}'\",\n                            feat_value,\n                            \"one of: bool, int, float, str\",\n                        )\n\n        # Validate embedding_features type\n        if embedding_features:\n            if not isinstance(embedding_features, dict):\n                raise InvalidValueType(\n                    \"embedding_features\", embedding_features, \"dict\"\n                )\n            if len(embedding_features) > MAX_NUMBER_OF_EMBEDDINGS:\n                raise InvalidNumberOfEmbeddings(len(embedding_features))\n            if (\n                model_type == ModelTypes.OBJECT_DETECTION\n                and len(embedding_features.keys()) > 1\n            ):\n                # Check that there is only 1 embedding feature for OD model types\n                raise ValueError(\n                    \"Object Detection models only support one embedding feature\"\n                )\n            if model_type == ModelTypes.GENERATIVE_LLM:\n                # Check reserved keys are not used\n                reserved_emb_feat_names = {\"prompt\", \"response\"}\n                if reserved_emb_feat_names & embedding_features.keys():\n                    raise KeyError(\n                        \"embedding features cannot use the reserved feature names ('prompt', 'response') \"\n                        \"for GENERATIVE_LLM models\"\n                    )\n            for emb_name, emb_obj in embedding_features.items():\n                _validate_mapping_key(emb_name, \"embedding features\")\n                # Must verify embedding type\n                if not isinstance(emb_obj, Embedding):\n                    raise InvalidValueType(\n                        f\"embedding feature '{emb_name}'\", emb_obj, \"Embedding\"\n                    )\n                emb_obj.validate(emb_name)\n\n        # Validate tag types\n        if tags:\n            if not isinstance(tags, dict):\n                raise InvalidValueType(\"tags\", tags, \"dict\")\n            wrong_tags = [\n                tag_name for tag_name in tags if tag_name in RESERVED_TAG_COLS\n            ]\n            if wrong_tags:\n                raise KeyError(\n                    f\"The following tag names are not allowed as they are reserved: {wrong_tags}\"\n                )\n            for tag_name, tag_value in tags.items():\n                _validate_mapping_key(tag_name, \"tags\")\n                val = convert_element(tag_value)\n                if val is not None and not isinstance(\n                    val, (str, bool, float, int)\n                ):\n                    raise InvalidValueType(\n                        f\"tag '{tag_name}'\",\n                        tag_value,\n                        \"one of: bool, int, float, str\",\n                    )\n                if isinstance(tag_name, str) and tag_name.endswith(\"_shap\"):\n                    raise ValueError(\n                        f\"tag {tag_name} must not be named with a `_shap` suffix\"\n                    )\n                if len(str(val)) > MAX_TAG_LENGTH:\n                    raise ValueError(\n                        f\"The number of characters for each tag must be less than or equal to \"\n                        f\"{MAX_TAG_LENGTH}. The tag {tag_name} with value {tag_value} has \"\n                        f\"{len(str(val))} characters.\"\n                    )\n                elif len(str(val)) > MAX_TAG_LENGTH_TRUNCATION:\n                    logger.warning(\n                        get_truncation_warning_message(\n                            \"tags\", MAX_TAG_LENGTH_TRUNCATION\n                        )\n                    )\n\n        # Check the timestamp present on the event\n        if prediction_timestamp is not None:\n            if not isinstance(prediction_timestamp, int):\n                raise InvalidValueType(\n                    \"prediction_timestamp\", prediction_timestamp, \"int\"\n                )\n            # Send warning if prediction is sent with future timestamp\n            now = int(time.time())\n            if prediction_timestamp > now:\n                logger.warning(\n                    \"Caution when sending a prediction with future timestamp.\"\n                    \"Arize only stores 2 years worth of data. For example, if you sent a prediction \"\n                    \"to Arize from 1.5 years ago, and now send a prediction with timestamp of a year in \"\n                    \"the future, the oldest 0.5 years will be dropped to maintain the 2 years worth of data \"\n                    \"requirement.\"\n                )\n            if not is_timestamp_in_range(now, prediction_timestamp):\n                raise ValueError(\n                    f\"prediction_timestamp: {prediction_timestamp} is out of range.\"\n                    f\"Prediction timestamps must be within {MAX_FUTURE_YEARS_FROM_CURRENT_TIME} year in the \"\n                    f\"future and {MAX_PAST_YEARS_FROM_CURRENT_TIME} years in the past from the current time.\"\n                )\n\n        # Validate GENERATIVE_LLM models requirements\n        if model_type == ModelTypes.GENERATIVE_LLM:\n            if prompt is not None:\n                if not isinstance(prompt, (str, Embedding)):\n                    raise TypeError(\n                        f\"prompt must be of type str or Embedding, but found {type(val)}\"\n                    )\n                if isinstance(prompt, str):\n                    prompt = _PromptOrResponseText(data=prompt)\n                # Validate content of prompt, type is either Embedding or _PromptOrResponseText\n                prompt.validate(\"prompt\")\n            if response is not None:\n                if not isinstance(response, (str, Embedding)):\n                    raise TypeError(\n                        f\"response must be of type str or Embedding, but found {type(val)}\"\n                    )\n                if isinstance(response, str):\n                    response = _PromptOrResponseText(data=response)\n                # Validate content of response, type is either Embedding or _PromptOrResponseText\n                response.validate(\"response\")\n\n            # Validate prompt templates workflow information\n            _validate_prompt_templates_and_llm_config(\n                prompt_template,\n                prompt_template_version,\n                llm_model_name,\n                llm_params,\n            )\n            # Validate llm run metadata\n            if llm_run_metadata is not None:\n                llm_run_metadata.validate()\n        else:\n            if prompt is not None or response is not None:\n                raise ValueError(\n                    \"The fields 'prompt' and 'response' must be None for model types other \"\n                    \"than GENERATIVE_LLM\"\n                )\n\n        # Construct the prediction\n        p = None\n        # Only set a default prediction label for generative LLM models if there is no explicit prediction\n        # label AND no actual label to ensure that generative LLM model prediction records will have a\n        # prediction label that can be used for metrics in the platform (as users will generally pass in\n        # actuals/user feedback only). We do not want to add the default prediction label in if actual labels\n        # are present so that latent actuals will still work.\n        if (\n            model_type == ModelTypes.GENERATIVE_LLM\n            and prediction_label is None\n            and actual_label is None\n        ):\n            prediction_label = \"1\"\n\n        if prediction_label is not None:\n            if model_version is not None and not isinstance(model_version, str):\n                raise InvalidValueType(\"model_version\", model_version, \"str\")\n            _validate_label(\n                prediction_or_actual=\"prediction\",\n                model_type=model_type,\n                label=convert_element(prediction_label),\n                embedding_features=embedding_features,\n            )\n            p = pb2.Prediction(\n                prediction_label=_get_label(\n                    prediction_or_actual=\"prediction\",\n                    value=prediction_label,\n                    model_type=model_type,\n                ),\n                model_version=model_version,\n            )\n            if features is not None:\n                converted_feats = convert_dictionary(features)\n                feats = pb2.Prediction(features=converted_feats)\n                p.MergeFrom(feats)\n\n            if embedding_features or prompt or response:\n                # NOTE: Deep copy is necessary to avoid side effects on the original input dictionary\n                combined_embedding_features = (\n                    {k: v for k, v in embedding_features.items()}\n                    if embedding_features\n                    else {}\n                )\n                # Map prompt as embedding features for generative models\n                if prompt is not None:\n                    combined_embedding_features.update({\"prompt\": prompt})\n                # Map response as embedding features for generative models\n                if response is not None:\n                    combined_embedding_features.update({\"response\": response})\n                converted_embedding_feats = convert_dictionary(\n                    combined_embedding_features\n                )\n                embedding_feats = pb2.Prediction(\n                    features=converted_embedding_feats\n                )\n                p.MergeFrom(embedding_feats)\n\n            if tags or llm_run_metadata:\n                joined_tags = copy.deepcopy(tags)\n                if llm_run_metadata:\n                    if llm_run_metadata.total_token_count is not None:\n                        joined_tags[\n                            LLM_RUN_METADATA_TOTAL_TOKEN_COUNT_TAG_NAME\n                        ] = llm_run_metadata.total_token_count\n                    if llm_run_metadata.prompt_token_count is not None:\n                        joined_tags[\n                            LLM_RUN_METADATA_PROMPT_TOKEN_COUNT_TAG_NAME\n                        ] = llm_run_metadata.prompt_token_count\n                    if llm_run_metadata.response_token_count is not None:\n                        joined_tags[\n                            LLM_RUN_METADATA_RESPONSE_TOKEN_COUNT_TAG_NAME\n                        ] = llm_run_metadata.response_token_count\n                    if llm_run_metadata.response_latency_ms is not None:\n                        joined_tags[\n                            LLM_RUN_METADATA_RESPONSE_LATENCY_MS_TAG_NAME\n                        ] = llm_run_metadata.response_latency_ms\n                converted_tags = convert_dictionary(joined_tags)\n                tgs = pb2.Prediction(tags=converted_tags)\n                p.MergeFrom(tgs)\n\n            if (\n                prompt_template\n                or prompt_template_version\n                or llm_model_name\n                or llm_params\n            ):\n                llm_fields = pb2.LLMFields(\n                    prompt_template=prompt_template or \"\",\n                    prompt_template_name=prompt_template_version or \"\",\n                    llm_model_name=llm_model_name or \"\",\n                    llm_params=convert_dictionary(llm_params),\n                )\n                p.MergeFrom(pb2.Prediction(llm_fields=llm_fields))\n\n            if prediction_timestamp is not None:\n                p.timestamp.MergeFrom(get_timestamp(prediction_timestamp))\n\n        # Validate and construct the optional actual\n        is_latent_tags = prediction_label is None and tags is not None\n        a = None\n        if actual_label or is_latent_tags:\n            a = pb2.Actual()\n            if actual_label is not None:\n                _validate_label(\n                    prediction_or_actual=\"actual\",\n                    model_type=model_type,\n                    label=convert_element(actual_label),\n                    embedding_features=embedding_features,\n                )\n                a.MergeFrom(\n                    pb2.Actual(\n                        actual_label=_get_label(\n                            prediction_or_actual=\"actual\",\n                            value=actual_label,\n                            model_type=model_type,\n                        )\n                    )\n                )\n            # Added to support delayed tags on actuals.\n            if tags is not None:\n                converted_tags = convert_dictionary(tags)\n                a.MergeFrom(pb2.Actual(tags=converted_tags))\n\n        # Validate and construct the optional feature importances\n        fi = None\n        if shap_values is not None and bool(shap_values):\n            for k, v in shap_values.items():\n                if not isinstance(convert_element(v), float):\n                    raise InvalidValueType(f\"feature '{k}'\", v, \"float\")\n                if isinstance(k, str) and k.endswith(\"_shap\"):\n                    raise ValueError(\n                        f\"feature {k} must not be named with a `_shap` suffix\"\n                    )\n            fi = pb2.FeatureImportances(feature_importances=shap_values)\n\n        if p is None and a is None and fi is None:\n            raise ValueError(\n                \"must provide at least one of prediction_label, actual_label, tags, or shap_values\"\n            )\n\n        env_params = None\n        if environment == Environments.TRAINING:\n            if p is None or a is None:\n                raise ValueError(\n                    \"Training records must have both Prediction and Actual\"\n                )\n            env_params = pb2.Record.EnvironmentParams(\n                training=pb2.Record.EnvironmentParams.Training()\n            )\n        elif environment == Environments.VALIDATION:\n            if p is None or a is None:\n                raise ValueError(\n                    \"Validation records must have both Prediction and Actual\"\n                )\n            env_params = pb2.Record.EnvironmentParams(\n                validation=pb2.Record.EnvironmentParams.Validation(\n                    batch_id=batch_id\n                )\n            )\n        elif environment == Environments.PRODUCTION:\n            env_params = pb2.Record.EnvironmentParams(\n                production=pb2.Record.EnvironmentParams.Production()\n            )\n\n        rec = pb2.Record(\n            space_key=self._space_key,\n            model_id=model_id,\n            prediction_id=prediction_id,\n            prediction=p,\n            actual=a,\n            feature_importances=fi,\n            environment_params=env_params,\n            is_generative_llm_record=BoolValue(\n                value=model_type == ModelTypes.GENERATIVE_LLM\n            ),\n        )\n        return self._post(record=rec, uri=self._uri, indexes=None)\n",
          "triple_sequences": [
            {
              "action_api": "raise AuthError()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "missing_space_id=not (self._space_id or self._space_key), missing_api_key=not self._api_key, method_name=\"log\"",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Stop execution on invalid input",
              "intention_id": "stop_execution_invalid_input"
            },
            {
              "action_api": "raise InvalidValueType()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "\"model_id\", model_id, \"str\"",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Stop execution on invalid input",
              "intention_id": "stop_execution_invalid_input"
            },
            {
              "action_api": "raise InvalidValueType()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "\"model_type\", model_type, \"arize.utils.ModelTypes\"",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Stop execution on invalid input",
              "intention_id": "stop_execution_invalid_input"
            },
            {
              "action_api": "raise InvalidValueType()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "\"environment\", environment, \"arize.utils.Environments\"",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Stop execution on invalid input",
              "intention_id": "stop_execution_invalid_input"
            },
            {
              "action_api": "raise ValueError()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "\"Batch ID must be a nonempty string if logging to validation environment.\"",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Stop execution on invalid input",
              "intention_id": "stop_execution_invalid_input"
            },
            {
              "action_api": "_validate_and_convert_prediction_id()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "prediction_id, environment, prediction_label, actual_label, shap_values",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "cast_dictionary()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "features",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "cast_dictionary()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "tags",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "_validate_mapping_key()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "feat_name, \"features\"",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "is_list_of()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "feat_value, str",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "convert_element()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "feat_value",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "raise InvalidValueType()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "f\"feature '{feat_name}'\", feat_value, \"one of: bool, int, float, str\"",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Stop execution on invalid input",
              "intention_id": "stop_execution_invalid_input"
            },
            {
              "action_api": "_validate_mapping_key()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "emb_name, \"embedding features\"",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "raise InvalidValueType()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "f\"embedding feature '{emb_name}'\", emb_obj, \"Embedding\"",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Stop execution on invalid input",
              "intention_id": "stop_execution_invalid_input"
            },
            {
              "action_api": "emb_obj.validate()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "emb_name",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "raise InvalidValueType()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "\"tags\", tags, \"dict\"",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Stop execution on invalid input",
              "intention_id": "stop_execution_invalid_input"
            },
            {
              "action_api": "raise KeyError()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "f\"The following tag names are not allowed as they are reserved: {wrong_tags}\"",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Stop execution on invalid input",
              "intention_id": "stop_execution_invalid_input"
            },
            {
              "action_api": "_validate_mapping_key()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "tag_name, \"tags\"",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "convert_element()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "tag_value",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "raise InvalidValueType()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "f\"tag '{tag_name}'\", tag_value, \"one of: bool, int, float, str\"",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Stop execution on invalid input",
              "intention_id": "stop_execution_invalid_input"
            },
            {
              "action_api": "logger.warning()",
              "action_description": "Process HTTP response content",
              "action_id": "process_http_response",
              "object": "get_truncation_warning_message(\"tags\", MAX_TAG_LENGTH_TRUNCATION)",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Access attribute value",
              "intention_id": "access_attribute_value"
            },
            {
              "action_api": "raise ValueError()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "f\"tag {tag_name} must not be named with a `_shap` suffix\"",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Stop execution on invalid input",
              "intention_id": "stop_execution_invalid_input"
            },
            {
              "action_api": "raise ValueError()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "f\"The number of characters for each tag must be less than or equal to {MAX_TAG_LENGTH}. The tag {tag_name} with value {tag_value} has {len(str(val))} characters.\"",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Stop execution on invalid input",
              "intention_id": "stop_execution_invalid_input"
            },
            {
              "action_api": "logger.warning()",
              "action_description": "Process HTTP response content",
              "action_id": "process_http_response",
              "object": "\"Caution when sending a prediction with future timestamp.\"",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Access attribute value",
              "intention_id": "access_attribute_value"
            },
            {
              "action_api": "raise ValueError()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "f\"prediction_timestamp: {prediction_timestamp} is out of range.\"",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Stop execution on invalid input",
              "intention_id": "stop_execution_invalid_input"
            },
            {
              "action_api": "_validate_prompt_templates_and_llm_config()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "prompt_template, prompt_template_version, llm_model_name, llm_params",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "llm_run_metadata.validate()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "",
              "object_description": "",
              "object_id": "",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "raise ValueError()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "\"The fields 'prompt' and 'response' must be None for model types other than GENERATIVE_LLM\"",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Stop execution on invalid input",
              "intention_id": "stop_execution_invalid_input"
            },
            {
              "action_api": "_validate_label()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "prediction_or_actual=\"prediction\", model_type=model_type, label=convert_element(prediction_label), embedding_features=embedding_features",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "_get_label()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "prediction_or_actual=\"prediction\", value=prediction_label, model_type=model_type",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "convert_dictionary()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "features",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "convert_dictionary()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "combined_embedding_features",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "convert_dictionary()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "joined_tags",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "convert_dictionary()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "llm_params",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "get_timestamp()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "prediction_timestamp",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "_validate_label()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "prediction_or_actual=\"actual\", model_type=model_type, label=convert_element(actual_label), embedding_features=embedding_features",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "_get_label()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "prediction_or_actual=\"actual\", value=actual_label, model_type=model_type",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "convert_dictionary()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "tags",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "convert_element()",
              "action_description": "Evaluates string as Python expression",
              "action_id": "eval_python_expr",
              "object": "v",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "raise InvalidValueType()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "f\"feature '{k}'\", v, \"float\"",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Stop execution on invalid input",
              "intention_id": "stop_execution_invalid_input"
            },
            {
              "action_api": "raise ValueError()",
              "action_description": "Raises HTTPError if response status code indicates error",
              "action_id": "raise_http_error",
              "object": "f\"feature {k} must not be named with a `_shap` suffix\"",
              "object_description": "Error message text",
              "object_id": "error_message",
              "intention_description": "Stop execution on invalid input",
              "intention_id": "stop_execution_invalid_input"
            },
            {
              "action_api": "self._post()",
              "action_description": "Sends HTTP POST request to URL",
              "action_id": "open_url_post",
              "object": "record=rec, uri=self._uri, indexes=None",
              "object_description": "API endpoint",
              "object_id": "api_endpoint",
              "intention_description": "Transmit encoded data via HTTP POST",
              "intention_id": "transmit_data_http_post"
            }
          ]
        }
      ]
    }
  ]
}