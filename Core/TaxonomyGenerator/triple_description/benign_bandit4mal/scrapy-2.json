{
  "metadata": {
    "package_name": "scrapy-2",
    "original_json_path": "/home2/blue/Documents/PyPIAgent/Codes/code_contexral/codesnippets/benign_bandit4mal/scrapy-2.12.0.json",
    "dataset_type": "benign_bandit4mal"
  },
  "code_files": [
    {
      "pyfile": "test_downloadermiddleware_httpcache.py",
      "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/scrapy-2.12.0/scrapy-2.12.0/tests/test_downloadermiddleware_httpcache.py",
      "line_number": "38",
      "type_description": "B836:rmtree",
      "context_snippet": "import shutil\nimport tempfile\nimport email.utils\nimport time\nimport unittest\nfrom scrapy.http import Request, Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\nclass _BaseTest(unittest.TestCase):\n    storage_class = \"scrapy.extensions.httpcache.DbmCacheStorage\"\n    policy_class = \"scrapy.extensions.httpcache.RFC2616Policy\"\n\n    def setUp(self):\n        self.yesterday = email.utils.formatdate(time.time() - 86400)\n        self.today = email.utils.formatdate()\n        self.tomorrow = email.utils.formatdate(time.time() + 86400)\n        self.crawler = get_crawler(Spider)\n        self.spider = self.crawler._create_spider(\"example.com\")\n        self.tmpdir = tempfile.mkdtemp()\n        self.request = Request(\"http://www.example.com\", headers={\"User-Agent\": \"test\"})\n        self.response = Response(\n            \"http://www.example.com\",\n            headers={\"Content-Type\": \"text/html\"},\n            body=b\"test body\",\n            status=202,\n        )\n        self.crawler.stats.open_spider(self.spider)\n\n    def tearDown(self):\n        self.crawler.stats.close_spider(self.spider, \"\")\n        shutil.rmtree(self.tmpdir)\n",
      "hash_value": "5c753886c4d742305ef1aab401704485",
      "severity": "High",
      "confidence": "Medium",
      "code_snippets": [
        {
          "snippet": "import shutil\nimport tempfile\nimport email.utils\nimport time\nimport unittest\nfrom scrapy.http import Request, Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\nclass _BaseTest(unittest.TestCase):\n    storage_class = \"scrapy.extensions.httpcache.DbmCacheStorage\"\n    policy_class = \"scrapy.extensions.httpcache.RFC2616Policy\"\n\n    def setUp(self):\n        self.yesterday = email.utils.formatdate(time.time() - 86400)\n        self.today = email.utils.formatdate()\n        self.tomorrow = email.utils.formatdate(time.time() + 86400)\n        self.crawler = get_crawler(Spider)\n        self.spider = self.crawler._create_spider(\"example.com\")\n        self.tmpdir = tempfile.mkdtemp()\n        self.request = Request(\"http://www.example.com\", headers={\"User-Agent\": \"test\"})\n        self.response = Response(\n            \"http://www.example.com\",\n            headers={\"Content-Type\": \"text/html\"},\n            body=b\"test body\",\n            status=202,\n        )\n        self.crawler.stats.open_spider(self.spider)\n\n    def tearDown(self):\n        self.crawler.stats.close_spider(self.spider, \"\")\n        shutil.rmtree(self.tmpdir)\n",
          "triple_sequences": [
            {
              "action_api": "email.utils.formatdate()",
              "action_description": "Parses string into datetime object",
              "action_id": "parse_datetime",
              "object": "time.time() - 86400",
              "object_description": "Delay duration in seconds",
              "object_id": "delay_duration",
              "intention_description": "Get datetime epoch",
              "intention_id": "get_datetime_epoch"
            },
            {
              "action_api": "email.utils.formatdate()",
              "action_description": "Parses string into datetime object",
              "action_id": "parse_datetime",
              "object": "",
              "object_description": "",
              "object_id": "",
              "intention_description": "Get datetime epoch",
              "intention_id": "get_datetime_epoch"
            },
            {
              "action_api": "email.utils.formatdate()",
              "action_description": "Parses string into datetime object",
              "action_id": "parse_datetime",
              "object": "time.time() + 86400",
              "object_description": "Delay duration in seconds",
              "object_id": "delay_duration",
              "intention_description": "Get datetime epoch",
              "intention_id": "get_datetime_epoch"
            },
            {
              "action_api": "get_crawler()",
              "action_description": "Instantiates class",
              "action_id": "init_grabber_class",
              "object": "Spider",
              "object_description": "Module name string",
              "object_id": "module_name_string",
              "intention_description": "Import required module",
              "intention_id": "import_required_module"
            },
            {
              "action_api": "self.crawler._create_spider()",
              "action_description": "Instantiates class",
              "action_id": "init_grabber_class",
              "object": "\"example.com\"",
              "object_description": "Domain name string",
              "object_id": "domain_name",
              "intention_description": "Import required module",
              "intention_id": "import_required_module"
            },
            {
              "action_api": "tempfile.mkdtemp()",
              "action_description": "Creates temporary directory and returns its path",
              "action_id": "create_temp_dir",
              "object": "",
              "object_description": "",
              "object_id": "",
              "intention_description": "Create temporary directory",
              "intention_id": "create_temporary_directory"
            },
            {
              "action_api": "Request()",
              "action_description": "Creates HTTP request object with specified URL, data, and headers",
              "action_id": "create_http_request",
              "object": "\"http://www.example.com\", headers={\"User-Agent\": \"test\"}",
              "object_description": "Request object for external domain",
              "object_id": "request_external_domain",
              "intention_description": "Prepare URL for HTTP request",
              "intention_id": "prepare_url_http_request"
            },
            {
              "action_api": "Response()",
              "action_description": "Process HTTP response content",
              "action_id": "process_http_response",
              "object": "\"http://www.example.com\", headers={\"Content-Type\": \"text/html\"}, body=b\"test body\", status=202",
              "object_description": "HTTP response content from remote server",
              "object_id": "http_response_remote_content",
              "intention_description": "Download remote content",
              "intention_id": "download_remote_content"
            },
            {
              "action_api": "self.crawler.stats.open_spider()",
              "action_description": "Opens file",
              "action_id": "open_file_app",
              "object": "self.spider",
              "object_description": "Module name string",
              "object_id": "module_name_string",
              "intention_description": "Open file",
              "intention_id": "open_file"
            },
            {
              "action_api": "self.crawler.stats.close_spider()",
              "action_description": "Closes the opened file",
              "action_id": "close_file",
              "object": "self.spider, \"\"",
              "object_description": "Module name string",
              "object_id": "module_name_string",
              "intention_description": "Close file",
              "intention_id": "close_file"
            },
            {
              "action_api": "shutil.rmtree()",
              "action_description": "Recursively deletes directory and its contents",
              "action_id": "delete_directory",
              "object": "self.tmpdir",
              "object_description": "Temporary directory",
              "object_id": "temporary_directory",
              "intention_description": "Delete directory content",
              "intention_id": "delete_directory_content"
            }
          ]
        }
      ]
    }
  ]
}