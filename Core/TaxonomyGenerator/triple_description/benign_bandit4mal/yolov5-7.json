{
  "metadata": {
    "package_name": "yolov5-7",
    "original_json_path": "/home2/blue/Documents/PyPIAgent/Codes/code_contexral/codesnippets/benign_bandit4mal/yolov5-7.0.14.json",
    "dataset_type": "benign_bandit4mal"
  },
  "code_files": [
    {
      "pyfile": "val.py",
      "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/yolov5-7.0.14/yolov5-7.0.14/yolov5/val.py",
      "line_number": "316",
      "type_description": "B815:write",
      "context_snippet": "def run(\n        data,\n        weights=None,  # model.pt path(s)\n        batch_size=None,  # batch size\n        batch=None,  # batch size\n        imgsz=None,  # inference size (pixels)\n        img=None,  # inference size (pixels)\n        conf_thres=0.001,  # confidence threshold\n        iou_thres=0.6,  # NMS IoU threshold\n        max_det=300,  # maximum detections per image\n        task='val',  # train, val, test, speed or study\n        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n        workers=8,  # max dataloader workers (per RANK in DDP mode)\n        single_cls=False,  # treat as single-class dataset\n        augment=False,  # augmented inference\n        verbose=False,  # verbose output\n        save_txt=False,  # save results to *.txt\n        save_hybrid=False,  # save label+prediction hybrid results to *.txt\n        save_conf=False,  # save confidences in --save-txt labels\n        save_json=False,  # save a COCO-JSON results file\n        project='runs/val',  # save to project/name\n        name='exp',  # save to project/name\n        exist_ok=False,  # existing project/name ok, do not increment\n        half=True,  # use FP16 half-precision inference\n        dnn=False,  # use OpenCV DNN for ONNX inference\n        model=None,\n        dataloader=None,\n        save_dir=Path(''),\n        plots=True,\n        callbacks=Callbacks(),\n        compute_loss=None,\n):\n    ...\n    # Export results as html\n    header = \"Class Images Labels P R mAP@.5 mAP@.5:.95\"\n    headers = header.split()\n    data = []\n    data.append(['all', seen, nt.sum(), f\"{float(mp):0.3f}\", f\"{float(mr):0.3f}\", f\"{float(map50):0.3f}\", f\"{float(map):0.3f}\"])\n    for i, c in enumerate(ap_class):\n        data.append([names[c], seen, nt[c], f\"{float(p[i]):0.3f}\", f\"{float(r[i]):0.3f}\", f\"{float(ap50[i]):0.3f}\", f\"{float(ap[i]):0.3f}\"])\n    results_df = pd.DataFrame(data,columns=headers)\n    results_html = results_df.to_html()\n    text_file = open(save_dir / \"results.html\", \"w\")\n    text_file.write(results_html)\n    text_file.close()\n    ...",
      "hash_value": "6bf89baf19f5e9a2bc44b017463df18f",
      "severity": "High",
      "confidence": "Medium",
      "code_snippets": [
        {
          "snippet": "def run(\n        data,\n        weights=None,  # model.pt path(s)\n        batch_size=None,  # batch size\n        batch=None,  # batch size\n        imgsz=None,  # inference size (pixels)\n        img=None,  # inference size (pixels)\n        conf_thres=0.001,  # confidence threshold\n        iou_thres=0.6,  # NMS IoU threshold\n        max_det=300,  # maximum detections per image\n        task='val',  # train, val, test, speed or study\n        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n        workers=8,  # max dataloader workers (per RANK in DDP mode)\n        single_cls=False,  # treat as single-class dataset\n        augment=False,  # augmented inference\n        verbose=False,  # verbose output\n        save_txt=False,  # save results to *.txt\n        save_hybrid=False,  # save label+prediction hybrid results to *.txt\n        save_conf=False,  # save confidences in --save-txt labels\n        save_json=False,  # save a COCO-JSON results file\n        project='runs/val',  # save to project/name\n        name='exp',  # save to project/name\n        exist_ok=False,  # existing project/name ok, do not increment\n        half=True,  # use FP16 half-precision inference\n        dnn=False,  # use OpenCV DNN for ONNX inference\n        model=None,\n        dataloader=None,\n        save_dir=Path(''),\n        plots=True,\n        callbacks=Callbacks(),\n        compute_loss=None,\n):\n    ...\n    # Export results as html\n    header = \"Class Images Labels P R mAP@.5 mAP@.5:.95\"\n    headers = header.split()\n    data = []\n    data.append(['all', seen, nt.sum(), f\"{float(mp):0.3f}\", f\"{float(mr):0.3f}\", f\"{float(map50):0.3f}\", f\"{float(map):0.3f}\"])\n    for i, c in enumerate(ap_class):\n        data.append([names[c], seen, nt[c], f\"{float(p[i]):0.3f}\", f\"{float(r[i]):0.3f}\", f\"{float(ap50[i]):0.3f}\", f\"{float(ap[i]):0.3f}\"])\n    results_df = pd.DataFrame(data,columns=headers)\n    results_html = results_df.to_html()\n    text_file = open(save_dir / \"results.html\", \"w\")\n    text_file.write(results_html)\n    text_file.close()\n    ...",
          "triple_sequences": [
            {
              "action_api": "str.split()",
              "action_description": "Basic path string operations (getting absolute path, base name, parent directory, splitting and joining paths)",
              "action_id": "path_string_operations",
              "object": "header",
              "object_description": "String containing environment data",
              "object_id": "string_environment_data",
              "intention_description": "Prepare string for further processing",
              "intention_id": "prepare_string_processing"
            },
            {
              "action_api": "pd.DataFrame()",
              "action_description": "Serialize Python object to JSON string",
              "action_id": "serialize_to_json",
              "object": "data, columns=headers",
              "object_description": "Structured file data",
              "object_id": "structured_file_data",
              "intention_description": "Serialize data",
              "intention_id": "serialize_data"
            },
            {
              "action_api": "DataFrame.to_html()",
              "action_description": "Serialize Python object to JSON string",
              "action_id": "serialize_to_json",
              "object": "results_df",
              "object_description": "Structured file data",
              "object_id": "structured_file_data",
              "intention_description": "Serialize data",
              "intention_id": "serialize_data"
            },
            {
              "action_api": "open()",
              "action_description": "File opening operations for writing (normal writing, binary writing)",
              "action_id": "basic_write_operations",
              "object": "save_dir / \"results.html\", \"w\"",
              "object_description": "File path",
              "object_id": "file_path",
              "intention_description": "Open file for writing",
              "intention_id": "open_file_writing"
            },
            {
              "action_api": "file.write()",
              "action_description": "File opening operations for writing (normal writing, binary writing)",
              "action_id": "basic_write_operations",
              "object": "results_html",
              "object_description": "File text",
              "object_id": "file_text",
              "intention_description": "Write file content",
              "intention_id": "write_file_content"
            },
            {
              "action_api": "file.close()",
              "action_description": "Closes the opened file",
              "action_id": "close_file",
              "object": "",
              "object_description": "",
              "object_id": "",
              "intention_description": "Close file",
              "intention_id": "close_file"
            }
          ]
        }
      ]
    }
  ]
}