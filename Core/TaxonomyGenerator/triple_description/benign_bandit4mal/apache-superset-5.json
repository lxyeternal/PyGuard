{
  "metadata": {
    "package_name": "apache-superset-5",
    "original_json_path": "/home2/blue/Documents/PyPIAgent/Codes/code_contexral/codesnippets/benign_bandit4mal/apache-superset-5.0.0rc2.json",
    "dataset_type": "benign_bandit4mal"
  },
  "code_files": [
    {
      "pyfile": "trino.py",
      "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/apache-superset-5.0.0rc2/apache-superset-5.0.0rc2/superset/db_engine_specs/trino.py",
      "line_number": "441",
      "type_description": "B820:get",
      "context_snippet": "@classmethod\n    def _expand_columns(cls, col: ResultSetColumnType) -> list[ResultSetColumnType]:\n        \"\"\"\n        Expand the given column out to one or more columns by analysing their types,\n        descending into ROWS and expanding out their inner fields recursively.\n\n        We can only navigate named fields in ROWs in this way, so we can't expand out\n        MAP or ARRAY types, nor fields in ROWs which have no name (in fact the trino\n        library doesn't correctly parse unnamed fields in ROWs). We won't be able to\n        expand ROWs which are nested underneath any of those types, either.\n\n        Expanded columns are named foo.bar.baz and we provide a query_as property to\n        instruct the base engine spec how to correctly query them: instead of quoting\n        the whole string they have to be quoted like \"foo\".\"bar\".\"baz\" and we then\n        alias them to the full dotted string for ease of reference.\n        \"\"\"\n        # pylint: disable=import-outside-toplevel\n        from trino.sqlalchemy import datatype\n\n        cols = [col]\n        col_type = col.get(\"type\")\n\n        if not isinstance(col_type, datatype.ROW):\n            return cols\n\n        for inner_name, inner_type in col_type.attr_types:\n            outer_name = col[\"name\"]\n            name = \".\".join([outer_name, inner_name])\n            query_name = \".\".join([f'\"{piece}\"' for piece in name.split(\".\")])\n            column_spec = cls.get_column_spec(str(inner_type))\n            is_dttm = column_spec.is_dttm if column_spec else False\n\n            inner_col = ResultSetColumnType(\n                name=name,\n                column_name=name,\n                type=inner_type,\n                is_dttm=is_dttm,\n                query_as=f'{query_name} AS \"{name}\"',\n            )\n            cols.extend(cls._expand_columns(inner_col))\n\n        return cols",
      "hash_value": "bc5cb7b9579f8eccc39720eed8b5d8e7",
      "severity": "High",
      "confidence": "Medium",
      "code_snippets": [
        {
          "snippet": "@classmethod\n    def _expand_columns(cls, col: ResultSetColumnType) -> list[ResultSetColumnType]:\n        \"\"\"\n        Expand the given column out to one or more columns by analysing their types,\n        descending into ROWS and expanding out their inner fields recursively.\n\n        We can only navigate named fields in ROWs in this way, so we can't expand out\n        MAP or ARRAY types, nor fields in ROWs which have no name (in fact the trino\n        library doesn't correctly parse unnamed fields in ROWs). We won't be able to\n        expand ROWs which are nested underneath any of those types, either.\n\n        Expanded columns are named foo.bar.baz and we provide a query_as property to\n        instruct the base engine spec how to correctly query them: instead of quoting\n        the whole string they have to be quoted like \"foo\".\"bar\".\"baz\" and we then\n        alias them to the full dotted string for ease of reference.\n        \"\"\"\n        # pylint: disable=import-outside-toplevel\n        from trino.sqlalchemy import datatype\n\n        cols = [col]\n        col_type = col.get(\"type\")\n\n        if not isinstance(col_type, datatype.ROW):\n            return cols\n\n        for inner_name, inner_type in col_type.attr_types:\n            outer_name = col[\"name\"]\n            name = \".\".join([outer_name, inner_name])\n            query_name = \".\".join([f'\"{piece}\"' for piece in name.split(\".\")])\n            column_spec = cls.get_column_spec(str(inner_type))\n            is_dttm = column_spec.is_dttm if column_spec else False\n\n            inner_col = ResultSetColumnType(\n                name=name,\n                column_name=name,\n                type=inner_type,\n                is_dttm=is_dttm,\n                query_as=f'{query_name} AS \"{name}\"',\n            )\n            cols.extend(cls._expand_columns(inner_col))\n\n        return cols",
          "triple_sequences": [
            {
              "action_api": "isinstance()",
              "action_description": "Checks if object is instance of specified type",
              "action_id": "check_instance_type",
              "object": "col_type, datatype.ROW",
              "object_description": "Module name string",
              "object_id": "module_name_string",
              "intention_description": "Determine required module presence",
              "intention_id": "determine_required_module_presence"
            },
            {
              "action_api": "str.split()",
              "action_description": "Basic path string operations (getting absolute path, base name, parent directory, splitting and joining paths)",
              "action_id": "path_string_operations",
              "object": "name, \".\"",
              "object_description": "Path segment",
              "object_id": "path_segment",
              "intention_description": "Prepare path for file operations",
              "intention_id": "prepare_path_file_operations"
            },
            {
              "action_api": "str.join()",
              "action_description": "Basic path string operations (getting absolute path, base name, parent directory, splitting and joining paths)",
              "action_id": "path_string_operations",
              "object": "[f'\"{piece}\"' for piece in name.split(\".\")]",
              "object_description": "Path segment",
              "object_id": "path_segment",
              "intention_description": "Prepare path for file operations",
              "intention_id": "prepare_path_file_operations"
            },
            {
              "action_api": "str()",
              "action_description": "Converts bytes to string using default codec",
              "action_id": "decode_bytes_default",
              "object": "inner_type",
              "object_description": "",
              "object_id": "",
              "intention_description": "Prepare string for further processing",
              "intention_id": "prepare_string_processing"
            },
            {
              "action_api": "cls.get_column_spec()",
              "action_description": "Import required module",
              "action_id": "import_required_module",
              "object": "str(inner_type)",
              "object_description": "Module name string",
              "object_id": "module_name_string",
              "intention_description": "Import required module",
              "intention_id": "import_required_module"
            },
            {
              "action_api": "ResultSetColumnType()",
              "action_description": "Instantiates class",
              "action_id": "init_grabber_class",
              "object": "name=name, column_name=name, type=inner_type, is_dttm=is_dttm, query_as=f'{query_name} AS \"{name}\"'",
              "object_description": "Function and arguments",
              "object_id": "function_with_arguments",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            },
            {
              "action_api": "cls._expand_columns()",
              "action_description": "Import required module",
              "action_id": "import_required_module",
              "object": "inner_col",
              "object_description": "",
              "object_id": "",
              "intention_description": "Prepare data for further processing",
              "intention_id": "prepare_data_processing"
            }
          ]
        }
      ]
    }
  ]
}