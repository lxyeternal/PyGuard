[
  {
    "metadata": {
      "package_name": "azure_cli-2.71.0",
      "total_matches": 5,
      "processing_date": "2025-05-18 01:49:39"
    }
  },
  {
    "pyfile": "custom.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/azure_cli-2.71.0/azure_cli-2.71.0/azure/cli/command_modules/aro/custom.py",
    "line_number": "390",
    "type_description": "B801:b64decode",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "389\t    file_mode = \"x\"\n390\t    yaml_data = b64decode(query_result.kubeconfig).decode('UTF-8')\n391\t    try:",
    "code_snippet": "def aro_get_admin_kubeconfig(client, resource_group_name, resource_name, file=\"kubeconfig\"):\n    query_result = client.open_shift_clusters.list_admin_credentials(resource_group_name, resource_name)\n    file_mode = \"x\"\n    yaml_data = b64decode(query_result.kubeconfig).decode('UTF-8')\n    try:\n        with open(file, file_mode, encoding=\"utf-8\") as f:\n            f.write(yaml_data)\n    except FileExistsError as e:\n        raise FileOperationError(f\"File {file} already exists.\") from e\n    logger.info(\"Kubeconfig written to file: %s\", file)",
    "pattern_analysis": {
      "api_sequence": [
        "client.open_shift_clusters.list_admin_credentials",
        "base64.b64decode",
        "open",
        "io.TextIOWrapper.write"
      ],
      "api_sequence_with_args": [
        "client.open_shift_clusters.list_admin_credentials(resource_group_name, resource_name)",
        "base64.b64decode(query_result.kubeconfig)",
        "open(file, file_mode, encoding=\"utf-8\")",
        "io.TextIOWrapper.write(yaml_data)"
      ],
      "mapped_sequence": [
        {
          "api_name": "client.open_shift_clusters.list_admin_credentials",
          "id": "import_dynamic",
          "description": "Dynamically imports specified module",
          "first_id": "code_execution",
          "second_id": "module_management",
          "third_id": "module_importing"
        },
        {
          "api_name": "base64.b64decode",
          "id": "decode_base64_to_bytes",
          "description": "Decodes base64-encoded string to bytes",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "base_encoding"
        },
        {
          "api_name": "open",
          "id": "basic_write_operations",
          "description": "Basic file opening operations for writing (normal writing, binary writing)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_opening"
        },
        {
          "api_name": "io.TextIOWrapper.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        }
      ],
      "contextual_code": "def aro_get_admin_kubeconfig(client, resource_group_name, resource_name, file=\"kubeconfig\"):\n    query_result = client.open_shift_clusters.list_admin_credentials(resource_group_name, resource_name)\n    file_mode = \"x\"\n    yaml_data = b64decode(query_result.kubeconfig).decode('UTF-8')\n    try:\n        with open(file, file_mode, encoding=\"utf-8\") as f:\n            f.write(yaml_data)\n    except FileExistsError as e:\n        raise FileOperationError(f\"File {file} already exists.\") from e"
    }
  },
  {
    "pyfile": "util.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/azure_cli-2.71.0/azure_cli-2.71.0/azure/cli/command_modules/storage/azcopy/util.py",
    "line_number": "239",
    "type_description": "B814:read",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "238\t                with open(install_location, 'wb') as f:\n239\t                    f.write(zip_file.read(fileName))\n240\t    elif url.endswith('gz'):",
    "code_snippet": "def _urlretrieve(url, install_location):\n    import io\n    logger.warning('Downloading AzCopy from %s', url)\n    req = urlopen(url)\n    compressedFile = io.BytesIO(req.read())\n    if url.endswith('zip'):\n        zip_file = zipfile.ZipFile(compressedFile)\n        for fileName in zip_file.namelist():\n            if fileName.endswith('azcopy') or fileName.endswith('azcopy.exe'):\n                with open(install_location, 'wb') as f:\n                    f.write(zip_file.read(fileName))\n    elif url.endswith('gz'):\n        import tarfile\n        with tarfile.open(fileobj=compressedFile, mode=\"r:gz\") as tar:\n            for tarinfo in tar:\n                if tarinfo.isfile() and tarinfo.name.endswith('azcopy'):\n                    with open(install_location, 'wb') as f:\n                        f.write(tar.extractfile(tarinfo).read())\n    else:\n        raise CLIError('Invalid downloading url {}'.format(url))",
    "pattern_analysis": {
      "api_sequence": [
        "logger.warning",
        "urlopen",
        "req.read",
        "io.BytesIO",
        "zipfile.ZipFile",
        "zip_file.namelist",
        "zip_file.read",
        "open",
        "f.write",
        "tarfile.open",
        "tarinfo.isfile",
        "tar.extractfile",
        "tar.extractfile.read",
        "open",
        "f.write"
      ],
      "api_sequence_with_args": [
        "logger.warning('Downloading AzCopy from %s', url)",
        "urlopen(url)",
        "req.read()",
        "io.BytesIO(req.read())",
        "zipfile.ZipFile(compressedFile)",
        "zip_file.namelist()",
        "zip_file.read(fileName)",
        "open(install_location, 'wb')",
        "f.write(zip_file.read(fileName))",
        "tarfile.open(fileobj=compressedFile, mode=\"r:gz\")",
        "tarinfo.isfile()",
        "tar.extractfile(tarinfo)",
        "tar.extractfile(tarinfo).read()",
        "open(install_location, 'wb')",
        "f.write(tar.extractfile(tarinfo).read())"
      ],
      "mapped_sequence": [
        {
          "api_name": "logger.warning",
          "id": "log_error",
          "description": "Logs error message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "urlopen",
          "id": "open_url_get",
          "description": "Opens URL with GET parameters",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "request_sending"
        },
        {
          "api_name": "req.read",
          "id": "read_response_body",
          "description": "Reads response body from HTTP response",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "response_processing"
        },
        {
          "api_name": "io.BytesIO",
          "id": "create_memory_bytes",
          "description": "Creates in-memory bytes buffer from encoded string",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "byte_encoding"
        },
        {
          "api_name": "zipfile.ZipFile",
          "id": "open_zip_read",
          "description": "Opens ZIP archive for reading",
          "first_id": "file_operations",
          "second_id": "compression_archiving",
          "third_id": "zip_operations"
        },
        {
          "api_name": "zip_file.namelist",
          "id": "list_files_directories",
          "description": "Lists files and directories in specified path",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "directory_operations"
        },
        {
          "api_name": "zip_file.read",
          "id": "basic_file_reading",
          "description": "Reading content from files (by lines or entire content)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_reading"
        },
        {
          "api_name": "open",
          "id": "basic_write_operations",
          "description": "Basic file opening operations for writing (normal writing, binary writing)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_opening"
        },
        {
          "api_name": "f.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "tarfile.open",
          "id": "open_zip_read",
          "description": "Opens ZIP archive for reading",
          "first_id": "file_operations",
          "second_id": "compression_archiving",
          "third_id": "zip_operations"
        },
        {
          "api_name": "tarinfo.isfile",
          "id": "check_file_is_file",
          "description": "Checks if specified path exists and is a file",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_checking"
        },
        {
          "api_name": "tar.extractfile",
          "id": "basic_file_reading",
          "description": "Reading content from files (by lines or entire content)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_reading"
        },
        {
          "api_name": "tar.extractfile.read",
          "id": "basic_file_reading",
          "description": "Reading content from files (by lines or entire content)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_reading"
        },
        {
          "api_name": "open",
          "id": "basic_write_operations",
          "description": "Basic file opening operations for writing (normal writing, binary writing)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_opening"
        },
        {
          "api_name": "f.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        }
      ],
      "contextual_code": "def _urlretrieve(url, install_location):\n    import io\n    logger.warning('Downloading AzCopy from %s', url)\n    req = urlopen(url)\n    compressedFile = io.BytesIO(req.read())\n    if url.endswith('zip'):\n        zip_file = zipfile.ZipFile(compressedFile)\n        for fileName in zip_file.namelist():\n            if fileName.endswith('azcopy') or fileName.endswith('azcopy.exe'):\n                with open(install_location, 'wb') as f:\n                    f.write(zip_file.read(fileName))\n    elif url.endswith('gz'):\n        import tarfile\n        with tarfile.open(fileobj=compressedFile, mode=\"r:gz\") as tar:\n            for tarinfo in tar:\n                if tarinfo.isfile() and tarinfo.name.endswith('azcopy'):\n                    with open(install_location, 'wb') as f:\n                        f.write(tar.extractfile(tarinfo).read())\n    else:\n        raise CLIError('Invalid downloading url {}'.format(url))"
    }
  },
  {
    "pyfile": "_network.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/azure_cli-2.71.0/azure_cli-2.71.0/azure/cli/command_modules/mysql/_network.py",
    "line_number": "229",
    "type_description": "B820:get",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "228\t        for subnet in vnet.get(\"subnets\", []):\n229\t            vnet_subnet_prefixes += (subnet.get(\"addressPrefixes\") if not subnet.get(\"addressPrefix\") else [subnet.get(\"addressPrefix\")])\n230\t        if subnet_address_pref in vnet_subnet_prefixes:",
    "code_snippet": "def _create_subnet_delegation(cmd, nw_subscription, resource_client, delegation_service_name, resource_group, vnet_name, subnet_name, location, server_name, subnet_address_pref, yes):\n    delegation = {\"name\": delegation_service_name, \"service_name\": delegation_service_name}\n\n    # subnet not exist\n    if not check_existence(resource_client, subnet_name, resource_group, 'Microsoft.Network', 'subnets', parent_name=vnet_name, parent_type='virtualNetworks'):\n        vnet = VNetShow(cli_ctx=cmd.cli_ctx)(command_args={\n            \"name\": vnet_name,\n            \"subscription\": nw_subscription,\n            \"resource_group\": resource_group\n        })\n\n        vnet_subnet_prefixes = []\n        for subnet in vnet.get(\"subnets\", []):\n            vnet_subnet_prefixes += (subnet.get(\"addressPrefixes\") if not subnet.get(\"addressPrefix\") else [subnet.get(\"addressPrefix\")])\n        if subnet_address_pref in vnet_subnet_prefixes:\n            raise ValidationError(f\"The Subnet (default) prefix {subnet_address_pref} is already taken by another Subnet in the Vnet. Please provide a different prefix for --subnet-prefix parameter\")\n\n        user_confirmation(\"Do you want to create a new Subnet {0} in resource group {1}\".format(subnet_name, resource_group), yes=yes)\n        logger.warning('Creating new Subnet \"%s\" in resource group \"%s\"', subnet_name, resource_group)\n        poller = SubnetCreate(cli_ctx=cmd.cli_ctx)(command_args={\n            \"name\": subnet_name,\n            \"vnet_name\": vnet_name,\n            \"subscription\": nw_subscription,\n            \"resource_group\": resource_group,\n            \"address_prefix\": subnet_address_pref,\n            \"delegated_services\": [delegation]\n        })\n        subnet = LongRunningOperation(cmd.cli_ctx)(poller)\n    # subnet exist\n    else:\n        subnet = SubnetShow(cli_ctx=cmd.cli_ctx)(command_args={\n            \"name\": subnet_name,\n            \"vnet_name\": vnet_name,\n            \"subscription\": nw_subscription,\n            \"resource_group\": resource_group\n        })\n        subnet_address_prefixes = [DEFAULT_SUBNET_ADDRESS_PREFIX] + subnet.get(\"addressPrefixes\") if not subnet.get(\"addressPrefix\") else [subnet.get(\"addressPrefix\")]\n        logger.warning('Using existing Subnet \"%s\" in resource group \"%s\"', subnet_name, resource_group)\n        if subnet_address_pref not in subnet_address_prefixes:\n            logger.warning(\"The prefix of the subnet you provided does not match the --subnet-prefix value %s. Using current prefix %s\", subnet_address_pref, subnet_address_prefixes)\n\n        # Add Delegation if not delegated already\n        if not subnet.get(\"delegations\", None):\n            logger.warning('Adding \"%s\" delegation to the existing subnet %s.', delegation_service_name, subnet_name)\n            poller = SubnetUpdate(cli_ctx=cmd.cli_ctx)(command_args={\n                \"name\": subnet_name,\n                \"vnet_name\": vnet_name,\n                \"subscription\": nw_subscription,\n                \"resource_group\": resource_group,\n                \"delegated_services\": [delegation]\n            })\n            subnet = LongRunningOperation(cmd.cli_ctx)(poller)\n        else:\n            for delgtn in subnet[\"delegations\"]:\n                if delgtn[\"serviceName\"] != delegation_service_name:\n                    raise CLIError(\"Can not use subnet with existing delegations other than {}\".format(\n                        delegation_service_name))\n\n    return subnet",
    "pattern_analysis": {
      "api_sequence": [
        "check_existence",
        "VNetShow",
        "vnet.get",
        "subnet.get",
        "subnet.get",
        "user_confirmation",
        "logger.warning",
        "SubnetCreate",
        "LongRunningOperation",
        "SubnetShow",
        "subnet.get",
        "logger.warning",
        "logger.warning",
        "subnet.get",
        "logger.warning",
        "SubnetUpdate",
        "LongRunningOperation"
      ],
      "api_sequence_with_args": [
        "check_existence(resource_client, subnet_name, resource_group, 'Microsoft.Network', 'subnets', parent_name=vnet_name, parent_type='virtualNetworks')",
        "VNetShow(cli_ctx=cmd.cli_ctx)(command_args={...})",
        "vnet.get('subnets', [])",
        "subnet.get('addressPrefixes')",
        "subnet.get('addressPrefix')",
        "user_confirmation('Do you want to create a new Subnet {0} in resource group {1}'.format(subnet_name, resource_group), yes=yes)",
        "logger.warning('Creating new Subnet \"%s\" in resource group \"%s\"', subnet_name, resource_group)",
        "SubnetCreate(cli_ctx=cmd.cli_ctx)(command_args={...})",
        "LongRunningOperation(cmd.cli_ctx)(poller)",
        "SubnetShow(cli_ctx=cmd.cli_ctx)(command_args={...})",
        "subnet.get('addressPrefix')",
        "logger.warning('Using existing Subnet \"%s\" in resource group \"%s\"', subnet_name, resource_group)",
        "logger.warning('The prefix of the subnet you provided does not match the --subnet-prefix value %s. Using current prefix %s', subnet_address_pref, subnet_address_prefixes)",
        "subnet.get('delegations', None)",
        "logger.warning('Adding \"%s\" delegation to the existing subnet %s.', delegation_service_name, subnet_name)",
        "SubnetUpdate(cli_ctx=cmd.cli_ctx)(command_args={...})",
        "LongRunningOperation(cmd.cli_ctx)(poller)"
      ],
      "mapped_sequence": [
        {
          "api_name": "check_existence",
          "id": "check_path_exists",
          "description": "Checks if specified path exists in filesystem",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_checking"
        },
        {
          "api_name": "VNetShow",
          "id": "get_process_info",
          "description": "Retrieves process information as dictionary",
          "first_id": "system_operations",
          "second_id": "process_management",
          "third_id": "process_information"
        },
        {
          "api_name": "vnet.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "subnet.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "subnet.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "user_confirmation",
          "id": "read_user_input",
          "description": "Reads user input from standard input",
          "first_id": "persistence_stealth",
          "second_id": "user_interaction",
          "third_id": "interface_control"
        },
        {
          "api_name": "logger.warning",
          "id": "log_error",
          "description": "Logs error message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "SubnetCreate",
          "id": "create_directory",
          "description": "Creates directory, ignoring if it already exists",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "directory_operations"
        },
        {
          "api_name": "LongRunningOperation",
          "id": "wait_process_completion",
          "description": "Waits for process to complete execution",
          "first_id": "system_operations",
          "second_id": "process_management",
          "third_id": "process_control"
        },
        {
          "api_name": "SubnetShow",
          "id": "get_process_info",
          "description": "Retrieves process information as dictionary",
          "first_id": "system_operations",
          "second_id": "process_management",
          "third_id": "process_information"
        },
        {
          "api_name": "subnet.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "logger.warning",
          "id": "log_error",
          "description": "Logs error message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "logger.warning",
          "id": "log_error",
          "description": "Logs error message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "subnet.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "logger.warning",
          "id": "log_error",
          "description": "Logs error message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "SubnetUpdate",
          "id": "set_file_attributes",
          "description": "Sets file attributes for specified file",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_attribute_management"
        },
        {
          "api_name": "LongRunningOperation",
          "id": "wait_process_completion",
          "description": "Waits for process to complete execution",
          "first_id": "system_operations",
          "second_id": "process_management",
          "third_id": "process_control"
        }
      ],
      "contextual_code": "def _create_subnet_delegation(cmd, nw_subscription, resource_client, delegation_service_name, resource_group, vnet_name, subnet_name, location, server_name, subnet_address_pref, yes):\n    delegation = {\"name\": delegation_service_name, \"service_name\": delegation_service_name}\n\n    # subnet not exist\n    if not check_existence(resource_client, subnet_name, resource_group, 'Microsoft.Network', 'subnets', parent_name=vnet_name, parent_type='virtualNetworks'):\n        vnet = VNetShow(cli_ctx=cmd.cli_ctx)(command_args={\n            \"name\": vnet_name,\n            \"subscription\": nw_subscription,\n            \"resource_group\": resource_group\n        })\n\n        vnet_subnet_prefixes = []\n        for subnet in vnet.get(\"subnets\", []):\n            vnet_subnet_prefixes += (subnet.get(\"addressPrefixes\") if not subnet.get(\"addressPrefix\") else [subnet.get(\"addressPrefix\")])\n        if subnet_address_pref in vnet_subnet_prefixes:\n            raise ValidationError(f\"The Subnet (default) prefix {subnet_address_pref} is already taken by another Subnet in the Vnet. Please provide a different prefix for --subnet-prefix parameter\")\n\n        user_confirmation(\"Do you want to create a new Subnet {0} in resource group {1}\".format(subnet_name, resource_group), yes=yes)\n        logger.warning('Creating new Subnet \"%s\" in resource group \"%s\"', subnet_name, resource_group)\n        poller = SubnetCreate(cli_ctx=cmd.cli_ctx)(command_args={\n            \"name\": subnet_name,\n            \"vnet_name\": vnet_name,\n            \"subscription\": nw_subscription,\n            \"resource_group\": resource_group,\n            \"address_prefix\": subnet_address_pref,\n            \"delegated_services\": [delegation]\n        })\n        subnet = LongRunningOperation(cmd.cli_ctx)(poller)\n    # subnet exist\n    else:\n        subnet = SubnetShow(cli_ctx=cmd.cli_ctx)(command_args={\n            \"name\": subnet_name,\n            \"vnet_name\": vnet_name,\n            \"subscription\": nw_subscription,\n            \"resource_group\": resource_group\n        })\n        subnet_address_prefixes = [DEFAULT_SUBNET_ADDRESS_PREFIX] + subnet.get(\"addressPrefixes\") if not subnet.get(\"addressPrefix\") else [subnet.get(\"addressPrefix\")]\n        logger.warning('Using existing Subnet \"%s\" in resource group \"%s\"', subnet_name, resource_group)\n        if subnet_address_pref not in subnet_address_prefixes:\n            logger.warning(\"The prefix of the subnet you provided does not match the --subnet-prefix value %s. Using current prefix %s\", subnet_address_pref, subnet_address_prefixes)\n\n        # Add Delegation if not delegated already\n        if not subnet.get(\"delegations\", None):\n            logger.warning('Adding \"%s\" delegation to the existing subnet %s.', delegation_service_name, subnet_name)\n            poller = SubnetUpdate(cli_ctx=cmd.cli_ctx)(command_args={\n                \"name\": subnet_name,\n                \"vnet_name\": vnet_name,\n                \"subscription\": nw_subscription,\n                \"resource_group\": resource_group,\n                \"delegated_services\": [delegation]\n            })\n            subnet = LongRunningOperation(cmd.cli_ctx)(poller)\n        else:\n            for delgtn in subnet[\"delegations\"]:\n                if delgtn[\"serviceName\"] != delegation_service_name:\n                    raise CLIError(\"Can not use subnet with existing delegations other than {}\".format(\n                        delegation_service_name))\n\n    return subnet"
    }
  },
  {
    "pyfile": "custom.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/azure_cli-2.71.0/azure_cli-2.71.0/azure/cli/command_modules/resource/custom.py",
    "line_number": "348",
    "type_description": "B818:urlopen",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "347\t    try:\n348\t        req = urlopen(url, context=_ssl_context())\n349\t        return req.read()",
    "code_snippet": "def _urlretrieve(url):\n    try:\n        req = urlopen(url, context=_ssl_context())\n        return req.read()\n    except Exception:  # pylint: disable=broad-except\n        raise CLIError('Unable to retrieve url {}'.format(url))\n\ndef _ssl_context():\n    return ssl.create_default_context()",
    "pattern_analysis": {
      "api_sequence": [
        "ssl.create_default_context",
        "urlopen",
        "req.read"
      ],
      "api_sequence_with_args": [
        "ssl.create_default_context()",
        "urlopen(url, context=ssl.create_default_context())",
        "req.read()"
      ],
      "mapped_sequence": [
        {
          "api_name": "ssl.create_default_context",
          "id": "create_ssl_context",
          "description": "Creates SSL context with specified protocol",
          "first_id": "persistence_stealth",
          "second_id": "stealth_techniques",
          "third_id": "warning_disabling"
        },
        {
          "api_name": "urlopen",
          "id": "open_url_get",
          "description": "Opens URL with GET parameters",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "request_sending"
        },
        {
          "api_name": "req.read",
          "id": "read_response_body",
          "description": "Reads response body from HTTP response",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "response_processing"
        }
      ],
      "contextual_code": "def _urlretrieve(url):\n    try:\n        req = urlopen(url, context=_ssl_context())\n        return req.read()\n    except Exception:  # pylint: disable=broad-except\n        raise CLIError('Unable to retrieve url {}'.format(url))\n\ndef _ssl_context():\n    return ssl.create_default_context()"
    }
  },
  {
    "pyfile": "_actions.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/azure_cli-2.71.0/azure_cli-2.71.0/azure/cli/command_modules/vm/_actions.py",
    "line_number": "266",
    "type_description": "B840:executor",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "265\t    if publisher_num > 1:\n266\t        with ThreadPoolExecutor(max_workers=_get_thread_count()) as executor:\n267\t            tasks = [executor.submit(_load_extension_images_from_publisher,",
    "code_snippet": "def load_extension_images_thru_services(cli_ctx, publisher, name, version, location,\n                                    show_latest=False, partial_match=True):\n    from concurrent.futures import ThreadPoolExecutor, as_completed\n    from packaging.version import parse  # pylint: disable=no-name-in-module,import-error\n    all_images = []\n    client = _compute_client_factory(cli_ctx)\n    if location is None:\n        location = get_one_of_subscription_locations(cli_ctx)\n\n    def _load_extension_images_from_publisher(publisher):\n        from azure.core.exceptions import ResourceNotFoundError\n        try:\n            types = client.virtual_machine_extension_images.list_types(location, publisher)\n        except ResourceNotFoundError as e:\n            # PIR image publishers might not have any extension images, exception could raise\n            logger.warning(str(e))\n            types = []\n        if name:\n            types = [t for t in types if _matched(name, t.name, partial_match)]\n        for t in types:\n            try:\n                versions = client.virtual_machine_extension_images.list_versions(\n                    location, publisher, t.name)\n            except ResourceNotFoundError as e:\n                logger.warning(str(e))\n                continue\n            if version:\n                versions = [v for v in versions if _matched(version, v.name, partial_match)]\n\n            if show_latest:\n                # pylint: disable=no-member\n                versions.sort(key=lambda v: parse(v.name), reverse=True)\n                try:\n                    all_images.append({\n                        'publisher': publisher,\n                        'name': t.name,\n                        'version': versions[0].name})\n                except IndexError:\n                    pass  # if no versions for this type continue to next type.\n            else:\n                for v in versions:\n                    all_images.append({\n                        'publisher': publisher,\n                        'name': t.name,\n                        'version': v.name})\n\n    publishers = client.virtual_machine_images.list_publishers(location=location)\n    if publisher:\n        publishers = [p for p in publishers if _matched(publisher, p.name, partial_match)]\n\n    publisher_num = len(publishers)\n    if publisher_num > 1:\n        with ThreadPoolExecutor(max_workers=_get_thread_count()) as executor:\n            tasks = [executor.submit(_load_extension_images_from_publisher,\n                                     p.name) for p in publishers]\n            for t in as_completed(tasks):\n                t.result()  # don't use the result but expose exceptions from the threads\n    elif publisher_num == 1:\n        _load_extension_images_from_publisher(publishers[0].name)\n\n    return all_images",
    "pattern_analysis": {
      "api_sequence": [
        "client.virtual_machine_images.list_publishers",
        "_matched",
        "len",
        "ThreadPoolExecutor",
        "_get_thread_count",
        "executor.submit",
        "as_completed",
        "t.result",
        "_load_extension_images_from_publisher",
        "client.virtual_machine_extension_images.list_types",
        "logger.warning",
        "_matched",
        "client.virtual_machine_extension_images.list_versions",
        "logger.warning",
        "_matched",
        "parse",
        "all_images.append"
      ],
      "api_sequence_with_args": [
        "client.virtual_machine_images.list_publishers(location=location)",
        "_matched(publisher, p.name, partial_match)",
        "len(publishers)",
        "ThreadPoolExecutor(max_workers=_get_thread_count())",
        "_get_thread_count()",
        "executor.submit(_load_extension_images_from_publisher, p.name)",
        "as_completed(tasks)",
        "t.result()",
        "_load_extension_images_from_publisher(publishers[0].name)",
        "client.virtual_machine_extension_images.list_types(location, publisher)",
        "logger.warning(str(e))",
        "_matched(name, t.name, partial_match)",
        "client.virtual_machine_extension_images.list_versions(location, publisher, t.name)",
        "logger.warning(str(e))",
        "_matched(version, v.name, partial_match)",
        "parse(v.name)",
        "all_images.append({'publisher': publisher, 'name': t.name, 'version': versions[0].name})"
      ],
      "mapped_sequence": [
        {
          "api_name": "client.virtual_machine_images.list_publishers",
          "id": "list_files_directories",
          "description": "Lists files and directories in specified path",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "directory_operations"
        },
        {
          "api_name": "_matched",
          "id": "compile_regex",
          "description": "Compiles regular expression pattern",
          "first_id": "code_execution",
          "second_id": "code_evaluation_execution",
          "third_id": "code_compilation"
        },
        {
          "api_name": "len",
          "id": "get_global_symbols",
          "description": "Retrieves global symbol table as dictionary",
          "first_id": "information_gathering",
          "second_id": "system_information_collection",
          "third_id": "user_information"
        },
        {
          "api_name": "ThreadPoolExecutor",
          "id": "init_thread_pool",
          "description": "Initializes thread pool executor with specified worker count",
          "first_id": "system_operations",
          "second_id": "thread_management",
          "third_id": "thread_pool_management"
        },
        {
          "api_name": "_get_thread_count",
          "id": "get_global_symbols",
          "description": "Retrieves global symbol table as dictionary",
          "first_id": "information_gathering",
          "second_id": "system_information_collection",
          "third_id": "user_information"
        },
        {
          "api_name": "executor.submit",
          "id": "submit_thread_function",
          "description": "Submits function to thread pool for execution",
          "first_id": "system_operations",
          "second_id": "thread_management",
          "third_id": "thread_pool_management"
        },
        {
          "api_name": "as_completed",
          "id": "wait_thread",
          "description": "Waits for thread to finish execution",
          "first_id": "system_operations",
          "second_id": "thread_management",
          "third_id": "thread_control"
        },
        {
          "api_name": "t.result",
          "id": "wait_thread",
          "description": "Waits for thread to finish execution",
          "first_id": "system_operations",
          "second_id": "thread_management",
          "third_id": "thread_control"
        },
        {
          "api_name": "_load_extension_images_from_publisher",
          "id": "get_global_symbols",
          "description": "Retrieves global symbol table as dictionary",
          "first_id": "information_gathering",
          "second_id": "system_information_collection",
          "third_id": "user_information"
        },
        {
          "api_name": "client.virtual_machine_extension_images.list_types",
          "id": "list_files_directories",
          "description": "Lists files and directories in specified path",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "directory_operations"
        },
        {
          "api_name": "logger.warning",
          "id": "log_error",
          "description": "Logs error message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "_matched",
          "id": "compile_regex",
          "description": "Compiles regular expression pattern",
          "first_id": "code_execution",
          "second_id": "code_evaluation_execution",
          "third_id": "code_compilation"
        },
        {
          "api_name": "client.virtual_machine_extension_images.list_versions",
          "id": "list_files_directories",
          "description": "Lists files and directories in specified path",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "directory_operations"
        },
        {
          "api_name": "logger.warning",
          "id": "log_error",
          "description": "Logs error message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "_matched",
          "id": "compile_regex",
          "description": "Compiles regular expression pattern",
          "first_id": "code_execution",
          "second_id": "code_evaluation_execution",
          "third_id": "code_compilation"
        },
        {
          "api_name": "parse",
          "id": "parse_datetime",
          "description": "Parses string into datetime object using format",
          "first_id": "system_operations",
          "second_id": "system_environment_operations",
          "third_id": "time_operations"
        },
        {
          "api_name": "all_images.append",
          "id": "serialize_to_json",
          "description": "Serializes Python object to JSON string",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        }
      ],
      "contextual_code": "def load_extension_images_thru_services(cli_ctx, publisher, name, version, location,\n                                    show_latest=False, partial_match=True):\n    from concurrent.futures import ThreadPoolExecutor, as_completed\n    from packaging.version import parse\n    all_images = []\n    client = _compute_client_factory(cli_ctx)\n    if location is None:\n        location = get_one_of_subscription_locations(cli_ctx)\n\n    def _load_extension_images_from_publisher(publisher):\n        from azure.core.exceptions import ResourceNotFoundError\n        try:\n            types = client.virtual_machine_extension_images.list_types(location, publisher)\n        except ResourceNotFoundError as e:\n            logger.warning(str(e))\n            types = []\n        if name:\n            types = [t for t in types if _matched(name, t.name, partial_match)]\n        for t in types:\n            try:\n                versions = client.virtual_machine_extension_images.list_versions(\n                    location, publisher, t.name)\n            except ResourceNotFoundError as e:\n                logger.warning(str(e))\n                continue\n            if version:\n                versions = [v for v in versions if _matched(version, v.name, partial_match)]\n\n            if show_latest:\n                versions.sort(key=lambda v: parse(v.name), reverse=True)\n                try:\n                    all_images.append({\n                        'publisher': publisher,\n                        'name': t.name,\n                        'version': versions[0].name})\n                except IndexError:\n                    pass\n            else:\n                for v in versions:\n                    all_images.append({\n                        'publisher': publisher,\n                        'name': t.name,\n                        'version': v.name})\n\n    publishers = client.virtual_machine_images.list_publishers(location=location)\n    if publisher:\n        publishers = [p for p in publishers if _matched(publisher, p.name, partial_match)]\n\n    publisher_num = len(publishers)\n    if publisher_num > 1:\n        with ThreadPoolExecutor(max_workers=_get_thread_count()) as executor:\n            tasks = [executor.submit(_load_extension_images_from_publisher,\n                                     p.name) for p in publishers]\n            for t in as_completed(tasks):\n                t.result()\n    elif publisher_num == 1:\n        _load_extension_images_from_publisher(publishers[0].name)\n\n    return all_images"
    }
  }
]