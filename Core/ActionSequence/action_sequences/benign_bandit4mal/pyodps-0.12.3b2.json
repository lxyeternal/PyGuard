[
  {
    "metadata": {
      "package_name": "pyodps-0.12.3b2",
      "total_matches": 2,
      "processing_date": "2025-05-18 01:49:39"
    }
  },
  {
    "pyfile": "tabletunnel.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/pyodps-0.12.3b2/pyodps-0.12.3b2/odps/tunnel/tabletunnel.py",
    "line_number": "243",
    "type_description": "B821:post",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "242\t        try:\n243\t            resp = self._client.post(\n244\t                url, {}, params=params, headers=headers, timeout=timeout\n245\t            )",
    "code_snippet": "def _init(self, async_mode, timeout):\n    params = self.get_common_params(downloads=\"\")\n    headers = self.get_common_headers(content_length=0, tags=self._tags)\n    if async_mode:\n        params[\"asyncmode\"] = \"true\"\n\n    url = self._table.table_resource()\n    ts = monotonic()\n    try:\n        resp = self._client.post(\n            url, {}, params=params, headers=headers, timeout=timeout\n        )\n    except requests.exceptions.ReadTimeout:\n        if callable(options.tunnel_session_create_timeout_callback):\n            options.tunnel_session_create_timeout_callback(*sys.exc_info())\n        raise\n    self.check_tunnel_response(resp)\n\n    delay_time = 0.1\n    self.parse(resp, obj=self)\n    while self.status == self.Status.Initiating:\n        if timeout and monotonic() - ts > timeout:\n            try:\n                raise TunnelReadTimeout(\n                    \"Waiting for tunnel ready timed out. id=%s, table=%s\"\n                    % (self.id, self._table.name)\n                )\n            except TunnelReadTimeout:\n                if callable(options.tunnel_session_create_timeout_callback):\n                    options.tunnel_session_create_timeout_callback(*sys.exc_info())\n                raise\n        time.sleep(delay_time)\n        delay_time = min(delay_time * 2, 5)\n        self.reload()\n    if self.schema is not None:\n        self.schema.build_snapshot()",
    "pattern_analysis": {
      "api_sequence": [
        "self.get_common_params",
        "self.get_common_headers",
        "self._table.table_resource",
        "monotonic",
        "self._client.post",
        "options.tunnel_session_create_timeout_callback",
        "self.check_tunnel_response",
        "self.parse",
        "monotonic",
        "time.sleep",
        "self.reload",
        "self.schema.build_snapshot"
      ],
      "api_sequence_with_args": [
        "self.get_common_params(downloads=\"\")",
        "self.get_common_headers(content_length=0, tags=self._tags)",
        "self._table.table_resource()",
        "monotonic()",
        "self._client.post(url, {}, params=params, headers=headers, timeout=timeout)",
        "options.tunnel_session_create_timeout_callback(*sys.exc_info())",
        "self.check_tunnel_response(resp)",
        "self.parse(resp, obj=self)",
        "monotonic()",
        "time.sleep(delay_time)",
        "self.reload()",
        "self.schema.build_snapshot()"
      ],
      "mapped_sequence": [
        {
          "api_name": "monotonic",
          "id": "get_current_time",
          "description": "Returns current time in seconds since epoch",
          "first_id": "system_operations",
          "second_id": "system_environment_operations",
          "third_id": "time_operations"
        },
        {
          "api_name": "self._client.post",
          "id": "send_http_post_timeout",
          "description": "Sends HTTP POST request with data and timeout",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "request_sending"
        },
        {
          "api_name": "options.tunnel_session_create_timeout_callback",
          "id": "log_error",
          "description": "Logs error message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "time.sleep",
          "id": "suspend_execution",
          "description": "Suspends execution for specified seconds",
          "first_id": "system_operations",
          "second_id": "system_environment_operations",
          "third_id": "time_operations"
        }
      ],
      "contextual_code": "def _init(self, async_mode, timeout):\n    params = self.get_common_params(downloads=\"\")\n    headers = self.get_common_headers(content_length=0, tags=self._tags)\n    if async_mode:\n        params[\"asyncmode\"] = \"true\"\n\n    url = self._table.table_resource()\n    ts = monotonic()\n    try:\n        resp = self._client.post(\n            url, {}, params=params, headers=headers, timeout=timeout\n        )\n    except requests.exceptions.ReadTimeout:\n        if callable(options.tunnel_session_create_timeout_callback):\n            options.tunnel_session_create_timeout_callback(*sys.exc_info())\n        raise\n    self.check_tunnel_response(resp)\n\n    delay_time = 0.1\n    self.parse(resp, obj=self)\n    while self.status == self.Status.Initiating:\n        if timeout and monotonic() - ts > timeout:\n            try:\n                raise TunnelReadTimeout(\n                    \"Waiting for tunnel ready timed out. id=%s, table=%s\"\n                    % (self.id, self._table.name)\n                )\n            except TunnelReadTimeout:\n                if callable(options.tunnel_session_create_timeout_callback):\n                    options.tunnel_session_create_timeout_callback(*sys.exc_info())\n                raise\n        time.sleep(delay_time)\n        delay_time = min(delay_time * 2, 5)\n        self.reload()\n    if self.schema is not None:\n        self.schema.build_snapshot()"
    }
  },
  {
    "pyfile": "tables.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/pyodps-0.12.3b2/pyodps-0.12.3b2/odps/examples/tables.py",
    "line_number": "628",
    "type_description": "B815:write",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "627\t    with open(cache_path, \"wb\") as f:\n628\t        f.write(compressed_content)\n629",
    "code_snippet": "def cache_newsgroup_tar(target_tar, target_dir, cache_dir):\n    tarfile.open(target_tar, \"r:gz\").extractall(path=target_dir)\n    os.unlink(target_tar)\n\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    train_path = os.path.join(target_dir, NEWSGROUP_TRAIN_FOLDER)\n    test_path = os.path.join(target_dir, NEWSGROUP_TEST_FOLDER)\n    cache_path = os.path.join(cache_dir, NEWSGROUP_CACHE_NAME)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    def load_files(path, encoding):\n        objs = []\n        for fn in glob.glob(os.path.join(path, \"*\")):\n            file_cat = os.path.basename(os.path.normpath(fn))\n            for sfn in glob.glob(os.path.join(fn, \"*\")):\n                file_id = os.path.basename(os.path.normpath(sfn))\n                with open(sfn, \"rb\") as f:\n                    objs.append((file_id, file_cat, f.read().decode(encoding)))\n\n        return objs\n\n    # Store a zipped pickle\n    cache = dict(\n        train=load_files(train_path, encoding=\"latin1\"),\n        test=load_files(test_path, encoding=\"latin1\"),\n    )\n    compressed_content = codecs.encode(pickle.dumps(cache), \"zlib_codec\")\n    with open(cache_path, \"wb\") as f:\n        f.write(compressed_content)\n\n    shutil.rmtree(target_dir)",
    "pattern_analysis": {
      "api_sequence": [
        "tarfile.open",
        "tarfile.TarFile.extractall",
        "os.unlink",
        "os.path.exists",
        "os.makedirs",
        "os.path.join",
        "os.path.join",
        "os.path.join",
        "os.path.exists",
        "os.makedirs",
        "glob.glob",
        "os.path.join",
        "os.path.basename",
        "os.path.normpath",
        "glob.glob",
        "os.path.join",
        "os.path.basename",
        "os.path.normpath",
        "open",
        "builtins.open.read",
        "codecs.encode",
        "pickle.dumps",
        "open",
        "builtins.open.write",
        "shutil.rmtree"
      ],
      "api_sequence_with_args": [
        "tarfile.open(target_tar, \"r:gz\")",
        "tarfile.TarFile.extractall(path=target_dir)",
        "os.unlink(target_tar)",
        "os.path.exists(target_dir)",
        "os.makedirs(target_dir)",
        "os.path.join(target_dir, NEWSGROUP_TRAIN_FOLDER)",
        "os.path.join(target_dir, NEWSGROUP_TEST_FOLDER)",
        "os.path.join(cache_dir, NEWSGROUP_CACHE_NAME)",
        "os.path.exists(cache_dir)",
        "os.makedirs(cache_dir)",
        "glob.glob(os.path.join(path, \"*\"))",
        "os.path.join(path, \"*\")",
        "os.path.basename(os.path.normpath(fn))",
        "os.path.normpath(fn)",
        "glob.glob(os.path.join(fn, \"*\"))",
        "os.path.join(fn, \"*\")",
        "os.path.basename(os.path.normpath(sfn))",
        "os.path.normpath(sfn)",
        "open(sfn, \"rb\")",
        "builtins.open.read().decode(encoding)",
        "codecs.encode(pickle.dumps(cache), \"zlib_codec\")",
        "pickle.dumps(cache)",
        "open(cache_path, \"wb\")",
        "builtins.open.write(compressed_content)",
        "shutil.rmtree(target_dir)"
      ],
      "mapped_sequence": [
        {
          "api_name": "tarfile.open",
          "id": "open_zip_read",
          "description": "Opens ZIP archive for reading",
          "first_id": "file_operations",
          "second_id": "compression_archiving",
          "third_id": "zip_operations"
        },
        {
          "api_name": "tarfile.TarFile.extractall",
          "id": "extract_zip_files",
          "description": "Extracts all files from ZIP archive to specified directory",
          "first_id": "file_operations",
          "second_id": "compression_archiving",
          "third_id": "zip_operations"
        },
        {
          "api_name": "os.unlink",
          "id": "delete_file",
          "description": "Deletes specified file from filesystem",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_transfer"
        },
        {
          "api_name": "os.path.exists",
          "id": "check_path_exists",
          "description": "Checks if specified path exists in filesystem",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_checking"
        },
        {
          "api_name": "os.makedirs",
          "id": "create_directory",
          "description": "Creates directory, ignoring if it already exists",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "directory_operations"
        },
        {
          "api_name": "os.path.join",
          "id": "path_string_operations",
          "description": "Basic path string operations (getting absolute path, base name, parent directory, splitting and joining paths)",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_path_operations"
        },
        {
          "api_name": "os.path.join",
          "id": "path_string_operations",
          "description": "Basic path string operations (getting absolute path, base name, parent directory, splitting and joining paths)",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_path_operations"
        },
        {
          "api_name": "os.path.join",
          "id": "path_string_operations",
          "description": "Basic path string operations (getting absolute path, base name, parent directory, splitting and joining paths)",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_path_operations"
        },
        {
          "api_name": "os.path.exists",
          "id": "check_path_exists",
          "description": "Checks if specified path exists in filesystem",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_checking"
        },
        {
          "api_name": "os.makedirs",
          "id": "create_directory",
          "description": "Creates directory, ignoring if it already exists",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "directory_operations"
        },
        {
          "api_name": "glob.glob",
          "id": "list_files_directories",
          "description": "Lists files and directories in specified path",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "directory_operations"
        },
        {
          "api_name": "os.path.join",
          "id": "path_string_operations",
          "description": "Basic path string operations (getting absolute path, base name, parent directory, splitting and joining paths)",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_path_operations"
        },
        {
          "api_name": "os.path.basename",
          "id": "path_string_operations",
          "description": "Basic path string operations (getting absolute path, base name, parent directory, splitting and joining paths)",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_path_operations"
        },
        {
          "api_name": "os.path.normpath",
          "id": "path_string_operations",
          "description": "Basic path string operations (getting absolute path, base name, parent directory, splitting and joining paths)",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_path_operations"
        },
        {
          "api_name": "glob.glob",
          "id": "list_files_directories",
          "description": "Lists files and directories in specified path",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "directory_operations"
        },
        {
          "api_name": "os.path.join",
          "id": "path_string_operations",
          "description": "Basic path string operations (getting absolute path, base name, parent directory, splitting and joining paths)",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_path_operations"
        },
        {
          "api_name": "os.path.basename",
          "id": "path_string_operations",
          "description": "Basic path string operations (getting absolute path, base name, parent directory, splitting and joining paths)",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_path_operations"
        },
        {
          "api_name": "os.path.normpath",
          "id": "path_string_operations",
          "description": "Basic path string operations (getting absolute path, base name, parent directory, splitting and joining paths)",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_path_operations"
        },
        {
          "api_name": "open",
          "id": "basic_read_operations",
          "description": "ic file opening operations for reading (normal reading, binary reading)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_opening"
        },
        {
          "api_name": "builtins.open.read",
          "id": "basic_file_reading",
          "description": "Reading content from files (by lines or entire content)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_reading"
        },
        {
          "api_name": "codecs.encode",
          "id": "compress_data_zlib",
          "description": "Compresses data using zlib compression",
          "first_id": "file_operations",
          "second_id": "compression_archiving",
          "third_id": "compression_decompression"
        },
        {
          "api_name": "pickle.dumps",
          "id": "serialize_to_json",
          "description": "Serializes Python object to JSON string",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "open",
          "id": "basic_write_operations",
          "description": "Basic file opening operations for writing (normal writing, binary writing)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_opening"
        },
        {
          "api_name": "builtins.open.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "shutil.rmtree",
          "id": "delete_directory",
          "description": "Recursively deletes directory and its contents",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "directory_operations"
        }
      ],
      "contextual_code": "def cache_newsgroup_tar(target_tar, target_dir, cache_dir):\n    tarfile.open(target_tar, \"r:gz\").extractall(path=target_dir)\n    os.unlink(target_tar)\n\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    train_path = os.path.join(target_dir, NEWSGROUP_TRAIN_FOLDER)\n    test_path = os.path.join(target_dir, NEWSGROUP_TEST_FOLDER)\n    cache_path = os.path.join(cache_dir, NEWSGROUP_CACHE_NAME)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    def load_files(path, encoding):\n        objs = []\n        for fn in glob.glob(os.path.join(path, \"*\")):\n            file_cat = os.path.basename(os.path.normpath(fn))\n            for sfn in glob.glob(os.path.join(fn, \"*\")):\n                file_id = os.path.basename(os.path.normpath(sfn))\n                with open(sfn, \"rb\") as f:\n                    objs.append((file_id, file_cat, f.read().decode(encoding)))\n        return objs\n\n    cache = dict(\n        train=load_files(train_path, encoding=\"latin1\"),\n        test=load_files(test_path, encoding=\"latin1\"),\n    )\n    compressed_content = codecs.encode(pickle.dumps(cache), \"zlib_codec\")\n    with open(cache_path, \"wb\") as f:\n        f.write(compressed_content)\n\n    shutil.rmtree(target_dir)"
    }
  }
]