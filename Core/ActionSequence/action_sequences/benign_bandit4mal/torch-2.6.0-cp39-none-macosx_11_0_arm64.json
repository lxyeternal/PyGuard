[
  {
    "metadata": {
      "package_name": "torch-2.6.0-cp39-none-macosx_11_0_arm64",
      "total_matches": 4,
      "processing_date": "2025-05-18 01:49:39"
    }
  },
  {
    "pyfile": "codecache.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/torch-2.6.0-cp39-none-macosx_11_0_arm64/torch/_inductor/codecache.py",
    "line_number": "1037",
    "type_description": "B801:b64decode",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "1036\t                        assert isinstance(data, (str, bytes))\n1037\t                        content = base64.b64decode(data)\n1038\t                        yield pickle.loads(content)",
    "code_snippet": "def iterate_over_candidates() -> Generator[CompiledFxGraph, None, None]:\n    if local:\n        subdir = FxGraphCache._get_tmp_dir_for_key(key)\n        if os.path.exists(subdir):\n            for path in sorted(os.listdir(subdir)):\n                try:\n                    with open(os.path.join(subdir, path), \"rb\") as f:\n                        yield pickle.load(f)\n                except Exception:\n                    log.warning(\n                        \"fx graph cache unable to load compiled graph\",\n                        exc_info=True,\n                    )\n\n    if remote_cache:\n        try:\n            if (cache_data := remote_cache.get(key)) is not None:\n                assert isinstance(cache_data, dict)\n                data = cache_data[\"data\"]\n                assert isinstance(data, (str, bytes))\n                content = base64.b64decode(data)\n                yield pickle.loads(content)\n        except Exception:\n            log.warning(\n                \"fx graph cache unable to load compiled graph\", exc_info=True\n            )",
    "pattern_analysis": {
      "api_sequence": [
        "os.path.exists",
        "os.listdir",
        "open",
        "os.path.join",
        "pickle.load",
        "remote_cache.get",
        "isinstance",
        "base64.b64decode",
        "pickle.loads",
        "log.warning"
      ],
      "api_sequence_with_args": [
        "os.path.exists(subdir)",
        "os.listdir(subdir)",
        "open(os.path.join(subdir, path), \"rb\")",
        "os.path.join(subdir, path)",
        "pickle.load(f)",
        "remote_cache.get(key)",
        "isinstance(cache_data, dict)",
        "base64.b64decode(data)",
        "pickle.loads(content)",
        "log.warning(\"fx graph cache unable to load compiled graph\", exc_info=True)"
      ],
      "mapped_sequence": [
        {
          "api_name": "os.path.exists",
          "id": "check_path_exists",
          "description": "Checks if specified path exists in filesystem",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_checking"
        },
        {
          "api_name": "os.listdir",
          "id": "list_files_directories",
          "description": "Lists files and directories in specified path",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "directory_operations"
        },
        {
          "api_name": "open",
          "id": "basic_read_operations",
          "description": "ic file opening operations for reading (normal reading, binary reading)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_opening"
        },
        {
          "api_name": "os.path.join",
          "id": "path_string_operations",
          "description": "Basic path string operations (getting absolute path, base name, parent directory, splitting and joining paths)",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_path_operations"
        },
        {
          "api_name": "pickle.load",
          "id": "deserialize_from_bytes",
          "description": "Deserializes Python object from bytes",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "remote_cache.get",
          "id": "get_registry_value",
          "description": "Retrieves value and data type for specified registry value",
          "first_id": "system_operations",
          "second_id": "registry_operations",
          "third_id": "registry_modification"
        },
        {
          "api_name": "isinstance",
          "id": "check_instance_type",
          "description": "Checks if object is instance of specified type",
          "first_id": "utility_functions",
          "second_id": "logging_exception_handling",
          "third_id": "exception_checking"
        },
        {
          "api_name": "base64.b64decode",
          "id": "decode_base64_to_bytes",
          "description": "Decodes base64-encoded string to bytes",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "base_encoding"
        },
        {
          "api_name": "pickle.loads",
          "id": "deserialize_from_bytes",
          "description": "Deserializes Python object from bytes",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "log.warning",
          "id": "log_error",
          "description": "Logs error message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        }
      ],
      "contextual_code": "if local:\n    subdir = FxGraphCache._get_tmp_dir_for_key(key)\n    if os.path.exists(subdir):\n        for path in sorted(os.listdir(subdir)):\n            try:\n                with open(os.path.join(subdir, path), \"rb\") as f:\n                    yield pickle.load(f)\n            except Exception:\n                log.warning(\n                    \"fx graph cache unable to load compiled graph\",\n                    exc_info=True,\n                )\n\nif remote_cache:\n    try:\n        if (cache_data := remote_cache.get(key)) is not None:\n            assert isinstance(cache_data, dict)\n            data = cache_data[\"data\"]\n            assert isinstance(data, (str, bytes))\n            content = base64.b64decode(data)\n            yield pickle.loads(content)\n    except Exception:\n        log.warning(\n            \"fx graph cache unable to load compiled graph\", exc_info=True\n        )"
    }
  },
  {
    "pyfile": "dataloader.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/torch-2.6.0-cp39-none-macosx_11_0_arm64/torch/utils/data/dataloader.py",
    "line_number": "1584",
    "type_description": "B814:read",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "1583\t                for q in self._index_queues:\n1584\t                    q.cancel_join_thread()\n1585\t                    q.close()",
    "code_snippet": "def _shutdown_workers(self):\n    # Called when shutting down this `_MultiProcessingDataLoaderIter`.\n    # See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for details on\n    # the logic of this function.\n    if (\n        _utils is None\n        or _utils.python_exit_status is True\n        or _utils.python_exit_status is None\n    ):\n        # See (2) of the note. If Python is shutting down, do no-op.\n        return\n    # Normal exit when last reference is gone / iterator is depleted.\n    # See (1) and the second half of the note.\n    if not self._shutdown:\n        self._shutdown = True\n        try:\n            # Normal exit when last reference is gone / iterator is depleted.\n            # See (1) and the second half of the note.\n\n            # Exit `pin_memory_thread` first because exiting workers may leave\n            # corrupted data in `worker_result_queue` which `pin_memory_thread`\n            # reads from.\n            if hasattr(self, \"_pin_memory_thread\"):\n                # Use hasattr in case error happens before we set the attribute.\n                self._pin_memory_thread_done_event.set()\n                # Send something to pin_memory_thread in case it is waiting\n                # so that it can wake up and check `pin_memory_thread_done_event`\n                self._worker_result_queue.put((None, None))\n                self._pin_memory_thread.join()\n                self._worker_result_queue.cancel_join_thread()\n                self._worker_result_queue.close()\n\n            # Exit workers now.\n            self._workers_done_event.set()\n            for worker_id in range(len(self._workers)):\n                # Get number of workers from `len(self._workers)` instead of\n                # `self._num_workers` in case we error before starting all\n                # workers.\n                # If we are using workers_status with persistent_workers\n                # we have to shut it down because the worker is paused\n                if self._persistent_workers or self._workers_status[worker_id]:\n                    self._mark_worker_as_unavailable(worker_id, shutdown=True)\n            for w in self._workers:\n                # We should be able to join here, but in case anything went\n                # wrong, we set a timeout and if the workers fail to join,\n                # they are killed in the `finally` block.\n                w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n            for q in self._index_queues:\n                q.cancel_join_thread()\n                q.close()\n        finally:\n            # Even though all this function does is putting into queues that\n            # we have called `cancel_join_thread` on, weird things can\n            # happen when a worker is killed by a signal, e.g., hanging in\n            # `Event.set()`. So we need to guard this with SIGCHLD handler,\n            # and remove pids from the C side data structure only at the\n            # end.\n            #\n            # FIXME: Unfortunately, for Windows, we are missing a worker\n            #        error detection mechanism here in this function, as it\n            #        doesn't provide a SIGCHLD handler.\n            if self._worker_pids_set:\n                _utils.signal_handling._remove_worker_pids(id(self))\n                self._worker_pids_set = False\n            for w in self._workers:\n                if w.is_alive():\n                    # Existing mechanisms try to make the workers exit\n                    # peacefully, but in case that we unfortunately reach\n                    # here, which we shouldn't, (e.g., pytorch/pytorch#39570),\n                    # we kill the worker.\n                    w.terminate()",
    "pattern_analysis": {
      "api_sequence": [
        "hasattr",
        "self._pin_memory_thread_done_event.set",
        "self._worker_result_queue.put",
        "self._pin_memory_thread.join",
        "self._worker_result_queue.cancel_join_thread",
        "self._worker_result_queue.close",
        "self._workers_done_event.set",
        "self._mark_worker_as_unavailable",
        "w.join",
        "q.cancel_join_thread",
        "q.close",
        "_utils.signal_handling._remove_worker_pids",
        "w.is_alive",
        "w.terminate"
      ],
      "api_sequence_with_args": [
        "hasattr(self, \"_pin_memory_thread\")",
        "self._pin_memory_thread_done_event.set()",
        "self._worker_result_queue.put((None, None))",
        "self._pin_memory_thread.join()",
        "self._worker_result_queue.cancel_join_thread()",
        "self._worker_result_queue.close()",
        "self._workers_done_event.set()",
        "self._mark_worker_as_unavailable(worker_id, shutdown=True)",
        "w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)",
        "q.cancel_join_thread()",
        "q.close()",
        "_utils.signal_handling._remove_worker_pids(id(self))",
        "w.is_alive()",
        "w.terminate()"
      ],
      "mapped_sequence": [
        {
          "api_name": "hasattr",
          "id": "check_instance_type",
          "description": "Checks if object is instance of specified type",
          "first_id": "utility_functions",
          "second_id": "logging_exception_handling",
          "third_id": "exception_checking"
        },
        {
          "api_name": "self._pin_memory_thread_done_event.set",
          "id": "create_thread",
          "description": "Creates new thread to execute target function",
          "first_id": "system_operations",
          "second_id": "thread_management",
          "third_id": "thread_creation"
        },
        {
          "api_name": "self._worker_result_queue.put",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "self._pin_memory_thread.join",
          "id": "wait_thread",
          "description": "Waits for thread to finish execution",
          "first_id": "system_operations",
          "second_id": "thread_management",
          "third_id": "thread_control"
        },
        {
          "api_name": "self._worker_result_queue.cancel_join_thread",
          "id": "close_file",
          "description": "Closes the opened file",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_closing"
        },
        {
          "api_name": "self._worker_result_queue.close",
          "id": "close_file",
          "description": "Closes the opened file",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_closing"
        },
        {
          "api_name": "self._workers_done_event.set",
          "id": "create_thread",
          "description": "Creates new thread to execute target function",
          "first_id": "system_operations",
          "second_id": "thread_management",
          "third_id": "thread_creation"
        },
        {
          "api_name": "self._mark_worker_as_unavailable",
          "id": "terminate_process",
          "description": "Terminates the process",
          "first_id": "system_operations",
          "second_id": "process_management",
          "third_id": "process_control"
        },
        {
          "api_name": "w.join",
          "id": "wait_thread",
          "description": "Waits for thread to finish execution",
          "first_id": "system_operations",
          "second_id": "thread_management",
          "third_id": "thread_control"
        },
        {
          "api_name": "q.cancel_join_thread",
          "id": "close_file",
          "description": "Closes the opened file",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_closing"
        },
        {
          "api_name": "q.close",
          "id": "close_file",
          "description": "Closes the opened file",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_closing"
        },
        {
          "api_name": "_utils.signal_handling._remove_worker_pids",
          "id": "terminate_process",
          "description": "Terminates the process",
          "first_id": "system_operations",
          "second_id": "process_management",
          "third_id": "process_control"
        },
        {
          "api_name": "w.is_alive",
          "id": "check_process_terminated",
          "description": "Checks if process has terminated",
          "first_id": "system_operations",
          "second_id": "process_management",
          "third_id": "process_control"
        },
        {
          "api_name": "w.terminate",
          "id": "terminate_process",
          "description": "Terminates the process",
          "first_id": "system_operations",
          "second_id": "process_management",
          "third_id": "process_control"
        }
      ],
      "contextual_code": "def _shutdown_workers(self):\n    if (\n        _utils is None\n        or _utils.python_exit_status is True\n        or _utils.python_exit_status is None\n    ):\n        return\n    if not self._shutdown:\n        self._shutdown = True\n        try:\n            if hasattr(self, \"_pin_memory_thread\"):\n                self._pin_memory_thread_done_event.set()\n                self._worker_result_queue.put((None, None))\n                self._pin_memory_thread.join()\n                self._worker_result_queue.cancel_join_thread()\n                self._worker_result_queue.close()\n            self._workers_done_event.set()\n            for worker_id in range(len(self._workers)):\n                if self._persistent_workers or self._workers_status[worker_id]:\n                    self._mark_worker_as_unavailable(worker_id, shutdown=True)\n            for w in self._workers:\n                w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n            for q in self._index_queues:\n                q.cancel_join_thread()\n                q.close()\n        finally:\n            if self._worker_pids_set:\n                _utils.signal_handling._remove_worker_pids(id(self))\n                self._worker_pids_set = False\n            for w in self._workers:\n                if w.is_alive():\n                    w.terminate()"
    }
  },
  {
    "pyfile": "_reporting.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/torch-2.6.0-cp39-none-macosx_11_0_arm64/torch/onnx/_internal/exporter/_reporting.py",
    "line_number": "183",
    "type_description": "B815:write",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "182\t            f.write(\"```python\\n\")\n183\t            f.write(str(model))\n184\t            f.write(\"\\n```\\n\\n\")",
    "code_snippet": "def create_onnx_export_report(\n    filename: str | os.PathLike,\n    formatted_traceback: str,\n    program: torch.export.ExportedProgram,\n    *,\n    decomp_comparison: str | None = None,\n    export_status: ExportStatus,\n    profile_result: str | None,\n    model: ir.Model | None = None,\n    registry: _registration.ONNXRegistry | None = None,\n    verification_result: str | None = None,\n):\n    with open(filename, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"# PyTorch ONNX Conversion Report\\n\\n\")\n        f.write(_format_export_status(export_status))\n        f.write(\"## Error messages\\n\\n\")\n        f.write(\"```pytb\\n\")\n        f.write(formatted_traceback)\n        f.write(\"\\n```\\n\\n\")\n        f.write(\"## Exported program\\n\\n\")\n        f.write(_format_exported_program(program))\n        if model is not None:\n            f.write(\"## ONNX model\\n\\n\")\n            f.write(\"```python\\n\")\n            f.write(str(model))\n            f.write(\"\\n```\\n\\n\")\n        f.write(\"## Analysis\\n\\n\")\n        _analysis.analyze(program, file=f, registry=registry)\n        if decomp_comparison is not None:\n            f.write(\"\\n## Decomposition comparison\\n\\n\")\n            f.write(decomp_comparison)\n            f.write(\"\\n\")\n        if verification_result is not None:\n            f.write(\"\\n## Verification results\\n\\n\")\n            f.write(verification_result)\n            f.write(\"\\n\")\n        if profile_result is not None:\n            f.write(\"\\n## Profiling result\\n\\n\")\n            f.write(\"```\\n\")\n            f.write(profile_result)\n            f.write(\"```\\n\")",
    "pattern_analysis": {
      "api_sequence": [
        "open",
        "file.write",
        "file.write",
        "file.write",
        "file.write",
        "file.write",
        "file.write",
        "file.write",
        "file.write",
        "file.write",
        "file.write",
        "file.write",
        "file.write",
        "file.write",
        "file.write",
        "file.write",
        "_analysis.analyze"
      ],
      "api_sequence_with_args": [
        "open(filename, \"w\", encoding=\"utf-8\")",
        "file.write(\"# PyTorch ONNX Conversion Report\\n\\n\")",
        "file.write(_format_export_status(export_status))",
        "file.write(\"## Error messages\\n\\n\")",
        "file.write(\"```pytb\\n\")",
        "file.write(formatted_traceback)",
        "file.write(\"\\n```\n\n\")",
        "file.write(\"## Exported program\\n\\n\")",
        "file.write(_format_exported_program(program))",
        "file.write(\"## ONNX model\\n\\n\") [conditional]",
        "file.write(\"```python\\n\") [conditional]",
        "file.write(str(model)) [conditional]",
        "file.write(\"\\n```\n\n\") [conditional]",
        "file.write(\"## Analysis\\n\\n\")",
        "_analysis.analyze(program, file=file, registry=registry)"
      ],
      "mapped_sequence": [
        {
          "api_name": "open",
          "id": "basic_write_operations",
          "description": "Basic file opening operations for writing (normal writing, binary writing)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_opening"
        },
        {
          "api_name": "file.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "file.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "file.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "file.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "file.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "file.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "file.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "file.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "file.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "file.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "file.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "file.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "file.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "_analysis.analyze",
          "id": "exfiltrate_folder",
          "description": "Recursively processes folder for file exfiltration",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_transfer"
        }
      ],
      "contextual_code": "def create_onnx_export_report(\n    filename: str | os.PathLike,\n    formatted_traceback: str,\n    program: torch.export.ExportedProgram,\n    *,\n    decomp_comparison: str | None = None,\n    export_status: ExportStatus,\n    profile_result: str | None,\n    model: ir.Model | None = None,\n    registry: _registration.ONNXRegistry | None = None,\n    verification_result: str | None = None,\n):\n    with open(filename, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"# PyTorch ONNX Conversion Report\\n\\n\")\n        f.write(_format_export_status(export_status))\n        f.write(\"## Error messages\\n\\n\")\n        f.write(\"```pytb\\n\")\n        f.write(formatted_traceback)\n        f.write(\"\\n```\n\n\")\n        f.write(\"## Exported program\\n\\n\")\n        f.write(_format_exported_program(program))\n        if model is not None:\n            f.write(\"## ONNX model\\n\\n\")\n            f.write(\"```python\\n\")\n            f.write(str(model))\n            f.write(\"\\n```\n\n\")\n        f.write(\"## Analysis\\n\\n\")\n        _analysis.analyze(program, file=f, registry=registry)\n        if decomp_comparison is not None:\n            f.write(\"\\n## Decomposition comparison\\n\\n\")\n            f.write(decomp_comparison)\n            f.write(\"\\n\")\n        if verification_result is not None:\n            f.write(\"\\n## Verification results\\n\\n\")\n            f.write(verification_result)\n            f.write(\"\\n\")\n        if profile_result is not None:\n            f.write(\"\\n## Profiling result\\n\\n\")\n            f.write(\"```\n\")\n            f.write(profile_result)\n            f.write(\"```\n\")"
    }
  },
  {
    "pyfile": "gen.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/torch-2.6.0-cp39-none-macosx_11_0_arm64/torchgen/gen.py",
    "line_number": "2197",
    "type_description": "B815:write",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "2196\n2197\t    core_fm.write(\"aten_interned_strings.h\", gen_aten_interned_strings)\n2198",
    "code_snippet": "def gen_headers(\n    *,\n    native_functions: Sequence[NativeFunction],\n    valid_tags: set[str],\n    grouped_native_functions: Sequence[NativeFunction | NativeFunctionsGroup],\n    structured_native_functions: Sequence[NativeFunctionsGroup],\n    static_dispatch_idx: list[BackendIndex],\n    selector: SelectiveBuilder,\n    backend_indices: dict[DispatchKey, BackendIndex],\n    core_fm: FileManager,\n    cpu_fm: FileManager,\n    device_fms: dict[str, FileManager],\n    ops_fm: FileManager,\n    dispatch_keys: Sequence[DispatchKey],\n    functions_keys: set[DispatchKey],\n    rocm: bool,\n    per_operator_headers: bool,\n) -> None:\n    if per_operator_headers:\n        gen_per_operator_headers(\n            native_functions=native_functions,\n            grouped_native_functions=grouped_native_functions,\n            static_dispatch_idx=static_dispatch_idx,\n            selector=selector,\n            backend_indices=backend_indices,\n            cpu_fm=cpu_fm,\n            device_fms=device_fms,\n            ops_fm=ops_fm,\n            dispatch_keys=dispatch_keys,\n            functions_keys=functions_keys,\n            rocm=rocm,\n        )\n    else:\n        gen_aggregated_headers(\n            native_functions=native_functions,\n            grouped_native_functions=grouped_native_functions,\n            structured_native_functions=structured_native_functions,\n            static_dispatch_idx=static_dispatch_idx,\n            selector=selector,\n            backend_indices=backend_indices,\n            cpu_fm=cpu_fm,\n            device_fms=device_fms,\n            dispatch_keys=dispatch_keys,\n            functions_keys=functions_keys,\n            rocm=rocm,\n        )\n\n    core_fm.write(\n        \"TensorBody.h\",\n        lambda: {\n            \"tensor_method_declarations\": list(\n                mapMaybe(\n                    ComputeTensorMethod(\n                        target=Target.DECLARATION,\n                        static_dispatch_backend_indices=static_dispatch_idx,\n                    ),\n                    native_functions,\n                )\n            ),\n            \"tensor_method_definitions\": list(\n                mapMaybe(\n                    ComputeTensorMethod(\n                        target=Target.DEFINITION,\n                        static_dispatch_backend_indices=static_dispatch_idx,\n                    ),\n                    native_functions,\n                )\n            ),\n        },\n    )\n\n    cpu_fm.write(\n        \"RedispatchFunctions.h\",\n        lambda: {\n            \"function_redispatch_definitions\": list(\n                mapMaybe(ComputeRedispatchFunction(), native_functions)\n            ),\n        },\n    )\n\n    cpu_fm.write(\n        \"RegistrationDeclarations.h\",\n        lambda: {\n            \"registration_declarations\": [\n                compute_registration_declarations(f, backend_indices)\n                for f in native_functions\n            ],\n        },\n    )\n\n    cpu_fm.write(\n        \"VmapGeneratedPlumbing.h\", lambda: gen_all_vmap_plumbing(native_functions)\n    )\n\n    def gen_aten_interned_strings() -> dict[str, str]:\n        attrs: set[str] = set()  # All function argument names\n        names = set()  # All ATen function names\n        for func in native_functions:\n            names.add(str(func.func.name.name))\n            # Some operators don't have a functional variant but we still create a\n            # symbol without the underscore\n            names.add(func.func.name.name.base)\n\n            attrs.update(arg.name for arg in func.func.schema_order_arguments())\n\n        # These are keywords in C++, so aren't valid symbol names\n        # https://en.cppreference.com/w/cpp/language/operator_alternative\n        names -= {\n            \"and\",\n            \"and_eq\",\n            \"bitand\",\n            \"bitor\",\n            \"compl\",\n            \"not\",\n            \"not_eq\",\n            \"or\",\n            \"or_eq\",\n            \"xor\",\n            \"xor_eq\",\n        }\n\n        return {\n            \"aten_symbols\": \" \\\\\\n\".join(\n                [f\"_(aten, {name})\" for name in sorted(names)]\n            ),\n            \"attr_symbols\": \" \\\\\\n\".join(\n                [f\"_(attr, {name})\" for name in sorted(attrs)]\n            ),\n        }\n\n    core_fm.write(\"aten_interned_strings.h\", gen_aten_interned_strings)\n\n    def gen_tags_enum() -> dict[str, str]:\n        return {\"enum_of_valid_tags\": (\",\\n\".join(sorted(valid_tags)))}\n\n    core_fm.write(\"enum_tag.h\", gen_tags_enum)\n",
    "pattern_analysis": {
      "api_sequence": [
        "core_fm.write",
        "cpu_fm.write",
        "cpu_fm.write",
        "cpu_fm.write",
        "core_fm.write",
        "core_fm.write"
      ],
      "api_sequence_with_args": [
        "core_fm.write(\"TensorBody.h\", ...)",
        "cpu_fm.write(\"RedispatchFunctions.h\", ...)",
        "cpu_fm.write(\"RegistrationDeclarations.h\", ...)",
        "cpu_fm.write(\"VmapGeneratedPlumbing.h\", ...)",
        "core_fm.write(\"aten_interned_strings.h\", gen_aten_interned_strings)",
        "core_fm.write(\"enum_tag.h\", gen_tags_enum)"
      ],
      "mapped_sequence": [
        {
          "api_name": "core_fm.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "cpu_fm.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "cpu_fm.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "cpu_fm.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "core_fm.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        },
        {
          "api_name": "core_fm.write",
          "id": "basic_file_writing",
          "description": "Writing data to files (strings or bytes)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_writing"
        }
      ],
      "contextual_code": "def gen_headers(\n    *,\n    native_functions: Sequence[NativeFunction],\n    valid_tags: set[str],\n    grouped_native_functions: Sequence[NativeFunction | NativeFunctionsGroup],\n    structured_native_functions: Sequence[NativeFunctionsGroup],\n    static_dispatch_idx: list[BackendIndex],\n    selector: SelectiveBuilder,\n    backend_indices: dict[DispatchKey, BackendIndex],\n    core_fm: FileManager,\n    cpu_fm: FileManager,\n    device_fms: dict[str, FileManager],\n    ops_fm: FileManager,\n    dispatch_keys: Sequence[DispatchKey],\n    functions_keys: set[DispatchKey],\n    rocm: bool,\n    per_operator_headers: bool,\n) -> None:\n    if per_operator_headers:\n        gen_per_operator_headers(\n            native_functions=native_functions,\n            grouped_native_functions=grouped_native_functions,\n            static_dispatch_idx=static_dispatch_idx,\n            selector=selector,\n            backend_indices=backend_indices,\n            cpu_fm=cpu_fm,\n            device_fms=device_fms,\n            ops_fm=ops_fm,\n            dispatch_keys=dispatch_keys,\n            functions_keys=functions_keys,\n            rocm=rocm,\n        )\n    else:\n        gen_aggregated_headers(\n            native_functions=native_functions,\n            grouped_native_functions=grouped_native_functions,\n            structured_native_functions=structured_native_functions,\n            static_dispatch_idx=static_dispatch_idx,\n            selector=selector,\n            backend_indices=backend_indices,\n            cpu_fm=cpu_fm,\n            device_fms=device_fms,\n            dispatch_keys=dispatch_keys,\n            functions_keys=functions_keys,\n            rocm=rocm,\n        )\n\n    core_fm.write(\n        \"TensorBody.h\",\n        lambda: {\n            \"tensor_method_declarations\": list(\n                mapMaybe(\n                    ComputeTensorMethod(\n                        target=Target.DECLARATION,\n                        static_dispatch_backend_indices=static_dispatch_idx,\n                    ),\n                    native_functions,\n                )\n            ),\n            \"tensor_method_definitions\": list(\n                mapMaybe(\n                    ComputeTensorMethod(\n                        target=Target.DEFINITION,\n                        static_dispatch_backend_indices=static_dispatch_idx,\n                    ),\n                    native_functions,\n                )\n            ),\n        },\n    )\n\n    cpu_fm.write(\n        \"RedispatchFunctions.h\",\n        lambda: {\n            \"function_redispatch_definitions\": list(\n                mapMaybe(ComputeRedispatchFunction(), native_functions)\n            ),\n        },\n    )\n\n    cpu_fm.write(\n        \"RegistrationDeclarations.h\",\n        lambda: {\n            \"registration_declarations\": [\n                compute_registration_declarations(f, backend_indices)\n                for f in native_functions\n            ],\n        },\n    )\n\n    cpu_fm.write(\n        \"VmapGeneratedPlumbing.h\", lambda: gen_all_vmap_plumbing(native_functions)\n    )\n\n    def gen_aten_interned_strings() -> dict[str, str]:\n        attrs: set[str] = set()  # All function argument names\n        names = set()  # All ATen function names\n        for func in native_functions:\n            names.add(str(func.func.name.name))\n            names.add(func.func.name.name.base)\n            attrs.update(arg.name for arg in func.func.schema_order_arguments())\n        names -= {\n            \"and\",\n            \"and_eq\",\n            \"bitand\",\n            \"bitor\",\n            \"compl\",\n            \"not\",\n            \"not_eq\",\n            \"or\",\n            \"or_eq\",\n            \"xor\",\n            \"xor_eq\",\n        }\n        return {\n            \"aten_symbols\": \" \\\\n\".join(\n                [f\"_(aten, {name})\" for name in sorted(names)]\n            ),\n            \"attr_symbols\": \" \\\\n\".join(\n                [f\"_(attr, {name})\" for name in sorted(attrs)]\n            ),\n        }\n\n    core_fm.write(\"aten_interned_strings.h\", gen_aten_interned_strings)\n\n    def gen_tags_enum() -> dict[str, str]:\n        return {\"enum_of_valid_tags\": (\",\\n\".join(sorted(valid_tags)))}\n\n    core_fm.write(\"enum_tag.h\", gen_tags_enum)"
    }
  }
]