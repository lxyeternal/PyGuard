[
  {
    "metadata": {
      "package_name": "acryl_datahub-1.0.0.3rc2",
      "total_matches": 6,
      "processing_date": "2025-05-18 01:49:39"
    }
  },
  {
    "pyfile": "nifi.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/acryl_datahub-1.0.0.3rc2/acryl_datahub-1.0.0.3rc2/src/datahub/ingestion/source/nifi.py",
    "line_number": "822",
    "type_description": "B820:get",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "821\t            for event in events:\n822\t                event_time = parser.parse(event.get(\"eventTime\"))\n823\t                # datetime.strptime(",
    "code_snippet": "    def fetch_provenance_events(\n        self,\n        processor: NifiComponent,\n        eventType: str,\n        startDate: datetime,\n        endDate: Optional[datetime] = None,\n    ) -> Iterable[Dict]:\n        logger.debug(\n            f\"Fetching {eventType} provenance events for {processor.id}\\\n            of processor type {processor.type}, Start date: {startDate}, End date: {endDate}\"\n        )\n\n        provenance_response = self.submit_provenance_query(\n            processor, eventType, startDate, endDate\n        )\n\n        if provenance_response.ok:\n            provenance = provenance_response.json().get(\"provenance\", {})\n            provenance_uri = provenance.get(\"uri\")\n            logger.debug(f\"Retrieving provenance uri: {provenance_uri}\")\n            provenance_response = self.session.get(provenance_uri)\n            if provenance_response.ok:\n                provenance = provenance_response.json().get(\"provenance\", {})\n\n            attempts = 5  # wait for at most 5 attempts 5*1= 5 seconds\n            while (not provenance.get(\"finished\", False)) and attempts > 0:\n                logger.warning(\n                    f\"Provenance query not completed, attempts left : {attempts}\"\n                )\n                # wait until the uri returns percentcomplete 100\n                time.sleep(1)\n                provenance_response = self.session.get(provenance_uri)\n                attempts -= 1\n                if provenance_response.ok:\n                    provenance = provenance_response.json().get(\"provenance\", {})\n\n            events = provenance.get(\"results\", {}).get(\"provenanceEvents\", [])\n            last_event_time: Optional[datetime] = None\n            oldest_event_time: Optional[datetime] = None\n\n            for event in events:\n                event_time = parser.parse(event.get(\"eventTime\"))\n                # datetime.strptime(\n                #    event.get(\"eventTime\"), \"%m/%d/%Y %H:%M:%S.%f %Z\"\n                # )\n                if not last_event_time or event_time > last_event_time:\n                    last_event_time = event_time\n\n                if not oldest_event_time or event_time < oldest_event_time:\n                    oldest_event_time = event_time\n\n                yield event\n\n            processor.last_event_time = str(last_event_time)\n            self.delete_provenance(provenance_uri)\n\n            total = provenance.get(\"results\", {}).get(\"total\")\n            totalCount = provenance.get(\"results\", {}).get(\"totalCount\")\n            logger.debug(f\"Retrieved {totalCount} of {total}\")\n            if total != str(totalCount):\n                logger.debug(\"Trying to retrieve more events for the same processor\")\n                yield from self.fetch_provenance_events(\n                    processor, eventType, startDate, oldest_event_time\n                )\n        else:\n            self.report.warning(\n                f\"Provenance events could not be fetched for processor \\\n                    {processor.id} of type {processor.name}\",\n                self.config.site_url,\n            )\n            logger.warning(provenance_response.text)\n        return",
    "pattern_analysis": {
      "api_sequence": [
        "self.submit_provenance_query",
        "provenance_response.ok",
        "provenance_response.json",
        "dict.get",
        "dict.get",
        "logger.debug",
        "self.session.get",
        "provenance_response.ok",
        "provenance_response.json",
        "dict.get",
        "dict.get",
        "dict.get",
        "logger.warning",
        "time.sleep",
        "self.session.get",
        "dict.get",
        "provenance_response.ok",
        "provenance_response.json",
        "dict.get",
        "dict.get",
        "dict.get",
        "parser.parse",
        "event.get",
        "str",
        "self.delete_provenance",
        "dict.get",
        "dict.get",
        "logger.debug",
        "logger.debug",
        "self.fetch_provenance_events",
        "self.report.warning",
        "logger.warning",
        "provenance_response.text"
      ],
      "api_sequence_with_args": [
        "self.submit_provenance_query(processor, eventType, startDate, endDate)",
        "provenance_response.ok",
        "provenance_response.json()",
        "provenance.get(\"uri\")",
        "provenance_response.json().get(\"provenance\", {})",
        "logger.debug(f\"Retrieving provenance uri: {provenance_uri}\")",
        "self.session.get(provenance_uri)",
        "provenance_response.ok",
        "provenance_response.json()",
        "provenance.get(\"finished\", False)",
        "provenance.get(\"results\", {}).get(\"provenanceEvents\", [])",
        "provenance.get(\"results\", {}).get(\"total\")",
        "logger.warning(f\"Provenance query not completed, attempts left : {attempts}\")",
        "time.sleep(1)",
        "self.session.get(provenance_uri)",
        "provenance.get(\"finished\", False)",
        "provenance_response.ok",
        "provenance_response.json()",
        "provenance.get(\"provenance\", {})",
        "provenance.get(\"results\", {}).get(\"provenanceEvents\", [])",
        "event.get(\"eventTime\")",
        "parser.parse(event.get(\"eventTime\"))",
        "str(last_event_time)",
        "self.delete_provenance(provenance_uri)",
        "provenance.get(\"results\", {}).get(\"total\")",
        "provenance.get(\"results\", {}).get(\"totalCount\")",
        "logger.debug(f\"Retrieved {totalCount} of {total}\")",
        "logger.debug(\"Trying to retrieve more events for the same processor\")",
        "self.fetch_provenance_events(processor, eventType, startDate, oldest_event_time)",
        "self.report.warning(f\"Provenance events could not be fetched for processor {processor.id} of type {processor.name}\", self.config.site_url)",
        "logger.warning(provenance_response.text)",
        "provenance_response.text"
      ],
      "mapped_sequence": [
        {
          "api_name": "self.session.get",
          "id": "send_http_get",
          "description": "Sends HTTP GET request with parameters and timeout",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "request_sending"
        },
        {
          "api_name": "time.sleep",
          "id": "suspend_execution",
          "description": "Suspends execution for specified seconds",
          "first_id": "system_operations",
          "second_id": "system_environment_operations",
          "third_id": "time_operations"
        }
      ],
      "contextual_code": "if provenance_response.ok:\n    provenance = provenance_response.json().get(\"provenance\", {})\n    provenance_uri = provenance.get(\"uri\")\n    logger.debug(f\"Retrieving provenance uri: {provenance_uri}\")\n    provenance_response = self.session.get(provenance_uri)\n    if provenance_response.ok:\n        provenance = provenance_response.json().get(\"provenance\", {})\n    attempts = 5\n    while (not provenance.get(\"finished\", False)) and attempts > 0:\n        logger.warning(f\"Provenance query not completed, attempts left : {attempts}\")\n        time.sleep(1)\n        provenance_response = self.session.get(provenance_uri)\n        attempts -= 1\n        if provenance_response.ok:\n            provenance = provenance_response.json().get(\"provenance\", {})"
    }
  },
  {
    "pyfile": "tableau.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/acryl_datahub-1.0.0.3rc2/acryl_datahub-1.0.0.3rc2/src/datahub/ingestion/source/tableau/tableau.py",
    "line_number": "2434",
    "type_description": "B820:get",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "2433\t        database_field = datasource.get(c.DATABASE) or {}\n2434\t        database_id: Optional[str] = database_field.get(c.ID)\n2435\t        database_name: Optional[str] = database_field.get(c.NAME) or c.UNKNOWN.lower()",
    "code_snippet": "def parse_custom_sql(\n    self,\n    datasource: dict,\n    datasource_urn: str,\n    platform: str,\n    env: str,\n    platform_instance: Optional[str],\n    func_overridden_info: Optional[\n        Callable[\n            [\n                str,\n                Optional[str],\n                Optional[str],\n                Optional[Dict[str, str]],\n                Optional[TableauLineageOverrides],\n                Optional[Dict[str, str]],\n                Optional[Dict[str, str]],\n            ],\n            Tuple[Optional[str], Optional[str], str, str],\n        ]\n    ],\n) -> Optional[\"SqlParsingResult\"]:\n    database_field = datasource.get(c.DATABASE) or {}\n    database_id: Optional[str] = database_field.get(c.ID)\n    database_name: Optional[str] = database_field.get(c.NAME) or c.UNKNOWN.lower()\n    database_connection_type: Optional[str] = database_field.get(\n        c.CONNECTION_TYPE\n    ) or datasource.get(c.CONNECTION_TYPE)\n\n    if (\n        datasource.get(c.IS_UNSUPPORTED_CUSTOM_SQL) in (None, False)\n        and not self.config.force_extraction_of_lineage_from_custom_sql_queries\n    ):\n        logger.debug(f\"datasource {datasource_urn} is not created from custom sql\")\n        return None\n\n    if database_connection_type is None:\n        logger.debug(\n            f\"database information is missing from datasource {datasource_urn}\"\n        )\n        return None\n\n    query = datasource.get(c.QUERY)\n    if query is None:\n        logger.debug(\n            f\"raw sql query is not available for datasource {datasource_urn}\"\n        )\n        return None\n    query = self._clean_tableau_query_parameters(query)\n\n    logger.debug(f\"Parsing sql={query}\")\n\n    upstream_db = database_name\n\n    if func_overridden_info is not None:\n        # Override the information as per configuration\n        upstream_db, platform_instance, platform, _ = func_overridden_info(\n            database_connection_type,\n            database_name,\n            database_id,\n            self.config.platform_instance_map,\n            self.config.lineage_overrides,\n            self.config.database_hostname_to_platform_instance_map,\n            self.database_server_hostname_map,\n        )\n\n    logger.debug(\n        f\"Overridden info upstream_db={upstream_db}, platform_instance={platform_instance}, platform={platform}\"\n    )\n\n    parsed_result = create_lineage_sql_parsed_result(\n        query=query,\n        default_db=upstream_db,\n        platform=platform,\n        platform_instance=platform_instance,\n        env=env,\n        graph=self.ctx.graph,\n        schema_aware=not self.config.sql_parsing_disable_schema_awareness,\n    )\n\n    assert parsed_result is not None\n\n    if parsed_result.debug_info.table_error:\n        logger.warning(\n            f\"Failed to extract table lineage from datasource {datasource_urn}: {parsed_result.debug_info.table_error}\"\n        )\n        self.report.num_upstream_table_lineage_failed_parse_sql += 1\n    elif parsed_result.debug_info.column_error:\n        logger.warning(\n            f\"Failed to extract column level lineage from datasource {datasource_urn}: {parsed_result.debug_info.column_error}\"\n        )\n        self.report.num_upstream_fine_grained_lineage_failed_parse_sql += 1\n\n    return parsed_result\n",
    "pattern_analysis": {
      "api_sequence": [
        "dict.get",
        "dict.get",
        "dict.get",
        "dict.get",
        "dict.get",
        "dict.get",
        "self._clean_tableau_query_parameters",
        "create_lineage_sql_parsed_result"
      ],
      "api_sequence_with_args": [
        "datasource.get(c.DATABASE)",
        "database_field.get(c.ID)",
        "database_field.get(c.NAME)",
        "database_field.get(c.CONNECTION_TYPE)",
        "datasource.get(c.CONNECTION_TYPE)",
        "datasource.get(c.IS_UNSUPPORTED_CUSTOM_SQL)",
        "self._clean_tableau_query_parameters(query)",
        "create_lineage_sql_parsed_result(query=query, default_db=upstream_db, platform=platform, platform_instance=platform_instance, env=env, graph=self.ctx.graph, schema_aware=not self.config.sql_parsing_disable_schema_awareness)"
      ],
      "mapped_sequence": [
        {
          "api_name": "dict.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "dict.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "dict.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "dict.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "dict.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "dict.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "self._clean_tableau_query_parameters",
          "id": "exec_python_code",
          "description": "Dynamically executes Python code string",
          "first_id": "code_execution",
          "second_id": "code_evaluation_execution",
          "third_id": "string_execution"
        },
        {
          "api_name": "create_lineage_sql_parsed_result",
          "id": "exec_python_code",
          "description": "Dynamically executes Python code string",
          "first_id": "code_execution",
          "second_id": "code_evaluation_execution",
          "third_id": "string_execution"
        }
      ],
      "contextual_code": "def parse_custom_sql(\n    self,\n    datasource: dict,\n    datasource_urn: str,\n    platform: str,\n    env: str,\n    platform_instance: Optional[str],\n    func_overridden_info: Optional[\n        Callable[\n            [\n                str,\n                Optional[str],\n                Optional[str],\n                Optional[Dict[str, str]],\n                Optional[TableauLineageOverrides],\n                Optional[Dict[str, str]],\n                Optional[Dict[str, str]],\n            ],\n            Tuple[Optional[str], Optional[str], str, str],\n        ]\n    ],\n) -> Optional[\"SqlParsingResult\"]:\n    database_field = datasource.get(c.DATABASE) or {}\n    database_id: Optional[str] = database_field.get(c.ID)\n    database_name: Optional[str] = database_field.get(c.NAME) or c.UNKNOWN.lower()\n    database_connection_type: Optional[str] = database_field.get(\n        c.CONNECTION_TYPE\n    ) or datasource.get(c.CONNECTION_TYPE)\n\n    if (\n        datasource.get(c.IS_UNSUPPORTED_CUSTOM_SQL) in (None, False)\n        and not self.config.force_extraction_of_lineage_from_custom_sql_queries\n    ):\n        logger.debug(f\"datasource {datasource_urn} is not created from custom sql\")\n        return None\n\n    if database_connection_type is None:\n        logger.debug(\n            f\"database information is missing from datasource {datasource_urn}\"\n        )\n        return None\n\n    query = datasource.get(c.QUERY)\n    if query is None:\n        logger.debug(\n            f\"raw sql query is not available for datasource {datasource_urn}\"\n        )\n        return None\n    query = self._clean_tableau_query_parameters(query)\n\n    logger.debug(f\"Parsing sql={query}\")\n\n    upstream_db = database_name\n\n    if func_overridden_info is not None:\n        # Override the information as per configuration\n        upstream_db, platform_instance, platform, _ = func_overridden_info(\n            database_connection_type,\n            database_name,\n            database_id,\n            self.config.platform_instance_map,\n            self.config.lineage_overrides,\n            self.config.database_hostname_to_platform_instance_map,\n            self.database_server_hostname_map,\n        )\n\n    logger.debug(\n        f\"Overridden info upstream_db={upstream_db}, platform_instance={platform_instance}, platform={platform}\"\n    )\n\n    parsed_result = create_lineage_sql_parsed_result(\n        query=query,\n        default_db=upstream_db,\n        platform=platform,\n        platform_instance=platform_instance,\n        env=env,\n        graph=self.ctx.graph,\n        schema_aware=not self.config.sql_parsing_disable_schema_awareness,\n    )\n\n    assert parsed_result is not None\n\n    if parsed_result.debug_info.table_error:\n        logger.warning(\n            f\"Failed to extract table lineage from datasource {datasource_urn}: {parsed_result.debug_info.table_error}\"\n        )\n        self.report.num_upstream_table_lineage_failed_parse_sql += 1\n    elif parsed_result.debug_info.column_error:\n        logger.warning(\n            f\"Failed to extract column level lineage from datasource {datasource_urn}: {parsed_result.debug_info.column_error}\"\n        )\n        self.report.num_upstream_fine_grained_lineage_failed_parse_sql += 1\n\n    return parsed_result"
    }
  },
  {
    "pyfile": "data_resolver.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/acryl_datahub-1.0.0.3rc2/acryl_datahub-1.0.0.3rc2/src/datahub/ingestion/source/powerbi/rest_api_wrapper/data_resolver.py",
    "line_number": "241",
    "type_description": "B820:get",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "240\t                displayName=instance.get(Constant.DISPLAY_NAME),\n241\t                description=instance.get(Constant.DESCRIPTION, \"\"),\n242\t                embedUrl=instance.get(Constant.EMBED_URL),",
    "code_snippet": "def get_dashboards(self, workspace: Workspace) -> List[Dashboard]:\n    \"\"\"\n    Get the list of dashboard from PowerBi for the given workspace identifier\n\n    TODO: Pagination. As per REST API doc (https://docs.microsoft.com/en-us/rest/api/power-bi/dashboards/get\n    -dashboards), there is no information available on pagination\n    \"\"\"\n    dashboard_list_endpoint: str = self.get_dashboards_endpoint(workspace)\n\n    logger.debug(f\"Request to URL={dashboard_list_endpoint}\")\n    response = self._request_session.get(\n        dashboard_list_endpoint,\n        headers=self.get_authorization_header(),\n    )\n\n    response.raise_for_status()\n\n    dashboards_dict: List[Any] = response.json()[Constant.VALUE]\n\n    # Iterate through response and create a list of PowerBiAPI.Dashboard\n    dashboards: List[Dashboard] = [\n        Dashboard(\n            id=instance.get(Constant.ID),\n            isReadOnly=instance.get(Constant.IS_READ_ONLY),\n            displayName=instance.get(Constant.DISPLAY_NAME),\n            description=instance.get(Constant.DESCRIPTION, \"\"),\n            embedUrl=instance.get(Constant.EMBED_URL),\n            webUrl=instance.get(Constant.WEB_URL),\n            workspace_id=workspace.id,\n            workspace_name=workspace.name,\n            tiles=[],\n            users=[],\n            tags=[],\n        )\n        for instance in dashboards_dict\n        if (\n            instance is not None\n            and Constant.APP_ID\n            not in instance  # As we add dashboards to the App, Power BI starts\n            # providing duplicate dashboard information,\n            # where the duplicate includes an AppId, while the original dashboard does not.\n        )\n    ]\n\n    return dashboards",
    "pattern_analysis": {
      "api_sequence": [
        "self.get_dashboards_endpoint",
        "logger.debug",
        "self._request_session.get",
        "self.get_authorization_header",
        "response.raise_for_status",
        "response.json"
      ],
      "api_sequence_with_args": [
        "self.get_dashboards_endpoint(workspace)",
        "logger.debug(f\"Request to URL={dashboard_list_endpoint}\")",
        "self._request_session.get(dashboard_list_endpoint, headers=self.get_authorization_header())",
        "self.get_authorization_header()",
        "response.raise_for_status()",
        "response.json()"
      ],
      "mapped_sequence": [
        {
          "api_name": "self._request_session.get",
          "id": "send_http_get",
          "description": "Sends HTTP GET request with parameters and timeout",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "request_sending"
        },
        {
          "api_name": "response.raise_for_status",
          "id": "raise_http_error",
          "description": "Raises HTTPError if response status code indicates error",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "response_processing"
        },
        {
          "api_name": "response.json",
          "id": "deserialize_json_response",
          "description": "Deserializes JSON response body to Python object",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "response_processing"
        }
      ],
      "contextual_code": "def get_dashboards(self, workspace: Workspace) -> List[Dashboard]:\n    dashboard_list_endpoint: str = self.get_dashboards_endpoint(workspace)\n    logger.debug(f\"Request to URL={dashboard_list_endpoint}\")\n    response = self._request_session.get(\n        dashboard_list_endpoint,\n        headers=self.get_authorization_header(),\n    )\n    response.raise_for_status()\n    dashboards_dict: List[Any] = response.json()[Constant.VALUE]\n    # ... rest of code omitted for brevity"
    }
  },
  {
    "pyfile": "mode.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/acryl_datahub-1.0.0.3rc2/acryl_datahub-1.0.0.3rc2/src/datahub/ingestion/source/mode.py",
    "line_number": "762",
    "type_description": "B820:get",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "761\t        for data_source in data_sources:\n762\t            if data_source.get(\"id\", -1) == data_source_id:\n763\t                platform = self._get_datahub_friendly_platform(",
    "code_snippet": "    @lru_cache(maxsize=None)\n    def _get_platform_and_dbname(\n        self, data_source_id: int\n    ) -> Union[Tuple[str, str], Tuple[None, None]]:\n        data_sources = self._get_data_sources()\n\n        if not data_sources:\n            self.report.report_failure(\n                title=\"No Data Sources Found\",\n                message=\"Could not find data sources matching some ids\",\n                context=f\"Data Soutce ID: {data_source_id}\",\n            )\n            return None, None\n\n        for data_source in data_sources:\n            if data_source.get(\"id\", -1) == data_source_id:\n                platform = self._get_datahub_friendly_platform(\n                    data_source.get(\"adapter\", \"\"), data_source.get(\"name\", \"\")\n                )\n                database = data_source.get(\"database\", \"\")\n                # This is hacky but on bigquery we want to change the database if its default\n                # For lineage we need project_id.db.table\n                if platform == \"bigquery\" and database == \"default\":\n                    database = data_source.get(\"host\", \"\")\n                return platform, database\n        else:\n            self.report.report_warning(\n                title=\"Unable to construct upstream lineage\",\n                message=\"We did not find a data source / connection with a matching ID, meaning that we do not know the platform/database to use in lineage.\",\n                context=f\"Data Source ID: {data_source_id}\",\n            )\n        return None, None",
    "pattern_analysis": {
      "api_sequence": [
        "self._get_data_sources",
        "self.report.report_failure",
        "data_source.get",
        "self._get_datahub_friendly_platform",
        "data_source.get",
        "data_source.get",
        "data_source.get",
        "self.report.report_warning"
      ],
      "api_sequence_with_args": [
        "self._get_data_sources()",
        "self.report.report_failure(title=\"No Data Sources Found\", message=\"Could not find data sources matching some ids\", context=f\"Data Soutce ID: {data_source_id}\")",
        "data_source.get(\"id\", -1)",
        "self._get_datahub_friendly_platform(data_source.get(\"adapter\", \"\"), data_source.get(\"name\", \"\"))",
        "data_source.get(\"database\", \"\")",
        "data_source.get(\"host\", \"\")",
        "data_source.get(\"adapter\", \"\")",
        "self.report.report_warning(title=\"Unable to construct upstream lineage\", message=\"We did not find a data source / connection with a matching ID, meaning that we do not know the platform/database to use in lineage.\", context=f\"Data Source ID: {data_source_id}\")"
      ],
      "mapped_sequence": [
        {
          "api_name": "self._get_data_sources",
          "id": "import_dynamic",
          "description": "Dynamically imports specified module",
          "first_id": "code_execution",
          "second_id": "module_management",
          "third_id": "module_importing"
        },
        {
          "api_name": "self.report.report_failure",
          "id": "log_error",
          "description": "Logs error message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "data_source.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "self._get_datahub_friendly_platform",
          "id": "import_dynamic",
          "description": "Dynamically imports specified module",
          "first_id": "code_execution",
          "second_id": "module_management",
          "third_id": "module_importing"
        },
        {
          "api_name": "data_source.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "data_source.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "data_source.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "self.report.report_warning",
          "id": "log_info",
          "description": "Logs informational message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        }
      ],
      "contextual_code": "@lru_cache(maxsize=None)\ndef _get_platform_and_dbname(self, data_source_id: int) -> Union[Tuple[str, str], Tuple[None, None]]:\n    data_sources = self._get_data_sources()\n\n    if not data_sources:\n        self.report.report_failure(\n            title=\"No Data Sources Found\",\n            message=\"Could not find data sources matching some ids\",\n            context=f\"Data Soutce ID: {data_source_id}\",\n        )\n        return None, None\n\n    for data_source in data_sources:\n        if data_source.get(\"id\", -1) == data_source_id:\n            platform = self._get_datahub_friendly_platform(\n                data_source.get(\"adapter\", \"\"), data_source.get(\"name\", \"\")\n            )\n            database = data_source.get(\"database\", \"\")\n            if platform == \"bigquery\" and database == \"default\":\n                database = data_source.get(\"host\", \"\")\n            return platform, database\n    else:\n        self.report.report_warning(\n            title=\"Unable to construct upstream lineage\",\n            message=\"We did not find a data source / connection with a matching ID, meaning that we do not know the platform/database to use in lineage.\",\n            context=f\"Data Source ID: {data_source_id}\",\n        )\n    return None, None"
    }
  },
  {
    "pyfile": "superset.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/acryl_datahub-1.0.0.3rc2/acryl_datahub-1.0.0.3rc2/src/datahub/ingestion/source/superset.py",
    "line_number": "920",
    "type_description": "B820:get",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "919\t        for column in columns:\n920\t            column_name = column.get(\"column_name\", \"\")\n921\t            if not column_name:",
    "code_snippet": "def generate_physical_dataset_lineage(\n    self,\n    dataset_response: dict,\n    upstream_dataset: str,\n    datasource_urn: str,\n) -> UpstreamLineageClass:\n    # To generate column level lineage, we can manually decode the metadata\n    # to produce the ColumnLineageInfo\n    columns = dataset_response.get(\"result\", {}).get(\"columns\", [])\n    fine_grained_lineages: List[FineGrainedLineageClass] = []\n\n    for column in columns:\n        column_name = column.get(\"column_name\", \"\")\n        if not column_name:\n            continue\n\n        downstream = [make_schema_field_urn(datasource_urn, column_name)]\n        upstreams = [make_schema_field_urn(upstream_dataset, column_name)]\n        fine_grained_lineages.append(\n            FineGrainedLineageClass(\n                downstreamType=FineGrainedLineageDownstreamTypeClass.FIELD,\n                downstreams=downstream,\n                upstreamType=FineGrainedLineageUpstreamTypeClass.FIELD_SET,\n                upstreams=upstreams,\n            )\n        )\n\n    upstream_lineage = UpstreamLineageClass(\n        upstreams=[\n            UpstreamClass(\n                type=DatasetLineageTypeClass.TRANSFORMED,\n                dataset=upstream_dataset,\n            )\n        ],\n        fineGrainedLineages=fine_grained_lineages,\n    )\n    return upstream_lineage",
    "pattern_analysis": {
      "api_sequence": [
        "dict.get",
        "dict.get",
        "dict.get",
        "make_schema_field_urn",
        "make_schema_field_urn",
        "FineGrainedLineageClass",
        "UpstreamClass",
        "UpstreamLineageClass"
      ],
      "api_sequence_with_args": [
        "dataset_response.get(\"result\", {})",
        "dataset_response.get(\"result\", {}).get(\"columns\", [])",
        "column.get(\"column_name\", \"\")",
        "make_schema_field_urn(datasource_urn, column_name)",
        "make_schema_field_urn(upstream_dataset, column_name)",
        "FineGrainedLineageClass(downstreamType=FineGrainedLineageDownstreamTypeClass.FIELD, downstreams=downstream, upstreamType=FineGrainedLineageUpstreamTypeClass.FIELD_SET, upstreams=upstreams)",
        "UpstreamClass(type=DatasetLineageTypeClass.TRANSFORMED, dataset=upstream_dataset)",
        "UpstreamLineageClass(upstreams=[...], fineGrainedLineages=fine_grained_lineages)"
      ],
      "mapped_sequence": [
        {
          "api_name": "dict.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "dict.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "dict.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        }
      ],
      "contextual_code": "columns = dataset_response.get(\"result\", {}).get(\"columns\", [])\nfor column in columns:\n    column_name = column.get(\"column_name\", \"\")\n    if not column_name:\n        continue\n    downstream = [make_schema_field_urn(datasource_urn, column_name)]\n    upstreams = [make_schema_field_urn(upstream_dataset, column_name)]\n    fine_grained_lineages.append(\n        FineGrainedLineageClass(\n            downstreamType=FineGrainedLineageDownstreamTypeClass.FIELD,\n            downstreams=downstream,\n            upstreamType=FineGrainedLineageUpstreamTypeClass.FIELD_SET,\n            upstreams=upstreams,\n        )\n    )\nupstream_lineage = UpstreamLineageClass(\n    upstreams=[\n        UpstreamClass(\n            type=DatasetLineageTypeClass.TRANSFORMED,\n            dataset=upstream_dataset,\n        )\n    ],\n    fineGrainedLineages=fine_grained_lineages,\n)\nreturn upstream_lineage"
    }
  },
  {
    "pyfile": "tableau.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/acryl_datahub-1.0.0.3rc2/acryl_datahub-1.0.0.3rc2/src/datahub/ingestion/source/tableau/tableau.py",
    "line_number": "2260",
    "type_description": "B820:get",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "2259\t            logger.debug(\n2260\t                f\"published datasource {ds.get(c.NAME)} project_luid not found.\"\n2261\t                f\" Running get datasource query for {ds[c.LUID]}\"",
    "code_snippet": "def _get_published_datasource_project_luid(self, ds: dict) -> Optional[str]:\n    # This is fallback in case \"get all datasources\" query fails for some reason.\n    # It is possible due to https://github.com/tableau/server-client-python/issues/1210\n    if (\n        ds.get(c.LUID)\n        and ds[c.LUID] not in self.datasource_project_map\n        and self.report.get_all_datasources_query_failed\n    ):\n        logger.debug(\n            f\"published datasource {ds.get(c.NAME)} project_luid not found.\"\n            f\" Running get datasource query for {ds[c.LUID]}\"\n        )\n        # Query and update self.datasource_project_map with luid\n        self._query_published_datasource_for_project_luid(ds[c.LUID])\n\n    if (\n        ds.get(c.LUID)\n        and ds[c.LUID] in self.datasource_project_map\n        and self.datasource_project_map[ds[c.LUID]] in self.tableau_project_registry\n    ):\n        return self.datasource_project_map[ds[c.LUID]]\n\n    logger.debug(f\"published datasource {ds.get(c.NAME)} project_luid not found\")\n\n    return None",
    "pattern_analysis": {
      "api_sequence": [
        "dict.get",
        "logger.debug",
        "self._query_published_datasource_for_project_luid",
        "dict.get",
        "logger.debug"
      ],
      "api_sequence_with_args": [
        "ds.get(c.LUID)",
        "logger.debug(f\"published datasource {ds.get(c.NAME)} project_luid not found.\" f\" Running get datasource query for {ds[c.LUID]}\")",
        "self._query_published_datasource_for_project_luid(ds[c.LUID])",
        "ds.get(c.LUID)",
        "logger.debug(f\"published datasource {ds.get(c.NAME)} project_luid not found\")"
      ],
      "mapped_sequence": [
        {
          "api_name": "dict.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "logger.debug",
          "id": "log_info",
          "description": "Logs informational message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "self._query_published_datasource_for_project_luid",
          "id": "import_dynamic",
          "description": "Dynamically imports specified module",
          "first_id": "code_execution",
          "second_id": "module_management",
          "third_id": "module_importing"
        },
        {
          "api_name": "dict.get",
          "id": "deserialize_from_json",
          "description": "Deserializes JSON string to Python object",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "logger.debug",
          "id": "log_info",
          "description": "Logs informational message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        }
      ],
      "contextual_code": "def _get_published_datasource_project_luid(self, ds: dict) -> Optional[str]:\n    if (\n        ds.get(c.LUID)\n        and ds[c.LUID] not in self.datasource_project_map\n        and self.report.get_all_datasources_query_failed\n    ):\n        logger.debug(\n            f\"published datasource {ds.get(c.NAME)} project_luid not found.\"\n            f\" Running get datasource query for {ds[c.LUID]}\"\n        )\n        self._query_published_datasource_for_project_luid(ds[c.LUID])\n\n    if (\n        ds.get(c.LUID)\n        and ds[c.LUID] in self.datasource_project_map\n        and self.datasource_project_map[ds[c.LUID]] in self.tableau_project_registry\n    ):\n        return self.datasource_project_map[ds[c.LUID]]\n\n    logger.debug(f\"published datasource {ds.get(c.NAME)} project_luid not found\")\n    return None"
    }
  }
]