[
  {
    "metadata": {
      "package_name": "scrapy-2.12.0",
      "total_matches": 2,
      "processing_date": "2025-05-18 01:49:40"
    }
  },
  {
    "pyfile": "test_downloadermiddleware_httpcache.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/scrapy-2.12.0/scrapy-2.12.0/tests/test_downloadermiddleware_httpcache.py",
    "line_number": "38",
    "type_description": "B836:rmtree",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "37\t        self.crawler.stats.close_spider(self.spider, \"\")\n38\t        shutil.rmtree(self.tmpdir)\n39",
    "code_snippet": "import shutil\nimport tempfile\nimport email.utils\nimport time\nimport unittest\nfrom scrapy.http import Request, Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\nclass _BaseTest(unittest.TestCase):\n    storage_class = \"scrapy.extensions.httpcache.DbmCacheStorage\"\n    policy_class = \"scrapy.extensions.httpcache.RFC2616Policy\"\n\n    def setUp(self):\n        self.yesterday = email.utils.formatdate(time.time() - 86400)\n        self.today = email.utils.formatdate()\n        self.tomorrow = email.utils.formatdate(time.time() + 86400)\n        self.crawler = get_crawler(Spider)\n        self.spider = self.crawler._create_spider(\"example.com\")\n        self.tmpdir = tempfile.mkdtemp()\n        self.request = Request(\"http://www.example.com\", headers={\"User-Agent\": \"test\"})\n        self.response = Response(\n            \"http://www.example.com\",\n            headers={\"Content-Type\": \"text/html\"},\n            body=b\"test body\",\n            status=202,\n        )\n        self.crawler.stats.open_spider(self.spider)\n\n    def tearDown(self):\n        self.crawler.stats.close_spider(self.spider, \"\")\n        shutil.rmtree(self.tmpdir)\n",
    "pattern_analysis": {
      "api_sequence": [
        "time.time",
        "email.utils.formatdate",
        "time.time",
        "email.utils.formatdate",
        "email.utils.formatdate",
        "tempfile.mkdtemp",
        "scrapy.utils.test.get_crawler",
        "scrapy.crawler.Crawler._create_spider",
        "scrapy.http.Request",
        "scrapy.http.Response",
        "scrapy.stats.StatsCollector.open_spider",
        "scrapy.stats.StatsCollector.close_spider",
        "shutil.rmtree"
      ],
      "api_sequence_with_args": [
        "time.time()",
        "email.utils.formatdate(time.time() - 86400)",
        "time.time()",
        "email.utils.formatdate()",
        "email.utils.formatdate(time.time() + 86400)",
        "tempfile.mkdtemp()",
        "scrapy.utils.test.get_crawler(Spider)",
        "scrapy.crawler.Crawler._create_spider(\"example.com\")",
        "scrapy.http.Request(\"http://www.example.com\", headers={\"User-Agent\": \"test\"})",
        "scrapy.http.Response(\"http://www.example.com\", headers={\"Content-Type\": \"text/html\"}, body=b\"test body\", status=202)",
        "scrapy.stats.StatsCollector.open_spider(self.spider)",
        "scrapy.stats.StatsCollector.close_spider(self.spider, \"\")",
        "shutil.rmtree(self.tmpdir)"
      ],
      "mapped_sequence": [
        {
          "api_name": "time.time",
          "id": "get_current_time",
          "description": "Returns current time in seconds since epoch",
          "first_id": "system_operations",
          "second_id": "system_environment_operations",
          "third_id": "time_operations"
        },
        {
          "api_name": "email.utils.formatdate",
          "id": "format_local_time",
          "description": "Formats local time tuple as string",
          "first_id": "system_operations",
          "second_id": "system_environment_operations",
          "third_id": "time_operations"
        },
        {
          "api_name": "time.time",
          "id": "get_current_time",
          "description": "Returns current time in seconds since epoch",
          "first_id": "system_operations",
          "second_id": "system_environment_operations",
          "third_id": "time_operations"
        },
        {
          "api_name": "email.utils.formatdate",
          "id": "format_local_time",
          "description": "Formats local time tuple as string",
          "first_id": "system_operations",
          "second_id": "system_environment_operations",
          "third_id": "time_operations"
        },
        {
          "api_name": "email.utils.formatdate",
          "id": "format_local_time",
          "description": "Formats local time tuple as string",
          "first_id": "system_operations",
          "second_id": "system_environment_operations",
          "third_id": "time_operations"
        },
        {
          "api_name": "tempfile.mkdtemp",
          "id": "create_temp_dir",
          "description": "Creates temporary directory and returns its path",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "directory_operations"
        },
        {
          "api_name": "scrapy.utils.test.get_crawler",
          "id": "import_dynamic",
          "description": "Dynamically imports specified module",
          "first_id": "code_execution",
          "second_id": "module_management",
          "third_id": "module_importing"
        },
        {
          "api_name": "scrapy.crawler.Crawler._create_spider",
          "id": "import_dynamic",
          "description": "Dynamically imports specified module",
          "first_id": "code_execution",
          "second_id": "module_management",
          "third_id": "module_importing"
        },
        {
          "api_name": "scrapy.http.Request",
          "id": "create_http_request",
          "description": "Creates HTTP request object with specified URL, data, and headers",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "request_creation"
        },
        {
          "api_name": "scrapy.http.Response",
          "id": "get_response_body",
          "description": "Retrieves response body from HTTP response",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "response_processing"
        },
        {
          "api_name": "scrapy.stats.StatsCollector.open_spider",
          "id": "open_file_app",
          "description": "Opens file with associated application",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_opening"
        },
        {
          "api_name": "scrapy.stats.StatsCollector.close_spider",
          "id": "close_file",
          "description": "Closes the opened file",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_closing"
        },
        {
          "api_name": "shutil.rmtree",
          "id": "delete_directory",
          "description": "Recursively deletes directory and its contents",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "directory_operations"
        }
      ],
      "contextual_code": "import shutil\nimport tempfile\nimport email.utils\nimport time\nfrom scrapy.http import Request, Response\nfrom scrapy.utils.test import get_crawler\n\nclass _BaseTest(unittest.TestCase):\n    def setUp(self):\n        self.yesterday = email.utils.formatdate(time.time() - 86400)\n        self.today = email.utils.formatdate()\n        self.tomorrow = email.utils.formatdate(time.time() + 86400)\n        self.crawler = get_crawler(Spider)\n        self.spider = self.crawler._create_spider(\"example.com\")\n        self.tmpdir = tempfile.mkdtemp()\n        self.request = Request(\"http://www.example.com\", headers={\"User-Agent\": \"test\"})\n        self.response = Response(\n            \"http://www.example.com\",\n            headers={\"Content-Type\": \"text/html\"},\n            body=b\"test body\",\n            status=202,\n        )\n        self.crawler.stats.open_spider(self.spider)\n\n    def tearDown(self):\n        self.crawler.stats.close_spider(self.spider, \"\")\n        shutil.rmtree(self.tmpdir)"
    }
  },
  {
    "pyfile": "test_scheduler_base.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/scrapy-2.12.0/scrapy-2.12.0/tests/test_scheduler_base.py",
    "line_number": "137",
    "type_description": "B825:request",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "136\t        while self.scheduler.has_pending_requests():\n137\t            request = self.scheduler.next_request()\n138\t            dequeued.append(request.url)",
    "code_snippet": "def test_enqueue_dequeue(self):\n    self.assertFalse(self.scheduler.has_pending_requests())\n    for url in URLS:\n        self.assertTrue(self.scheduler.enqueue_request(Request(url)))\n        self.assertFalse(self.scheduler.enqueue_request(Request(url)))\n    self.assertTrue(self.scheduler.has_pending_requests)\n\n    dequeued = []\n    while self.scheduler.has_pending_requests():\n        request = self.scheduler.next_request()\n        dequeued.append(request.url)\n    self.assertEqual(set(dequeued), set(URLS))\n    self.assertFalse(self.scheduler.has_pending_requests())",
    "pattern_analysis": {
      "api_sequence": [],
      "api_sequence_with_args": [],
      "mapped_sequence": [],
      "contextual_code": ""
    }
  }
]