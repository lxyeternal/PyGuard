[
  {
    "metadata": {
      "package_name": "sudachipy-0.6.10",
      "total_matches": 1,
      "processing_date": "2025-05-18 01:49:40"
    }
  },
  {
    "pyfile": "command_line.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/sudachipy-0.6.10/sudachipy-0.6.10/py_src/sudachipy/command_line.py",
    "line_number": "124",
    "type_description": "B833:input",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "123\t        tokenizer_obj = dict_.create(mode=args.mode)\n124\t        input_ = fileinput.input(\n125\t            args.in_files, openhook=fileinput.hook_encoded(\"utf-8\"))\n126\t        run(tokenizer_obj, input_, output, print_all,",
    "code_snippet": "def _command_tokenize(args, print_usage):\n    if args.version:\n        print_version()\n        return\n\n    _input_files_checker(args, print_usage)\n\n    output = sys.stdout\n    if args.fpath_out:\n        output = open(args.fpath_out, \"w\", encoding=\"utf-8\")\n\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)\n\n    print_all = args.a\n    debug = args.d\n    if debug:\n        logger.warning(\"-d option is not implemented in python.\")\n\n    try:\n        dict_ = Dictionary(config_path=args.fpath_setting,\n                           dict_type=args.system_dict_type)\n        # empty matcher - get all POS tags\n        all_pos_matcher = dict_.pos_matcher([()])\n        # precompute output POS strings\n        pos_list = [\",\".join(ms) for ms in all_pos_matcher]\n\n        tokenizer_obj = dict_.create(mode=args.mode)\n        input_ = fileinput.input(\n            args.in_files, openhook=fileinput.hook_encoded(\"utf-8\"))\n        run(tokenizer_obj, input_, output, print_all,\n            pos_list, is_stdout=args.fpath_out is None)\n    finally:\n        if args.fpath_out:\n            output.close()",
    "pattern_analysis": {
      "api_sequence": [
        "open",
        "output.close"
      ],
      "api_sequence_with_args": [
        "open(args.fpath_out, \"w\", encoding=\"utf-8\")",
        "output.close()"
      ],
      "mapped_sequence": [
        {
          "api_name": "open",
          "id": "basic_write_operations",
          "description": "Basic file opening operations for writing (normal writing, binary writing)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_opening"
        },
        {
          "api_name": "output.close",
          "id": "close_file",
          "description": "Closes the opened file",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_closing"
        }
      ],
      "contextual_code": "def _command_tokenize(args, print_usage):\n    output = sys.stdout\n    if args.fpath_out:\n        output = open(args.fpath_out, \"w\", encoding=\"utf-8\")\n    try:\n        ... # omitted for brevity\n        run(tokenizer_obj, input_, output, print_all,\n            pos_list, is_stdout=args.fpath_out is None)\n    finally:\n        if args.fpath_out:\n            output.close()"
    }
  }
]