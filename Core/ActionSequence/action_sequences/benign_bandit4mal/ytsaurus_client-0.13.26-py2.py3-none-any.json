[
  {
    "metadata": {
      "package_name": "ytsaurus_client-0.13.26-py2.py3-none-any",
      "total_matches": 3,
      "processing_date": "2025-05-18 01:49:39"
    }
  },
  {
    "pyfile": "_dill.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/ytsaurus_client-0.13.26-py2.py3-none-any/yt/packages/dill/_dill.py",
    "line_number": "1867",
    "type_description": "B815:write",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "1866\t        if PY3:\n1867\t            pickler.write(bytes('c__builtin__\\nNoneType\\n', 'UTF-8'))\n1868\t        else:",
    "code_snippet": "@register(TypeType)\ndef save_type(pickler, obj, postproc_list=None):\n    if obj in _typemap:\n        log.info(\"T1: %s\" % obj)\n        pickler.save_reduce(_load_type, (_typemap[obj],), obj=obj)\n        log.info(\"# T1\")\n    elif obj.__bases__ == (tuple,) and all([hasattr(obj, attr) for attr in ('_fields','_asdict','_make','_replace')]):\n        # special case: namedtuples\n        log.info(\"T6: %s\" % obj)\n        if OLD37 or (not obj._field_defaults):\n            pickler.save_reduce(_create_namedtuple, (obj.__name__, obj._fields, obj.__module__), obj=obj)\n        else:\n            defaults = [obj._field_defaults[field] for field in obj._fields if field in obj._field_defaults]\n            pickler.save_reduce(_create_namedtuple, (obj.__name__, obj._fields, obj.__module__, defaults), obj=obj)\n        log.info(\"# T6\")\n        return\n\n    # special cases: NoneType, NotImplementedType, EllipsisType\n    elif obj is type(None):\n        log.info(\"T7: %s\" % obj)\n        #XXX: pickler.save_reduce(type, (None,), obj=obj)\n        if PY3:\n            pickler.write(bytes('c__builtin__\\nNoneType\\n', 'UTF-8'))\n        else:\n            pickler.write('c__builtin__\\nNoneType\\n')\n        log.info(\"# T7\")\n    elif obj is NotImplementedType:\n        log.info(\"T7: %s\" % obj)\n        pickler.save_reduce(type, (NotImplemented,), obj=obj)\n        log.info(\"# T7\")\n    elif obj is EllipsisType:\n        log.info(\"T7: %s\" % obj)\n        pickler.save_reduce(type, (Ellipsis,), obj=obj)\n        log.info(\"# T7\")\n\n    else:\n        obj_name = getattr(obj, '__qualname__', getattr(obj, '__name__', None))\n        _byref = getattr(pickler, '_byref', None)\n        obj_recursive = id(obj) in getattr(pickler, '_postproc', ())\n        incorrectly_named = not _locate_function(obj, pickler)\n        if not _byref and not obj_recursive and incorrectly_named: # not a function, but the name was held over\n            if issubclass(type(obj), type):\n                # thanks to Tom Stepleton pointing out pickler._session unneeded\n                _t = 'T2'\n                log.info(\"%s: %s\" % (_t, obj))\n                _dict = _dict_from_dictproxy(obj.__dict__)\n            else:\n                _t = 'T3'\n                log.info(\"%s: %s\" % (_t, obj))\n                _dict = obj.__dict__\n           #print (_dict)\n           #print (\"%s\\n%s\" % (type(obj), obj.__name__))\n           #print (\"%s\\n%s\" % (obj.__bases__, obj.__dict__))\n            for name in _dict.get(\"__slots__\", []):\n                del _dict[name]\n            if PY3 and obj_name != obj.__name__:\n                if postproc_list is None:\n                    postproc_list = []\n                postproc_list.append((setattr, (obj, '__qualname__', obj_name)))\n            _save_with_postproc(pickler, (_create_type, (\n                type(obj), obj.__name__, obj.__bases__, _dict\n            )), obj=obj, postproc_list=postproc_list)\n            log.info(\"# %s\" % _t)\n        else:\n            log.info(\"T4: %s\" % obj)\n            if incorrectly_named:\n                warnings.warn('Cannot locate reference to %r.' % (obj,), PicklingWarning)\n            if obj_recursive:\n                warnings.warn('Cannot pickle %r: %s.%s has recursive self-references that trigger a RecursionError.' % (obj, obj.__module__, obj_name), PicklingWarning)\n           #print (obj.__dict__)\n           #print (\"%s\\n%s\" % (type(obj), obj.__name__))\n           #print (\"%s\\n%s\" % (obj.__bases__, obj.__dict__))\n            StockPickler.save_global(pickler, obj, name=obj_name)\n            log.info(\"# T4\")\n    return",
    "pattern_analysis": {
      "api_sequence": [
        "log.info",
        "pickler.save_reduce",
        "log.info",
        "log.info",
        "pickler.save_reduce",
        "log.info",
        "pickler.save_reduce",
        "log.info",
        "pickler.write",
        "log.info",
        "pickler.write",
        "log.info",
        "pickler.save_reduce",
        "log.info",
        "pickler.save_reduce",
        "log.info",
        "getattr",
        "getattr",
        "getattr",
        "_locate_function",
        "issubclass",
        "log.info",
        "_dict_from_dictproxy",
        "log.info",
        "del",
        "getattr",
        "getattr",
        "setattr",
        "_save_with_postproc",
        "log.info",
        "log.info",
        "warnings.warn",
        "warnings.warn",
        "StockPickler.save_global",
        "log.info"
      ],
      "api_sequence_with_args": [
        "log.info(\"T1: %s\" % obj)",
        "pickler.save_reduce(_load_type, (_typemap[obj],), obj=obj)",
        "log.info(\"# T1\")",
        "log.info(\"T6: %s\" % obj)",
        "pickler.save_reduce(_create_namedtuple, (obj.__name__, obj._fields, obj.__module__), obj=obj)",
        "log.info(\"# T6\")",
        "pickler.save_reduce(_create_namedtuple, (obj.__name__, obj._fields, obj.__module__, defaults), obj=obj)",
        "log.info(\"# T6\")",
        "pickler.write(bytes('c__builtin__\\nNoneType\\n', 'UTF-8'))",
        "log.info(\"# T7\")",
        "pickler.write('c__builtin__\\nNoneType\\n')",
        "log.info(\"# T7\")",
        "pickler.save_reduce(type, (NotImplemented,), obj=obj)",
        "log.info(\"# T7\")",
        "pickler.save_reduce(type, (Ellipsis,), obj=obj)",
        "log.info(\"# T7\")",
        "getattr(obj, '__qualname__', getattr(obj, '__name__', None))",
        "getattr(pickler, '_byref', None)",
        "getattr(pickler, '_postproc', ())",
        "_locate_function(obj, pickler)",
        "issubclass(type(obj), type)",
        "log.info(\"%s: %s\" % (_t, obj))",
        "_dict_from_dictproxy(obj.__dict__)",
        "log.info(\"%s: %s\" % (_t, obj))",
        "del _dict[name]",
        "getattr(obj, '__name__')",
        "getattr(obj, '__qualname__', getattr(obj, '__name__', None))",
        "setattr(obj, '__qualname__', obj_name)",
        "_save_with_postproc(pickler, (_create_type, (type(obj), obj.__name__, obj.__bases__, _dict)), obj=obj, postproc_list=postproc_list)",
        "log.info(\"# %s\" % _t)",
        "log.info(\"T4: %s\" % obj)",
        "warnings.warn('Cannot locate reference to %r.' % (obj,), PicklingWarning)",
        "warnings.warn('Cannot pickle %r: %s.%s has recursive self-references that trigger a RecursionError.' % (obj, obj.__module__, obj_name), PicklingWarning)",
        "StockPickler.save_global(pickler, obj, name=obj_name)",
        "log.info(\"# T4\")"
      ],
      "mapped_sequence": [
        {
          "api_name": "log.info",
          "id": "log_info",
          "description": "Logs informational message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "pickler.save_reduce",
          "id": "serialize_to_json",
          "description": "Serializes Python object to JSON string",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "log.info",
          "id": "log_info",
          "description": "Logs informational message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "log.info",
          "id": "log_info",
          "description": "Logs informational message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "pickler.save_reduce",
          "id": "serialize_to_json",
          "description": "Serializes Python object to JSON string",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "log.info",
          "id": "log_info",
          "description": "Logs informational message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "pickler.save_reduce",
          "id": "serialize_to_json",
          "description": "Serializes Python object to JSON string",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "log.info",
          "id": "log_info",
          "description": "Logs informational message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "pickler.write",
          "id": "basic_write_operations",
          "description": "Basic file opening operations for writing (normal writing, binary writing)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_opening"
        },
        {
          "api_name": "log.info",
          "id": "log_info",
          "description": "Logs informational message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "pickler.write",
          "id": "basic_write_operations",
          "description": "Basic file opening operations for writing (normal writing, binary writing)",
          "first_id": "file_operations",
          "second_id": "file_reading_writing",
          "third_id": "file_opening"
        },
        {
          "api_name": "log.info",
          "id": "log_info",
          "description": "Logs informational message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "pickler.save_reduce",
          "id": "serialize_to_json",
          "description": "Serializes Python object to JSON string",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "log.info",
          "id": "log_info",
          "description": "Logs informational message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "pickler.save_reduce",
          "id": "serialize_to_json",
          "description": "Serializes Python object to JSON string",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "log.info",
          "id": "log_info",
          "description": "Logs informational message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "getattr",
          "id": "get_global_symbols",
          "description": "Retrieves global symbol table as dictionary",
          "first_id": "information_gathering",
          "second_id": "system_information_collection",
          "third_id": "user_information"
        },
        {
          "api_name": "getattr",
          "id": "get_global_symbols",
          "description": "Retrieves global symbol table as dictionary",
          "first_id": "information_gathering",
          "second_id": "system_information_collection",
          "third_id": "user_information"
        },
        {
          "api_name": "getattr",
          "id": "get_global_symbols",
          "description": "Retrieves global symbol table as dictionary",
          "first_id": "information_gathering",
          "second_id": "system_information_collection",
          "third_id": "user_information"
        },
        {
          "api_name": "_locate_function",
          "id": "find_packages",
          "description": "Finds all packages in current directory",
          "first_id": "code_execution",
          "second_id": "module_management",
          "third_id": "package_configuration"
        },
        {
          "api_name": "issubclass",
          "id": "check_instance_type",
          "description": "Checks if object is instance of specified type",
          "first_id": "utility_functions",
          "second_id": "logging_exception_handling",
          "third_id": "exception_checking"
        },
        {
          "api_name": "log.info",
          "id": "log_info",
          "description": "Logs informational message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "_dict_from_dictproxy",
          "id": "get_global_symbols",
          "description": "Retrieves global symbol table as dictionary",
          "first_id": "information_gathering",
          "second_id": "system_information_collection",
          "third_id": "user_information"
        },
        {
          "api_name": "log.info",
          "id": "log_info",
          "description": "Logs informational message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "del",
          "id": "delete_file",
          "description": "Deletes specified file from filesystem",
          "first_id": "file_operations",
          "second_id": "file_management",
          "third_id": "file_transfer"
        },
        {
          "api_name": "getattr",
          "id": "get_global_symbols",
          "description": "Retrieves global symbol table as dictionary",
          "first_id": "information_gathering",
          "second_id": "system_information_collection",
          "third_id": "user_information"
        },
        {
          "api_name": "getattr",
          "id": "get_global_symbols",
          "description": "Retrieves global symbol table as dictionary",
          "first_id": "information_gathering",
          "second_id": "system_information_collection",
          "third_id": "user_information"
        },
        {
          "api_name": "setattr",
          "id": "set_builtin_attr",
          "description": "Sets attribute on builtins object",
          "first_id": "persistence_stealth",
          "second_id": "stealth_techniques",
          "third_id": "warning_disabling"
        },
        {
          "api_name": "_save_with_postproc",
          "id": "serialize_to_json",
          "description": "Serializes Python object to JSON string",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "log.info",
          "id": "log_info",
          "description": "Logs informational message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "log.info",
          "id": "log_info",
          "description": "Logs informational message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "warnings.warn",
          "id": "log_error",
          "description": "Logs error message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "warnings.warn",
          "id": "log_error",
          "description": "Logs error message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "StockPickler.save_global",
          "id": "serialize_to_json",
          "description": "Serializes Python object to JSON string",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        },
        {
          "api_name": "log.info",
          "id": "log_info",
          "description": "Logs informational message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        }
      ],
      "contextual_code": "@register(TypeType)\ndef save_type(pickler, obj, postproc_list=None):\n    if obj in _typemap:\n        log.info(\"T1: %s\" % obj)\n        pickler.save_reduce(_load_type, (_typemap[obj],), obj=obj)\n        log.info(\"# T1\")\n    elif obj.__bases__ == (tuple,) and all([hasattr(obj, attr) for attr in ('_fields','_asdict','_make','_replace')]):\n        log.info(\"T6: %s\" % obj)\n        if OLD37 or (not obj._field_defaults):\n            pickler.save_reduce(_create_namedtuple, (obj.__name__, obj._fields, obj.__module__), obj=obj)\n        else:\n            defaults = [obj._field_defaults[field] for field in obj._fields if field in obj._field_defaults]\n            pickler.save_reduce(_create_namedtuple, (obj.__name__, obj._fields, obj.__module__, defaults), obj=obj)\n        log.info(\"# T6\")\n        return\n    elif obj is type(None):\n        log.info(\"T7: %s\" % obj)\n        if PY3:\n            pickler.write(bytes('c__builtin__\\nNoneType\\n', 'UTF-8'))\n        else:\n            pickler.write('c__builtin__\\nNoneType\\n')\n        log.info(\"# T7\")\n    elif obj is NotImplementedType:\n        log.info(\"T7: %s\" % obj)\n        pickler.save_reduce(type, (NotImplemented,), obj=obj)\n        log.info(\"# T7\")\n    elif obj is EllipsisType:\n        log.info(\"T7: %s\" % obj)\n        pickler.save_reduce(type, (Ellipsis,), obj=obj)\n        log.info(\"# T7\")\n    else:\n        obj_name = getattr(obj, '__qualname__', getattr(obj, '__name__', None))\n        _byref = getattr(pickler, '_byref', None)\n        obj_recursive = id(obj) in getattr(pickler, '_postproc', ())\n        incorrectly_named = not _locate_function(obj, pickler)\n        if not _byref and not obj_recursive and incorrectly_named:\n            if issubclass(type(obj), type):\n                _t = 'T2'\n                log.info(\"%s: %s\" % (_t, obj))\n                _dict = _dict_from_dictproxy(obj.__dict__)\n            else:\n                _t = 'T3'\n                log.info(\"%s: %s\" % (_t, obj))\n                _dict = obj.__dict__\n            for name in _dict.get(\"__slots__\", []):\n                del _dict[name]\n            if PY3 and obj_name != obj.__name__:\n                if postproc_list is None:\n                    postproc_list = []\n                postproc_list.append((setattr, (obj, '__qualname__', obj_name)))\n            _save_with_postproc(pickler, (_create_type, (\n                type(obj), obj.__name__, obj.__bases__, _dict\n            )), obj=obj, postproc_list=postproc_list)\n            log.info(\"# %s\" % _t)\n        else:\n            log.info(\"T4: %s\" % obj)\n            if incorrectly_named:\n                warnings.warn('Cannot locate reference to %r.' % (obj,), PicklingWarning)\n            if obj_recursive:\n                warnings.warn('Cannot pickle %r: %s.%s has recursive self-references that trigger a RecursionError.' % (obj, obj.__module__, obj_name), PicklingWarning)\n            StockPickler.save_global(pickler, obj, name=obj_name)\n            log.info(\"# T4\")\n    return"
    }
  },
  {
    "pyfile": "connectionpool.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/ytsaurus_client-0.13.26-py2.py3-none-any/yt/packages/urllib3/connectionpool.py",
    "line_number": "274",
    "type_description": "B807:close",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "273\t            log.debug(\"Resetting dropped connection: %s\", self.host)\n274\t            conn.close()\n275\t            if getattr(conn, \"auto_open\", 1) == 0:",
    "code_snippet": "def _get_conn(self, timeout=None):\n    \"\"\"\n    Get a connection. Will return a pooled connection if one is available.\n\n    If no connections are available and :prop:`.block` is ``False``, then a\n    fresh connection is returned.\n\n    :param timeout:\n        Seconds to wait before giving up and raising\n        :class:`urllib3.exceptions.EmptyPoolError` if the pool is empty and\n        :prop:`.block` is ``True``.\n    \"\"\"\n    conn = None\n    try:\n        conn = self.pool.get(block=self.block, timeout=timeout)\n\n    except AttributeError:  # self.pool is None\n        raise ClosedPoolError(self, \"Pool is closed.\")\n\n    except queue.Empty:\n        if self.block:\n            raise EmptyPoolError(\n                self,\n                \"Pool reached maximum size and no more connections are allowed.\",\n            )\n        pass  # Oh well, we'll create a new connection then\n\n    # If this is a persistent connection, check if it got disconnected\n    if conn and is_connection_dropped(conn):\n        log.debug(\"Resetting dropped connection: %s\", self.host)\n        conn.close()\n        if getattr(conn, \"auto_open\", 1) == 0:\n            # This is a proxied connection that has been mutated by\n            # http.client._tunnel() and cannot be reused (since it would\n            # attempt to bypass the proxy)\n            conn = None\n\n    return conn or self._new_conn()",
    "pattern_analysis": {
      "api_sequence": [
        "self.pool.get",
        "is_connection_dropped",
        "log.debug",
        "conn.close",
        "getattr",
        "self._new_conn"
      ],
      "api_sequence_with_args": [
        "self.pool.get(block=self.block, timeout=timeout)",
        "is_connection_dropped(conn)",
        "log.debug(\"Resetting dropped connection: %s\", self.host)",
        "conn.close()",
        "getattr(conn, \"auto_open\", 1)",
        "self._new_conn()"
      ],
      "mapped_sequence": [
        {
          "api_name": "self.pool.get",
          "id": "create_http_connection",
          "description": "Creates HTTP connection to specified host",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "request_creation"
        },
        {
          "api_name": "is_connection_dropped",
          "id": "check_process_terminated",
          "description": "Checks if process has terminated",
          "first_id": "system_operations",
          "second_id": "process_management",
          "third_id": "process_control"
        },
        {
          "api_name": "log.debug",
          "id": "log_info",
          "description": "Logs informational message",
          "first_id": "data_exfiltration",
          "second_id": "data_collection_operations",
          "third_id": "data_acquisition"
        },
        {
          "api_name": "conn.close",
          "id": "close_http_response",
          "description": "Closes the HTTP response object",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "response_processing"
        },
        {
          "api_name": "getattr",
          "id": "get_global_symbols",
          "description": "Retrieves global symbol table as dictionary",
          "first_id": "information_gathering",
          "second_id": "system_information_collection",
          "third_id": "user_information"
        },
        {
          "api_name": "self._new_conn",
          "id": "create_http_connection",
          "description": "Creates HTTP connection to specified host",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "request_creation"
        }
      ],
      "contextual_code": "def _get_conn(self, timeout=None):\n    conn = None\n    try:\n        conn = self.pool.get(block=self.block, timeout=timeout)\n    except AttributeError:  # self.pool is None\n        raise ClosedPoolError(self, \"Pool is closed.\")\n    except queue.Empty:\n        if self.block:\n            raise EmptyPoolError(\n                self,\n                \"Pool reached maximum size and no more connections are allowed.\",\n            )\n        pass  # Oh well, we'll create a new connection then\n    if conn and is_connection_dropped(conn):\n        log.debug(\"Resetting dropped connection: %s\", self.host)\n        conn.close()\n        if getattr(conn, \"auto_open\", 1) == 0:\n            conn = None\n    return conn or self._new_conn()"
    }
  },
  {
    "pyfile": "connectionpool.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/ytsaurus_client-0.13.26-py2.py3-none-any/yt/packages/urllib3/connectionpool.py",
    "line_number": "440",
    "type_description": "B826:getresponse",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "439\t                # Python 2.7, use buffering of HTTP responses\n440\t                httplib_response = conn.getresponse(buffering=True)\n441\t            except TypeError:",
    "code_snippet": "    def _make_request(\n        self, conn, method, url, timeout=_Default, chunked=False, **httplib_request_kw\n    ):\n        \"\"\"\n        Perform a request on a given urllib connection object taken from our\n        pool.\n\n        :param conn:\n            a connection from one of our connection pools\n\n        :param timeout:\n            Socket timeout in seconds for the request. This can be a\n            float or integer, which will set the same timeout value for\n            the socket connect and the socket read, or an instance of\n            :class:`urllib3.util.Timeout`, which gives you more fine-grained\n            control over your timeouts.\n        \"\"\"\n        self.num_requests += 1\n\n        timeout_obj = self._get_timeout(timeout)\n        timeout_obj.start_connect()\n        conn.timeout = timeout_obj.connect_timeout\n\n        # Trigger any extra validation we need to do.\n        try:\n            self._validate_conn(conn)\n        except (SocketTimeout, BaseSSLError) as e:\n            # Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\n            self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n            raise\n\n        # conn.request() calls http.client.*.request, not the method in\n        # urllib3.request. It also calls makefile (recv) on the socket.\n        try:\n            if chunked:\n                conn.request_chunked(method, url, **httplib_request_kw)\n            else:\n                conn.request(method, url, **httplib_request_kw)\n\n        # We are swallowing BrokenPipeError (errno.EPIPE) since the server is\n        # legitimately able to close the connection after sending a valid response.\n        # With this behaviour, the received response is still readable.\n        except BrokenPipeError:\n            # Python 3\n            pass\n        except IOError as e:\n            # Python 2 and macOS/Linux\n            # EPIPE and ESHUTDOWN are BrokenPipeError on Python 2, and EPROTOTYPE is needed on macOS\n            # https://erickt.github.io/blog/2014/11/19/adventures-in-debugging-a-potential-osx-kernel-bug/\n            if e.errno not in {\n                errno.EPIPE,\n                errno.ESHUTDOWN,\n                errno.EPROTOTYPE,\n            }:\n                raise\n\n        # Reset the timeout for the recv() on the socket\n        read_timeout = timeout_obj.read_timeout\n\n        # App Engine doesn't have a sock attr\n        if getattr(conn, \"sock\", None):\n            # In Python 3 socket.py will catch EAGAIN and return None when you\n            # try and read into the file pointer created by http.client, which\n            # instead raises a BadStatusLine exception. Instead of catching\n            # the exception and assuming all BadStatusLine exceptions are read\n            # timeouts, check for a zero timeout before making the request.\n            if read_timeout == 0:\n                raise ReadTimeoutError(\n                    self, url, \"Read timed out. (read timeout=%s)\" % read_timeout\n                )\n            if read_timeout is Timeout.DEFAULT_TIMEOUT:\n                conn.sock.settimeout(socket.getdefaulttimeout())\n            else:  # None or a value\n                conn.sock.settimeout(read_timeout)\n\n        # Receive the response from the server\n        try:\n            try:\n                # Python 2.7, use buffering of HTTP responses\n                httplib_response = conn.getresponse(buffering=True)\n            except TypeError:\n                # Python 3\n                try:\n                    httplib_response = conn.getresponse()\n                except BaseException as e:\n                    # Remove the TypeError from the exception chain in\n                    # Python 3 (including for exceptions like SystemExit).\n                    # Otherwise it looks like a bug in the code.\n                    six.raise_from(e, None)\n        except (SocketTimeout, BaseSSLError, SocketError) as e:\n            self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n            raise\n\n        # AppEngine doesn't have a version attr.\n        http_version = getattr(conn, \"_http_vsn_str\", \"HTTP/?\")\n        log.debug(\n            '%s://%s:%s \"%s %s %s\" %s %s',\n            self.scheme,\n            self.host,\n            self.port,\n            method,\n            url,\n            http_version,\n            httplib_response.status,\n            httplib_response.length,\n        )\n\n        try:\n            assert_header_parsing(httplib_response.msg)\n        except (HeaderParsingError, TypeError) as hpe:  # Platform-specific: Python 3\n            log.warning(\n                \"Failed to parse headers (url=%s): %s\",\n                self._absolute_url(url),\n                hpe,\n                exc_info=True,\n            )\n\n        return httplib_response",
    "pattern_analysis": {
      "api_sequence": [
        "self._get_timeout",
        "timeout_obj.start_connect",
        "self._validate_conn",
        "self._raise_timeout",
        "conn.request_chunked",
        "conn.request",
        "conn.getresponse",
        "six.raise_from",
        "self._raise_timeout",
        "conn.sock.settimeout",
        "conn.sock.settimeout",
        "conn.getresponse",
        "assert_header_parsing"
      ],
      "api_sequence_with_args": [
        "self._get_timeout(timeout)",
        "timeout_obj.start_connect()",
        "self._validate_conn(conn)",
        "self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)",
        "conn.request_chunked(method, url, **httplib_request_kw)",
        "conn.request(method, url, **httplib_request_kw)",
        "conn.getresponse(buffering=True)",
        "six.raise_from(e, None)",
        "self._raise_timeout(err=e, url=url, timeout_value=read_timeout)",
        "conn.sock.settimeout(socket.getdefaulttimeout())",
        "conn.sock.settimeout(read_timeout)",
        "conn.getresponse()",
        "assert_header_parsing(httplib_response.msg)"
      ],
      "mapped_sequence": [
        {
          "api_name": "self._get_timeout",
          "id": "set_socket_timeout",
          "description": "Sets timeout for socket operations",
          "first_id": "basic_network_operations",
          "second_id": "socket_communication",
          "third_id": "socket_configuration"
        },
        {
          "api_name": "timeout_obj.start_connect",
          "id": "set_socket_timeout",
          "description": "Sets timeout for socket operations",
          "first_id": "basic_network_operations",
          "second_id": "socket_communication",
          "third_id": "socket_configuration"
        },
        {
          "api_name": "self._validate_conn",
          "id": "create_http_connection",
          "description": "Creates HTTP connection to specified host",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "request_creation"
        },
        {
          "api_name": "self._raise_timeout",
          "id": "handle_socket_timeout",
          "description": "Handles socket timeout exception",
          "first_id": "basic_network_operations",
          "second_id": "socket_communication",
          "third_id": "socket_configuration"
        },
        {
          "api_name": "conn.request_chunked",
          "id": "send_http_custom",
          "description": "Sends HTTP request with specified method and parameters",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "request_sending"
        },
        {
          "api_name": "conn.request",
          "id": "send_http_custom",
          "description": "Sends HTTP request with specified method and parameters",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "request_sending"
        },
        {
          "api_name": "conn.getresponse",
          "id": "read_response_body",
          "description": "Reads response body from HTTP response",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "response_processing"
        },
        {
          "api_name": "six.raise_from",
          "id": "handle_socket_timeout",
          "description": "Handles socket timeout exception",
          "first_id": "basic_network_operations",
          "second_id": "socket_communication",
          "third_id": "socket_configuration"
        },
        {
          "api_name": "self._raise_timeout",
          "id": "handle_socket_timeout",
          "description": "Handles socket timeout exception",
          "first_id": "basic_network_operations",
          "second_id": "socket_communication",
          "third_id": "socket_configuration"
        },
        {
          "api_name": "conn.sock.settimeout",
          "id": "set_socket_timeout",
          "description": "Sets timeout for socket operations",
          "first_id": "basic_network_operations",
          "second_id": "socket_communication",
          "third_id": "socket_configuration"
        },
        {
          "api_name": "conn.sock.settimeout",
          "id": "set_socket_timeout",
          "description": "Sets timeout for socket operations",
          "first_id": "basic_network_operations",
          "second_id": "socket_communication",
          "third_id": "socket_configuration"
        },
        {
          "api_name": "conn.getresponse",
          "id": "read_response_body",
          "description": "Reads response body from HTTP response",
          "first_id": "basic_network_operations",
          "second_id": "http_requests",
          "third_id": "response_processing"
        },
        {
          "api_name": "assert_header_parsing",
          "id": "deserialize_from_bytes",
          "description": "Deserializes Python object from bytes",
          "first_id": "data_transformation_processing",
          "second_id": "data_encoding",
          "third_id": "specific_format_encoding"
        }
      ],
      "contextual_code": "def _make_request(self, conn, method, url, timeout=_Default, chunked=False, **httplib_request_kw):\n    self.num_requests += 1\n    timeout_obj = self._get_timeout(timeout)\n    timeout_obj.start_connect()\n    conn.timeout = timeout_obj.connect_timeout\n    try:\n        self._validate_conn(conn)\n    except (SocketTimeout, BaseSSLError) as e:\n        self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n        raise\n    try:\n        if chunked:\n            conn.request_chunked(method, url, **httplib_request_kw)\n        else:\n            conn.request(method, url, **httplib_request_kw)\n    except BrokenPipeError:\n        pass\n    except IOError as e:\n        if e.errno not in {errno.EPIPE, errno.ESHUTDOWN, errno.EPROTOTYPE}:\n            raise\n    read_timeout = timeout_obj.read_timeout\n    if getattr(conn, \"sock\", None):\n        if read_timeout == 0:\n            raise ReadTimeoutError(self, url, \"Read timed out. (read timeout=%s)\" % read_timeout)\n        if read_timeout is Timeout.DEFAULT_TIMEOUT:\n            conn.sock.settimeout(socket.getdefaulttimeout())\n        else:\n            conn.sock.settimeout(read_timeout)\n    try:\n        try:\n            httplib_response = conn.getresponse(buffering=True)\n        except TypeError:\n            try:\n                httplib_response = conn.getresponse()\n            except BaseException as e:\n                six.raise_from(e, None)\n    except (SocketTimeout, BaseSSLError, SocketError) as e:\n        self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n        raise\n    try:\n        assert_header_parsing(httplib_response.msg)\n    except (HeaderParsingError, TypeError) as hpe:\n        log.warning(\"Failed to parse headers (url=%s): %s\", self._absolute_url(url), hpe, exc_info=True)\n    return httplib_response"
    }
  }
]