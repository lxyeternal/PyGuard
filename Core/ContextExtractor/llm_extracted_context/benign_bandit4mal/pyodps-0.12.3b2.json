[
  {
    "metadata": {
      "package_name": "pyodps-0.12.3b2",
      "total_matches": 2,
      "processing_date": "2025-05-18 01:49:39"
    }
  },
  {
    "pyfile": "tabletunnel.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/pyodps-0.12.3b2/pyodps-0.12.3b2/odps/tunnel/tabletunnel.py",
    "line_number": "243",
    "type_description": "B821:post",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "242\t        try:\n243\t            resp = self._client.post(\n244\t                url, {}, params=params, headers=headers, timeout=timeout\n245\t            )",
    "context_snippet": "def _init(self, async_mode, timeout):\n    params = self.get_common_params(downloads=\"\")\n    headers = self.get_common_headers(content_length=0, tags=self._tags)\n    if async_mode:\n        params[\"asyncmode\"] = \"true\"\n\n    url = self._table.table_resource()\n    ts = monotonic()\n    try:\n        resp = self._client.post(\n            url, {}, params=params, headers=headers, timeout=timeout\n        )\n    except requests.exceptions.ReadTimeout:\n        if callable(options.tunnel_session_create_timeout_callback):\n            options.tunnel_session_create_timeout_callback(*sys.exc_info())\n        raise\n    self.check_tunnel_response(resp)\n\n    delay_time = 0.1\n    self.parse(resp, obj=self)\n    while self.status == self.Status.Initiating:\n        if timeout and monotonic() - ts > timeout:\n            try:\n                raise TunnelReadTimeout(\n                    \"Waiting for tunnel ready timed out. id=%s, table=%s\"\n                    % (self.id, self._table.name)\n                )\n            except TunnelReadTimeout:\n                if callable(options.tunnel_session_create_timeout_callback):\n                    options.tunnel_session_create_timeout_callback(*sys.exc_info())\n                raise\n        time.sleep(delay_time)\n        delay_time = min(delay_time * 2, 5)\n        self.reload()\n    if self.schema is not None:\n        self.schema.build_snapshot()",
    "hash_value": "2db9bb13df72de0091084d38a07abb69"
  },
  {
    "pyfile": "tables.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/pyodps-0.12.3b2/pyodps-0.12.3b2/odps/examples/tables.py",
    "line_number": "628",
    "type_description": "B815:write",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "627\t    with open(cache_path, \"wb\") as f:\n628\t        f.write(compressed_content)\n629",
    "context_snippet": "def cache_newsgroup_tar(target_tar, target_dir, cache_dir):\n    tarfile.open(target_tar, \"r:gz\").extractall(path=target_dir)\n    os.unlink(target_tar)\n\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    train_path = os.path.join(target_dir, NEWSGROUP_TRAIN_FOLDER)\n    test_path = os.path.join(target_dir, NEWSGROUP_TEST_FOLDER)\n    cache_path = os.path.join(cache_dir, NEWSGROUP_CACHE_NAME)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    def load_files(path, encoding):\n        objs = []\n        for fn in glob.glob(os.path.join(path, \"*\")):\n            file_cat = os.path.basename(os.path.normpath(fn))\n            for sfn in glob.glob(os.path.join(fn, \"*\")):\n                file_id = os.path.basename(os.path.normpath(sfn))\n                with open(sfn, \"rb\") as f:\n                    objs.append((file_id, file_cat, f.read().decode(encoding)))\n\n        return objs\n\n    # Store a zipped pickle\n    cache = dict(\n        train=load_files(train_path, encoding=\"latin1\"),\n        test=load_files(test_path, encoding=\"latin1\"),\n    )\n    compressed_content = codecs.encode(pickle.dumps(cache), \"zlib_codec\")\n    with open(cache_path, \"wb\") as f:\n        f.write(compressed_content)\n\n    shutil.rmtree(target_dir)",
    "hash_value": "320f27155407bf6204ec48cdaeba9082"
  }
]