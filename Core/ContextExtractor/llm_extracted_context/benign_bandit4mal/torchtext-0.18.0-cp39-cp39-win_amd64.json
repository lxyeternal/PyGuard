[
  {
    "metadata": {
      "package_name": "torchtext-0.18.0-cp39-cp39-win_amd64",
      "total_matches": 1,
      "processing_date": "2025-05-18 01:49:40"
    }
  },
  {
    "pyfile": "vectors.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/torchtext-0.18.0-cp39-cp39-win_amd64/torchtext/vocab/vectors.py",
    "line_number": "98",
    "type_description": "B819:urlretrieve",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "97\t                        try:\n98\t                            urlretrieve(url, dest, reporthook=reporthook(t))\n99\t                        except KeyboardInterrupt as e:  # remove the partial zip file",
    "context_snippet": "def cache(self, name, cache, url=None, max_vectors=None):\n    import ssl\n\n    ssl._create_default_https_context = ssl._create_unverified_context\n    if os.path.isfile(name):\n        path = name\n        if max_vectors:\n            file_suffix = \"_{}.pt\".format(max_vectors)\n        else:\n            file_suffix = \".pt\"\n        path_pt = os.path.join(cache, os.path.basename(name)) + file_suffix\n    else:\n        path = os.path.join(cache, name)\n        if max_vectors:\n            file_suffix = \"_{}.pt\".format(max_vectors)\n        else:\n            file_suffix = \".pt\"\n        path_pt = path + file_suffix\n\n    if not os.path.isfile(path_pt):\n        if not os.path.isfile(path) and url:\n            logger.info(\"Downloading vectors from {}\".format(url))\n            if not os.path.exists(cache):\n                os.makedirs(cache)\n            dest = os.path.join(cache, os.path.basename(url))\n            if not os.path.isfile(dest):\n                with tqdm(unit=\"B\", unit_scale=True, miniters=1, desc=dest) as t:\n                    try:\n                        urlretrieve(url, dest, reporthook=reporthook(t))\n                    except KeyboardInterrupt as e:  # remove the partial zip file\n                        os.remove(dest)\n                        raise e\n            logger.info(\"Extracting vectors into {}\".format(cache))\n            ext = os.path.splitext(dest)[1][1:]\n            if ext == \"zip\":\n                with zipfile.ZipFile(dest, \"r\") as zf:\n                    zf.extractall(cache)\n            elif ext == \"gz\":\n                if dest.endswith(\".tar.gz\"):\n                    with tarfile.open(dest, \"r:gz\") as tar:\n                        tar.extractall(path=cache)\n        if not os.path.isfile(path):\n            raise RuntimeError(\"no vectors found at {}\".format(path))\n\n        logger.info(\"Loading vectors from {}\".format(path))\n        ext = os.path.splitext(path)[1][1:]\n        if ext == \"gz\":\n            open_file = gzip.open\n        else:\n            open_file = open\n\n        vectors_loaded = 0\n        with open_file(path, \"rb\") as f:\n            num_lines, dim = _infer_shape(f)\n            if not max_vectors or max_vectors > num_lines:\n                max_vectors = num_lines\n\n            itos, vectors, dim = [], torch.zeros((max_vectors, dim)), None\n\n            for line in tqdm(f, total=max_vectors):\n                # Explicitly splitting on \" \" is important, so we don't\n                # get rid of Unicode non-breaking spaces in the vectors.\n                entries = line.rstrip().split(b\" \")\n\n                word, entries = entries[0], entries[1:]\n                if dim is None and len(entries) > 1:\n                    dim = len(entries)\n                elif len(entries) == 1:\n                    logger.warning(\n                        \"Skipping token {} with 1-dimensional \" \"vector {}; likely a header\".format(word, entries)\n                    )\n                    continue\n                elif dim != len(entries):\n                    raise RuntimeError(\n                        \"Vector for token {} has {} dimensions, but previously \"\n                        \"read vectors have {} dimensions. All vectors must have \"\n                        \"the same number of dimensions.\".format(word, len(entries), dim)\n                    )\n\n                try:\n                    if isinstance(word, bytes):\n                        word = word.decode(\"utf-8\")\n                except UnicodeDecodeError:\n                    logger.info(\"Skipping non-UTF8 token {}\".format(repr(word)))\n                    continue\n\n                vectors[vectors_loaded] = torch.tensor([float(x) for x in entries])\n                vectors_loaded += 1\n                itos.append(word)\n\n                if vectors_loaded == max_vectors:\n                    break\n\n        self.itos = itos\n        self.stoi = {word: i for i, word in enumerate(itos)}\n        self.vectors = torch.Tensor(vectors).view(-1, dim)\n        self.dim = dim\n        logger.info(\"Saving vectors to {}\".format(path_pt))\n        if not os.path.exists(cache):\n            os.makedirs(cache)\n        torch.save((self.itos, self.stoi, self.vectors, self.dim), path_pt)\n    else:\n        logger.info(\"Loading vectors from {}\".format(path_pt))\n        self.itos, self.stoi, self.vectors, self.dim = torch.load(path_pt)",
    "hash_value": "cfc988d19997303c46067a15ddf79127"
  }
]