[
  {
    "metadata": {
      "package_name": "aimrecords-0.0.7",
      "total_matches": 1,
      "processing_date": "2025-05-18 01:49:39"
    }
  },
  {
    "pyfile": "writer.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/aimrecords-0.0.7/aimrecords-0.0.7/aimrecords/record_storage/writer.py",
    "line_number": "249",
    "type_description": "B815:write",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "248\n249\t            self.current_data_file.write(bucket_data)\n250",
    "context_snippet": "def _finalize_current_bucket(self):\n    current_bucket_offset = self.current_data_file.tell()\n    offset_b = current_bucket_offset.to_bytes(BUCKET_OFFSET_SIZE,\n                                              ENDIANNESS)\n    records_num_b = self.records_num.to_bytes(RECORDS_NUM_SIZE, ENDIANNESS)\n\n    self.bucket_offsets_file.write(offset_b)\n    self.bucket_offsets_file.write(records_num_b)\n\n    with open(current_bucket_fname(self.path), 'rb') as f_in:\n        # depending on size of current_bucket we may want to read it in\n        # chunks depending on compression we need to handle this differently\n        bucket_data = f_in.read()\n\n        if self.compression == COMPRESSION_GZIP:\n            bucket_comp_obj = io.BytesIO(b'')\n            with gzip.GzipFile(fileobj=bucket_comp_obj, mode='wb') \\\n                    as writer:\n                writer.write(bucket_data)\n            bucket_data = bucket_comp_obj.getvalue()\n\n        self.current_data_file.write(bucket_data)\n\n    self.buckets_num += 1\n    self.current_data_file.flush()\n    self.bucket_offsets_file.flush()\n    self.current_bucket_file.truncate(0)\n    self.current_bucket_file.seek(0)\n\n    self.save_metadata()",
    "hash_value": "e85fc3f243d5b76006cfa151ff41efbc"
  }
]