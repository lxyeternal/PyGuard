[
  {
    "metadata": {
      "package_name": "g4f-0.5.0.4",
      "total_matches": 2,
      "processing_date": "2025-05-18 01:49:39"
    }
  },
  {
    "pyfile": "DfeHub.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/g4f-0.5.0.4/g4f-0.5.0.4/g4f/Provider/deprecated/DfeHub.py",
    "line_number": "51",
    "type_description": "B821:post",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "50\n51\t        response = requests.post(\"https://chat.dfehub.com/api/openai/v1/chat/completions\",\n52\t            headers=headers, json=json_data, timeout=3)\n53",
    "context_snippet": "from __future__ import annotations\n\nimport json\nimport re\nimport time\n\nimport requests\n\nfrom ...typing import Any, CreateResult\nfrom ..base_provider import AbstractProvider\n\n\nclass DfeHub(AbstractProvider):\n    url                   = \"https://chat.dfehub.com/\"\n    supports_stream       = True\n    supports_gpt_35_turbo = True\n\n    @staticmethod\n    def create_completion(\n        model: str,\n        messages: list[dict[str, str]],\n        stream: bool, **kwargs: Any) -> CreateResult:\n        \n        headers = {\n            \"authority\"         : \"chat.dfehub.com\",\n            \"accept\"            : \"*/*\",\n            \"accept-language\"   : \"en,fr-FR;q=0.9,fr;q=0.8,es-ES;q=0.7,es;q=0.6,en-US;q=0.5,am;q=0.4,de;q=0.3\",\n            \"content-type\"      : \"application/json\",\n            \"origin\"            : \"https://chat.dfehub.com\",\n            \"referer\"           : \"https://chat.dfehub.com/\",\n            \"sec-ch-ua\"         : '\"Not.A/Brand\";v=\"8\", \"Chromium\";v=\"114\", \"Google Chrome\";v=\"114\"',\n            \"sec-ch-ua-mobile\"  : \"?0\",\n            \"sec-ch-ua-platform\": '\"macOS\"',\n            \"sec-fetch-dest\"    : \"empty\",\n            \"sec-fetch-mode\"    : \"cors\",\n            \"sec-fetch-site\"    : \"same-origin\",\n            \"user-agent\"        : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n            \"x-requested-with\"  : \"XMLHttpRequest\",\n        }\n\n        json_data = {\n            \"messages\"          : messages,\n            \"model\"             : \"gpt-3.5-turbo\",\n            \"temperature\"       : kwargs.get(\"temperature\", 0.5),\n            \"presence_penalty\"  : kwargs.get(\"presence_penalty\", 0),\n            \"frequency_penalty\" : kwargs.get(\"frequency_penalty\", 0),\n            \"top_p\"             : kwargs.get(\"top_p\", 1),\n            \"stream\"            : True\n        }\n        \n        response = requests.post(\"https://chat.dfehub.com/api/openai/v1/chat/completions\",\n            headers=headers, json=json_data, timeout=3)\n\n        for chunk in response.iter_lines():\n            if b\"detail\" in chunk:\n                delay = re.findall(r\"\\d+\\.\\d+\", chunk.decode())\n                delay = float(delay[-1])\n                time.sleep(delay)\n                yield from DfeHub.create_completion(model, messages, stream, **kwargs)\n            if b\"content\" in chunk:\n                data = json.loads(chunk.decode().split(\"data: \")[1])\n                yield (data[\"choices\"][0][\"delta\"][\"content\"])",
    "hash_value": "ed09e3b6bfc36581c8a6b0176f0d6085"
  },
  {
    "pyfile": "AutonomousAI.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/g4f-0.5.0.4/g4f-0.5.0.4/g4f/Provider/not_working/AutonomousAI.py",
    "line_number": "70",
    "type_description": "B821:post",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "69\n70\t            async with session.post(api_endpoint, json=data, proxy=proxy) as response:\n71\t                await raise_for_status(response)",
    "context_snippet": "from aiohttp import ClientSession\nimport base64\nimport json\nfrom ...requests.raise_for_status import raise_for_status\nfrom ...providers.response import FinishReason\n\nclass AutonomousAI(AsyncGeneratorProvider, ProviderModelMixin):\n    url = \"https://www.autonomous.ai/anon/\"\n    api_endpoints = {\n        \"llama\": \"https://chatgpt.autonomous.ai/api/v1/ai/chat\",\n        \"qwen_coder\": \"https://chatgpt.autonomous.ai/api/v1/ai/chat\",\n        \"hermes\": \"https://chatgpt.autonomous.ai/api/v1/ai/chat-hermes\",\n        \"vision\": \"https://chatgpt.autonomous.ai/api/v1/ai/chat-vision\",\n        \"summary\": \"https://chatgpt.autonomous.ai/api/v1/ai/summary\"\n    }\n    \n    working = False\n    supports_stream = True\n    supports_system_message = True\n    supports_message_history = True\n    \n    default_model = \"llama\"\n    models = [default_model, \"qwen_coder\", \"hermes\", \"vision\", \"summary\"]\n    \n    model_aliases = {\n        \"llama-3.3-70b\": default_model,\n        \"qwen-2.5-coder-32b\": \"qwen_coder\",\n        \"hermes-3\": \"hermes\",\n        \"llama-3.2-90b\": \"vision\",\n        \"llama-3.2-70b\": \"summary\",\n    }\n\n    @classmethod\n    async def create_async_generator(\n        cls,\n        model: str,\n        messages: Messages,\n        proxy: str = None,\n        stream: bool = False,\n        **kwargs\n    ) -> AsyncResult:\n        api_endpoint = cls.api_endpoints[model]\n        headers = {\n            'accept': '*/*',\n            'accept-language': 'en-US,en;q=0.9',\n            'content-type': 'application/json',\n            'country-code': 'US',\n            'origin': 'https://www.autonomous.ai',\n            'referer': 'https://www.autonomous.ai/',\n            'time-zone': 'America/New_York',\n            'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'\n        }\n\n        async with ClientSession(headers=headers) as session:\n            message_json = json.dumps(messages)\n            encoded_message = base64.b64encode(message_json.encode()).decode(errors=\"ignore\")\n            \n            data = {\n                \"messages\": encoded_message,\n                \"threadId\": model,\n                \"stream\": stream,\n                \"aiAgent\": model\n            }\n            \n            async with session.post(api_endpoint, json=data, proxy=proxy) as response:\n                await raise_for_status(response)\n                async for chunk in response.content:\n                    if chunk:\n                        chunk_str = chunk.decode()\n                        if chunk_str == \"data: [DONE]\":\n                            continue\n                        \n                        try:\n                            # Remove \"data: \" prefix and parse JSON\n                            chunk_data = json.loads(chunk_str.replace(\"data: \", \"\"))\n                            if \"choices\" in chunk_data and chunk_data[\"choices\"]:\n                                delta = chunk_data[\"choices\"][0].get(\"delta\", {})\n                                if \"content\" in delta and delta[\"content\"]:\n                                    yield delta[\"content\"]\n                            if \"finish_reason\" in chunk_data and chunk_data[\"finish_reason\"]:\n                                yield FinishReason(chunk_data[\"finish_reason\"])\n                        except json.JSONDecodeError:\n                            continue",
    "hash_value": "9c928f54ee13f0252b57c3499ce926ac"
  }
]