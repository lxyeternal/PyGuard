[
  {
    "metadata": {
      "package_name": "torch-2.6.0-cp39-none-macosx_11_0_arm64",
      "total_matches": 4,
      "processing_date": "2025-05-18 01:49:39"
    }
  },
  {
    "pyfile": "codecache.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/torch-2.6.0-cp39-none-macosx_11_0_arm64/torch/_inductor/codecache.py",
    "line_number": "1037",
    "type_description": "B801:b64decode",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "1036\t                        assert isinstance(data, (str, bytes))\n1037\t                        content = base64.b64decode(data)\n1038\t                        yield pickle.loads(content)",
    "context_snippet": "def iterate_over_candidates() -> Generator[CompiledFxGraph, None, None]:\n    if local:\n        subdir = FxGraphCache._get_tmp_dir_for_key(key)\n        if os.path.exists(subdir):\n            for path in sorted(os.listdir(subdir)):\n                try:\n                    with open(os.path.join(subdir, path), \"rb\") as f:\n                        yield pickle.load(f)\n                except Exception:\n                    log.warning(\n                        \"fx graph cache unable to load compiled graph\",\n                        exc_info=True,\n                    )\n\n    if remote_cache:\n        try:\n            if (cache_data := remote_cache.get(key)) is not None:\n                assert isinstance(cache_data, dict)\n                data = cache_data[\"data\"]\n                assert isinstance(data, (str, bytes))\n                content = base64.b64decode(data)\n                yield pickle.loads(content)\n        except Exception:\n            log.warning(\n                \"fx graph cache unable to load compiled graph\", exc_info=True\n            )",
    "hash_value": "b11b34ae906e0ba787b801576916a32e"
  },
  {
    "pyfile": "dataloader.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/torch-2.6.0-cp39-none-macosx_11_0_arm64/torch/utils/data/dataloader.py",
    "line_number": "1584",
    "type_description": "B814:read",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "1583\t                for q in self._index_queues:\n1584\t                    q.cancel_join_thread()\n1585\t                    q.close()",
    "context_snippet": "def _shutdown_workers(self):\n    # Called when shutting down this `_MultiProcessingDataLoaderIter`.\n    # See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for details on\n    # the logic of this function.\n    if (\n        _utils is None\n        or _utils.python_exit_status is True\n        or _utils.python_exit_status is None\n    ):\n        # See (2) of the note. If Python is shutting down, do no-op.\n        return\n    # Normal exit when last reference is gone / iterator is depleted.\n    # See (1) and the second half of the note.\n    if not self._shutdown:\n        self._shutdown = True\n        try:\n            # Normal exit when last reference is gone / iterator is depleted.\n            # See (1) and the second half of the note.\n\n            # Exit `pin_memory_thread` first because exiting workers may leave\n            # corrupted data in `worker_result_queue` which `pin_memory_thread`\n            # reads from.\n            if hasattr(self, \"_pin_memory_thread\"):\n                # Use hasattr in case error happens before we set the attribute.\n                self._pin_memory_thread_done_event.set()\n                # Send something to pin_memory_thread in case it is waiting\n                # so that it can wake up and check `pin_memory_thread_done_event`\n                self._worker_result_queue.put((None, None))\n                self._pin_memory_thread.join()\n                self._worker_result_queue.cancel_join_thread()\n                self._worker_result_queue.close()\n\n            # Exit workers now.\n            self._workers_done_event.set()\n            for worker_id in range(len(self._workers)):\n                # Get number of workers from `len(self._workers)` instead of\n                # `self._num_workers` in case we error before starting all\n                # workers.\n                # If we are using workers_status with persistent_workers\n                # we have to shut it down because the worker is paused\n                if self._persistent_workers or self._workers_status[worker_id]:\n                    self._mark_worker_as_unavailable(worker_id, shutdown=True)\n            for w in self._workers:\n                # We should be able to join here, but in case anything went\n                # wrong, we set a timeout and if the workers fail to join,\n                # they are killed in the `finally` block.\n                w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n            for q in self._index_queues:\n                q.cancel_join_thread()\n                q.close()\n        finally:\n            # Even though all this function does is putting into queues that\n            # we have called `cancel_join_thread` on, weird things can\n            # happen when a worker is killed by a signal, e.g., hanging in\n            # `Event.set()`. So we need to guard this with SIGCHLD handler,\n            # and remove pids from the C side data structure only at the\n            # end.\n            #\n            # FIXME: Unfortunately, for Windows, we are missing a worker\n            #        error detection mechanism here in this function, as it\n            #        doesn't provide a SIGCHLD handler.\n            if self._worker_pids_set:\n                _utils.signal_handling._remove_worker_pids(id(self))\n                self._worker_pids_set = False\n            for w in self._workers:\n                if w.is_alive():\n                    # Existing mechanisms try to make the workers exit\n                    # peacefully, but in case that we unfortunately reach\n                    # here, which we shouldn't, (e.g., pytorch/pytorch#39570),\n                    # we kill the worker.\n                    w.terminate()",
    "hash_value": "365b5177a66b1c3eb93bd3041d3071ee"
  },
  {
    "pyfile": "_reporting.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/torch-2.6.0-cp39-none-macosx_11_0_arm64/torch/onnx/_internal/exporter/_reporting.py",
    "line_number": "183",
    "type_description": "B815:write",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "182\t            f.write(\"```python\\n\")\n183\t            f.write(str(model))\n184\t            f.write(\"\\n```\\n\\n\")",
    "context_snippet": "def create_onnx_export_report(\n    filename: str | os.PathLike,\n    formatted_traceback: str,\n    program: torch.export.ExportedProgram,\n    *,\n    decomp_comparison: str | None = None,\n    export_status: ExportStatus,\n    profile_result: str | None,\n    model: ir.Model | None = None,\n    registry: _registration.ONNXRegistry | None = None,\n    verification_result: str | None = None,\n):\n    with open(filename, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"# PyTorch ONNX Conversion Report\\n\\n\")\n        f.write(_format_export_status(export_status))\n        f.write(\"## Error messages\\n\\n\")\n        f.write(\"```pytb\\n\")\n        f.write(formatted_traceback)\n        f.write(\"\\n```\\n\\n\")\n        f.write(\"## Exported program\\n\\n\")\n        f.write(_format_exported_program(program))\n        if model is not None:\n            f.write(\"## ONNX model\\n\\n\")\n            f.write(\"```python\\n\")\n            f.write(str(model))\n            f.write(\"\\n```\\n\\n\")\n        f.write(\"## Analysis\\n\\n\")\n        _analysis.analyze(program, file=f, registry=registry)\n        if decomp_comparison is not None:\n            f.write(\"\\n## Decomposition comparison\\n\\n\")\n            f.write(decomp_comparison)\n            f.write(\"\\n\")\n        if verification_result is not None:\n            f.write(\"\\n## Verification results\\n\\n\")\n            f.write(verification_result)\n            f.write(\"\\n\")\n        if profile_result is not None:\n            f.write(\"\\n## Profiling result\\n\\n\")\n            f.write(\"```\\n\")\n            f.write(profile_result)\n            f.write(\"```\\n\")",
    "hash_value": "caba2fe1a4dde14b857a4c56d7ed5321"
  },
  {
    "pyfile": "gen.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/torch-2.6.0-cp39-none-macosx_11_0_arm64/torchgen/gen.py",
    "line_number": "2197",
    "type_description": "B815:write",
    "severity": "High",
    "confidence": "Medium",
    "original_snippet": "2196\n2197\t    core_fm.write(\"aten_interned_strings.h\", gen_aten_interned_strings)\n2198",
    "context_snippet": "def gen_headers(\n    *,\n    native_functions: Sequence[NativeFunction],\n    valid_tags: set[str],\n    grouped_native_functions: Sequence[NativeFunction | NativeFunctionsGroup],\n    structured_native_functions: Sequence[NativeFunctionsGroup],\n    static_dispatch_idx: list[BackendIndex],\n    selector: SelectiveBuilder,\n    backend_indices: dict[DispatchKey, BackendIndex],\n    core_fm: FileManager,\n    cpu_fm: FileManager,\n    device_fms: dict[str, FileManager],\n    ops_fm: FileManager,\n    dispatch_keys: Sequence[DispatchKey],\n    functions_keys: set[DispatchKey],\n    rocm: bool,\n    per_operator_headers: bool,\n) -> None:\n    if per_operator_headers:\n        gen_per_operator_headers(\n            native_functions=native_functions,\n            grouped_native_functions=grouped_native_functions,\n            static_dispatch_idx=static_dispatch_idx,\n            selector=selector,\n            backend_indices=backend_indices,\n            cpu_fm=cpu_fm,\n            device_fms=device_fms,\n            ops_fm=ops_fm,\n            dispatch_keys=dispatch_keys,\n            functions_keys=functions_keys,\n            rocm=rocm,\n        )\n    else:\n        gen_aggregated_headers(\n            native_functions=native_functions,\n            grouped_native_functions=grouped_native_functions,\n            structured_native_functions=structured_native_functions,\n            static_dispatch_idx=static_dispatch_idx,\n            selector=selector,\n            backend_indices=backend_indices,\n            cpu_fm=cpu_fm,\n            device_fms=device_fms,\n            dispatch_keys=dispatch_keys,\n            functions_keys=functions_keys,\n            rocm=rocm,\n        )\n\n    core_fm.write(\n        \"TensorBody.h\",\n        lambda: {\n            \"tensor_method_declarations\": list(\n                mapMaybe(\n                    ComputeTensorMethod(\n                        target=Target.DECLARATION,\n                        static_dispatch_backend_indices=static_dispatch_idx,\n                    ),\n                    native_functions,\n                )\n            ),\n            \"tensor_method_definitions\": list(\n                mapMaybe(\n                    ComputeTensorMethod(\n                        target=Target.DEFINITION,\n                        static_dispatch_backend_indices=static_dispatch_idx,\n                    ),\n                    native_functions,\n                )\n            ),\n        },\n    )\n\n    cpu_fm.write(\n        \"RedispatchFunctions.h\",\n        lambda: {\n            \"function_redispatch_definitions\": list(\n                mapMaybe(ComputeRedispatchFunction(), native_functions)\n            ),\n        },\n    )\n\n    cpu_fm.write(\n        \"RegistrationDeclarations.h\",\n        lambda: {\n            \"registration_declarations\": [\n                compute_registration_declarations(f, backend_indices)\n                for f in native_functions\n            ],\n        },\n    )\n\n    cpu_fm.write(\n        \"VmapGeneratedPlumbing.h\", lambda: gen_all_vmap_plumbing(native_functions)\n    )\n\n    def gen_aten_interned_strings() -> dict[str, str]:\n        attrs: set[str] = set()  # All function argument names\n        names = set()  # All ATen function names\n        for func in native_functions:\n            names.add(str(func.func.name.name))\n            # Some operators don't have a functional variant but we still create a\n            # symbol without the underscore\n            names.add(func.func.name.name.base)\n\n            attrs.update(arg.name for arg in func.func.schema_order_arguments())\n\n        # These are keywords in C++, so aren't valid symbol names\n        # https://en.cppreference.com/w/cpp/language/operator_alternative\n        names -= {\n            \"and\",\n            \"and_eq\",\n            \"bitand\",\n            \"bitor\",\n            \"compl\",\n            \"not\",\n            \"not_eq\",\n            \"or\",\n            \"or_eq\",\n            \"xor\",\n            \"xor_eq\",\n        }\n\n        return {\n            \"aten_symbols\": \" \\\\\\n\".join(\n                [f\"_(aten, {name})\" for name in sorted(names)]\n            ),\n            \"attr_symbols\": \" \\\\\\n\".join(\n                [f\"_(attr, {name})\" for name in sorted(attrs)]\n            ),\n        }\n\n    core_fm.write(\"aten_interned_strings.h\", gen_aten_interned_strings)\n\n    def gen_tags_enum() -> dict[str, str]:\n        return {\"enum_of_valid_tags\": (\",\\n\".join(sorted(valid_tags)))}\n\n    core_fm.write(\"enum_tag.h\", gen_tags_enum)\n",
    "hash_value": "a4dd410d77153c1ec301b19660cb462e"
  }
]