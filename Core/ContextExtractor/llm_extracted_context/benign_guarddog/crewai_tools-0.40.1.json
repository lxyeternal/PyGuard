[
  {
    "metadata": {
      "package_name": "crewai_tools-0.40.1",
      "report_path": "/home2/blue/Documents/PyPIAgent/Codes/tool_detect/detect_output/study/guarddog/benign/crewai_tools-0.40.1.txt",
      "total_matches": 4
    }
  },
  {
    "pyfile": "brave_search_tool.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/crewai_tools-0.40.1/crewai_tools-0.40.1/crewai_tools/tools/brave_search_tool/brave_search_tool.py",
    "line_number": "87",
    "type_description": "exfiltrate-sensitive-data",
    "original_snippet": "response = requests.get(self.search_url, headers=headers, params=payload)",
    "context_snippet": "import datetime\nimport os\nimport time\nfrom typing import Any, ClassVar, Optional, Type\n\nimport requests\nfrom crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\n\n\ndef _save_results_to_file(content: str) -> None:\n    \"\"\"Saves the search results to a file.\"\"\"\n    filename = f\"search_results_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.txt\"\n    with open(filename, \"w\") as file:\n        file.write(content)\n    print(f\"Results saved to {filename}\")\n\n\nclass BraveSearchToolSchema(BaseModel):\n    \"\"\"Input for BraveSearchTool.\"\"\"\n\n    search_query: str = Field(\n        ..., description=\"Mandatory search query you want to use to search the internet\"\n    )\n\n\nclass BraveSearchTool(BaseTool):\n    \"\"\"\n    BraveSearchTool - A tool for performing web searches using the Brave Search API.\n\n    This module provides functionality to search the internet using Brave's Search API,\n    supporting customizable result counts and country-specific searches.\n\n    Dependencies:\n        - requests\n        - pydantic\n        - python-dotenv (for API key management)\n    \"\"\"\n\n    name: str = \"Brave Web Search the internet\"\n    description: str = (\n        \"A tool that can be used to search the internet with a search_query.\"\n    )\n    args_schema: Type[BaseModel] = BraveSearchToolSchema\n    search_url: str = \"https://api.search.brave.com/res/v1/web/search\"\n    country: Optional[str] = \"\"\n    n_results: int = 10\n    save_file: bool = False\n    _last_request_time: ClassVar[float] = 0\n    _min_request_interval: ClassVar[float] = 1.0  # seconds\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if \"BRAVE_API_KEY\" not in os.environ:\n            raise ValueError(\n                \"BRAVE_API_KEY environment variable is required for BraveSearchTool\"\n            )\n\n    def _run(\n        self,\n        **kwargs: Any,\n    ) -> Any:\n        current_time = time.time()\n        if (current_time - self._last_request_time) < self._min_request_interval:\n            time.sleep(\n                self._min_request_interval - (current_time - self._last_request_time)\n            )\n        BraveSearchTool._last_request_time = time.time()\n        try:\n            search_query = kwargs.get(\"search_query\") or kwargs.get(\"query\")\n            if not search_query:\n                raise ValueError(\"Search query is required\")\n\n            save_file = kwargs.get(\"save_file\", self.save_file)\n            n_results = kwargs.get(\"n_results\", self.n_results)\n\n            payload = {\"q\": search_query, \"count\": n_results}\n\n            if self.country != \"\":\n                payload[\"country\"] = self.country\n\n            headers = {\n                \"X-Subscription-Token\": os.environ[\"BRAVE_API_KEY\"],\n                \"Accept\": \"application/json\",\n            }\n\n            response = requests.get(self.search_url, headers=headers, params=payload)\n            response.raise_for_status()  # Handle non-200 responses\n            results = response.json()\n\n            if \"web\" in results:\n                results = results[\"web\"][\"results\"]\n                string = []\n                for result in results:\n                    try:\n                        string.append(\n                            \"\\n\".join(\n                                [\n                                    f\"Title: {result['title']}\",\n                                    f\"Link: {result['url']}\",\n                                    f\"Snippet: {result['description']}\",\n                                    \"---\",\n                                ]\n                            )\n                        )\n                    except KeyError:\n                        continue\n\n            content = \"\\n\".join(string)\n        except requests.RequestException as e:\n            return f\"Error performing search: {str(e)}\"\n        except KeyError as e:\n            return f\"Error parsing search results: {str(e)}\"\n        if save_file:\n            _save_results_to_file(content)\n            return f\"\\nSearch results: {content}\\n\"\n        else:\n            return content",
    "hash_value": "467099438ffc5e58ee7ce793e0d4bf5d",
    "detection_index": 1
  },
  {
    "pyfile": "serper_dev_tool.py",
    "full_path": "/home2/blue/Documents/PyPIAgent/Dataset/study/unzip_benign/crewai_tools-0.40.1/crewai_tools-0.40.1/crewai_tools/tools/serper_dev_tool/serper_dev_tool.py",
    "line_number": "169",
    "type_description": "exfiltrate-sensitive-data",
    "original_snippet": "response = requests.post(\nsearch_url, headers=headers, json=json.loads(payload), timeout=10\n)",
    "context_snippet": "import os\nimport json\nimport requests\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass SerperDevTool(BaseTool):\n    ...\n    def _make_api_request(self, search_query: str, search_type: str) -> dict:\n        \"\"\"Make API request to Serper.\"\"\"\n        search_url = self._get_search_url(search_type)\n        payload = {\"q\": search_query, \"num\": self.n_results}\n\n        if self.country != \"\":\n            payload[\"gl\"] = self.country\n        if self.location != \"\":\n            payload[\"location\"] = self.location\n        if self.locale != \"\":\n            payload[\"hl\"] = self.locale\n\n        headers = {\n            \"X-API-KEY\": os.environ[\"SERPER_API_KEY\"],\n            \"content-type\": \"application/json\",\n        }\n        payload = json.dumps(payload)\n\n        response = None\n        try:\n            response = requests.post(\n                search_url, headers=headers, json=json.loads(payload), timeout=10\n            )\n            response.raise_for_status()\n            results = response.json()\n            if not results:\n                logger.error(\"Empty response from Serper API\")\n                raise ValueError(\"Empty response from Serper API\")\n            return results\n        except requests.exceptions.RequestException as e:\n            error_msg = f\"Error making request to Serper API: {e}\"\n            if response is not None and hasattr(response, \"content\"):\n                error_msg += f\"\\nResponse content: {response.content}\"\n            logger.error(error_msg)\n            raise\n        except json.JSONDecodeError as e:\n            if response is not None and hasattr(response, \"content\"):\n                logger.error(f\"Error decoding JSON response: {e}\")\n                logger.error(f\"Response content: {response.content}\")\n            else:\n                logger.error(\n                    f\"Error decoding JSON response: {e} (No response content available)\"\n                )\n            raise\n\n# Data dependencies:\n# - search_url: set by self._get_search_url(search_type)\n# - headers: uses os.environ[\"SERPER_API_KEY\"]\n# - payload: built from search_query, self.n_results, self.country, self.location, self.locale\n# - requests: imported\n# - json: imported\n# - logger: imported and configured\n# - response: local variable",
    "hash_value": "d531e85b7faaf2efd2b8a82ba1263c7a",
    "detection_index": 1
  }
]