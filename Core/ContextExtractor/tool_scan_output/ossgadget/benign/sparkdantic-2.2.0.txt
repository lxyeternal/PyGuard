[31m--[ [0m[34mMatch #[0m[33m1[0m[34m of [0m[33m6[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/sparkdantic-2.2.0/src/sparkdantic/model.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m239 | [0m[35m    Raises:[0m
[30;1m240 | [0m[35m        TypeError: If the type is not recognized in the type map.[0m
[30;1m241 | [0m[35m    """[0m
[30;1m242 | [0m[35m    spark_type = _type_mapping.get(t)[0m
[30;1m243 | [0m[35m    if spark_type is None:[0m
[30;1m244 | [0m[35m        raise TypeError(f'Type {t} not recognized')[0m
[30;1m245 | [0m[35m    return spark_type[0m

[31m--[ [0m[34mMatch #[0m[33m2[0m[34m of [0m[33m6[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/sparkdantic-2.2.0/src/sparkdantic/model.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m157 | [0m[35m            name = _get_field_alias(name, info, mode)[0m
[30;1m158 | [0m[35m[0m
[30;1m159 | [0m[35m        field_info_extra = info.json_schema_extra or {}[0m
[30;1m160 | [0m[35m        override = field_info_extra.get('spark_type')[0m
[30;1m161 | [0m[35m        annotation_or_return_type = _get_annotation_or_return_type(info)[0m
[30;1m162 | [0m[35m        field_type = _get_union_type_arg(annotation_or_return_type)[0m
[30;1m163 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m3[0m[34m of [0m[33m6[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/sparkdantic-2.2.0/PKG-INFO[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m50 | [0m[35m[0m
[30;1m51 | [0m[35mWith PySpark:[0m
[30;1m52 | [0m[35m[0m
[30;1m53 | [0m[35m```shell[0m
[30;1m54 | [0m[35mpip install "sparkdantic[pyspark]"[0m
[30;1m55 | [0m[35m```[0m
[30;1m56 | [0m[35m[0m
[30;1m57 | [0m[35m### Supported PySpark versions[0m

[31m--[ [0m[34mMatch #[0m[33m4[0m[34m of [0m[33m6[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/sparkdantic-2.2.0/PKG-INFO[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m42 | [0m[35m[0m
[30;1m43 | [0m[35mWithout PySpark:[0m
[30;1m44 | [0m[35m[0m
[30;1m45 | [0m[35m```shell[0m
[30;1m46 | [0m[35mpip install sparkdantic[0m
[30;1m47 | [0m[35m```[0m
[30;1m48 | [0m[35m[0m
[30;1m49 | [0m[35m> Note: only **JSON** schema generation features are available without PySpark installed. If you att[0m

[31m--[ [0m[34mMatch #[0m[33m5[0m[34m of [0m[33m6[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/sparkdantic-2.2.0/README.md[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m30 | [0m[35m[0m
[30;1m31 | [0m[35mWith PySpark:[0m
[30;1m32 | [0m[35m[0m
[30;1m33 | [0m[35m```shell[0m
[30;1m34 | [0m[35mpip install "sparkdantic[pyspark]"[0m
[30;1m35 | [0m[35m```[0m
[30;1m36 | [0m[35m[0m
[30;1m37 | [0m[35m### Supported PySpark versions[0m

[31m--[ [0m[34mMatch #[0m[33m6[0m[34m of [0m[33m6[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/sparkdantic-2.2.0/README.md[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m22 | [0m[35m[0m
[30;1m23 | [0m[35mWithout PySpark:[0m
[30;1m24 | [0m[35m[0m
[30;1m25 | [0m[35m```shell[0m
[30;1m26 | [0m[35mpip install sparkdantic[0m
[30;1m27 | [0m[35m```[0m
[30;1m28 | [0m[35m[0m
[30;1m29 | [0m[35m> Note: only **JSON** schema generation features are available without PySpark installed. If you att[0m

6 matches found.
