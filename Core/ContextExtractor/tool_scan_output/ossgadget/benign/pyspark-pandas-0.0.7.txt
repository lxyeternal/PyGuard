[31m--[ [0m[34mMatch #[0m[33m1[0m[34m of [0m[33m9[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/pyspark-pandas-0.0.7/pyspark_pandas/dataframe_rdd.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m378 | [0m[35m    #     pd.algos.is_monotonic_float64(bins.values[:, col])[0][0m
[30;1m379 | [0m[35m    #     for col in range(bins.shape[1]))[0m
[30;1m380 | [0m[35m    # pd.algos.is_monotonic_float64(bins.values[:, 1])[0m
[30;1m381 | [0m[35m    # # find the count of values in each bin on all dataframes[0m
[30;1m382 | [0m[35m    # # count the values in each bin on all distributed frames[0m
[30;1m383 | [0m[35m    # log.debug('count values for each bin on each distributed dataframe')[0m
[30;1m384 | [0m[35m    # def make_histogram(key_df):[0m

[31m--[ [0m[34mMatch #[0m[33m2[0m[34m of [0m[33m9[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/pyspark-pandas-0.0.7/pyspark_pandas/dataframe_rdd.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m309 | [0m[35m        Assume the rdd has the form: (unique_key, pandas.DataFrame(...))[0m
[30;1m310 | [0m[35m[0m
[30;1m311 | [0m[35m        counts_bykey - a list of (key, num_vals_per_column) tuples for every[0m
[30;1m312 | [0m[35m            (key, dataframe) in the rdd you wish to sample from.  This is both[0m
[30;1m313 | [0m[35m            an optimization and a way to sample from a specific subset[0m
[30;1m314 | [0m[35m            of frames in the rdd.  By default, counts_bykey does not[0m
[30;1m315 | [0m[35m            count null values.  If min_bound or max_bound are also[0m

[31m--[ [0m[34mMatch #[0m[33m3[0m[34m of [0m[33m9[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/pyspark-pandas-0.0.7/pyspark_pandas/dataframe_rdd.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m214 | [0m[35m                                     min_bound=min_bound, max_bound=max_bound)[0m
[30;1m215 | [0m[35m            # pivot = self.mean(min_bound=min_bound, max_bound=max_bound)[0m
[30;1m216 | [0m[35m            # TODO: try: pivot = self.mean()  # per column mean on first try,[0m
[30;1m217 | [0m[35m            # then next try shouldn't be a mean but some sort of gradient[0m
[30;1m218 | [0m[35m            # function that approaches the median.  maybe similar to:[0m
[30;1m219 | [0m[35m            # pivot = (oldmean/(oldmax-oldmin) + mean/(max-min)) / 2*(max-min)[0m
[30;1m220 | [0m[35m            # TODO: have sampleValue return updated counts via with_counts=True[0m

[31m--[ [0m[34mMatch #[0m[33m4[0m[34m of [0m[33m9[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/pyspark-pandas-0.0.7/pyspark_pandas/dataframe_rdd.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m93 | [0m[35m    def get_nbytes(self, index=True, columns=True, values=True,[0m
[30;1m94 | [0m[35m                   per_partition=False, per_frame=False):[0m
[30;1m95 | [0m[35m        """[0m
[30;1m96 | [0m[35m        Find out the total number of bytes in memory the distributed frames[0m
[30;1m97 | [0m[35m        use.[0m
[30;1m98 | [0m[35m[0m
[30;1m99 | [0m[35m        If `per_partition` is True, calculate the bytes for each partition.[0m

[31m--[ [0m[34mMatch #[0m[33m5[0m[34m of [0m[33m9[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/pyspark-pandas-0.0.7/README[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m1 | [0m[35mThere is already an existing project,[0m
[30;1m2 | [0m[35m[SparklingPandas](https://github.com/holdenk/sparklingpandas), that[0m
[30;1m3 | [0m[35mintegrates pandas and pyspark.  You should look at that one as this[0m
[30;1m4 | [0m[35mproject may get merged into that one.  This project aims to provide[0m
[30;1m5 | [0m[35museful tools and algorithms for distributing Pandas objects on Spark.[0m
[30;1m6 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m6[0m[34m of [0m[33m9[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/pyspark-pandas-0.0.7/pyspark_pandas/examples.py[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m23 | [0m[35m[0m
[30;1m24 | [0m[35mdef get_rdd(seed=0):[0m
[30;1m25 | [0m[35m    np.random.seed(seed)[0m
[30;1m26 | [0m[35m    sc = spark_context()[0m
[30;1m27 | [0m[35m    rdd = sc.parallelize([0m
[30;1m28 | [0m[35m        [('key.%s' % key, make_frame()) for key in range(10)], 4)[0m
[30;1m29 | [0m[35m    rdd = DataFrameRDD(rdd)[0m

[31m--[ [0m[34mMatch #[0m[33m7[0m[34m of [0m[33m9[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/pyspark-pandas-0.0.7/README.md[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m1 | [0m[35mThere is already an existing project,[0m
[30;1m2 | [0m[35m[SparklingPandas](https://github.com/holdenk/sparklingpandas), that[0m
[30;1m3 | [0m[35mintegrates pandas and pyspark.  You should look at that one as this[0m
[30;1m4 | [0m[35mproject may get merged into that one.  This project aims to provide[0m
[30;1m5 | [0m[35museful tools and algorithms for distributing Pandas objects on Spark.[0m
[30;1m6 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m8[0m[34m of [0m[33m9[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/pyspark-pandas-0.0.7/code_linter.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m101 | [0m[35m    for file_name in files:[0m
[30;1m102 | [0m[35m        if not matches_file(file_name, check.get('match_files', [])):[0m
[30;1m103 | [0m[35m            continue[0m
[30;1m104 | [0m[35m        if matches_file(file_name, check.get('ignore_files', [])):[0m
[30;1m105 | [0m[35m            continue[0m
[30;1m106 | [0m[35m        print 'checking file: %s' % file_name[0m
[30;1m107 | [0m[35m        process = subprocess.Popen(check['command'] % file_name,[0m

[31m--[ [0m[34mMatch #[0m[33m9[0m[34m of [0m[33m9[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/pyspark-pandas-0.0.7/code_linter.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m99 | [0m[35m    clean = True[0m
[30;1m100 | [0m[35m    print check['start_msg'][0m
[30;1m101 | [0m[35m    for file_name in files:[0m
[30;1m102 | [0m[35m        if not matches_file(file_name, check.get('match_files', [])):[0m
[30;1m103 | [0m[35m            continue[0m
[30;1m104 | [0m[35m        if matches_file(file_name, check.get('ignore_files', [])):[0m
[30;1m105 | [0m[35m            continue[0m

9 matches found.
