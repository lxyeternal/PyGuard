[31m--[ [0m[34mMatch #[0m[33m1[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000800[0m
       Tag: [34mSecurity.Backdoor.Obfuscation[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32mbase64|encodedcommand|obfuscate[0m
[30;1m59 | [0m[35m            )[0m
[30;1m60 | [0m[35m[0m
[30;1m61 | [0m[35m    def _upload_notebook(self, path: str, compiled_code: str) -> None:[0m
[30;1m62 | [0m[35m        b64_encoded_content = base64.b64encode(compiled_code.encode()).decode()[0m
[30;1m63 | [0m[35m        response = requests.post([0m
[30;1m64 | [0m[35m            f"https://{self.credentials.host}/api/2.0/workspace/import",[0m
[30;1m65 | [0m[35m            headers=self.auth_header,[0m

[31m--[ [0m[34mMatch #[0m[33m2[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000800[0m
       Tag: [34mSecurity.Backdoor.Obfuscation[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32mbase64|encodedcommand|obfuscate[0m
[30;1m1 | [0m[35mimport base64[0m
[30;1m2 | [0m[35mimport time[0m
[30;1m3 | [0m[35mimport requests[0m
[30;1m4 | [0m[35mfrom typing import Any, Dict, Callable, Iterable[0m

[31m--[ [0m[34mMatch #[0m[33m3[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000800[0m
       Tag: [34mSecurity.Backdoor.Obfuscation[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/connections.py[0m
   Pattern: [32mbase64|encodedcommand|obfuscate[0m
[30;1m431 | [0m[35m                    transport = THttpClient.THttpClient(conn_url)[0m
[30;1m432 | [0m[35m[0m
[30;1m433 | [0m[35m                    raw_token = "token:{}".format(creds.token).encode()[0m
[30;1m434 | [0m[35m                    token = base64.standard_b64encode(raw_token).decode()[0m
[30;1m435 | [0m[35m                    transport.setCustomHeaders({"Authorization": "Basic {}".format(token)})[0m
[30;1m436 | [0m[35m[0m
[30;1m437 | [0m[35m                    conn = hive.connect([0m

[31m--[ [0m[34mMatch #[0m[33m4[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000800[0m
       Tag: [34mSecurity.Backdoor.Obfuscation[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/connections.py[0m
   Pattern: [32mbase64|encodedcommand|obfuscate[0m
[30;1m43 | [0m[35mexcept ImportError:[0m
[30;1m44 | [0m[35m    pass  # done deliberately: setting modules to None explicitly violates MyPy contracts by degradi[0m
[30;1m45 | [0m[35m[0m
[30;1m46 | [0m[35mimport base64[0m
[30;1m47 | [0m[35mimport time[0m
[30;1m48 | [0m[35m[0m
[30;1m49 | [0m[35mlogger = AdapterLogger("Spark")[0m

[31m--[ [0m[34mMatch #[0m[33m5[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/connections.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m631 | [0m[35m                sasl_client = SASLClient(host, kerberos_service_name, mechanism=sasl_auth)[0m
[30;1m632 | [0m[35m            elif sasl_auth == "PLAIN":[0m
[30;1m633 | [0m[35m                sasl_client = SASLClient([0m
[30;1m634 | [0m[35m                    host, mechanism=sasl_auth, username=username, password=password[0m
[30;1m635 | [0m[35m                )[0m
[30;1m636 | [0m[35m            else:[0m
[30;1m637 | [0m[35m                raise AssertionError[0m

[31m--[ [0m[34mMatch #[0m[33m6[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/connections.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m631 | [0m[35m                sasl_client = SASLClient(host, kerberos_service_name, mechanism=sasl_auth)[0m
[30;1m632 | [0m[35m            elif sasl_auth == "PLAIN":[0m
[30;1m633 | [0m[35m                sasl_client = SASLClient([0m
[30;1m634 | [0m[35m                    host, mechanism=sasl_auth, username=username, password=password[0m
[30;1m635 | [0m[35m                )[0m
[30;1m636 | [0m[35m            else:[0m
[30;1m637 | [0m[35m                raise AssertionError[0m

[31m--[ [0m[34mMatch #[0m[33m7[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/connections.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m624 | [0m[35m            if password is None:[0m
[30;1m625 | [0m[35m                # Password doesn't matter in NONE mode, just needs[0m
[30;1m626 | [0m[35m                # to be nonempty.[0m
[30;1m627 | [0m[35m                password = "x"[0m
[30;1m628 | [0m[35m[0m
[30;1m629 | [0m[35m        def sasl_factory() -> SASLClient:[0m
[30;1m630 | [0m[35m            if sasl_auth == "GSSAPI":[0m

[31m--[ [0m[34mMatch #[0m[33m8[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/connections.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m622 | [0m[35m        else:[0m
[30;1m623 | [0m[35m            sasl_auth = "PLAIN"[0m
[30;1m624 | [0m[35m            if password is None:[0m
[30;1m625 | [0m[35m                # Password doesn't matter in NONE mode, just needs[0m
[30;1m626 | [0m[35m                # to be nonempty.[0m
[30;1m627 | [0m[35m                password = "x"[0m
[30;1m628 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m9[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/connections.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m621 | [0m[35m            sasl_auth = "GSSAPI"[0m
[30;1m622 | [0m[35m        else:[0m
[30;1m623 | [0m[35m            sasl_auth = "PLAIN"[0m
[30;1m624 | [0m[35m            if password is None:[0m
[30;1m625 | [0m[35m                # Password doesn't matter in NONE mode, just needs[0m
[30;1m626 | [0m[35m                # to be nonempty.[0m
[30;1m627 | [0m[35m                password = "x"[0m

[31m--[ [0m[34mMatch #[0m[33m10[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/connections.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m601 | [0m[35m    username: str,[0m
[30;1m602 | [0m[35m    auth: str,[0m
[30;1m603 | [0m[35m    kerberos_service_name: str,[0m
[30;1m604 | [0m[35m    password: Optional[str] = None,[0m
[30;1m605 | [0m[35m) -> "thrift_sasl.TSaslClientTransport":[0m
[30;1m606 | [0m[35m    transport = None[0m
[30;1m607 | [0m[35m    if port is None:[0m

[31m--[ [0m[34mMatch #[0m[33m11[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/connections.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m462 | [0m[35m                            username=creds.user,[0m
[30;1m463 | [0m[35m                            auth=creds.auth,[0m
[30;1m464 | [0m[35m                            kerberos_service_name=creds.kerberos_service_name,[0m
[30;1m465 | [0m[35m                            password=creds.password,[0m
[30;1m466 | [0m[35m                            configuration=creds.server_side_parameters,[0m
[30;1m467 | [0m[35m                        )  # noqa[0m
[30;1m468 | [0m[35m                    handle = PyhiveConnectionWrapper(conn)[0m

[31m--[ [0m[34mMatch #[0m[33m12[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/connections.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m462 | [0m[35m                            username=creds.user,[0m
[30;1m463 | [0m[35m                            auth=creds.auth,[0m
[30;1m464 | [0m[35m                            kerberos_service_name=creds.kerberos_service_name,[0m
[30;1m465 | [0m[35m                            password=creds.password,[0m
[30;1m466 | [0m[35m                            configuration=creds.server_side_parameters,[0m
[30;1m467 | [0m[35m                        )  # noqa[0m
[30;1m468 | [0m[35m                    handle = PyhiveConnectionWrapper(conn)[0m

[31m--[ [0m[34mMatch #[0m[33m13[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/connections.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m449 | [0m[35m                            username=creds.user,[0m
[30;1m450 | [0m[35m                            auth=creds.auth,[0m
[30;1m451 | [0m[35m                            kerberos_service_name=creds.kerberos_service_name,[0m
[30;1m452 | [0m[35m                            password=creds.password,[0m
[30;1m453 | [0m[35m                        )[0m
[30;1m454 | [0m[35m                        conn = hive.connect([0m
[30;1m455 | [0m[35m                            thrift_transport=transport,[0m

[31m--[ [0m[34mMatch #[0m[33m14[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/connections.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m449 | [0m[35m                            username=creds.user,[0m
[30;1m450 | [0m[35m                            auth=creds.auth,[0m
[30;1m451 | [0m[35m                            kerberos_service_name=creds.kerberos_service_name,[0m
[30;1m452 | [0m[35m                            password=creds.password,[0m
[30;1m453 | [0m[35m                        )[0m
[30;1m454 | [0m[35m                        conn = hive.connect([0m
[30;1m455 | [0m[35m                            thrift_transport=transport,[0m

[31m--[ [0m[34mMatch #[0m[33m15[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/connections.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m73 | [0m[35m    endpoint: Optional[str] = None[0m
[30;1m74 | [0m[35m    token: Optional[str] = None[0m
[30;1m75 | [0m[35m    user: Optional[str] = None[0m
[30;1m76 | [0m[35m    password: Optional[str] = None[0m
[30;1m77 | [0m[35m    port: int = 443[0m
[30;1m78 | [0m[35m    auth: Optional[str] = None[0m
[30;1m79 | [0m[35m    kerberos_service_name: Optional[str] = None[0m

[31m--[ [0m[34mMatch #[0m[33m16[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000702[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Environment[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/README.md[0m
   Pattern: [32m(env|environment).{1,50}(get|post|curl|nc|invoke-restmethod)[0m
[30;1m41 | [0m[35m[0m
[30;1m42 | [0m[35m## Running locally[0m
[30;1m43 | [0m[35m[0m
[30;1m44 | [0m[35mA `docker-compose` environment starts a Spark Thrift server and a Postgres database as a Hive Metast[0m
[30;1m45 | [0m[35mNote: dbt-spark now supports Spark 3.3.2.[0m
[30;1m46 | [0m[35m[0m
[30;1m47 | [0m[35mThe following command starts two docker containers:[0m

[31m--[ [0m[34mMatch #[0m[33m17[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000702[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Environment[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/.gitignore[0m
   Pattern: [32m(env|environment).{1,50}(get|post|curl|nc|invoke-restmethod)[0m
[30;1m90 | [0m[35m# pipenv[0m
[30;1m91 | [0m[35m#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.[0m
[30;1m92 | [0m[35m#   However, in case of collaboration, if having platform-specific dependencies or dependencies[0m
[30;1m93 | [0m[35m#   having no cross-platform support, pipenv may install dependencies that don't work, or not[0m
[30;1m94 | [0m[35m#   install all needed dependencies.[0m
[30;1m95 | [0m[35m#Pipfile.lock[0m
[30;1m96 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m18[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000702[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Environment[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/.gitignore[0m
   Pattern: [32m(env|environment).{1,50}(get|post|curl|nc|invoke-restmethod)[0m
[30;1m88 | [0m[35m# .python-version[0m
[30;1m89 | [0m[35m[0m
[30;1m90 | [0m[35m# pipenv[0m
[30;1m91 | [0m[35m#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.[0m
[30;1m92 | [0m[35m#   However, in case of collaboration, if having platform-specific dependencies or dependencies[0m
[30;1m93 | [0m[35m#   having no cross-platform support, pipenv may install dependencies that don't work, or not[0m
[30;1m94 | [0m[35m#   install all needed dependencies.[0m

[31m--[ [0m[34mMatch #[0m[33m19[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000702[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Environment[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dbt_spark-1.9.2/PKG-INFO[0m
   Pattern: [32m(env|environment).{1,50}(get|post|curl|nc|invoke-restmethod)[0m
[30;1m82 | [0m[35m[0m
[30;1m83 | [0m[35m## Running locally[0m
[30;1m84 | [0m[35m[0m
[30;1m85 | [0m[35mA `docker-compose` environment starts a Spark Thrift server and a Postgres database as a Hive Metast[0m
[30;1m86 | [0m[35mNote: dbt-spark now supports Spark 3.3.2.[0m
[30;1m87 | [0m[35m[0m
[30;1m88 | [0m[35mThe following command starts two docker containers:[0m

[31m--[ [0m[34mMatch #[0m[33m20[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/snapshot.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m140 | [0m[35m[0m
[30;1m141 | [0m[35m  {% else %}[0m
[30;1m142 | [0m[35m[0m
[30;1m143 | [0m[35m      {% set columns = config.get("snapshot_table_column_names") or get_snapshot_table_column_names([0m
[30;1m144 | [0m[35m[0m
[30;1m145 | [0m[35m      {{ adapter.valid_snapshot_target(target_relation, columns) }}[0m
[30;1m146 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m21[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/snapshot.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m93 | [0m[35m  {%- set strategy_name = config.get('strategy') -%}[0m
[30;1m94 | [0m[35m  {%- set unique_key = config.get('unique_key') %}[0m
[30;1m95 | [0m[35m  {%- set file_format = config.get('file_format') or 'parquet' -%}[0m
[30;1m96 | [0m[35m  {%- set grant_config = config.get('grants') -%}[0m
[30;1m97 | [0m[35m[0m
[30;1m98 | [0m[35m  {% set target_relation_exists, target_relation = get_or_create_relation([0m
[30;1m99 | [0m[35m          database=none,[0m

[31m--[ [0m[34mMatch #[0m[33m22[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/snapshot.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m92 | [0m[35m[0m
[30;1m93 | [0m[35m  {%- set strategy_name = config.get('strategy') -%}[0m
[30;1m94 | [0m[35m  {%- set unique_key = config.get('unique_key') %}[0m
[30;1m95 | [0m[35m  {%- set file_format = config.get('file_format') or 'parquet' -%}[0m
[30;1m96 | [0m[35m  {%- set grant_config = config.get('grants') -%}[0m
[30;1m97 | [0m[35m[0m
[30;1m98 | [0m[35m  {% set target_relation_exists, target_relation = get_or_create_relation([0m

[31m--[ [0m[34mMatch #[0m[33m23[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/snapshot.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m91 | [0m[35m  {%- set target_table = model.get('alias', model.get('name')) -%}[0m
[30;1m92 | [0m[35m[0m
[30;1m93 | [0m[35m  {%- set strategy_name = config.get('strategy') -%}[0m
[30;1m94 | [0m[35m  {%- set unique_key = config.get('unique_key') %}[0m
[30;1m95 | [0m[35m  {%- set file_format = config.get('file_format') or 'parquet' -%}[0m
[30;1m96 | [0m[35m  {%- set grant_config = config.get('grants') -%}[0m
[30;1m97 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m24[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/snapshot.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m90 | [0m[35m[0m
[30;1m91 | [0m[35m  {%- set target_table = model.get('alias', model.get('name')) -%}[0m
[30;1m92 | [0m[35m[0m
[30;1m93 | [0m[35m  {%- set strategy_name = config.get('strategy') -%}[0m
[30;1m94 | [0m[35m  {%- set unique_key = config.get('unique_key') %}[0m
[30;1m95 | [0m[35m  {%- set file_format = config.get('file_format') or 'parquet' -%}[0m
[30;1m96 | [0m[35m  {%- set grant_config = config.get('grants') -%}[0m

[31m--[ [0m[34mMatch #[0m[33m25[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/snapshot.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m88 | [0m[35m[0m
[30;1m89 | [0m[35m{% materialization snapshot, adapter='spark' %}[0m
[30;1m90 | [0m[35m[0m
[30;1m91 | [0m[35m  {%- set target_table = model.get('alias', model.get('name')) -%}[0m
[30;1m92 | [0m[35m[0m
[30;1m93 | [0m[35m  {%- set strategy_name = config.get('strategy') -%}[0m
[30;1m94 | [0m[35m  {%- set unique_key = config.get('unique_key') %}[0m

[31m--[ [0m[34mMatch #[0m[33m26[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/snapshot.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m88 | [0m[35m[0m
[30;1m89 | [0m[35m{% materialization snapshot, adapter='spark' %}[0m
[30;1m90 | [0m[35m[0m
[30;1m91 | [0m[35m  {%- set target_table = model.get('alias', model.get('name')) -%}[0m
[30;1m92 | [0m[35m[0m
[30;1m93 | [0m[35m  {%- set strategy_name = config.get('strategy') -%}[0m
[30;1m94 | [0m[35m  {%- set unique_key = config.get('unique_key') %}[0m

[31m--[ [0m[34mMatch #[0m[33m27[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/snapshot.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m25 | [0m[35m    on DBT_INTERNAL_SOURCE.{{ columns.dbt_scd_id }} = DBT_INTERNAL_DEST.{{ columns.dbt_scd_id }}[0m
[30;1m26 | [0m[35m    when matched[0m
[30;1m27 | [0m[35m     {% if config.get("dbt_valid_to_current") %}[0m
[30;1m28 | [0m[35m       and ( DBT_INTERNAL_DEST.{{ columns.dbt_valid_to }} = {{ config.get('dbt_valid_to_current') }}[0m
[30;1m29 | [0m[35m             DBT_INTERNAL_DEST.{{ columns.dbt_valid_to }} is null )[0m
[30;1m30 | [0m[35m     {% else %}[0m
[30;1m31 | [0m[35m       and DBT_INTERNAL_DEST.{{ columns.dbt_valid_to }} is null[0m

[31m--[ [0m[34mMatch #[0m[33m28[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/snapshot.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m24 | [0m[35m    {% endif %}[0m
[30;1m25 | [0m[35m    on DBT_INTERNAL_SOURCE.{{ columns.dbt_scd_id }} = DBT_INTERNAL_DEST.{{ columns.dbt_scd_id }}[0m
[30;1m26 | [0m[35m    when matched[0m
[30;1m27 | [0m[35m     {% if config.get("dbt_valid_to_current") %}[0m
[30;1m28 | [0m[35m       and ( DBT_INTERNAL_DEST.{{ columns.dbt_valid_to }} = {{ config.get('dbt_valid_to_current') }}[0m
[30;1m29 | [0m[35m             DBT_INTERNAL_DEST.{{ columns.dbt_valid_to }} is null )[0m
[30;1m30 | [0m[35m     {% else %}[0m

[31m--[ [0m[34mMatch #[0m[33m29[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/snapshot.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m13 | [0m[35m[0m
[30;1m14 | [0m[35m[0m
[30;1m15 | [0m[35m{% macro spark__snapshot_merge_sql(target, source, insert_cols) -%}[0m
[30;1m16 | [0m[35m    {%- set columns = config.get("snapshot_table_column_names") or get_snapshot_table_column_names()[0m
[30;1m17 | [0m[35m[0m
[30;1m18 | [0m[35m    merge into {{ target }} as DBT_INTERNAL_DEST[0m
[30;1m19 | [0m[35m    {% if target.is_iceberg %}[0m

[31m--[ [0m[34mMatch #[0m[33m30[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/snapshot.sql[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m31 | [0m[35m       and DBT_INTERNAL_DEST.{{ columns.dbt_valid_to }} is null[0m
[30;1m32 | [0m[35m     {% endif %}[0m
[30;1m33 | [0m[35m     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')[0m
[30;1m34 | [0m[35m        then update[0m
[30;1m35 | [0m[35m        set {{ columns.dbt_valid_to }} = DBT_INTERNAL_SOURCE.{{ columns.dbt_valid_to }}[0m
[30;1m36 | [0m[35m[0m
[30;1m37 | [0m[35m    when not matched[0m
[30;1m38 | [0m[35m     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'[0m

[31m--[ [0m[34mMatch #[0m[33m31[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m266 | [0m[35m            )[0m
[30;1m267 | [0m[35m[0m
[30;1m268 | [0m[35m    def submit(self, compiled_code: str) -> None:[0m
[30;1m269 | [0m[35m        if self.parsed_model["config"].get("create_notebook", False):[0m
[30;1m270 | [0m[35m            self._submit_through_notebook(compiled_code, {"existing_cluster_id": self.cluster_id})[0m
[30;1m271 | [0m[35m        else:[0m
[30;1m272 | [0m[35m            context = DBContext(self.credentials, self.cluster_id, self.auth_header)[0m

[31m--[ [0m[34mMatch #[0m[33m32[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m244 | [0m[35m[0m
[30;1m245 | [0m[35m    def status(self, context_id: str, command_id: str) -> Dict[str, Any]:[0m
[30;1m246 | [0m[35m        # https://docs.databricks.com/dev-tools/api/1.2/index.html#get-information-about-a-command[0m
[30;1m247 | [0m[35m        response = requests.get([0m
[30;1m248 | [0m[35m            f"https://{self.host}/api/1.2/commands/status",[0m
[30;1m249 | [0m[35m            headers=self.auth_header,[0m
[30;1m250 | [0m[35m            params={[0m

[31m--[ [0m[34mMatch #[0m[33m33[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m228 | [0m[35m[0m
[30;1m229 | [0m[35m    def execute(self, context_id: str, command: str) -> str:[0m
[30;1m230 | [0m[35m        # https://docs.databricks.com/dev-tools/api/1.2/index.html#run-a-command[0m
[30;1m231 | [0m[35m        response = requests.post([0m
[30;1m232 | [0m[35m            f"https://{self.host}/api/1.2/commands/execute",[0m
[30;1m233 | [0m[35m            headers=self.auth_header,[0m
[30;1m234 | [0m[35m            json={[0m

[31m--[ [0m[34mMatch #[0m[33m34[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m207 | [0m[35m[0m
[30;1m208 | [0m[35m    def destroy(self, context_id: str) -> str:[0m
[30;1m209 | [0m[35m        # https://docs.databricks.com/dev-tools/api/1.2/index.html#delete-an-execution-context[0m
[30;1m210 | [0m[35m        response = requests.post([0m
[30;1m211 | [0m[35m            f"https://{self.host}/api/1.2/contexts/destroy",[0m
[30;1m212 | [0m[35m            headers=self.auth_header,[0m
[30;1m213 | [0m[35m            json={[0m

[31m--[ [0m[34mMatch #[0m[33m35[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m193 | [0m[35m[0m
[30;1m194 | [0m[35m    def create(self) -> str:[0m
[30;1m195 | [0m[35m        # https://docs.databricks.com/dev-tools/api/1.2/index.html#create-an-execution-context[0m
[30;1m196 | [0m[35m        response = requests.post([0m
[30;1m197 | [0m[35m            f"https://{self.host}/api/1.2/contexts/create",[0m
[30;1m198 | [0m[35m            headers=self.auth_header,[0m
[30;1m199 | [0m[35m            json={[0m

[31m--[ [0m[34mMatch #[0m[33m36[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m177 | [0m[35m[0m
[30;1m178 | [0m[35mclass JobClusterPythonJobHelper(BaseDatabricksHelper):[0m
[30;1m179 | [0m[35m    def check_credentials(self) -> None:[0m
[30;1m180 | [0m[35m        if not self.parsed_model["config"].get("job_cluster_config", None):[0m
[30;1m181 | [0m[35m            raise ValueError("job_cluster_config is required for commands submission method.")[0m
[30;1m182 | [0m[35m[0m
[30;1m183 | [0m[35m    def submit(self, compiled_code: str) -> None:[0m

[31m--[ [0m[34mMatch #[0m[33m37[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m125 | [0m[35m        )[0m
[30;1m126 | [0m[35m[0m
[30;1m127 | [0m[35m        # get end state to return to user[0m
[30;1m128 | [0m[35m        run_output = requests.get([0m
[30;1m129 | [0m[35m            f"https://{self.credentials.host}" f"/api/2.1/jobs/runs/get-output?run_id={run_id}",[0m
[30;1m130 | [0m[35m            headers=self.auth_header,[0m
[30;1m131 | [0m[35m        )[0m

[31m--[ [0m[34mMatch #[0m[33m38[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m92 | [0m[35m        for lib in additional_libs:[0m
[30;1m93 | [0m[35m            libraries.append(lib)[0m
[30;1m94 | [0m[35m        job_spec.update({"libraries": libraries})  # type: ignore[0m
[30;1m95 | [0m[35m        submit_response = requests.post([0m
[30;1m96 | [0m[35m            f"https://{self.credentials.host}/api/2.1/jobs/runs/submit",[0m
[30;1m97 | [0m[35m            headers=self.auth_header,[0m
[30;1m98 | [0m[35m            json=job_spec,[0m

[31m--[ [0m[34mMatch #[0m[33m39[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m85 | [0m[35m        # PYPI packages[0m
[30;1m86 | [0m[35m        packages = self.parsed_model["config"].get("packages", [])[0m
[30;1m87 | [0m[35m        # additional format of packages[0m
[30;1m88 | [0m[35m        additional_libs = self.parsed_model["config"].get("additional_libs", [])[0m
[30;1m89 | [0m[35m        libraries = [][0m
[30;1m90 | [0m[35m        for package in packages:[0m
[30;1m91 | [0m[35m            libraries.append({"pypi": {"package": package}})[0m

[31m--[ [0m[34mMatch #[0m[33m40[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m83 | [0m[35m        }[0m
[30;1m84 | [0m[35m        job_spec.update(cluster_spec)  # updates 'new_cluster' config[0m
[30;1m85 | [0m[35m        # PYPI packages[0m
[30;1m86 | [0m[35m        packages = self.parsed_model["config"].get("packages", [])[0m
[30;1m87 | [0m[35m        # additional format of packages[0m
[30;1m88 | [0m[35m        additional_libs = self.parsed_model["config"].get("additional_libs", [])[0m
[30;1m89 | [0m[35m        libraries = [][0m

[31m--[ [0m[34mMatch #[0m[33m41[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m60 | [0m[35m[0m
[30;1m61 | [0m[35m    def _upload_notebook(self, path: str, compiled_code: str) -> None:[0m
[30;1m62 | [0m[35m        b64_encoded_content = base64.b64encode(compiled_code.encode()).decode()[0m
[30;1m63 | [0m[35m        response = requests.post([0m
[30;1m64 | [0m[35m            f"https://{self.credentials.host}/api/2.0/workspace/import",[0m
[30;1m65 | [0m[35m            headers=self.auth_header,[0m
[30;1m66 | [0m[35m            json={[0m

[31m--[ [0m[34mMatch #[0m[33m42[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m46 | [0m[35m        )[0m
[30;1m47 | [0m[35m[0m
[30;1m48 | [0m[35m    def _create_work_dir(self, path: str) -> None:[0m
[30;1m49 | [0m[35m        response = requests.post([0m
[30;1m50 | [0m[35m            f"https://{self.credentials.host}/api/2.0/workspace/mkdirs",[0m
[30;1m51 | [0m[35m            headers=self.auth_header,[0m
[30;1m52 | [0m[35m            json={[0m

[31m--[ [0m[34mMatch #[0m[33m43[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m35 | [0m[35m        return self.parsed_model["config"].get("cluster_id", self.credentials.cluster_id)[0m
[30;1m36 | [0m[35m[0m
[30;1m37 | [0m[35m    def get_timeout(self) -> int:[0m
[30;1m38 | [0m[35m        timeout = self.parsed_model["config"].get("timeout", DEFAULT_TIMEOUT)[0m
[30;1m39 | [0m[35m        if timeout <= 0:[0m
[30;1m40 | [0m[35m            raise ValueError("Timeout must be a positive integer")[0m
[30;1m41 | [0m[35m        return timeout[0m

[31m--[ [0m[34mMatch #[0m[33m44[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m32 | [0m[35m[0m
[30;1m33 | [0m[35m    @property[0m
[30;1m34 | [0m[35m    def cluster_id(self) -> str:[0m
[30;1m35 | [0m[35m        return self.parsed_model["config"].get("cluster_id", self.credentials.cluster_id)[0m
[30;1m36 | [0m[35m[0m
[30;1m37 | [0m[35m    def get_timeout(self) -> int:[0m
[30;1m38 | [0m[35m        timeout = self.parsed_model["config"].get("timeout", DEFAULT_TIMEOUT)[0m

[31m--[ [0m[34mMatch #[0m[33m45[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m38 | [0m[35m        timeout = self.parsed_model["config"].get("timeout", DEFAULT_TIMEOUT)[0m
[30;1m39 | [0m[35m        if timeout <= 0:[0m
[30;1m40 | [0m[35m            raise ValueError("Timeout must be a positive integer")[0m
[30;1m41 | [0m[35m        return timeout[0m
[30;1m42 | [0m[35m[0m
[30;1m43 | [0m[35m    def check_credentials(self) -> None:[0m
[30;1m44 | [0m[35m        raise NotImplementedError([0m
[30;1m45 | [0m[35m            "Overwrite this method to check specific requirement for current submission method"[0m

[31m--[ [0m[34mMatch #[0m[33m46[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m36 | [0m[35m[0m
[30;1m37 | [0m[35m    def get_timeout(self) -> int:[0m
[30;1m38 | [0m[35m        timeout = self.parsed_model["config"].get("timeout", DEFAULT_TIMEOUT)[0m
[30;1m39 | [0m[35m        if timeout <= 0:[0m
[30;1m40 | [0m[35m            raise ValueError("Timeout must be a positive integer")[0m
[30;1m41 | [0m[35m        return timeout[0m
[30;1m42 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m47[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m35 | [0m[35m        return self.parsed_model["config"].get("cluster_id", self.credentials.cluster_id)[0m
[30;1m36 | [0m[35m[0m
[30;1m37 | [0m[35m    def get_timeout(self) -> int:[0m
[30;1m38 | [0m[35m        timeout = self.parsed_model["config"].get("timeout", DEFAULT_TIMEOUT)[0m
[30;1m39 | [0m[35m        if timeout <= 0:[0m
[30;1m40 | [0m[35m            raise ValueError("Timeout must be a positive integer")[0m
[30;1m41 | [0m[35m        return timeout[0m

[31m--[ [0m[34mMatch #[0m[33m48[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m1 | [0m[35mimport base64[0m
[30;1m2 | [0m[35mimport time[0m
[30;1m3 | [0m[35mimport requests[0m
[30;1m4 | [0m[35mfrom typing import Any, Dict, Callable, Iterable[0m
[30;1m5 | [0m[35mimport uuid[0m
[30;1m6 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m49[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/python_submissions.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m1 | [0m[35mimport base64[0m
[30;1m2 | [0m[35mimport time[0m
[30;1m3 | [0m[35mimport requests[0m
[30;1m4 | [0m[35mfrom typing import Any, Dict, Callable, Iterable[0m
[30;1m5 | [0m[35mimport uuid[0m

[31m--[ [0m[34mMatch #[0m[33m50[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/impl.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m528 | [0m[35m        if ([0m
[30;1m529 | [0m[35m            config[0m
[30;1m530 | [0m[35m            and hasattr(config, "_extra")[0m
[30;1m531 | [0m[35m            and (file_format := config._extra.get("file_format"))[0m
[30;1m532 | [0m[35m        ):[0m
[30;1m533 | [0m[35m            if file_format in table_formats_within_file_formats:[0m
[30;1m534 | [0m[35m                table_format = file_format[0m

[31m--[ [0m[34mMatch #[0m[33m51[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/impl.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m296 | [0m[35m                table_schema=relation.schema,[0m
[30;1m297 | [0m[35m                table_name=relation.name,[0m
[30;1m298 | [0m[35m                table_type=relation.type,[0m
[30;1m299 | [0m[35m                table_owner=str(metadata.get(KEY_TABLE_OWNER)),[0m
[30;1m300 | [0m[35m                table_stats=table_stats,[0m
[30;1m301 | [0m[35m                column=column["col_name"],[0m
[30;1m302 | [0m[35m                column_index=idx,[0m

[31m--[ [0m[34mMatch #[0m[33m52[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/impl.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m288 | [0m[35m        rows = [row for row in raw_rows[0:pos] if not row["col_name"].startswith("#")][0m
[30;1m289 | [0m[35m        metadata = {col["col_name"]: col["data_type"] for col in raw_rows[pos + 1 :]}[0m
[30;1m290 | [0m[35m[0m
[30;1m291 | [0m[35m        raw_table_stats = metadata.get(KEY_TABLE_STATISTICS)[0m
[30;1m292 | [0m[35m        table_stats = SparkColumn.convert_table_stats(raw_table_stats)[0m
[30;1m293 | [0m[35m        return [[0m
[30;1m294 | [0m[35m            SparkColumn([0m

[31m--[ [0m[34mMatch #[0m[33m53[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/impl.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m280 | [0m[35m    ) -> List[SparkColumn]:[0m
[30;1m281 | [0m[35m        # Convert the Row to a dict[0m
[30;1m282 | [0m[35m        dict_rows = [dict(zip(row._keys, row._values)) for row in raw_rows][0m
[30;1m283 | [0m[35m        # Find the separator between the rows and the metadata provided[0m
[30;1m284 | [0m[35m        # by the DESCRIBE TABLE EXTENDED statement[0m
[30;1m285 | [0m[35m        pos = self.find_table_information_separator(dict_rows)[0m
[30;1m286 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m54[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/connections.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m300 | [0m[35m            raise DbtDatabaseError(poll_state.errorMessage)[0m
[30;1m301 | [0m[35m[0m
[30;1m302 | [0m[35m        elif state not in STATE_SUCCESS:[0m
[30;1m303 | [0m[35m            status_type = ThriftState._VALUES_TO_NAMES.get(state, "Unknown<{!r}>".format(state))[0m
[30;1m304 | [0m[35m            raise DbtDatabaseError("Query failed with status: {}".format(status_type))[0m
[30;1m305 | [0m[35m[0m
[30;1m306 | [0m[35m        logger.debug("Poll status: {}, query complete".format(state))[0m

[31m--[ [0m[34mMatch #[0m[33m55[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/connections.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m44 | [0m[35m    pass  # done deliberately: setting modules to None explicitly violates MyPy contracts by degradi[0m
[30;1m45 | [0m[35m[0m
[30;1m46 | [0m[35mimport base64[0m
[30;1m47 | [0m[35mimport time[0m
[30;1m48 | [0m[35m[0m
[30;1m49 | [0m[35mlogger = AdapterLogger("Spark")[0m
[30;1m50 | [0m[35m[0m
[30;1m51 | [0m[35mNUMBERS = DECIMALS + (int, float)[0m

[31m--[ [0m[34mMatch #[0m[33m56[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/adapters/spark/connections.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m43 | [0m[35mexcept ImportError:[0m
[30;1m44 | [0m[35m    pass  # done deliberately: setting modules to None explicitly violates MyPy contracts by degradi[0m
[30;1m45 | [0m[35m[0m
[30;1m46 | [0m[35mimport base64[0m
[30;1m47 | [0m[35mimport time[0m
[30;1m48 | [0m[35m[0m
[30;1m49 | [0m[35mlogger = AdapterLogger("Spark")[0m
[30;1m50 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m57[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/hatch.toml[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m60 | [0m[35mcheck-sdist = [[0m
[30;1m61 | [0m[35m    "check-wheel-contents dist/*.whl --ignore W007,W008",[0m
[30;1m62 | [0m[35m    "find ./dist/dbt_spark-*.gz -maxdepth 1 -type f | xargs python -m pip install --force-reinstall [0m
[30;1m63 | [0m[35m    "pip freeze | grep dbt-spark",[0m
[30;1m64 | [0m[35m][0m
[30;1m65 | [0m[35mdocker-prod = "docker build -f docker/Dockerfile -t dbt-spark ."[0m

[31m--[ [0m[34mMatch #[0m[33m58[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/hatch.toml[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m59 | [0m[35m][0m
[30;1m60 | [0m[35mcheck-sdist = [[0m
[30;1m61 | [0m[35m    "check-wheel-contents dist/*.whl --ignore W007,W008",[0m
[30;1m62 | [0m[35m    "find ./dist/dbt_spark-*.gz -maxdepth 1 -type f | xargs python -m pip install --force-reinstall [0m
[30;1m63 | [0m[35m    "pip freeze | grep dbt-spark",[0m
[30;1m64 | [0m[35m][0m
[30;1m65 | [0m[35mdocker-prod = "docker build -f docker/Dockerfile -t dbt-spark ."[0m

[31m--[ [0m[34mMatch #[0m[33m59[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/hatch.toml[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m59 | [0m[35m][0m
[30;1m60 | [0m[35mcheck-sdist = [[0m
[30;1m61 | [0m[35m    "check-wheel-contents dist/*.whl --ignore W007,W008",[0m
[30;1m62 | [0m[35m    "find ./dist/dbt_spark-*.gz -maxdepth 1 -type f | xargs python -m pip install --force-reinstall [0m
[30;1m63 | [0m[35m    "pip freeze | grep dbt-spark",[0m
[30;1m64 | [0m[35m][0m
[30;1m65 | [0m[35mdocker-prod = "docker build -f docker/Dockerfile -t dbt-spark ."[0m

[31m--[ [0m[34mMatch #[0m[33m60[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/hatch.toml[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m55 | [0m[35mcheck-wheel = [[0m
[30;1m56 | [0m[35m    "twine check dist/*",[0m
[30;1m57 | [0m[35m    "find ./dist/dbt_spark-*.whl -maxdepth 1 -type f | xargs python -m pip install --force-reinstall[0m
[30;1m58 | [0m[35m    "pip freeze | grep dbt-spark",[0m
[30;1m59 | [0m[35m][0m
[30;1m60 | [0m[35mcheck-sdist = [[0m
[30;1m61 | [0m[35m    "check-wheel-contents dist/*.whl --ignore W007,W008",[0m

[31m--[ [0m[34mMatch #[0m[33m61[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/hatch.toml[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m54 | [0m[35m][0m
[30;1m55 | [0m[35mcheck-wheel = [[0m
[30;1m56 | [0m[35m    "twine check dist/*",[0m
[30;1m57 | [0m[35m    "find ./dist/dbt_spark-*.whl -maxdepth 1 -type f | xargs python -m pip install --force-reinstall[0m
[30;1m58 | [0m[35m    "pip freeze | grep dbt-spark",[0m
[30;1m59 | [0m[35m][0m
[30;1m60 | [0m[35mcheck-sdist = [[0m

[31m--[ [0m[34mMatch #[0m[33m62[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/hatch.toml[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m54 | [0m[35m][0m
[30;1m55 | [0m[35mcheck-wheel = [[0m
[30;1m56 | [0m[35m    "twine check dist/*",[0m
[30;1m57 | [0m[35m    "find ./dist/dbt_spark-*.whl -maxdepth 1 -type f | xargs python -m pip install --force-reinstall[0m
[30;1m58 | [0m[35m    "pip freeze | grep dbt-spark",[0m
[30;1m59 | [0m[35m][0m
[30;1m60 | [0m[35mcheck-sdist = [[0m

[31m--[ [0m[34mMatch #[0m[33m63[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/README.md[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m74 | [0m[35mConnecting to the local spark instance:[0m
[30;1m75 | [0m[35m[0m
[30;1m76 | [0m[35m* The Spark UI should be available at [http://localhost:4040/sqlserver/](http://localhost:4040/sqlse[0m
[30;1m77 | [0m[35m* The endpoint for SQL-based testing is at `http://localhost:10000` and can be referenced with the H[0m
[30;1m78 | [0m[35m[0m
[30;1m79 | [0m[35mNote that the Hive metastore data is persisted under `./.hive-metastore/`, and the Spark-produced da[0m
[30;1m80 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m64[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/README.md[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m73 | [0m[35m[0m
[30;1m74 | [0m[35mConnecting to the local spark instance:[0m
[30;1m75 | [0m[35m[0m
[30;1m76 | [0m[35m* The Spark UI should be available at [http://localhost:4040/sqlserver/](http://localhost:4040/sqlse[0m
[30;1m77 | [0m[35m* The endpoint for SQL-based testing is at `http://localhost:10000` and can be referenced with the H[0m
[30;1m78 | [0m[35m[0m
[30;1m79 | [0m[35mNote that the Hive metastore data is persisted under `./.hive-metastore/`, and the Spark-produced da[0m

[31m--[ [0m[34mMatch #[0m[33m65[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/README.md[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m50 | [0m[35mdocker-compose up -d[0m
[30;1m51 | [0m[35m```[0m
[30;1m52 | [0m[35m[0m
[30;1m53 | [0m[35mIt will take a bit of time for the instance to start, you can check the logs of the two containers.[0m
[30;1m54 | [0m[35mIf the instance doesn't start correctly, try the complete reset command listed below and then try st[0m
[30;1m55 | [0m[35m[0m
[30;1m56 | [0m[35mCreate a profile like this one:[0m

[31m--[ [0m[34mMatch #[0m[33m66[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/README.md[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m44 | [0m[35mA `docker-compose` environment starts a Spark Thrift server and a Postgres database as a Hive Metast[0m
[30;1m45 | [0m[35mNote: dbt-spark now supports Spark 3.3.2.[0m
[30;1m46 | [0m[35m[0m
[30;1m47 | [0m[35mThe following command starts two docker containers:[0m
[30;1m48 | [0m[35m[0m
[30;1m49 | [0m[35m```sh[0m
[30;1m50 | [0m[35mdocker-compose up -d[0m

[31m--[ [0m[34mMatch #[0m[33m67[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/incremental/column_helpers.sql[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m3 | [0m[35m[0m
[30;1m4 | [0m[35m  {%- if merge_update_columns and merge_exclude_columns -%}[0m
[30;1m5 | [0m[35m    {{ exceptions.raise_compiler_error([0m
[30;1m6 | [0m[35m        'Model cannot specify merge_update_columns and merge_exclude_columns. Please update model to[0m
[30;1m7 | [0m[35m    )}}[0m
[30;1m8 | [0m[35m  {%- elif merge_update_columns -%}[0m
[30;1m9 | [0m[35m    {%- set update_columns = merge_update_columns -%}[0m

[31m--[ [0m[34mMatch #[0m[33m68[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/incremental/strategies.sql[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m57 | [0m[35m      using {{ source }} as DBT_INTERNAL_SOURCE[0m
[30;1m58 | [0m[35m      on {{ predicates | join(' and ') }}[0m
[30;1m59 | [0m[35m[0m
[30;1m60 | [0m[35m      when matched then update set[0m
[30;1m61 | [0m[35m        {% if update_columns -%}{%- for column_name in update_columns %}[0m
[30;1m62 | [0m[35m            {{ column_name }} = DBT_INTERNAL_SOURCE.{{ column_name }}[0m
[30;1m63 | [0m[35m            {%- if not loop.last %}, {%- endif %}[0m

[31m--[ [0m[34mMatch #[0m[33m69[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/incremental/strategies.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m82 | [0m[35m      Ensure you are using a `partition_by` column that is of grain {{ config.get('batch_size') }}.[0m
[30;1m83 | [0m[35m    {%- endset %}[0m
[30;1m84 | [0m[35m[0m
[30;1m85 | [0m[35m    {%- if not config.get('partition_by') -%}[0m
[30;1m86 | [0m[35m      {{ exceptions.raise_compiler_error(missing_partition_key_microbatch_msg) }}[0m
[30;1m87 | [0m[35m    {%- endif -%}[0m
[30;1m88 | [0m[35m    {{ get_insert_overwrite_sql(source, target, existing) }}[0m

[31m--[ [0m[34mMatch #[0m[33m70[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/incremental/strategies.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m79 | [0m[35m    {#-- microbatch wraps insert_overwrite, and requires a partition_by config #}[0m
[30;1m80 | [0m[35m    {% set missing_partition_key_microbatch_msg -%}[0m
[30;1m81 | [0m[35m      dbt-spark 'microbatch' incremental strategy requires a `partition_by` config.[0m
[30;1m82 | [0m[35m      Ensure you are using a `partition_by` column that is of grain {{ config.get('batch_size') }}.[0m
[30;1m83 | [0m[35m    {%- endset %}[0m
[30;1m84 | [0m[35m[0m
[30;1m85 | [0m[35m    {%- if not config.get('partition_by') -%}[0m

[31m--[ [0m[34mMatch #[0m[33m71[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/incremental/strategies.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m30 | [0m[35m  {%- set predicates = [] if incremental_predicates is none else [] + incremental_predicates -%}[0m
[30;1m31 | [0m[35m  {%- set dest_columns = adapter.get_columns_in_relation(target) -%}[0m
[30;1m32 | [0m[35m  {%- set merge_update_columns = config.get('merge_update_columns') -%}[0m
[30;1m33 | [0m[35m  {%- set merge_exclude_columns = config.get('merge_exclude_columns') -%}[0m
[30;1m34 | [0m[35m  {%- set update_columns = get_merge_update_columns(merge_update_columns, merge_exclude_columns, des[0m
[30;1m35 | [0m[35m[0m
[30;1m36 | [0m[35m  {% if unique_key %}[0m

[31m--[ [0m[34mMatch #[0m[33m72[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/incremental/strategies.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m29 | [0m[35m  {# need dest_columns for merge_exclude_columns, default to use "*" #}[0m
[30;1m30 | [0m[35m  {%- set predicates = [] if incremental_predicates is none else [] + incremental_predicates -%}[0m
[30;1m31 | [0m[35m  {%- set dest_columns = adapter.get_columns_in_relation(target) -%}[0m
[30;1m32 | [0m[35m  {%- set merge_update_columns = config.get('merge_update_columns') -%}[0m
[30;1m33 | [0m[35m  {%- set merge_exclude_columns = config.get('merge_exclude_columns') -%}[0m
[30;1m34 | [0m[35m  {%- set update_columns = get_merge_update_columns(merge_update_columns, merge_exclude_columns, des[0m
[30;1m35 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m73[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/incremental/incremental.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m13 | [0m[35m  {%- set partition_by = config.get('partition_by', none) -%}[0m
[30;1m14 | [0m[35m  {%- set language = model['language'] -%}[0m
[30;1m15 | [0m[35m  {%- set on_schema_change = incremental_validate_on_schema_change(config.get('on_schema_change'), d[0m
[30;1m16 | [0m[35m  {%- set incremental_predicates = config.get('predicates', none) or config.get('incremental_predica[0m
[30;1m17 | [0m[35m  {%- set target_relation = this -%}[0m
[30;1m18 | [0m[35m  {%- set existing_relation = load_relation(this) -%}[0m
[30;1m19 | [0m[35m  {% set tmp_relation = this.incorporate(path = {"identifier": this.identifier ~ '__dbt_tmp'}) -%}[0m

[31m--[ [0m[34mMatch #[0m[33m74[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/incremental/incremental.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m13 | [0m[35m  {%- set partition_by = config.get('partition_by', none) -%}[0m
[30;1m14 | [0m[35m  {%- set language = model['language'] -%}[0m
[30;1m15 | [0m[35m  {%- set on_schema_change = incremental_validate_on_schema_change(config.get('on_schema_change'), d[0m
[30;1m16 | [0m[35m  {%- set incremental_predicates = config.get('predicates', none) or config.get('incremental_predica[0m
[30;1m17 | [0m[35m  {%- set target_relation = this -%}[0m
[30;1m18 | [0m[35m  {%- set existing_relation = load_relation(this) -%}[0m
[30;1m19 | [0m[35m  {% set tmp_relation = this.incorporate(path = {"identifier": this.identifier ~ '__dbt_tmp'}) -%}[0m

[31m--[ [0m[34mMatch #[0m[33m75[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/incremental/incremental.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m12 | [0m[35m  {%- set unique_key = config.get('unique_key', none) -%}[0m
[30;1m13 | [0m[35m  {%- set partition_by = config.get('partition_by', none) -%}[0m
[30;1m14 | [0m[35m  {%- set language = model['language'] -%}[0m
[30;1m15 | [0m[35m  {%- set on_schema_change = incremental_validate_on_schema_change(config.get('on_schema_change'), d[0m
[30;1m16 | [0m[35m  {%- set incremental_predicates = config.get('predicates', none) or config.get('incremental_predica[0m
[30;1m17 | [0m[35m  {%- set target_relation = this -%}[0m
[30;1m18 | [0m[35m  {%- set existing_relation = load_relation(this) -%}[0m

[31m--[ [0m[34mMatch #[0m[33m76[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/incremental/incremental.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m10 | [0m[35m  {#-- Set vars --#}[0m
[30;1m11 | [0m[35m[0m
[30;1m12 | [0m[35m  {%- set unique_key = config.get('unique_key', none) -%}[0m
[30;1m13 | [0m[35m  {%- set partition_by = config.get('partition_by', none) -%}[0m
[30;1m14 | [0m[35m  {%- set language = model['language'] -%}[0m
[30;1m15 | [0m[35m  {%- set on_schema_change = incremental_validate_on_schema_change(config.get('on_schema_change'), d[0m
[30;1m16 | [0m[35m  {%- set incremental_predicates = config.get('predicates', none) or config.get('incremental_predica[0m

[31m--[ [0m[34mMatch #[0m[33m77[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/incremental/incremental.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m9 | [0m[35m[0m
[30;1m10 | [0m[35m  {#-- Set vars --#}[0m
[30;1m11 | [0m[35m[0m
[30;1m12 | [0m[35m  {%- set unique_key = config.get('unique_key', none) -%}[0m
[30;1m13 | [0m[35m  {%- set partition_by = config.get('partition_by', none) -%}[0m
[30;1m14 | [0m[35m  {%- set language = model['language'] -%}[0m
[30;1m15 | [0m[35m  {%- set on_schema_change = incremental_validate_on_schema_change(config.get('on_schema_change'), d[0m

[31m--[ [0m[34mMatch #[0m[33m78[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/incremental/incremental.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m2 | [0m[35m  {#-- Validate early so we don't run SQL if the file_format + strategy combo is invalid --#}[0m
[30;1m3 | [0m[35m  {%- set raw_file_format = config.get('file_format', default='parquet') -%}[0m
[30;1m4 | [0m[35m  {%- set raw_strategy = config.get('incremental_strategy') or 'append' -%}[0m
[30;1m5 | [0m[35m  {%- set grant_config = config.get('grants') -%}[0m
[30;1m6 | [0m[35m[0m
[30;1m7 | [0m[35m  {%- set file_format = dbt_spark_validate_get_file_format(raw_file_format) -%}[0m
[30;1m8 | [0m[35m  {%- set strategy = dbt_spark_validate_get_incremental_strategy(raw_strategy, file_format) -%}[0m

[31m--[ [0m[34mMatch #[0m[33m79[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/incremental/incremental.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m1 | [0m[35m{% materialization incremental, adapter='spark', supported_languages=['sql', 'python'] -%}[0m
[30;1m2 | [0m[35m  {#-- Validate early so we don't run SQL if the file_format + strategy combo is invalid --#}[0m
[30;1m3 | [0m[35m  {%- set raw_file_format = config.get('file_format', default='parquet') -%}[0m
[30;1m4 | [0m[35m  {%- set raw_strategy = config.get('incremental_strategy') or 'append' -%}[0m
[30;1m5 | [0m[35m  {%- set grant_config = config.get('grants') -%}[0m
[30;1m6 | [0m[35m[0m
[30;1m7 | [0m[35m  {%- set file_format = dbt_spark_validate_get_file_format(raw_file_format) -%}[0m

[31m--[ [0m[34mMatch #[0m[33m80[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/incremental/incremental.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m1 | [0m[35m{% materialization incremental, adapter='spark', supported_languages=['sql', 'python'] -%}[0m
[30;1m2 | [0m[35m  {#-- Validate early so we don't run SQL if the file_format + strategy combo is invalid --#}[0m
[30;1m3 | [0m[35m  {%- set raw_file_format = config.get('file_format', default='parquet') -%}[0m
[30;1m4 | [0m[35m  {%- set raw_strategy = config.get('incremental_strategy') or 'append' -%}[0m
[30;1m5 | [0m[35m  {%- set grant_config = config.get('grants') -%}[0m
[30;1m6 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m81[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/.gitignore[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m169 | [0m[35m.vscode/[0m
[30;1m170 | [0m[35m.venv/[0m
[30;1m171 | [0m[35m*.code-workspace[0m
[30;1m172 | [0m[35m# Vim[0m
[30;1m173 | [0m[35m*.swp[0m
[30;1m174 | [0m[35m[0m
[30;1m175 | [0m[35m# other[0m
[30;1m176 | [0m[35m.hive-metastore/[0m

[31m--[ [0m[34mMatch #[0m[33m82[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/.gitignore[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m27 | [0m[35mMANIFEST[0m
[30;1m28 | [0m[35m[0m
[30;1m29 | [0m[35m# PyInstaller[0m
[30;1m30 | [0m[35m#  Usually these files are written by a python script from a template[0m
[30;1m31 | [0m[35m#  before PyInstaller builds the exe, so as to inject date/other infos into it.[0m
[30;1m32 | [0m[35m*.manifest[0m
[30;1m33 | [0m[35m*.spec[0m

[31m--[ [0m[34mMatch #[0m[33m83[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/PKG-INFO[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m115 | [0m[35mConnecting to the local spark instance:[0m
[30;1m116 | [0m[35m[0m
[30;1m117 | [0m[35m* The Spark UI should be available at [http://localhost:4040/sqlserver/](http://localhost:4040/sqlse[0m
[30;1m118 | [0m[35m* The endpoint for SQL-based testing is at `http://localhost:10000` and can be referenced with the H[0m
[30;1m119 | [0m[35m[0m
[30;1m120 | [0m[35mNote that the Hive metastore data is persisted under `./.hive-metastore/`, and the Spark-produced da[0m
[30;1m121 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m84[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/PKG-INFO[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m114 | [0m[35m[0m
[30;1m115 | [0m[35mConnecting to the local spark instance:[0m
[30;1m116 | [0m[35m[0m
[30;1m117 | [0m[35m* The Spark UI should be available at [http://localhost:4040/sqlserver/](http://localhost:4040/sqlse[0m
[30;1m118 | [0m[35m* The endpoint for SQL-based testing is at `http://localhost:10000` and can be referenced with the H[0m
[30;1m119 | [0m[35m[0m
[30;1m120 | [0m[35mNote that the Hive metastore data is persisted under `./.hive-metastore/`, and the Spark-produced da[0m

[31m--[ [0m[34mMatch #[0m[33m85[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/PKG-INFO[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m91 | [0m[35mdocker-compose up -d[0m
[30;1m92 | [0m[35m```[0m
[30;1m93 | [0m[35m[0m
[30;1m94 | [0m[35mIt will take a bit of time for the instance to start, you can check the logs of the two containers.[0m
[30;1m95 | [0m[35mIf the instance doesn't start correctly, try the complete reset command listed below and then try st[0m
[30;1m96 | [0m[35m[0m
[30;1m97 | [0m[35mCreate a profile like this one:[0m

[31m--[ [0m[34mMatch #[0m[33m86[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/PKG-INFO[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m85 | [0m[35mA `docker-compose` environment starts a Spark Thrift server and a Postgres database as a Hive Metast[0m
[30;1m86 | [0m[35mNote: dbt-spark now supports Spark 3.3.2.[0m
[30;1m87 | [0m[35m[0m
[30;1m88 | [0m[35mThe following command starts two docker containers:[0m
[30;1m89 | [0m[35m[0m
[30;1m90 | [0m[35m```sh[0m
[30;1m91 | [0m[35mdocker-compose up -d[0m

[31m--[ [0m[34mMatch #[0m[33m87[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/seed.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m59 | [0m[35m    create table {{ this.render() }} ([0m
[30;1m60 | [0m[35m        {%- for col_name in agate_table.column_names -%}[0m
[30;1m61 | [0m[35m            {%- set inferred_type = adapter.convert_type(agate_table, loop.index0) -%}[0m
[30;1m62 | [0m[35m            {%- set type = column_override.get(col_name, inferred_type) -%}[0m
[30;1m63 | [0m[35m            {%- set column_name = (col_name | string) -%}[0m
[30;1m64 | [0m[35m            {{ adapter.quote_seed_column(column_name, quote_seed_column) }} {{ type }} {%- if not lo[0m
[30;1m65 | [0m[35m        {%- endfor -%}[0m

[31m--[ [0m[34mMatch #[0m[33m88[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/seed.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m53 | [0m[35m[0m
[30;1m54 | [0m[35m{% macro spark__create_csv_table(model, agate_table) %}[0m
[30;1m55 | [0m[35m  {%- set column_override = model['config'].get('column_types', {}) -%}[0m
[30;1m56 | [0m[35m  {%- set quote_seed_column = model['config'].get('quote_columns', None) -%}[0m
[30;1m57 | [0m[35m[0m
[30;1m58 | [0m[35m  {% set sql %}[0m
[30;1m59 | [0m[35m    create table {{ this.render() }} ([0m

[31m--[ [0m[34mMatch #[0m[33m89[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/seed.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m52 | [0m[35m[0m
[30;1m53 | [0m[35m[0m
[30;1m54 | [0m[35m{% macro spark__create_csv_table(model, agate_table) %}[0m
[30;1m55 | [0m[35m  {%- set column_override = model['config'].get('column_types', {}) -%}[0m
[30;1m56 | [0m[35m  {%- set quote_seed_column = model['config'].get('quote_columns', None) -%}[0m
[30;1m57 | [0m[35m[0m
[30;1m58 | [0m[35m  {% set sql %}[0m

[31m--[ [0m[34mMatch #[0m[33m90[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/seed.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m31 | [0m[35m          {% for row in chunk -%}[0m
[30;1m32 | [0m[35m              ({%- for col_name in agate_table.column_names -%}[0m
[30;1m33 | [0m[35m                  {%- set inferred_type = adapter.convert_type(agate_table, loop.index0) -%}[0m
[30;1m34 | [0m[35m                  {%- set type = column_override.get(col_name, inferred_type) -%}[0m
[30;1m35 | [0m[35m                    cast({{ get_binding_char() }} as {{type}})[0m
[30;1m36 | [0m[35m                  {%- if not loop.last%},{%- endif %}[0m
[30;1m37 | [0m[35m              {%- endfor -%})[0m

[31m--[ [0m[34mMatch #[0m[33m91[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/seed.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m15 | [0m[35m{% macro spark__load_csv_rows(model, agate_table) %}[0m
[30;1m16 | [0m[35m[0m
[30;1m17 | [0m[35m  {% set batch_size = get_batch_size() %}[0m
[30;1m18 | [0m[35m  {% set column_override = model['config'].get('column_types', {}) %}[0m
[30;1m19 | [0m[35m[0m
[30;1m20 | [0m[35m  {% set statements = [] %}[0m
[30;1m21 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m92[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/clone.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m25 | [0m[35m  {%- endif -%}[0m
[30;1m26 | [0m[35m[0m
[30;1m27 | [0m[35m  {%- set other_existing_relation = load_cached_relation(defer_relation) -%}[0m
[30;1m28 | [0m[35m  {%- set file_format = config.get('file_format', validator=validation.any[basestring]) -%}[0m
[30;1m29 | [0m[35m[0m
[30;1m30 | [0m[35m  -- If this is a database that can do zero-copy cloning of tables, and the other relation is a tabl[0m
[30;1m31 | [0m[35m  -- Otherwise, this will be a view[0m

[31m--[ [0m[34mMatch #[0m[33m93[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m360 | [0m[35m{% endmacro %}[0m
[30;1m361 | [0m[35m[0m
[30;1m362 | [0m[35m{% macro spark__alter_column_comment(relation, column_dict) %}[0m
[30;1m363 | [0m[35m  {% if config.get('file_format', validator=validation.any[basestring]) in ['delta', 'hudi', 'iceber[0m
[30;1m364 | [0m[35m    {% for column_name in column_dict %}[0m
[30;1m365 | [0m[35m      {% set comment = column_dict[column_name]['description'] %}[0m
[30;1m366 | [0m[35m      {% set escaped_comment = comment | replace('\'', '\\\'') %}[0m

[31m--[ [0m[34mMatch #[0m[33m94[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m254 | [0m[35m    )[0m
[30;1m255 | [0m[35m  {% endif %}[0m
[30;1m256 | [0m[35m  {{ comment_clause() }}[0m
[30;1m257 | [0m[35m  {%- set contract_config = config.get('contract') -%}[0m
[30;1m258 | [0m[35m  {%- if contract_config.enforced -%}[0m
[30;1m259 | [0m[35m    {{ get_assert_columns_equivalent(sql) }}[0m
[30;1m260 | [0m[35m  {%- endif %}[0m

[31m--[ [0m[34mMatch #[0m[33m95[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m188 | [0m[35m[0m
[30;1m189 | [0m[35m{% macro spark__persist_constraints(relation, model) %}[0m
[30;1m190 | [0m[35m  {%- set contract_config = config.get('contract') -%}[0m
[30;1m191 | [0m[35m  {% if contract_config.enforced and config.get('file_format', 'delta') == 'delta' %}[0m
[30;1m192 | [0m[35m    {% do alter_table_add_constraints(relation, model.constraints) %}[0m
[30;1m193 | [0m[35m    {% do alter_column_set_constraints(relation, model.columns) %}[0m
[30;1m194 | [0m[35m  {% endif %}[0m

[31m--[ [0m[34mMatch #[0m[33m96[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m187 | [0m[35m{% endmacro %}[0m
[30;1m188 | [0m[35m[0m
[30;1m189 | [0m[35m{% macro spark__persist_constraints(relation, model) %}[0m
[30;1m190 | [0m[35m  {%- set contract_config = config.get('contract') -%}[0m
[30;1m191 | [0m[35m  {% if contract_config.enforced and config.get('file_format', 'delta') == 'delta' %}[0m
[30;1m192 | [0m[35m    {% do alter_table_add_constraints(relation, model.constraints) %}[0m
[30;1m193 | [0m[35m    {% do alter_column_set_constraints(relation, model.columns) %}[0m

[31m--[ [0m[34mMatch #[0m[33m97[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m153 | [0m[35m      {% else %}[0m
[30;1m154 | [0m[35m        create table {{ relation }}[0m
[30;1m155 | [0m[35m      {% endif %}[0m
[30;1m156 | [0m[35m      {%- set contract_config = config.get('contract') -%}[0m
[30;1m157 | [0m[35m      {%- if contract_config.enforced -%}[0m
[30;1m158 | [0m[35m        {{ get_assert_columns_equivalent(compiled_code) }}[0m
[30;1m159 | [0m[35m        {%- set compiled_code = get_select_subquery(compiled_code) %}[0m

[31m--[ [0m[34mMatch #[0m[33m98[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m148 | [0m[35m    {%- if temporary -%}[0m
[30;1m149 | [0m[35m      {{ create_temporary_view(relation, compiled_code) }}[0m
[30;1m150 | [0m[35m    {%- else -%}[0m
[30;1m151 | [0m[35m      {% if config.get('file_format', validator=validation.any[basestring]) in ['delta', 'iceberg'] [0m
[30;1m152 | [0m[35m        create or replace table {{ relation }}[0m
[30;1m153 | [0m[35m      {% else %}[0m
[30;1m154 | [0m[35m        create table {{ relation }}[0m

[31m--[ [0m[34mMatch #[0m[33m99[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m109 | [0m[35m[0m
[30;1m110 | [0m[35m{% macro spark__clustered_cols(label, required=false) %}[0m
[30;1m111 | [0m[35m  {%- set cols = config.get('clustered_by', validator=validation.any[list, basestring]) -%}[0m
[30;1m112 | [0m[35m  {%- set buckets = config.get('buckets', validator=validation.any[int]) -%}[0m
[30;1m113 | [0m[35m  {%- if (cols is not none) and (buckets is not none) %}[0m
[30;1m114 | [0m[35m    {%- if cols is string -%}[0m
[30;1m115 | [0m[35m      {%- set cols = [cols] -%}[0m

[31m--[ [0m[34mMatch #[0m[33m100[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m108 | [0m[35m{%- endmacro -%}[0m
[30;1m109 | [0m[35m[0m
[30;1m110 | [0m[35m{% macro spark__clustered_cols(label, required=false) %}[0m
[30;1m111 | [0m[35m  {%- set cols = config.get('clustered_by', validator=validation.any[list, basestring]) -%}[0m
[30;1m112 | [0m[35m  {%- set buckets = config.get('buckets', validator=validation.any[int]) -%}[0m
[30;1m113 | [0m[35m  {%- if (cols is not none) and (buckets is not none) %}[0m
[30;1m114 | [0m[35m    {%- if cols is string -%}[0m

[31m--[ [0m[34mMatch #[0m[33m101[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m88 | [0m[35m{%- endmacro -%}[0m
[30;1m89 | [0m[35m[0m
[30;1m90 | [0m[35m{% macro spark__partition_cols(label, required=false) %}[0m
[30;1m91 | [0m[35m  {%- set cols = config.get('partition_by', validator=validation.any[list, basestring]) -%}[0m
[30;1m92 | [0m[35m  {%- if cols is not none %}[0m
[30;1m93 | [0m[35m    {%- if cols is string -%}[0m
[30;1m94 | [0m[35m      {%- set cols = [cols] -%}[0m

[31m--[ [0m[34mMatch #[0m[33m102[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m73 | [0m[35m  {%- set raw_persist_docs = config.get('persist_docs', {}) -%}[0m
[30;1m74 | [0m[35m[0m
[30;1m75 | [0m[35m  {%- if raw_persist_docs is mapping -%}[0m
[30;1m76 | [0m[35m    {%- set raw_relation = raw_persist_docs.get('relation', false) -%}[0m
[30;1m77 | [0m[35m      {%- if raw_relation -%}[0m
[30;1m78 | [0m[35m      comment '{{ model.description | replace("'", "\\'") }}'[0m
[30;1m79 | [0m[35m      {% endif %}[0m

[31m--[ [0m[34mMatch #[0m[33m103[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m70 | [0m[35m{%- endmacro -%}[0m
[30;1m71 | [0m[35m[0m
[30;1m72 | [0m[35m{% macro spark__comment_clause() %}[0m
[30;1m73 | [0m[35m  {%- set raw_persist_docs = config.get('persist_docs', {}) -%}[0m
[30;1m74 | [0m[35m[0m
[30;1m75 | [0m[35m  {%- if raw_persist_docs is mapping -%}[0m
[30;1m76 | [0m[35m    {%- set raw_relation = raw_persist_docs.get('relation', false) -%}[0m

[31m--[ [0m[34mMatch #[0m[33m104[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m49 | [0m[35m    {%- if unique_key is not none and options is none -%}[0m
[30;1m50 | [0m[35m      {%- set options = {'primaryKey': config.get('unique_key')} -%}[0m
[30;1m51 | [0m[35m    {%- elif unique_key is not none and options is not none and 'primaryKey' not in options -%}[0m
[30;1m52 | [0m[35m      {%- set _ = options.update({'primaryKey': config.get('unique_key')}) -%}[0m
[30;1m53 | [0m[35m    {%- elif options is not none and 'primaryKey' in options and options['primaryKey'] != unique_key[0m
[30;1m54 | [0m[35m      {{ exceptions.raise_compiler_error("unique_key and options('primaryKey') should be the same co[0m
[30;1m55 | [0m[35m    {%- endif %}[0m

[31m--[ [0m[34mMatch #[0m[33m105[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m47 | [0m[35m  {%- if config.get('file_format') == 'hudi' -%}[0m
[30;1m48 | [0m[35m    {%- set unique_key = config.get('unique_key') -%}[0m
[30;1m49 | [0m[35m    {%- if unique_key is not none and options is none -%}[0m
[30;1m50 | [0m[35m      {%- set options = {'primaryKey': config.get('unique_key')} -%}[0m
[30;1m51 | [0m[35m    {%- elif unique_key is not none and options is not none and 'primaryKey' not in options -%}[0m
[30;1m52 | [0m[35m      {%- set _ = options.update({'primaryKey': config.get('unique_key')}) -%}[0m
[30;1m53 | [0m[35m    {%- elif options is not none and 'primaryKey' in options and options['primaryKey'] != unique_key[0m

[31m--[ [0m[34mMatch #[0m[33m106[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m45 | [0m[35m{% macro spark__options_clause() -%}[0m
[30;1m46 | [0m[35m  {%- set options = config.get('options') -%}[0m
[30;1m47 | [0m[35m  {%- if config.get('file_format') == 'hudi' -%}[0m
[30;1m48 | [0m[35m    {%- set unique_key = config.get('unique_key') -%}[0m
[30;1m49 | [0m[35m    {%- if unique_key is not none and options is none -%}[0m
[30;1m50 | [0m[35m      {%- set options = {'primaryKey': config.get('unique_key')} -%}[0m
[30;1m51 | [0m[35m    {%- elif unique_key is not none and options is not none and 'primaryKey' not in options -%}[0m

[31m--[ [0m[34mMatch #[0m[33m107[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m44 | [0m[35m[0m
[30;1m45 | [0m[35m{% macro spark__options_clause() -%}[0m
[30;1m46 | [0m[35m  {%- set options = config.get('options') -%}[0m
[30;1m47 | [0m[35m  {%- if config.get('file_format') == 'hudi' -%}[0m
[30;1m48 | [0m[35m    {%- set unique_key = config.get('unique_key') -%}[0m
[30;1m49 | [0m[35m    {%- if unique_key is not none and options is none -%}[0m
[30;1m50 | [0m[35m      {%- set options = {'primaryKey': config.get('unique_key')} -%}[0m

[31m--[ [0m[34mMatch #[0m[33m108[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m43 | [0m[35m{%- endmacro -%}[0m
[30;1m44 | [0m[35m[0m
[30;1m45 | [0m[35m{% macro spark__options_clause() -%}[0m
[30;1m46 | [0m[35m  {%- set options = config.get('options') -%}[0m
[30;1m47 | [0m[35m  {%- if config.get('file_format') == 'hudi' -%}[0m
[30;1m48 | [0m[35m    {%- set unique_key = config.get('unique_key') -%}[0m
[30;1m49 | [0m[35m    {%- if unique_key is not none and options is none -%}[0m

[31m--[ [0m[34mMatch #[0m[33m109[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m30 | [0m[35m{%- endmacro -%}[0m
[30;1m31 | [0m[35m[0m
[30;1m32 | [0m[35m{% macro spark__location_clause() %}[0m
[30;1m33 | [0m[35m  {%- set location_root = config.get('location_root', validator=validation.any[basestring]) -%}[0m
[30;1m34 | [0m[35m  {%- set identifier = model['alias'] -%}[0m
[30;1m35 | [0m[35m  {%- if location_root is not none %}[0m
[30;1m36 | [0m[35m    location '{{ location_root }}/{{ identifier }}'[0m

[31m--[ [0m[34mMatch #[0m[33m110[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m18 | [0m[35m{%- endmacro -%}[0m
[30;1m19 | [0m[35m[0m
[30;1m20 | [0m[35m{% macro spark__file_format_clause() %}[0m
[30;1m21 | [0m[35m  {%- set file_format = config.get('file_format', validator=validation.any[basestring]) -%}[0m
[30;1m22 | [0m[35m  {%- if file_format is not none %}[0m
[30;1m23 | [0m[35m    using {{ file_format }}[0m
[30;1m24 | [0m[35m  {%- endif %}[0m

[31m--[ [0m[34mMatch #[0m[33m111[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m3 | [0m[35m{%- endmacro -%}[0m
[30;1m4 | [0m[35m[0m
[30;1m5 | [0m[35m{% macro spark__tblproperties_clause() -%}[0m
[30;1m6 | [0m[35m  {%- set tblproperties = config.get('tblproperties') -%}[0m
[30;1m7 | [0m[35m  {%- if tblproperties is not none %}[0m
[30;1m8 | [0m[35m    tblproperties ([0m
[30;1m9 | [0m[35m      {%- for prop in tblproperties -%}[0m

[31m--[ [0m[34mMatch #[0m[33m112[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/adapters.sql[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m172 | [0m[35m  {%- elif language == 'python' -%}[0m
[30;1m173 | [0m[35m    {#--[0m
[30;1m174 | [0m[35m    N.B. Python models _can_ write to temp views HOWEVER they use a different session[0m
[30;1m175 | [0m[35m    and have already expired by the time they need to be used (I.E. in merges for incremental models[0m
[30;1m176 | [0m[35m[0m
[30;1m177 | [0m[35m    TODO: Deep dive into spark sessions to see if we can reuse a single session for an entire[0m
[30;1m178 | [0m[35m    dbt invocation.[0m

[31m--[ [0m[34mMatch #[0m[33m113[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/table.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m98 | [0m[35m  msg = f"{type(df)} is not a supported type for dbt Python materialization"[0m
[30;1m99 | [0m[35m  raise Exception(msg)[0m
[30;1m100 | [0m[35m[0m
[30;1m101 | [0m[35mdf.write.mode("overwrite").format("{{ config.get('file_format', 'delta') }}").option("overwriteSchem[0m
[30;1m102 | [0m[35m{%- endmacro -%}[0m
[30;1m103 | [0m[35m[0m
[30;1m104 | [0m[35m{%macro py_script_comment()%}[0m

[31m--[ [0m[34mMatch #[0m[33m114[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/table.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m16 | [0m[35m  -- create or replace table instead of dropping, so we don't have the table unavailable[0m
[30;1m17 | [0m[35m  {% if old_relation is not none %}[0m
[30;1m18 | [0m[35m    {% set is_delta = (old_relation.is_delta and config.get('file_format', validator=validation.any[[0m
[30;1m19 | [0m[35m    {% set is_iceberg = (old_relation.is_iceberg and config.get('file_format', validator=validation.[0m
[30;1m20 | [0m[35m    {% set old_relation_type = old_relation.type %}[0m
[30;1m21 | [0m[35m  {% else %}[0m
[30;1m22 | [0m[35m    {% set is_delta = false %}[0m

[31m--[ [0m[34mMatch #[0m[33m115[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/table.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m15 | [0m[35m  -- in case if the existing and future table is delta or iceberg, we want to do a[0m
[30;1m16 | [0m[35m  -- create or replace table instead of dropping, so we don't have the table unavailable[0m
[30;1m17 | [0m[35m  {% if old_relation is not none %}[0m
[30;1m18 | [0m[35m    {% set is_delta = (old_relation.is_delta and config.get('file_format', validator=validation.any[[0m
[30;1m19 | [0m[35m    {% set is_iceberg = (old_relation.is_iceberg and config.get('file_format', validator=validation.[0m
[30;1m20 | [0m[35m    {% set old_relation_type = old_relation.type %}[0m
[30;1m21 | [0m[35m  {% else %}[0m

[31m--[ [0m[34mMatch #[0m[33m116[0m[34m of [0m[33m116[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dbt_spark-1.9.2/dbt/include/spark/macros/materializations/table.sql[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m1 | [0m[35m{% materialization table, adapter = 'spark', supported_languages=['sql', 'python'] %}[0m
[30;1m2 | [0m[35m  {%- set language = model['language'] -%}[0m
[30;1m3 | [0m[35m  {%- set identifier = model['alias'] -%}[0m
[30;1m4 | [0m[35m  {%- set grant_config = config.get('grants') -%}[0m
[30;1m5 | [0m[35m[0m
[30;1m6 | [0m[35m  {%- set old_relation = adapter.get_relation(database=database, schema=schema, identifier=identifie[0m
[30;1m7 | [0m[35m  {%- set target_relation = api.Relation.create(identifier=identifier,[0m

116 matches found.
