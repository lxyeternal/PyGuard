[31m--[ [0m[34mMatch #[0m[33m1[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000800[0m
       Tag: [34mSecurity.Backdoor.Obfuscation[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/_test_utils.py[0m
   Pattern: [32mbase64|encodedcommand|obfuscate[0m
[30;1m76 | [0m[35m    else:[0m
[30;1m77 | [0m[35m        check.failed("Unreachable")[0m
[30;1m78 | [0m[35m    dbfs_client = files.DbfsAPI(client.api_client)[0m
[30;1m79 | [0m[35m    contents = base64.b64encode(source.encode("utf-8")).decode("utf-8")[0m
[30;1m80 | [0m[35m    if dbfs_path is None:[0m
[30;1m81 | [0m[35m        with dbfs_tempdir(dbfs_client) as tempdir:[0m
[30;1m82 | [0m[35m            script_path = os.path.join(tempdir, "script.py")[0m

[31m--[ [0m[34mMatch #[0m[33m2[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000800[0m
       Tag: [34mSecurity.Backdoor.Obfuscation[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/_test_utils.py[0m
   Pattern: [32mbase64|encodedcommand|obfuscate[0m
[30;1m1 | [0m[35mimport base64[0m
[30;1m2 | [0m[35mimport inspect[0m
[30;1m3 | [0m[35mimport os[0m
[30;1m4 | [0m[35mimport subprocess[0m

[31m--[ [0m[34mMatch #[0m[33m3[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000702[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Environment[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m(env|environment).{1,50}(get|post|curl|nc|invoke-restmethod)[0m
[30;1m1005 | [0m[35m    return Field([0m
[30;1m1006 | [0m[35m        [Shape(fields={"name": name, "key": key, "scope": scope})],[0m
[30;1m1007 | [0m[35m        description=([0m
[30;1m1008 | [0m[35m            "Databricks secrets to be exported as environment variables. Since runs will execute in"[0m
[30;1m1009 | [0m[35m            " the Databricks runtime environment, environment variables (such as those required for"[0m
[30;1m1010 | [0m[35m            " a `StringSource` config variable) will not be accessible to Dagster. These variables"[0m
[30;1m1011 | [0m[35m            " must be stored as Databricks secrets and specified here, which will ensure they are"[0m

[31m--[ [0m[34mMatch #[0m[33m4[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000702[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Environment[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m(env|environment).{1,50}(get|post|curl|nc|invoke-restmethod)[0m
[30;1m446 | [0m[35m            " launching the driver and workers. To specify an additional set of"[0m
[30;1m447 | [0m[35m            " SPARK_DAEMON_JAVA_OPTS, we recommend appending them to $SPARK_DAEMON_JAVA_OPTS as"[0m
[30;1m448 | [0m[35m            " shown in the example below. This ensures that all default Databricks managed"[0m
[30;1m449 | [0m[35m            " environmental variables are included as well. Example Spark environment variables:"[0m
[30;1m450 | [0m[35m            ' {"SPARK_WORKER_MEMORY": "28000m", "SPARK_LOCAL_DIRS": "/local_disk0"} or'[0m
[30;1m451 | [0m[35m            ' {"SPARK_DAEMON_JAVA_OPTS": "$SPARK_DAEMON_JAVA_OPTS'[0m
[30;1m452 | [0m[35m            ' -Dspark.shuffle.service.enabled=true"}'[0m

[31m--[ [0m[34mMatch #[0m[33m5[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m937 | [0m[35m            fields={[0m
[30;1m938 | [0m[35m                "secret_scope": _define_secret_scope(),[0m
[30;1m939 | [0m[35m                "access_key_key": access_key_key,[0m
[30;1m940 | [0m[35m                "secret_key_key": secret_key_key,[0m
[30;1m941 | [0m[35m            }[0m
[30;1m942 | [0m[35m        ),[0m
[30;1m943 | [0m[35m        description="S3 storage secret configuration",[0m

[31m--[ [0m[34mMatch #[0m[33m6[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m937 | [0m[35m            fields={[0m
[30;1m938 | [0m[35m                "secret_scope": _define_secret_scope(),[0m
[30;1m939 | [0m[35m                "access_key_key": access_key_key,[0m
[30;1m940 | [0m[35m                "secret_key_key": secret_key_key,[0m
[30;1m941 | [0m[35m            }[0m
[30;1m942 | [0m[35m        ),[0m
[30;1m943 | [0m[35m        description="S3 storage secret configuration",[0m

[31m--[ [0m[34mMatch #[0m[33m7[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m927 | [0m[35m        description="The key of a Databricks secret containing the S3 access key ID.",[0m
[30;1m928 | [0m[35m        is_required=True,[0m
[30;1m929 | [0m[35m    )[0m
[30;1m930 | [0m[35m    secret_key_key = Field([0m
[30;1m931 | [0m[35m        String,[0m
[30;1m932 | [0m[35m        description="The key of a Databricks secret containing the S3 secret access key.",[0m
[30;1m933 | [0m[35m        is_required=True,[0m

[31m--[ [0m[34mMatch #[0m[33m8[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m696 | [0m[35mdef _define_docker_image_conf() -> Field:[0m
[30;1m697 | [0m[35m    docker_basic_auth = Shape([0m
[30;1m698 | [0m[35m        {[0m
[30;1m699 | [0m[35m            "password": Field(String),[0m
[30;1m700 | [0m[35m            "username": Field(String),[0m
[30;1m701 | [0m[35m        }[0m
[30;1m702 | [0m[35m    )[0m

[31m--[ [0m[34mMatch #[0m[33m9[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000800[0m
       Tag: [34mSecurity.Backdoor.Obfuscation[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32mbase64|encodedcommand|obfuscate[0m
[30;1m315 | [0m[35m[0m
[30;1m316 | [0m[35m        block = file_obj.read(block_size)[0m
[30;1m317 | [0m[35m        while block:[0m
[30;1m318 | [0m[35m            data = base64.b64encode(block).decode("utf-8")[0m
[30;1m319 | [0m[35m            dbfs_service.add_block(data=data, handle=handle)[0m
[30;1m320 | [0m[35m            block = file_obj.read(block_size)[0m
[30;1m321 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m10[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000800[0m
       Tag: [34mSecurity.Backdoor.Obfuscation[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32mbase64|encodedcommand|obfuscate[0m
[30;1m292 | [0m[35m            bytes_read += check.not_none(jdoc.bytes_read)[0m
[30;1m293 | [0m[35m            jdoc = dbfs_service.read(path=dbfs_path, offset=bytes_read, length=block_size)[0m
[30;1m294 | [0m[35m            jdoc_data = check.not_none(jdoc.data, f"read file {dbfs_path} with no data")[0m
[30;1m295 | [0m[35m            data += base64.b64decode(jdoc_data)[0m
[30;1m296 | [0m[35m[0m
[30;1m297 | [0m[35m        return data[0m
[30;1m298 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m11[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000800[0m
       Tag: [34mSecurity.Backdoor.Obfuscation[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32mbase64|encodedcommand|obfuscate[0m
[30;1m287 | [0m[35m[0m
[30;1m288 | [0m[35m        jdoc = dbfs_service.read(path=dbfs_path, length=block_size)[0m
[30;1m289 | [0m[35m        jdoc_data = check.not_none(jdoc.data, f"read file {dbfs_path} with no data")[0m
[30;1m290 | [0m[35m        data += base64.b64decode(jdoc_data)[0m
[30;1m291 | [0m[35m        while jdoc.bytes_read == block_size:[0m
[30;1m292 | [0m[35m            bytes_read += check.not_none(jdoc.bytes_read)[0m
[30;1m293 | [0m[35m            jdoc = dbfs_service.read(path=dbfs_path, offset=bytes_read, length=block_size)[0m

[31m--[ [0m[34mMatch #[0m[33m12[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000800[0m
       Tag: [34mSecurity.Backdoor.Obfuscation[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32mbase64|encodedcommand|obfuscate[0m
[30;1m1 | [0m[35mimport base64[0m
[30;1m2 | [0m[35mimport logging[0m
[30;1m3 | [0m[35mimport os[0m
[30;1m4 | [0m[35mimport time[0m

[31m--[ [0m[34mMatch #[0m[33m13[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000702[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Environment[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m(env|environment).{1,50}(get|post|curl|nc|invoke-restmethod)[0m
[30;1m619 | [0m[35m[0m
[30;1m620 | [0m[35m    Instances of this class will be created when a Databricks step is launched and will contain[0m
[30;1m621 | [0m[35m    all configuration and secrets required to set up storage and environment variables within[0m
[30;1m622 | [0m[35m    the Databricks environment. The instance will be serialized and uploaded to Databricks[0m
[30;1m623 | [0m[35m    by the step launcher, then deserialized as part of the 'main' script when the job is running[0m
[30;1m624 | [0m[35m    in Databricks.[0m
[30;1m625 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m14[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000702[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Environment[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m(env|environment).{1,50}(get|post|curl|nc|invoke-restmethod)[0m
[30;1m561 | [0m[35m        return out[0m
[30;1m562 | [0m[35m[0m
[30;1m563 | [0m[35m    def create_remote_config(self) -> "DatabricksConfig":[0m
[30;1m564 | [0m[35m        env_variables = self.get_dagster_env_variables()[0m
[30;1m565 | [0m[35m        env_variables.update(self.env_variables)[0m
[30;1m566 | [0m[35m        databricks_config = DatabricksConfig([0m
[30;1m567 | [0m[35m            env_variables=env_variables,[0m

[31m--[ [0m[34mMatch #[0m[33m15[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000702[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Environment[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m(env|environment).{1,50}(get|post|curl|nc|invoke-restmethod)[0m
[30;1m201 | [0m[35m        run_config: Mapping[str, Any],[0m
[30;1m202 | [0m[35m        permissions: Mapping[str, Any],[0m
[30;1m203 | [0m[35m        databricks_host: Optional[str],[0m
[30;1m204 | [0m[35m        secrets_to_env_variables: Sequence[Mapping[str, Any]],[0m
[30;1m205 | [0m[35m        staging_prefix: str,[0m
[30;1m206 | [0m[35m        wait_for_logs: bool,[0m
[30;1m207 | [0m[35m        max_completion_wait_time_seconds: int,[0m

[31m--[ [0m[34mMatch #[0m[33m16[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m675 | [0m[35m[0m
[30;1m676 | [0m[35m        # Boto will use these.[0m
[30;1m677 | [0m[35m        os.environ["AWS_ACCESS_KEY_ID"] = access_key[0m
[30;1m678 | [0m[35m        os.environ["AWS_SECRET_ACCESS_KEY"] = secret_key[0m
[30;1m679 | [0m[35m[0m
[30;1m680 | [0m[35m    def setup_adls2_storage(self, adls2_storage: Mapping[str, Any], dbutils: Any, sc: Any) -> None:[0m
[30;1m681 | [0m[35m        """Obtain an Azure Storage Account key from Databricks secrets and export so Spark can use i[0m

[31m--[ [0m[34mMatch #[0m[33m17[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m671 | [0m[35m        # Spark APIs will use this.[0m
[30;1m672 | [0m[35m        # See https://docs.databricks.com/data/data-sources/aws/amazon-s3.html#alternative-1-set-aws[0m
[30;1m673 | [0m[35m        sc._jsc.hadoopConfiguration().set("fs.s3n.awsAccessKeyId", access_key)  # noqa: SLF001[0m
[30;1m674 | [0m[35m        sc._jsc.hadoopConfiguration().set("fs.s3n.awsSecretAccessKey", secret_key)  # noqa: SLF001[0m
[30;1m675 | [0m[35m[0m
[30;1m676 | [0m[35m        # Boto will use these.[0m
[30;1m677 | [0m[35m        os.environ["AWS_ACCESS_KEY_ID"] = access_key[0m

[31m--[ [0m[34mMatch #[0m[33m18[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m666 | [0m[35m        scope = s3_storage["secret_scope"][0m
[30;1m667 | [0m[35m[0m
[30;1m668 | [0m[35m        access_key = dbutils.secrets.get(scope=scope, key=s3_storage["access_key_key"])[0m
[30;1m669 | [0m[35m        secret_key = dbutils.secrets.get(scope=scope, key=s3_storage["secret_key_key"])[0m
[30;1m670 | [0m[35m[0m
[30;1m671 | [0m[35m        # Spark APIs will use this.[0m
[30;1m672 | [0m[35m        # See https://docs.databricks.com/data/data-sources/aws/amazon-s3.html#alternative-1-set-aws[0m

[31m--[ [0m[34mMatch #[0m[33m19[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000701[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Token[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m(npm owner|password|htpasswd|auth_?token|secret_?key|private_?key|authorized_keys?|npmrc|\.ssh|usersecrets?|api_?keys|nuget\.config|\.identityservice)[0m
[30;1m666 | [0m[35m        scope = s3_storage["secret_scope"][0m
[30;1m667 | [0m[35m[0m
[30;1m668 | [0m[35m        access_key = dbutils.secrets.get(scope=scope, key=s3_storage["access_key_key"])[0m
[30;1m669 | [0m[35m        secret_key = dbutils.secrets.get(scope=scope, key=s3_storage["secret_key_key"])[0m
[30;1m670 | [0m[35m[0m
[30;1m671 | [0m[35m        # Spark APIs will use this.[0m
[30;1m672 | [0m[35m        # See https://docs.databricks.com/data/data-sources/aws/amazon-s3.html#alternative-1-set-aws[0m

[31m--[ [0m[34mMatch #[0m[33m20[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000800[0m
       Tag: [34mSecurity.Backdoor.Obfuscation[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32mbase64|encodedcommand|obfuscate[0m
[30;1m474 | [0m[35m                    return None[0m
[30;1m475 | [0m[35m                read_response = self.dbfs_client.read(log_path)[0m
[30;1m476 | [0m[35m                assert read_response.data[0m
[30;1m477 | [0m[35m                content = base64.b64decode(read_response.data).decode("utf-8")[0m
[30;1m478 | [0m[35m                chunk = content[self.log_position :][0m
[30;1m479 | [0m[35m                self.log_position = len(content)[0m
[30;1m480 | [0m[35m                return chunk[0m

[31m--[ [0m[34mMatch #[0m[33m21[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000800[0m
       Tag: [34mSecurity.Backdoor.Obfuscation[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32mbase64|encodedcommand|obfuscate[0m
[30;1m412 | [0m[35m            message_data = check.not_none(raw_message.data, "Read message with null data.")[0m
[30;1m413 | [0m[35m            # Files written to dbfs using the Python IO interface used in PipesDbfsMessageWriter are[0m
[30;1m414 | [0m[35m            # base64-encoded.[0m
[30;1m415 | [0m[35m            return base64.b64decode(message_data).decode("utf-8")[0m
[30;1m416 | [0m[35m        # An error here is an expected result, since an IOError will be thrown if the next message[0m
[30;1m417 | [0m[35m        # chunk doesn't yet exist. Swallowing the error here is equivalent to doing a no-op on a[0m
[30;1m418 | [0m[35m        # status check showing a non-existent file.[0m

[31m--[ [0m[34mMatch #[0m[33m22[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000800[0m
       Tag: [34mSecurity.Backdoor.Obfuscation[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32mbase64|encodedcommand|obfuscate[0m
[30;1m411 | [0m[35m            raw_message = self.dbfs_client.read(message_path)[0m
[30;1m412 | [0m[35m            message_data = check.not_none(raw_message.data, "Read message with null data.")[0m
[30;1m413 | [0m[35m            # Files written to dbfs using the Python IO interface used in PipesDbfsMessageWriter are[0m
[30;1m414 | [0m[35m            # base64-encoded.[0m
[30;1m415 | [0m[35m            return base64.b64decode(message_data).decode("utf-8")[0m
[30;1m416 | [0m[35m        # An error here is an expected result, since an IOError will be thrown if the next message[0m
[30;1m417 | [0m[35m        # chunk doesn't yet exist. Swallowing the error here is equivalent to doing a no-op on a[0m

[31m--[ [0m[34mMatch #[0m[33m23[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000800[0m
       Tag: [34mSecurity.Backdoor.Obfuscation[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32mbase64|encodedcommand|obfuscate[0m
[30;1m344 | [0m[35m        """[0m
[30;1m345 | [0m[35m        with dbfs_tempdir(self.dbfs_client) as tempdir:[0m
[30;1m346 | [0m[35m            path = os.path.join(tempdir, _CONTEXT_FILENAME)[0m
[30;1m347 | [0m[35m            contents = base64.b64encode(json.dumps(context).encode("utf-8")).decode("utf-8")[0m
[30;1m348 | [0m[35m            self.dbfs_client.put(path, contents=contents, overwrite=True)[0m
[30;1m349 | [0m[35m            yield {"path": path}[0m
[30;1m350 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m24[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000800[0m
       Tag: [34mSecurity.Backdoor.Obfuscation[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32mbase64|encodedcommand|obfuscate[0m
[30;1m1 | [0m[35mimport base64[0m
[30;1m2 | [0m[35mimport json[0m
[30;1m3 | [0m[35mimport os[0m
[30;1m4 | [0m[35mimport random[0m

[31m--[ [0m[34mMatch #[0m[33m25[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000702[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration.Environment[0m
  Severity: [36mImportant[0m, Confidence: [36mHigh[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32m(env|environment).{1,50}(get|post|curl|nc|invoke-restmethod)[0m
[30;1m222 | [0m[35m                    break[0m
[30;1m223 | [0m[35m[0m
[30;1m224 | [0m[35m        else:[0m
[30;1m225 | [0m[35m            pipes_env_vars = session.get_bootstrap_env_vars()[0m
[30;1m226 | [0m[35m[0m
[30;1m227 | [0m[35m            submit_task_dict["new_cluster"]["spark_env_vars"] = {[0m
[30;1m228 | [0m[35m                **submit_task_dict["new_cluster"].get("spark_env_vars", {}),[0m

[31m--[ [0m[34mMatch #[0m[33m26[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/ops.py[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m203 | [0m[35m    class DatabricksSubmitRunOpConfig(Config):[0m
[30;1m204 | [0m[35m        poll_interval_seconds: float = Field([0m
[30;1m205 | [0m[35m            default=_poll_interval_seconds,[0m
[30;1m206 | [0m[35m            description="Check whether the Databricks Job is done at this interval, in seconds.",[0m
[30;1m207 | [0m[35m        )[0m
[30;1m208 | [0m[35m        max_wait_time_seconds: float = Field([0m
[30;1m209 | [0m[35m            default=_max_wait_time_seconds,[0m

[31m--[ [0m[34mMatch #[0m[33m27[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/ops.py[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m91 | [0m[35m    class DatabricksRunNowOpConfig(Config):[0m
[30;1m92 | [0m[35m        poll_interval_seconds: float = Field([0m
[30;1m93 | [0m[35m            default=_poll_interval_seconds,[0m
[30;1m94 | [0m[35m            description="Check whether the Databricks Job is done at this interval, in seconds.",[0m
[30;1m95 | [0m[35m        )[0m
[30;1m96 | [0m[35m        max_wait_time_seconds: float = Field([0m
[30;1m97 | [0m[35m            default=_max_wait_time_seconds,[0m

[31m--[ [0m[34mMatch #[0m[33m28[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/ops.py[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m13 | [0m[35mfrom pydantic import Field[0m
[30;1m14 | [0m[35m[0m
[30;1m15 | [0m[35mDEFAULT_POLL_INTERVAL_SECONDS = 10[0m
[30;1m16 | [0m[35m# wait at most 24 hours by default for run execution[0m
[30;1m17 | [0m[35mDEFAULT_MAX_WAIT_TIME_SECONDS = 24 * 60 * 60[0m
[30;1m18 | [0m[35mfrom dagster import Config[0m
[30;1m19 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m29[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/_test_utils.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m1 | [0m[35mimport base64[0m
[30;1m2 | [0m[35mimport inspect[0m
[30;1m3 | [0m[35mimport os[0m
[30;1m4 | [0m[35mimport subprocess[0m
[30;1m5 | [0m[35mimport textwrap[0m

[31m--[ [0m[34mMatch #[0m[33m30[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/resources.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m64 | [0m[35m    def has_token_or_oauth_credentials(cls, values: dict[str, Any]) -> dict[str, Any]:[0m
[30;1m65 | [0m[35m        token = values.get("token")[0m
[30;1m66 | [0m[35m        oauth_credentials = values.get("oauth_credentials")[0m
[30;1m67 | [0m[35m        azure_credentials = values.get("azure_credentials")[0m
[30;1m68 | [0m[35m        present = [True for v in [token, oauth_credentials, azure_credentials] if v is not None][0m
[30;1m69 | [0m[35m        if len(present) > 1:[0m
[30;1m70 | [0m[35m            raise ValueError([0m

[31m--[ [0m[34mMatch #[0m[33m31[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/resources.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m63 | [0m[35m    @model_validator(mode="before")[0m
[30;1m64 | [0m[35m    def has_token_or_oauth_credentials(cls, values: dict[str, Any]) -> dict[str, Any]:[0m
[30;1m65 | [0m[35m        token = values.get("token")[0m
[30;1m66 | [0m[35m        oauth_credentials = values.get("oauth_credentials")[0m
[30;1m67 | [0m[35m        azure_credentials = values.get("azure_credentials")[0m
[30;1m68 | [0m[35m        present = [True for v in [token, oauth_credentials, azure_credentials] if v is not None][0m
[30;1m69 | [0m[35m        if len(present) > 1:[0m

[31m--[ [0m[34mMatch #[0m[33m32[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/resources.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m62 | [0m[35m[0m
[30;1m63 | [0m[35m    @model_validator(mode="before")[0m
[30;1m64 | [0m[35m    def has_token_or_oauth_credentials(cls, values: dict[str, Any]) -> dict[str, Any]:[0m
[30;1m65 | [0m[35m        token = values.get("token")[0m
[30;1m66 | [0m[35m        oauth_credentials = values.get("oauth_credentials")[0m
[30;1m67 | [0m[35m        azure_credentials = values.get("azure_credentials")[0m
[30;1m68 | [0m[35m        present = [True for v in [token, oauth_credentials, azure_credentials] if v is not None][0m

[31m--[ [0m[34mMatch #[0m[33m33[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/resources.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m47 | [0m[35m    azure_credentials: Optional[AzureServicePrincipalCredentials] = Field([0m
[30;1m48 | [0m[35m        default=None,[0m
[30;1m49 | [0m[35m        description=([0m
[30;1m50 | [0m[35m            "Azure service principal credentials. See"[0m
[30;1m51 | [0m[35m            " https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth#requirements-for-oau[0m
[30;1m52 | [0m[35m        ),[0m
[30;1m53 | [0m[35m    )[0m

[31m--[ [0m[34mMatch #[0m[33m34[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/resources.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m40 | [0m[35m    oauth_credentials: Optional[OauthCredentials] = Field([0m
[30;1m41 | [0m[35m        default=None,[0m
[30;1m42 | [0m[35m        description=([0m
[30;1m43 | [0m[35m            "Databricks OAuth credentials for using a service principal. See"[0m
[30;1m44 | [0m[35m            " https://docs.databricks.com/en/dev-tools/auth.html#oauth-2-0"[0m
[30;1m45 | [0m[35m        ),[0m
[30;1m46 | [0m[35m    )[0m

[31m--[ [0m[34mMatch #[0m[33m35[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/resources.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m25 | [0m[35m[0m
[30;1m26 | [0m[35m    azure_client_id: str = Field(description="The client ID of the Azure service principal")[0m
[30;1m27 | [0m[35m    azure_client_secret: str = Field(description="The client secret of the Azure service principal")[0m
[30;1m28 | [0m[35m    azure_tenant_id: str = Field(description="The tenant ID of the Azure service principal")[0m
[30;1m29 | [0m[35m[0m
[30;1m30 | [0m[35m[0m
[30;1m31 | [0m[35mclass DatabricksClientResource(ConfigurableResource, IAttachDifferentObjectToOpContext):[0m

[31m--[ [0m[34mMatch #[0m[33m36[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/resources.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m24 | [0m[35m    """[0m
[30;1m25 | [0m[35m[0m
[30;1m26 | [0m[35m    azure_client_id: str = Field(description="The client ID of the Azure service principal")[0m
[30;1m27 | [0m[35m    azure_client_secret: str = Field(description="The client secret of the Azure service principal")[0m
[30;1m28 | [0m[35m    azure_tenant_id: str = Field(description="The tenant ID of the Azure service principal")[0m
[30;1m29 | [0m[35m[0m
[30;1m30 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m37[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/resources.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m23 | [0m[35m    See https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth#--azure-service-principal-[0m
[30;1m24 | [0m[35m    """[0m
[30;1m25 | [0m[35m[0m
[30;1m26 | [0m[35m    azure_client_id: str = Field(description="The client ID of the Azure service principal")[0m
[30;1m27 | [0m[35m    azure_client_secret: str = Field(description="The client secret of the Azure service principal")[0m
[30;1m28 | [0m[35m    azure_tenant_id: str = Field(description="The tenant ID of the Azure service principal")[0m
[30;1m29 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m38[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/resources.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m18 | [0m[35m[0m
[30;1m19 | [0m[35m[0m
[30;1m20 | [0m[35mclass AzureServicePrincipalCredentials(Config):[0m
[30;1m21 | [0m[35m    """Azure service principal credentials for Azure Databricks.[0m
[30;1m22 | [0m[35m[0m
[30;1m23 | [0m[35m    See https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth#--azure-service-principal-[0m
[30;1m24 | [0m[35m    """[0m

[31m--[ [0m[34mMatch #[0m[33m39[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m818 | [0m[35m            "Base parameters to be used for each run of this job. "[0m
[30;1m819 | [0m[35m            "If the notebook takes a parameter that is not specified in the job's base_parameters "[0m
[30;1m820 | [0m[35m            "or the run-now override parameters, the default value from the notebook will be used. "[0m
[30;1m821 | [0m[35m            "Retrieve these parameters in a notebook by using dbutils.widgets.get()."[0m
[30;1m822 | [0m[35m        ),[0m
[30;1m823 | [0m[35m        is_required=False,[0m
[30;1m824 | [0m[35m    )[0m

[31m--[ [0m[34mMatch #[0m[33m40[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m776 | [0m[35m            "If you specify the idempotency token, upon failure you can retry until the request "[0m
[30;1m777 | [0m[35m            "succeeds. Databricks guarantees that exactly one run will be launched with that "[0m
[30;1m778 | [0m[35m            "idempotency token. "[0m
[30;1m779 | [0m[35m            "This token should have at most 64 characters."[0m
[30;1m780 | [0m[35m        ),[0m
[30;1m781 | [0m[35m        is_required=False,[0m
[30;1m782 | [0m[35m    )[0m

[31m--[ [0m[34mMatch #[0m[33m41[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m689 | [0m[35m            }[0m
[30;1m690 | [0m[35m        ),[0m
[30;1m691 | [0m[35m        is_required=False,[0m
[30;1m692 | [0m[35m        description="Optional webhooks to trigger at different stages of job execution",[0m
[30;1m693 | [0m[35m    )[0m
[30;1m694 | [0m[35m[0m
[30;1m695 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m42[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m266 | [0m[35m        description=([0m
[30;1m267 | [0m[35m            "Attributes related to clusters running on Amazon Web Services. "[0m
[30;1m268 | [0m[35m            "If not specified at cluster creation, a set of default values is used. "[0m
[30;1m269 | [0m[35m            "See aws_attributes at https://docs.databricks.com/dev-tools/api/latest/clusters.html."[0m
[30;1m270 | [0m[35m        ),[0m
[30;1m271 | [0m[35m        is_required=False,[0m
[30;1m272 | [0m[35m    )[0m

[31m--[ [0m[34mMatch #[0m[33m43[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m265 | [0m[35m        ),[0m
[30;1m266 | [0m[35m        description=([0m
[30;1m267 | [0m[35m            "Attributes related to clusters running on Amazon Web Services. "[0m
[30;1m268 | [0m[35m            "If not specified at cluster creation, a set of default values is used. "[0m
[30;1m269 | [0m[35m            "See aws_attributes at https://docs.databricks.com/dev-tools/api/latest/clusters.html."[0m
[30;1m270 | [0m[35m        ),[0m
[30;1m271 | [0m[35m        is_required=False,[0m

[31m--[ [0m[34mMatch #[0m[33m44[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m147 | [0m[35m    canned_acl = Field([0m
[30;1m148 | [0m[35m        String,[0m
[30;1m149 | [0m[35m        description=([0m
[30;1m150 | [0m[35m            "(Optional) Set canned access control list, e.g. bucket-owner-full-control.If"[0m
[30;1m151 | [0m[35m            " canned_acl is set, the cluster instance profile must have s3:PutObjectAcl permission"[0m
[30;1m152 | [0m[35m            " on the destination bucket and prefix. The full list of possible canned ACLs can be"[0m
[30;1m153 | [0m[35m            " found at"[0m

[31m--[ [0m[34mMatch #[0m[33m45[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m88 | [0m[35m            "Additional tags for cluster resources. Databricks tags all cluster resources (e.g.,"[0m
[30;1m89 | [0m[35m            " AWS instances and EBS volumes) with these tags in addition to default_tags. Note: -"[0m
[30;1m90 | [0m[35m            " Tags are not supported on legacy node types such as compute-optimized and"[0m
[30;1m91 | [0m[35m            " memory-optimized - Databricks allows at most 45 custom tagsMore restrictions may"[0m
[30;1m92 | [0m[35m            " apply if using Azure Databricks; refer to the official docs for further details."[0m
[30;1m93 | [0m[35m        ),[0m
[30;1m94 | [0m[35m        is_required=False,[0m

[31m--[ [0m[34mMatch #[0m[33m46[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m1105 | [0m[35m            ),[0m
[30;1m1106 | [0m[35m        ),[0m
[30;1m1107 | [0m[35m        description=([0m
[30;1m1108 | [0m[35m            "Azure service principal oauth credentials for interacting with the Databricks REST API"[0m
[30;1m1109 | [0m[35m            " via a service"[0m
[30;1m1110 | [0m[35m            " principal. See"[0m
[30;1m1111 | [0m[35m            " https://learn.microsoft.com/en-us/azure/databricks/administration-guide/users-groups/s[0m

[31m--[ [0m[34mMatch #[0m[33m47[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m1099 | [0m[35m                        str, is_required=True, description="Azure service principal client secret"[0m
[30;1m1100 | [0m[35m                    ),[0m
[30;1m1101 | [0m[35m                    "azure_tenant_id": Field([0m
[30;1m1102 | [0m[35m                        str, is_required=True, description="Azure service principal tenant ID"[0m
[30;1m1103 | [0m[35m                    ),[0m
[30;1m1104 | [0m[35m                }[0m
[30;1m1105 | [0m[35m            ),[0m

[31m--[ [0m[34mMatch #[0m[33m48[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m1096 | [0m[35m                        str, is_required=True, description="Azure service principal client ID"[0m
[30;1m1097 | [0m[35m                    ),[0m
[30;1m1098 | [0m[35m                    "azure_client_secret": Field([0m
[30;1m1099 | [0m[35m                        str, is_required=True, description="Azure service principal client secret"[0m
[30;1m1100 | [0m[35m                    ),[0m
[30;1m1101 | [0m[35m                    "azure_tenant_id": Field([0m
[30;1m1102 | [0m[35m                        str, is_required=True, description="Azure service principal tenant ID"[0m

[31m--[ [0m[34mMatch #[0m[33m49[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m1093 | [0m[35m            Shape([0m
[30;1m1094 | [0m[35m                fields={[0m
[30;1m1095 | [0m[35m                    "azure_client_id": Field([0m
[30;1m1096 | [0m[35m                        str, is_required=True, description="Azure service principal client ID"[0m
[30;1m1097 | [0m[35m                    ),[0m
[30;1m1098 | [0m[35m                    "azure_client_secret": Field([0m
[30;1m1099 | [0m[35m                        str, is_required=True, description="Azure service principal client secret"[0m

[31m--[ [0m[34mMatch #[0m[33m50[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m762 | [0m[35m    timeout_seconds = Field([0m
[30;1m763 | [0m[35m        Int,[0m
[30;1m764 | [0m[35m        description=([0m
[30;1m765 | [0m[35m            "An optional timeout applied to each run of this job. "[0m
[30;1m766 | [0m[35m            "The default behavior is to have no timeout."[0m
[30;1m767 | [0m[35m        ),[0m
[30;1m768 | [0m[35m        is_required=False,[0m

[31m--[ [0m[34mMatch #[0m[33m51[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m712 | [0m[35m    url = Field(str, description="Image URL")[0m
[30;1m713 | [0m[35m    return Field([0m
[30;1m714 | [0m[35m        Shape({"basic_auth": basic_auth, "url": url}),[0m
[30;1m715 | [0m[35m        description="Optional Docker image to use as base image for the cluster",[0m
[30;1m716 | [0m[35m        is_required=False,[0m
[30;1m717 | [0m[35m    )[0m
[30;1m718 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m52[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m703 | [0m[35m    basic_auth = Field([0m
[30;1m704 | [0m[35m        docker_basic_auth,[0m
[30;1m705 | [0m[35m        description=([0m
[30;1m706 | [0m[35m            "Authentication information for pulling down docker image. Leave unconfigured if the"[0m
[30;1m707 | [0m[35m            " image is stored in AWS ECR you have an instance profile configured which already has"[0m
[30;1m708 | [0m[35m            " permissions to pull the image from the configured URL"[0m
[30;1m709 | [0m[35m        ),[0m

[31m--[ [0m[34mMatch #[0m[33m53[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m533 | [0m[35m        String,[0m
[30;1m534 | [0m[35m        description=([0m
[30;1m535 | [0m[35m            "The repository where the package can be found. "[0m
[30;1m536 | [0m[35m            "If not specified, the default pip index is used."[0m
[30;1m537 | [0m[35m        ),[0m
[30;1m538 | [0m[35m        is_required=False,[0m
[30;1m539 | [0m[35m    )[0m

[31m--[ [0m[34mMatch #[0m[33m54[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m432 | [0m[35m        description=([0m
[30;1m433 | [0m[35m            "The configuration for storing init scripts. Any number of scripts can be "[0m
[30;1m434 | [0m[35m            "specified. The scripts are executed sequentially in the order provided. "[0m
[30;1m435 | [0m[35m            "If cluster_log_conf is specified, init script logs are sent to "[0m
[30;1m436 | [0m[35m            "<destination>/<cluster-id>/init_scripts."[0m
[30;1m437 | [0m[35m        ),[0m
[30;1m438 | [0m[35m        is_required=False,[0m

[31m--[ [0m[34mMatch #[0m[33m55[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m420 | [0m[35m    ssh_public_keys = Field([0m
[30;1m421 | [0m[35m        [String],[0m
[30;1m422 | [0m[35m        description=([0m
[30;1m423 | [0m[35m            "SSH public key contents that will be added to each Spark node in this cluster. The"[0m
[30;1m424 | [0m[35m            " corresponding private keys can be used to login with the user name ubuntu on port"[0m
[30;1m425 | [0m[35m            " 2200. Up to 10 keys can be specified."[0m
[30;1m426 | [0m[35m        ),[0m

[31m--[ [0m[34mMatch #[0m[33m56[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m386 | [0m[35m            }[0m
[30;1m387 | [0m[35m        ),[0m
[30;1m388 | [0m[35m        description=([0m
[30;1m389 | [0m[35m            "The nodes used in the cluster. Either the node types or an instance pool "[0m
[30;1m390 | [0m[35m            "can be specified."[0m
[30;1m391 | [0m[35m        ),[0m
[30;1m392 | [0m[35m        is_required=True,[0m

[31m--[ [0m[34mMatch #[0m[33m57[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m372 | [0m[35m        String,[0m
[30;1m373 | [0m[35m        description=([0m
[30;1m374 | [0m[35m            "The optional ID of the instance pool to use for launching the driver node. If not"[0m
[30;1m375 | [0m[35m            " specified, the driver node will be launched in the same instance pool as the workers"[0m
[30;1m376 | [0m[35m            " (specified by the instance_pool_id configuration value)."[0m
[30;1m377 | [0m[35m        ),[0m
[30;1m378 | [0m[35m    )[0m

[31m--[ [0m[34mMatch #[0m[33m58[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m348 | [0m[35m        String,[0m
[30;1m349 | [0m[35m        description=([0m
[30;1m350 | [0m[35m            "The node type of the Spark driver. "[0m
[30;1m351 | [0m[35m            "This field is optional; if unset, the driver node type is set as the "[0m
[30;1m352 | [0m[35m            "same value as node_type_id defined above."[0m
[30;1m353 | [0m[35m        ),[0m
[30;1m354 | [0m[35m        is_required=False,[0m

[31m--[ [0m[34mMatch #[0m[33m59[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m347 | [0m[35m    driver_node_type_id = Field([0m
[30;1m348 | [0m[35m        String,[0m
[30;1m349 | [0m[35m        description=([0m
[30;1m350 | [0m[35m            "The node type of the Spark driver. "[0m
[30;1m351 | [0m[35m            "This field is optional; if unset, the driver node type is set as the "[0m
[30;1m352 | [0m[35m            "same value as node_type_id defined above."[0m
[30;1m353 | [0m[35m        ),[0m

[31m--[ [0m[34mMatch #[0m[33m60[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m338 | [0m[35m            "This field encodes, through a single value, the resources available to each "[0m
[30;1m339 | [0m[35m            "of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned "[0m
[30;1m340 | [0m[35m            "and optimized for memory or compute intensive workloads. "[0m
[30;1m341 | [0m[35m            "A list of available node types can be retrieved by using the List node types API "[0m
[30;1m342 | [0m[35m            "call. This field is required."[0m
[30;1m343 | [0m[35m        ),[0m
[30;1m344 | [0m[35m        is_required=True,[0m

[31m--[ [0m[34mMatch #[0m[33m61[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m338 | [0m[35m            "This field encodes, through a single value, the resources available to each "[0m
[30;1m339 | [0m[35m            "of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned "[0m
[30;1m340 | [0m[35m            "and optimized for memory or compute intensive workloads. "[0m
[30;1m341 | [0m[35m            "A list of available node types can be retrieved by using the List node types API "[0m
[30;1m342 | [0m[35m            "call. This field is required."[0m
[30;1m343 | [0m[35m        ),[0m
[30;1m344 | [0m[35m        is_required=True,[0m

[31m--[ [0m[34mMatch #[0m[33m62[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m182 | [0m[35m                    Int,[0m
[30;1m183 | [0m[35m                    description=([0m
[30;1m184 | [0m[35m                        "The first first_on_demand nodes of the cluster will be placed on on-demand"[0m
[30;1m185 | [0m[35m                        " instances. If this value is greater than 0, the cluster driver node will"[0m
[30;1m186 | [0m[35m                        " be placed on an on-demand instance. If this value is greater than or"[0m
[30;1m187 | [0m[35m                        " equal to the current cluster size, all nodes will be placed on on-demand"[0m
[30;1m188 | [0m[35m                        " instances. If this value is less than the current cluster size,"[0m

[31m--[ [0m[34mMatch #[0m[33m63[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/configs.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m87 | [0m[35m        description=([0m
[30;1m88 | [0m[35m            "Additional tags for cluster resources. Databricks tags all cluster resources (e.g.,"[0m
[30;1m89 | [0m[35m            " AWS instances and EBS volumes) with these tags in addition to default_tags. Note: -"[0m
[30;1m90 | [0m[35m            " Tags are not supported on legacy node types such as compute-optimized and"[0m
[30;1m91 | [0m[35m            " memory-optimized - Databricks allows at most 45 custom tagsMore restrictions may"[0m
[30;1m92 | [0m[35m            " apply if using Azure Databricks; refer to the official docs for further details."[0m
[30;1m93 | [0m[35m        ),[0m

[31m--[ [0m[34mMatch #[0m[33m64[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_step_main.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m49 | [0m[35m    time_posted_last_batch = time.time()[0m
[30;1m50 | [0m[35m    while not done:[0m
[30;1m51 | [0m[35m        try:[0m
[30;1m52 | [0m[35m            event_or_done = events_queue.get(timeout=1)[0m
[30;1m53 | [0m[35m            if event_or_done == DONE:[0m
[30;1m54 | [0m[35m                done = True[0m
[30;1m55 | [0m[35m            else:[0m

[31m--[ [0m[34mMatch #[0m[33m65[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_step_main.py[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m96 | [0m[35m            with open(setup_filepath, "rb") as handle:[0m
[30;1m97 | [0m[35m                databricks_config = pickle.load(handle)[0m
[30;1m98 | [0m[35m[0m
[30;1m99 | [0m[35m            # sc and dbutils are globally defined in the Databricks runtime.[0m
[30;1m100 | [0m[35m            databricks_config.setup(dbutils, sc)  # noqa: F821  # type: ignore[0m
[30;1m101 | [0m[35m[0m
[30;1m102 | [0m[35m            with open(step_run_ref_filepath, "rb") as handle:[0m

[31m--[ [0m[34mMatch #[0m[33m66[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_step_main.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m86 | [0m[35m            # Extract any zip files to a temporary directory and add that temporary directory[0m
[30;1m87 | [0m[35m            # to the site path so the contained files can be imported.[0m
[30;1m88 | [0m[35m            #[0m
[30;1m89 | [0m[35m            # We can't rely on pip or other packaging tools because the zipped files might not[0m
[30;1m90 | [0m[35m            # even be Python packages.[0m
[30;1m91 | [0m[35m            with zipfile.ZipFile(dagster_job_zip) as zf:[0m
[30;1m92 | [0m[35m                zf.extractall(tmp)[0m

[31m--[ [0m[34mMatch #[0m[33m67[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_step_main.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m14 | [0m[35mimport site[0m
[30;1m15 | [0m[35mimport sys[0m
[30;1m16 | [0m[35mimport tempfile[0m
[30;1m17 | [0m[35mimport time[0m
[30;1m18 | [0m[35mimport traceback[0m
[30;1m19 | [0m[35mimport zipfile[0m
[30;1m20 | [0m[35mfrom contextlib import redirect_stderr, redirect_stdout[0m
[30;1m21 | [0m[35mfrom io import StringIO[0m

[31m--[ [0m[34mMatch #[0m[33m68[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_step_main.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m1 | [0m[35m"""The main entrypoint for ops executed using the DatabricksPySparkStepLauncher.[0m
[30;1m2 | [0m[35m[0m
[30;1m3 | [0m[35mThis script is launched on Databricks using a `spark_python_task` and is passed the following[0m
[30;1m4 | [0m[35mparameters:[0m
[30;1m5 | [0m[35m[0m
[30;1m6 | [0m[35m- the DBFS path to the pickled `step_run_ref` file[0m

[31m--[ [0m[34mMatch #[0m[33m69[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m581 | [0m[35m            "cluster_id should be string like `1234-123456-abcdefgh` got:"[0m
[30;1m582 | [0m[35m            f" `{cluster_instance.cluster_id}`",[0m
[30;1m583 | [0m[35m        )[0m
[30;1m584 | [0m[35m        cluster = self.client.workspace_client.clusters.get(cluster_id)[0m
[30;1m585 | [0m[35m        log_config = cluster.cluster_log_conf[0m
[30;1m586 | [0m[35m        if log_config is None:[0m
[30;1m587 | [0m[35m            log.warn([0m

[31m--[ [0m[34mMatch #[0m[33m70[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m543 | [0m[35m                )[0m
[30;1m544 | [0m[35m            ],[0m
[30;1m545 | [0m[35m            idempotency_token=run_config.get("idempotency_token"),[0m
[30;1m546 | [0m[35m            timeout_seconds=run_config.get("timeout_seconds"),[0m
[30;1m547 | [0m[35m            health=jobs.JobsHealthRules.from_dict({"rules": run_config["job_health_settings"]})[0m
[30;1m548 | [0m[35m            if "job_health_settings" in run_config[0m
[30;1m549 | [0m[35m            else None,[0m

[31m--[ [0m[34mMatch #[0m[33m71[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m542 | [0m[35m                    },[0m
[30;1m543 | [0m[35m                )[0m
[30;1m544 | [0m[35m            ],[0m
[30;1m545 | [0m[35m            idempotency_token=run_config.get("idempotency_token"),[0m
[30;1m546 | [0m[35m            timeout_seconds=run_config.get("timeout_seconds"),[0m
[30;1m547 | [0m[35m            health=jobs.JobsHealthRules.from_dict({"rules": run_config["job_health_settings"]})[0m
[30;1m548 | [0m[35m            if "job_health_settings" in run_config[0m

[31m--[ [0m[34mMatch #[0m[33m72[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m530 | [0m[35m        )[0m
[30;1m531 | [0m[35m[0m
[30;1m532 | [0m[35m        return self.client.workspace_client.jobs.submit([0m
[30;1m533 | [0m[35m            run_name=run_config.get("run_name"),[0m
[30;1m534 | [0m[35m            tasks=[[0m
[30;1m535 | [0m[35m                jobs.SubmitTask.from_dict([0m
[30;1m536 | [0m[35m                    {[0m

[31m--[ [0m[34mMatch #[0m[33m73[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m517 | [0m[35m        # Only one task should be able to be chosen really; make sure of that here.[0m
[30;1m518 | [0m[35m        check.invariant([0m
[30;1m519 | [0m[35m            sum([0m
[30;1m520 | [0m[35m                task.get(key) is not None[0m
[30;1m521 | [0m[35m                for key in [[0m
[30;1m522 | [0m[35m                    "notebook_task",[0m
[30;1m523 | [0m[35m                    "spark_python_task",[0m

[31m--[ [0m[34mMatch #[0m[33m74[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m491 | [0m[35m        # since they're imported by our scripts.[0m
[30;1m492 | [0m[35m        # Add them if they're not already added by users in config.[0m
[30;1m493 | [0m[35m        libraries = list(run_config.get("libraries", []))[0m
[30;1m494 | [0m[35m        install_default_libraries = run_config.get("install_default_libraries", True)[0m
[30;1m495 | [0m[35m        if install_default_libraries:[0m
[30;1m496 | [0m[35m            python_libraries = {[0m
[30;1m497 | [0m[35m                x["pypi"]["package"].split("==")[0].replace("_", "-")[0m

[31m--[ [0m[34mMatch #[0m[33m75[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m490 | [0m[35m        # We'll always need some libraries, namely dagster/dagster_databricks/dagster_pyspark,[0m
[30;1m491 | [0m[35m        # since they're imported by our scripts.[0m
[30;1m492 | [0m[35m        # Add them if they're not already added by users in config.[0m
[30;1m493 | [0m[35m        libraries = list(run_config.get("libraries", []))[0m
[30;1m494 | [0m[35m        install_default_libraries = run_config.get("install_default_libraries", True)[0m
[30;1m495 | [0m[35m        if install_default_libraries:[0m
[30;1m496 | [0m[35m            python_libraries = {[0m

[31m--[ [0m[34mMatch #[0m[33m76[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m476 | [0m[35m            else:[0m
[30;1m477 | [0m[35m                new_cluster["autoscale"] = cluster_size["autoscale"][0m
[30;1m478 | [0m[35m[0m
[30;1m479 | [0m[35m            tags = new_cluster.get("custom_tags", {})[0m
[30;1m480 | [0m[35m            if isinstance(tags, list):[0m
[30;1m481 | [0m[35m                tags = {x["key"]: x["value"] for x in tags}[0m
[30;1m482 | [0m[35m            tags["__dagster_version"] = dagster.__version__[0m

[31m--[ [0m[34mMatch #[0m[33m77[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m442 | [0m[35m        """Submit a new run using the 'Runs submit' API."""[0m
[30;1m443 | [0m[35m        existing_cluster_id = run_config["cluster"].get("existing")[0m
[30;1m444 | [0m[35m[0m
[30;1m445 | [0m[35m        new_cluster = run_config["cluster"].get("new")[0m
[30;1m446 | [0m[35m[0m
[30;1m447 | [0m[35m        # The Databricks API needs different keys to be present in API calls depending[0m
[30;1m448 | [0m[35m        # on new/existing cluster, so we need to process the new_cluster[0m

[31m--[ [0m[34mMatch #[0m[33m78[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m440 | [0m[35m[0m
[30;1m441 | [0m[35m    def submit_run(self, run_config: Mapping[str, Any], task: Mapping[str, Any]) -> int:[0m
[30;1m442 | [0m[35m        """Submit a new run using the 'Runs submit' API."""[0m
[30;1m443 | [0m[35m        existing_cluster_id = run_config["cluster"].get("existing")[0m
[30;1m444 | [0m[35m[0m
[30;1m445 | [0m[35m        new_cluster = run_config["cluster"].get("new")[0m
[30;1m446 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m79[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m207 | [0m[35m        host = host if host else os.getenv("DATABRICKS_HOST")[0m
[30;1m208 | [0m[35m        if host is None:[0m
[30;1m209 | [0m[35m            raise ValueError([0m
[30;1m210 | [0m[35m                "Must provide host explicitly or in DATABRICKS_HOST env var when providing"[0m
[30;1m211 | [0m[35m                " credentials explicitly"[0m
[30;1m212 | [0m[35m            )[0m
[30;1m213 | [0m[35m        return host[0m

[31m--[ [0m[34mMatch #[0m[33m80[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m195 | [0m[35m            or (azure_tenant_id and not azure_client_id and not azure_client_secret)[0m
[30;1m196 | [0m[35m        ):[0m
[30;1m197 | [0m[35m            raise ValueError([0m
[30;1m198 | [0m[35m                "If using azure service principal auth, azure_client_id, azure_client_secret, and"[0m
[30;1m199 | [0m[35m                " azure_tenant_id must be provided"[0m
[30;1m200 | [0m[35m            )[0m
[30;1m201 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m81[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m186 | [0m[35m            oauth_client_secret and not oauth_client_id[0m
[30;1m187 | [0m[35m        ):[0m
[30;1m188 | [0m[35m            raise ValueError([0m
[30;1m189 | [0m[35m                "If using databricks service principal oauth credentials, both oauth_client_id and"[0m
[30;1m190 | [0m[35m                " oauth_client_secret must be provided"[0m
[30;1m191 | [0m[35m            )[0m
[30;1m192 | [0m[35m        if ([0m

[31m--[ [0m[34mMatch #[0m[33m82[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m51 | [0m[35m        azure_client_id: Optional[str],[0m
[30;1m52 | [0m[35m        azure_client_secret: Optional[str],[0m
[30;1m53 | [0m[35m        azure_tenant_id: Optional[str],[0m
[30;1m54 | [0m[35m    ):[0m
[30;1m55 | [0m[35m        """Initialize the Databricks Workspace client. Users may provide explicit credentials for a [0m
[30;1m56 | [0m[35m        service principal oauth credentials, or azure service principal credentials. If no credentia[0m
[30;1m57 | [0m[35m        the underlying WorkspaceClient from `databricks.sdk` will attempt to read credentials from t[0m
[30;1m58 | [0m[35m        from the `~/.databrickscfg` file. For more information, see the Databricks SDK docs on vario[0m

[31m--[ [0m[34mMatch #[0m[33m83[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m49 | [0m[35m,[0m
[30;1m50 | [0m[35m        oauth_client_secret: Optional[str],[0m
[30;1m51 | [0m[35m        azure_client_id: Optional[str],[0m
[30;1m52 | [0m[35m        azure_client_secret: Optional[str],[0m
[30;1m53 | [0m[35m        azure_tenant_id: Optional[str],[0m
[30;1m54 | [0m[35m    ):[0m
[30;1m55 | [0m[35m        """Initialize the Databricks Workspace client. Users may provide explicit credentials for a [0m
[30;1m56 | [0m[35m        service principal oauth credentials, or azure service principal credentials. If no credentia[0m
[30;1m57 | [0m[35m        the underlying WorkspaceClient from `databricks.sdk` will attempt to read credentials from t[0m

[31m--[ [0m[34mMatch #[0m[33m84[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m1 | [0m[35mimport base64[0m
[30;1m2 | [0m[35mimport logging[0m
[30;1m3 | [0m[35mimport os[0m
[30;1m4 | [0m[35mimport time[0m
[30;1m5 | [0m[35mfrom collections.abc import Mapping[0m
[30;1m6 | [0m[35mfrom enum import Enum[0m
[30;1m7 | [0m[35mfrom importlib.metadata import version[0m
[30;1m8 | [0m[35mfrom typing import IO, Any, Final, Optional[0m

[31m--[ [0m[34mMatch #[0m[33m85[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m1 | [0m[35mimport base64[0m
[30;1m2 | [0m[35mimport logging[0m
[30;1m3 | [0m[35mimport os[0m
[30;1m4 | [0m[35mimport time[0m
[30;1m5 | [0m[35mfrom collections.abc import Mapping[0m

[31m--[ [0m[34mMatch #[0m[33m86[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks.py[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m26 | [0m[35mfrom dagster_databricks.types import DatabricksRunState[0m
[30;1m27 | [0m[35mfrom dagster_databricks.version import __version__[0m
[30;1m28 | [0m[35m[0m
[30;1m29 | [0m[35m# wait at most 24 hours by default for run execution[0m
[30;1m30 | [0m[35mDEFAULT_RUN_MAX_WAIT_TIME_SEC: Final = 24 * 60 * 60[0m
[30;1m31 | [0m[35m[0m
[30;1m32 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m87[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m708 | [0m[35m            key = secret["key"][0m
[30;1m709 | [0m[35m            scope = secret["scope"][0m
[30;1m710 | [0m[35m            print(f"Exporting {name} from Databricks secret {key}, scope {scope}")  # noqa: T201[0m
[30;1m711 | [0m[35m            val = dbutils.secrets.get(scope=scope, key=key)[0m
[30;1m712 | [0m[35m            os.environ[name] = val[0m

[31m--[ [0m[34mMatch #[0m[33m88[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m679 | [0m[35m[0m
[30;1m680 | [0m[35m    def setup_adls2_storage(self, adls2_storage: Mapping[str, Any], dbutils: Any, sc: Any) -> None:[0m
[30;1m681 | [0m[35m        """Obtain an Azure Storage Account key from Databricks secrets and export so Spark can use i[0m
[30;1m682 | [0m[35m        storage_account_key = dbutils.secrets.get([0m
[30;1m683 | [0m[35m            scope=adls2_storage["secret_scope"], key=adls2_storage["storage_account_key_key"][0m
[30;1m684 | [0m[35m        )[0m
[30;1m685 | [0m[35m        # Spark APIs will use this.[0m

[31m--[ [0m[34mMatch #[0m[33m89[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m666 | [0m[35m        scope = s3_storage["secret_scope"][0m
[30;1m667 | [0m[35m[0m
[30;1m668 | [0m[35m        access_key = dbutils.secrets.get(scope=scope, key=s3_storage["access_key_key"])[0m
[30;1m669 | [0m[35m        secret_key = dbutils.secrets.get(scope=scope, key=s3_storage["secret_key_key"])[0m
[30;1m670 | [0m[35m[0m
[30;1m671 | [0m[35m        # Spark APIs will use this.[0m
[30;1m672 | [0m[35m        # See https://docs.databricks.com/data/data-sources/aws/amazon-s3.html#alternative-1-set-aws[0m

[31m--[ [0m[34mMatch #[0m[33m90[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m665 | [0m[35m        """Obtain AWS credentials from Databricks secrets and export so both Spark and boto can use [0m
[30;1m666 | [0m[35m        scope = s3_storage["secret_scope"][0m
[30;1m667 | [0m[35m[0m
[30;1m668 | [0m[35m        access_key = dbutils.secrets.get(scope=scope, key=s3_storage["access_key_key"])[0m
[30;1m669 | [0m[35m        secret_key = dbutils.secrets.get(scope=scope, key=s3_storage["secret_key_key"])[0m
[30;1m670 | [0m[35m[0m
[30;1m671 | [0m[35m        # Spark APIs will use this.[0m

[31m--[ [0m[34mMatch #[0m[33m91[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m336 | [0m[35m        run = self.databricks_runner.client.workspace_client.jobs.get_run([0m
[30;1m337 | [0m[35m            databricks_run_id[0m
[30;1m338 | [0m[35m        ).as_dict()[0m
[30;1m339 | [0m[35m        run_page_url = run.get("run_page_url")[0m
[30;1m340 | [0m[35m        if run_page_url:[0m
[30;1m341 | [0m[35m            metadata["databricks_run_url"] = MetadataValue.url(run_page_url)[0m
[30;1m342 | [0m[35m        return DagsterEvent.from_step([0m

[31m--[ [0m[34mMatch #[0m[33m92[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m263 | [0m[35m            oauth_client_secret=oauth_credentials.get("client_secret"),[0m
[30;1m264 | [0m[35m            azure_client_id=azure_credentials.get("azure_client_id"),[0m
[30;1m265 | [0m[35m            azure_client_secret=azure_credentials.get("azure_client_secret"),[0m
[30;1m266 | [0m[35m            azure_tenant_id=azure_credentials.get("azure_tenant_id"),[0m
[30;1m267 | [0m[35m            poll_interval_sec=poll_interval_sec,[0m
[30;1m268 | [0m[35m            max_wait_time_sec=max_completion_wait_time_seconds,[0m
[30;1m269 | [0m[35m        )[0m

[31m--[ [0m[34mMatch #[0m[33m93[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m262 | [0m[35m            oauth_client_id=oauth_credentials.get("client_id"),[0m
[30;1m263 | [0m[35m            oauth_client_secret=oauth_credentials.get("client_secret"),[0m
[30;1m264 | [0m[35m            azure_client_id=azure_credentials.get("azure_client_id"),[0m
[30;1m265 | [0m[35m            azure_client_secret=azure_credentials.get("azure_client_secret"),[0m
[30;1m266 | [0m[35m            azure_tenant_id=azure_credentials.get("azure_tenant_id"),[0m
[30;1m267 | [0m[35m            poll_interval_sec=poll_interval_sec,[0m
[30;1m268 | [0m[35m            max_wait_time_sec=max_completion_wait_time_seconds,[0m

[31m--[ [0m[34mMatch #[0m[33m94[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m261 | [0m[35m            token=databricks_token,[0m
[30;1m262 | [0m[35m            oauth_client_id=oauth_credentials.get("client_id"),[0m
[30;1m263 | [0m[35m            oauth_client_secret=oauth_credentials.get("client_secret"),[0m
[30;1m264 | [0m[35m            azure_client_id=azure_credentials.get("azure_client_id"),[0m
[30;1m265 | [0m[35m            azure_client_secret=azure_credentials.get("azure_client_secret"),[0m
[30;1m266 | [0m[35m            azure_tenant_id=azure_credentials.get("azure_tenant_id"),[0m
[30;1m267 | [0m[35m            poll_interval_sec=poll_interval_sec,[0m

[31m--[ [0m[34mMatch #[0m[33m95[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m260 | [0m[35m            host=databricks_host,[0m
[30;1m261 | [0m[35m            token=databricks_token,[0m
[30;1m262 | [0m[35m            oauth_client_id=oauth_credentials.get("client_id"),[0m
[30;1m263 | [0m[35m            oauth_client_secret=oauth_credentials.get("client_secret"),[0m
[30;1m264 | [0m[35m            azure_client_id=azure_credentials.get("azure_client_id"),[0m
[30;1m265 | [0m[35m            azure_client_secret=azure_credentials.get("azure_client_secret"),[0m
[30;1m266 | [0m[35m            azure_tenant_id=azure_credentials.get("azure_tenant_id"),[0m

[31m--[ [0m[34mMatch #[0m[33m96[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m259 | [0m[35m        self.databricks_runner = DatabricksJobRunner([0m
[30;1m260 | [0m[35m            host=databricks_host,[0m
[30;1m261 | [0m[35m            token=databricks_token,[0m
[30;1m262 | [0m[35m            oauth_client_id=oauth_credentials.get("client_id"),[0m
[30;1m263 | [0m[35m            oauth_client_secret=oauth_credentials.get("client_secret"),[0m
[30;1m264 | [0m[35m            azure_client_id=azure_credentials.get("azure_client_id"),[0m
[30;1m265 | [0m[35m            azure_client_secret=azure_credentials.get("azure_client_secret"),[0m

[31m--[ [0m[34mMatch #[0m[33m97[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m684 | [0m[35m        )[0m
[30;1m685 | [0m[35m        # Spark APIs will use this.[0m
[30;1m686 | [0m[35m        # See https://docs.microsoft.com/en-gb/azure/databricks/data/data-sources/azure/azure-datala[0m
[30;1m687 | [0m[35m        # sc is globally defined in the Databricks runtime and points to the Spark context[0m
[30;1m688 | [0m[35m        sc._jsc.hadoopConfiguration().set(  # noqa: SLF001[0m
[30;1m689 | [0m[35m            "fs.azure.account.key.{}.dfs.core.windows.net".format([0m
[30;1m690 | [0m[35m                adls2_storage["storage_account_name"][0m

[31m--[ [0m[34mMatch #[0m[33m98[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m602 | [0m[35m        return f"dbfs://{path}"[0m
[30;1m603 | [0m[35m[0m
[30;1m604 | [0m[35m    def _internal_dbfs_path(self, run_id: str, step_key: str, filename: str) -> str:[0m
[30;1m605 | [0m[35m        """Scripts running on Databricks should access DBFS at /dbfs/."""[0m
[30;1m606 | [0m[35m        path = "/".join([0m
[30;1m607 | [0m[35m            [[0m
[30;1m608 | [0m[35m                self.staging_prefix,[0m

[31m--[ [0m[34mMatch #[0m[33m99[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m469 | [0m[35m        if "cluster_permissions" in self.permissions:[0m
[30;1m470 | [0m[35m            if "existing" in self.run_config["cluster"]:[0m
[30;1m471 | [0m[35m                raise ValueError([0m
[30;1m472 | [0m[35m                    "Attempting to update permissions of an existing cluster. "[0m
[30;1m473 | [0m[35m                    "This is dangerous and thus unsupported."[0m
[30;1m474 | [0m[35m                )[0m
[30;1m475 | [0m[35m            cluster_permissions = self._format_permissions(self.permissions["cluster_permissions"])[0m

[31m--[ [0m[34mMatch #[0m[33m100[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m465 | [0m[35m            client.permissions.update("jobs", str(job_id), access_control_list=job_permissions)[0m
[30;1m466 | [0m[35m            log.info("Successfully updated cluster permissions")[0m
[30;1m467 | [0m[35m[0m
[30;1m468 | [0m[35m        # Update cluster permissions[0m
[30;1m469 | [0m[35m        if "cluster_permissions" in self.permissions:[0m
[30;1m470 | [0m[35m            if "existing" in self.run_config["cluster"]:[0m
[30;1m471 | [0m[35m                raise ValueError([0m

[31m--[ [0m[34mMatch #[0m[33m101[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m454 | [0m[35m            )[0m
[30;1m455 | [0m[35m            return[0m
[30;1m456 | [0m[35m[0m
[30;1m457 | [0m[35m        # Update job permissions[0m
[30;1m458 | [0m[35m        if "job_permissions" in self.permissions:[0m
[30;1m459 | [0m[35m            job_permissions = self._format_permissions(self.permissions["job_permissions"])[0m
[30;1m460 | [0m[35m            job_id = check.not_none([0m

[31m--[ [0m[34mMatch #[0m[33m102[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m626 | [0m[35m    The `setup` method handles the actual setup prior to op execution on the Databricks side.[0m
[30;1m627 | [0m[35m[0m
[30;1m628 | [0m[35m    This config is separated out from the regular Dagster run config system because the setup[0m
[30;1m629 | [0m[35m    is done by the 'main' script before entering a Dagster context (i.e. using `run_step_from_ref`).[0m
[30;1m630 | [0m[35m    We use a separate class to avoid coupling the setup to the format of the `step_run_ref` object.[0m
[30;1m631 | [0m[35m    """[0m
[30;1m632 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m103[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m620 | [0m[35m    Instances of this class will be created when a Databricks step is launched and will contain[0m
[30;1m621 | [0m[35m    all configuration and secrets required to set up storage and environment variables within[0m
[30;1m622 | [0m[35m    the Databricks environment. The instance will be serialized and uploaded to Databricks[0m
[30;1m623 | [0m[35m    by the step launcher, then deserialized as part of the 'main' script when the job is running[0m
[30;1m624 | [0m[35m    in Databricks.[0m
[30;1m625 | [0m[35m[0m
[30;1m626 | [0m[35m    The `setup` method handles the actual setup prior to op execution on the Databricks side.[0m

[31m--[ [0m[34mMatch #[0m[33m104[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m418 | [0m[35m                max_retries=4,[0m
[30;1m419 | [0m[35m            )[0m
[30;1m420 | [0m[35m        # if you poll before the Databricks process has had a chance to create the file,[0m
[30;1m421 | [0m[35m        # we expect to get this error[0m
[30;1m422 | [0m[35m        except DatabricksError as e:[0m
[30;1m423 | [0m[35m            if e.error_code == "RESOURCE_DOES_NOT_EXIST":[0m
[30;1m424 | [0m[35m                return [][0m

[31m--[ [0m[34mMatch #[0m[33m105[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/databricks_pyspark_step_launcher.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m4 | [0m[35mimport pickle[0m
[30;1m5 | [0m[35mimport sys[0m
[30;1m6 | [0m[35mimport tempfile[0m
[30;1m7 | [0m[35mimport time[0m
[30;1m8 | [0m[35mimport zlib[0m
[30;1m9 | [0m[35mfrom collections.abc import Iterator, Mapping, Sequence[0m
[30;1m10 | [0m[35mfrom typing import Any, Optional, cast[0m
[30;1m11 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m106[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m490 | [0m[35m    def _get_log_path(self, params: PipesParams) -> Optional[str]:[0m
[30;1m491 | [0m[35m        if self.log_path is None:[0m
[30;1m492 | [0m[35m            cluster_driver_log_root = ([0m
[30;1m493 | [0m[35m                params["extras"].get("cluster_driver_log_root") if "extras" in params else None[0m
[30;1m494 | [0m[35m            )[0m
[30;1m495 | [0m[35m            if cluster_driver_log_root is None:[0m
[30;1m496 | [0m[35m                return None[0m

[31m--[ [0m[34mMatch #[0m[33m107[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m231 | [0m[35m            }[0m
[30;1m232 | [0m[35m[0m
[30;1m233 | [0m[35m        submit_task_dict["tags"] = {[0m
[30;1m234 | [0m[35m            **submit_task_dict.get("tags", {}),[0m
[30;1m235 | [0m[35m            **session.default_remote_invocation_info,[0m
[30;1m236 | [0m[35m        }[0m
[30;1m237 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m108[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m225 | [0m[35m            pipes_env_vars = session.get_bootstrap_env_vars()[0m
[30;1m226 | [0m[35m[0m
[30;1m227 | [0m[35m            submit_task_dict["new_cluster"]["spark_env_vars"] = {[0m
[30;1m228 | [0m[35m                **submit_task_dict["new_cluster"].get("spark_env_vars", {}),[0m
[30;1m229 | [0m[35m                **(self.env or {}),[0m
[30;1m230 | [0m[35m                **pipes_env_vars,[0m
[30;1m231 | [0m[35m            }[0m

[31m--[ [0m[34mMatch #[0m[33m109[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m209 | [0m[35m[0m
[30;1m210 | [0m[35m            for task_type in self.get_task_fields_which_support_cli_parameters():[0m
[30;1m211 | [0m[35m                if task_type in submit_task_dict:[0m
[30;1m212 | [0m[35m                    existing_params = submit_task_dict[task_type].get("parameters", [])[0m
[30;1m213 | [0m[35m[0m
[30;1m214 | [0m[35m                    # merge the existing parameters with the CLI arguments[0m
[30;1m215 | [0m[35m                    for key, value in cli_args.items():[0m

[31m--[ [0m[34mMatch #[0m[33m110[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m97 | [0m[35m        if task.existing_cluster_id is not None:[0m
[30;1m98 | [0m[35m            cluster = self.client.clusters.get(cluster_id=task.existing_cluster_id).as_dict()[0m
[30;1m99 | [0m[35m[0m
[30;1m100 | [0m[35m            if cluster.get("cluster_log_conf", {}).get("dbfs", None):[0m
[30;1m101 | [0m[35m                existing_cluster_has_logging_configured = True[0m
[30;1m102 | [0m[35m[0m
[30;1m103 | [0m[35m        if ([0m

[31m--[ [0m[34mMatch #[0m[33m111[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m97 | [0m[35m        if task.existing_cluster_id is not None:[0m
[30;1m98 | [0m[35m            cluster = self.client.clusters.get(cluster_id=task.existing_cluster_id).as_dict()[0m
[30;1m99 | [0m[35m[0m
[30;1m100 | [0m[35m            if cluster.get("cluster_log_conf", {}).get("dbfs", None):[0m
[30;1m101 | [0m[35m                existing_cluster_has_logging_configured = True[0m
[30;1m102 | [0m[35m[0m
[30;1m103 | [0m[35m        if ([0m

[31m--[ [0m[34mMatch #[0m[33m112[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m95 | [0m[35m[0m
[30;1m96 | [0m[35m        existing_cluster_has_logging_configured = False[0m
[30;1m97 | [0m[35m        if task.existing_cluster_id is not None:[0m
[30;1m98 | [0m[35m            cluster = self.client.clusters.get(cluster_id=task.existing_cluster_id).as_dict()[0m
[30;1m99 | [0m[35m[0m
[30;1m100 | [0m[35m            if cluster.get("cluster_log_conf", {}).get("dbfs", None):[0m
[30;1m101 | [0m[35m                existing_cluster_has_logging_configured = True[0m

[31m--[ [0m[34mMatch #[0m[33m113[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m90 | [0m[35m        # include log readers if the user is writing their logs to DBFS[0m
[30;1m91 | [0m[35m[0m
[30;1m92 | [0m[35m        new_cluster_logging_configured = ([0m
[30;1m93 | [0m[35m            task.as_dict().get("new_cluster", {}).get("cluster_log_conf", {}).get("dbfs", None)[0m
[30;1m94 | [0m[35m        )[0m
[30;1m95 | [0m[35m[0m
[30;1m96 | [0m[35m        existing_cluster_has_logging_configured = False[0m

[31m--[ [0m[34mMatch #[0m[33m114[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m90 | [0m[35m        # include log readers if the user is writing their logs to DBFS[0m
[30;1m91 | [0m[35m[0m
[30;1m92 | [0m[35m        new_cluster_logging_configured = ([0m
[30;1m93 | [0m[35m            task.as_dict().get("new_cluster", {}).get("cluster_log_conf", {}).get("dbfs", None)[0m
[30;1m94 | [0m[35m        )[0m
[30;1m95 | [0m[35m[0m
[30;1m96 | [0m[35m        existing_cluster_has_logging_configured = False[0m

[31m--[ [0m[34mMatch #[0m[33m115[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m90 | [0m[35m        # include log readers if the user is writing their logs to DBFS[0m
[30;1m91 | [0m[35m[0m
[30;1m92 | [0m[35m        new_cluster_logging_configured = ([0m
[30;1m93 | [0m[35m            task.as_dict().get("new_cluster", {}).get("cluster_log_conf", {}).get("dbfs", None)[0m
[30;1m94 | [0m[35m        )[0m
[30;1m95 | [0m[35m[0m
[30;1m96 | [0m[35m        existing_cluster_has_logging_configured = False[0m

[31m--[ [0m[34mMatch #[0m[33m116[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m203 | [0m[35m        submit_task_dict: dict[str, Any],[0m
[30;1m204 | [0m[35m    ) -> dict[str, Any]:[0m
[30;1m205 | [0m[35m        if "existing_cluster_id" in submit_task_dict:[0m
[30;1m206 | [0m[35m            # we can't set env vars on an existing cluster[0m
[30;1m207 | [0m[35m            # so we must use CLI to pass Pipes params[0m
[30;1m208 | [0m[35m            cli_args = session.get_bootstrap_cli_arguments()  # this is a mapping[0m
[30;1m209 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m117[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m66 | [0m[35m        forward_termination: bool = True,[0m
[30;1m67 | [0m[35m    ):[0m
[30;1m68 | [0m[35m        self.client = client[0m
[30;1m69 | [0m[35m        self.env = env[0m
[30;1m70 | [0m[35m        self.context_injector = check.opt_inst_param([0m
[30;1m71 | [0m[35m            context_injector,[0m
[30;1m72 | [0m[35m            "context_injector",[0m
[30;1m73 | [0m[35m            PipesContextInjector,[0m

[31m--[ [0m[34mMatch #[0m[33m118[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m39 | [0m[35m[0m
[30;1m40 | [0m[35m    Args:[0m
[30;1m41 | [0m[35m        client (WorkspaceClient): A databricks `WorkspaceClient` object.[0m
[30;1m42 | [0m[35m        env (Optional[Mapping[str,str]]: An optional dict of environment[0m
[30;1m43 | [0m[35m            variables to pass to the databricks job.[0m
[30;1m44 | [0m[35m        context_injector (Optional[PipesContextInjector]): A context injector to use to inject[0m
[30;1m45 | [0m[35m            context into the k8s container process. Defaults to :py:class:`PipesDbfsContextInjector`[0m

[31m--[ [0m[34mMatch #[0m[33m119[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m4 | [0m[35mimport random[0m
[30;1m5 | [0m[35mimport string[0m
[30;1m6 | [0m[35mimport sys[0m
[30;1m7 | [0m[35mimport time[0m
[30;1m8 | [0m[35mfrom collections.abc import Iterator, Mapping, Sequence[0m
[30;1m9 | [0m[35mfrom contextlib import ExitStack, contextmanager[0m
[30;1m10 | [0m[35mfrom typing import Any, Literal, Optional, TextIO, Union[0m
[30;1m11 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m120[0m[34m of [0m[33m120[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/dagster_databricks/pipes.py[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m1 | [0m[35mimport base64[0m
[30;1m2 | [0m[35mimport json[0m
[30;1m3 | [0m[35mimport os[0m
[30;1m4 | [0m[35mimport random[0m
[30;1m5 | [0m[35mimport string[0m

120 matches found.
