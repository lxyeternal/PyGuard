Run started:2025-04-12 14:49:25.943736

Test results:
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/setup.py:83
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
82	        (
83	            "Warning: {} exists.\n\n"
84	            "If you recently updated transformers to 3.0 or later, this is expected,\n"
85	            "but it may prevent transformers from installing in editable mode.\n\n"

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/setup.py:91
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
90	    )
91	    shutil.rmtree(stale_egg_info)
92	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/setup.py:266
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
265	        with open(target, "w", encoding="utf-8", newline="\n") as f:
266	            f.write("\n".join(content))
267	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/setup.py:457
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
456	    version="4.51.2",  # expected format is one of x.y.z.dev0, or x.y.z.rc1 or x.y.z (no to dashes, yes to dots)
457	    author="The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)",
458	    author_email="transformers@huggingface.co",

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/setup.py:460
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
459	    description="State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow",
460	    long_description=open("README.md", "r", encoding="utf-8").read(),
461	    long_description_content_type="text/markdown",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/setup.py:464
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
463	    license="Apache 2.0 License",
464	    url="https://github.com/huggingface/transformers",
465	    package_dir={"": "src"},

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/agent_types.py:218
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
217	        if self._path is not None:
218	            tensor, self.samplerate = sf.read(self._path)
219	            self._tensor = torch.tensor(tensor)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/agent_types.py:233
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
232	            self._path = os.path.join(directory, str(uuid.uuid4()) + ".wav")
233	            sf.write(self._path, self._tensor, samplerate=self.samplerate)
234	            return self._path

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/agent_types.py:233
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
232	            self._path = os.path.join(directory, str(uuid.uuid4()) + ".wav")
233	            sf.write(self._path, self._tensor, samplerate=self.samplerate)
234	            return self._path

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/agents.py:727
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
726	        version="4.51.0",
727	        message="Switch to smolagents instead, with the same functionalities and similar API (https://huggingface.co/docs/smolagents/index)",
728	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/agents.py:785
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
784	        version="4.51.0",
785	        message="Switch to smolagents instead, with the same functionalities and similar API (https://huggingface.co/docs/smolagents/index)",
786	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/agents.py:1240
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1239	        version="4.51.0",
1240	        message="Switch to smolagents instead, with the same functionalities and similar API (https://huggingface.co/docs/smolagents/index)",
1241	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/llm_engine.py:79
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
78	        version="4.51.0",
79	        message="Switch to smolagents instead, with the same functionalities and similar API (https://huggingface.co/docs/smolagents/index)",
80	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/prompts.py:476
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
475	
476	SYSTEM_PROMPT_PLAN_STRUCTURED = """Output a step-by-step plan to solve the task using the given tools.
477	This plan should involve individual tasks based on the available tools, that if executed correctly will yield the correct answer. Each step should be structured as follows:
478	Step #n: {
479	  "description": <description of what the step does and its output>
480	  "tool": <tool to use>,
481	  "params": {
482	      <parameters to pass to the tool as a valid dict>
483	  }
484	  "output_var": <output variable name>
485	}
486	Each step must be necessary to reach the final answer. Steps should reuse outputs produced by earlier steps. The last step must be the final answer.
487	
488	Below are some examples:
489	
490	Example 1:
491	------
492	Inputs:
493	---
494	Task:
495	How many encoder blocks were in the first attention-only ML architecture published?
496	
497	[FACTS LIST]:
498	### 1. Facts given in the task
499	- The paper first introduced an attention-only ML architecture.
500	- The specific information required is the page number where the number of encoder blocks is stated.
501	- No local files are provided for access.
502	
503	### 2. Facts to look up
504	- The title and authors of the paper that first introduced an attention-only ML architecture.
505	  - Source: Online search (e.g., Google Scholar, arXiv, or other academic databases)
506	- The full text of the identified paper.
507	  - Source: Online academic repositories (e.g., arXiv, journal websites)
508	- The specific page number in the paper where the number of encoder blocks is mentioned.
509	  - Source: The content of the identified paper
510	
511	### 3. Facts to derive
512	- By identifying the correct paper and locating the specific page, we will derive the page number where the number of encoder blocks is stated.
513	  - Logical steps: Identify the correct paper, access its content, search for the term "encoder blocks," and note the page number where this information is found.
514	```
515	
516	[STEP 1 TOOL CALL]: {'tool_name': 'code interpreter', 'tool_arguments': '# Step 1: Identify the title and authors of the paper that first introduced an attention-only ML architecture.\nanswer = ask_search_agent(query="Can you find the title and authors of the paper that first introduced an attention-only machine learning architecture? Please provide the full citation.")\nprint(answer)'}
517	[OUTPUT OF STEP 1] Observation: **Title**: Attention Is All You Need
518	**Authors**: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
519	[STEP 2 TOOL CALL]: {'tool_name': 'code interpreter', 'tool_arguments': '# Step 1: Find the full text of the identified paper on arXiv\\npaper_url = "https://arxiv.org/pdf/1706.03762.pdf"\\nprint(paper_url)'}
520	[OUTPUT OF STEP 2] Observation: https://arxiv.org/pdf/1706.03762.pdf
521	---
522	
523	Output plan:
524	---
525	Step #1: {
526	  "description": "Open the PDF of the paper from the provided URL and search within the text of the paper for the  mention of "encoder blocks"",
527	  "tool": "inspect_file_as_text",
528	  "params": {
529	    "file_path": "https://arxiv.org/pdf/1706.03762.pdf",
530	    "question": "On which page is the number of encoder blocks mentioned?"
531	  },
532	  "output_var": "page_number"
533	}
534	
535	Step #2: {
536	  "description": "Provide the final answer",
537	  "tool": "final_answer",
538	  "params": {
539	      "answer": "{page_number}"
540	  },
541	  "output_var": ""
542	}
543	------
544	
545	Example 2:
546	------
547	Inputs:
548	---
549	Task:
550	How many golf balls fits into a Boeing-747?
551	
552	[FACTS LIST]:
553	### 1. Facts given in the task
554	- The task requires calculating the number of golf balls that fir into a Boeing-747
555	### 2. Facts to look up
556	- The volume of a golf ball
557	- The volume of a Boeing-747
558	### 3. Facts to derive
559	- Once the volumes are known the final answer can be calculated
560	---
561	Output plan:
562	---
563	Step #1: {
564	  "description": "Find the volume of a Boeing-747",
565	  "tool": "web_search",
566	  "params": {
567	      "query": "What is the internal volume of a Boeing-747 in cubic meters?"
568	  },
569	  "output_var": "boeing_volume"
570	}
571	
572	Step #2: {
573	  "description": "Find the volume of a standard golf ball",
574	  "tool": "ask_search_agent",
575	  "params": {
576	      "query": "What is the volume of a standard golf ball in cubic centimeters?"
577	  },
578	  "output_var": "golf_ball_volume"
579	}
580	
581	Step #3: {
582	  "description": "Convert the volume of a golf ball from cubic centimeters to cubic meters. Calculate the number of golf balls that fit into the Boeing-747 by dividing the internal volume of the Boeing-747 by the volume of a golf ball.",
583	  "tool": "python_code",
584	  "params": {
585	      "code": "golf_ball_volume_m3 = golf_ball_volume / 1e6\nnumber_of_golf_balls = boeing_volume / golf_ball_volume_m3"
586	  },
587	  "output_var": "number_of_golf_balls"
588	}
589	
590	Step #4: {
591	  "description": "Provide the final answer",
592	  "tool": "final_answer",
593	  "params": {
594	      "answer": "{number_of_golf_balls}"
595	  },
596	  "output_var": ""
597	}
598	------
599	Above example were using tools that might not exist for you.
600	Your goal is to create a plan to solve the task."""
601	
602	USER_PROMPT_PLAN_STRUCTURED = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/prompts.py:622
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
621	
622	SYSTEM_PROMPT_PLAN_UPDATE_STRUCTURED = """Output a step-by-step plan to solve the task using the given tools.
623	This plan should involve individual tasks based on the available tools, that if executed correctly will yield the correct answer. Each step should be structured as follows:
624	Step #n: {{
625	  "description": <description of what the step does and its output>
626	  "tool": <tool to use>,
627	  "params": {{
628	      <parameters to pass to the tool as a valid dict>
629	  }}
630	  "output_var": <output variable name>
631	}}
632	Each step must be necessary to reach the final answer. Steps should reuse outputs produced by earlier steps. The last step must be the final answer.
633	
634	Below are some examples:
635	
636	Example 1:
637	------
638	Inputs:
639	---
640	Task:
641	How many encoder blocks were in the first attention-only ML architecture published?
642	
643	[FACTS LIST]:
644	### 1. Facts given in the task
645	- The paper first introduced an attention-only ML architecture.
646	- The specific information required is the page number where the number of encoder blocks is stated.
647	- No local files are provided for access.
648	
649	### 2. Facts to look up
650	- The title and authors of the paper that first introduced an attention-only ML architecture.
651	  - Source: Online search (e.g., Google Scholar, arXiv, or other academic databases)
652	- The full text of the identified paper.
653	  - Source: Online academic repositories (e.g., arXiv, journal websites)
654	- The specific page number in the paper where the number of encoder blocks is mentioned.
655	  - Source: The content of the identified paper
656	
657	### 3. Facts to derive
658	- By identifying the correct paper and locating the specific page, we will derive the page number where the number of encoder blocks is stated.
659	  - Logical steps: Identify the correct paper, access its content, search for the term "encoder blocks," and note the page number where this information is found.
660	```
661	
662	[STEP 1 TOOL CALL]: {{'tool_name': 'code interpreter', 'tool_arguments': '# Step 1: Identify the title and authors of the paper that first introduced an attention-only ML architecture.\nanswer = ask_search_agent(query="Can you find the title and authors of the paper that first introduced an attention-only machine learning architecture? Please provide the full citation.")\nprint(answer)'}}
663	[OUTPUT OF STEP 1] Observation: **Title**: Attention Is All You Need
664	**Authors**: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
665	[STEP 2 TOOL CALL]: {{'tool_name': 'code interpreter', 'tool_arguments': '# Step 1: Find the full text of the identified paper on arXiv\\npaper_url = "https://arxiv.org/pdf/1706.03762.pdf"\\nprint(paper_url)'}}
666	[OUTPUT OF STEP 2] Observation: https://arxiv.org/pdf/1706.03762.pdf
667	---
668	
669	Output plan:
670	---
671	Step #1: {{
672	  "description": "Open the PDF of the paper from the provided URL and search within the text of the paper for the  mention of "encoder blocks"",
673	  "tool": "inspect_file_as_text",
674	  "params": {{
675	    "file_path": "https://arxiv.org/pdf/1706.03762.pdf",
676	    "question": "On which page is the number of encoder blocks mentioned?"
677	  }},
678	  "output_var": "page_number"
679	}}
680	
681	Step #2: {{
682	  "description": "Provide the final answer",
683	  "tool": "final_answer",
684	  "params": {{
685	      "answer": "{{page_number}}"
686	  }},
687	  "output_var": ""
688	}}
689	------
690	
691	Example 2:
692	------
693	Inputs:
694	---
695	Task:
696	How many golf balls fits into a Boeing-747?
697	
698	[FACTS LIST]:
699	### 1. Facts given in the task
700	- The task requires calculating the number of golf balls that fir into a Boeing-747
701	### 2. Facts to look up
702	- The volume of a golf ball
703	- The volume of a Boeing-747
704	### 3. Facts to derive
705	- Once the volumes are known the final answer can be calculated
706	---
707	Output plan:
708	---
709	Step #1: {{
710	  "description": "Find the volume of a Boeing-747",
711	  "tool": "web_search",
712	  "params": {{
713	      "query": "What is the internal volume of a Boeing-747 in cubic meters?"
714	  }},
715	  "output_var": "boeing_volume"
716	}}
717	
718	Step #2: {{
719	  "description": "Find the volume of a standard golf ball",
720	  "tool": "ask_search_agent",
721	  "params": {{
722	      "query": "What is the volume of a standard golf ball in cubic centimeters?"
723	  }},
724	  "output_var": "golf_ball_volume"
725	}}
726	
727	Step #3: {{
728	  "description": "Convert the volume of a golf ball from cubic centimeters to cubic meters. Calculate the number of golf balls that fit into the Boeing-747 by dividing the internal volume of the Boeing-747 by the volume of a golf ball.",
729	  "tool": "python_code",
730	  "params": {{
731	      "code": "golf_ball_volume_m3 = golf_ball_volume / 1e6\nnumber_of_golf_balls = boeing_volume / golf_ball_volume_m3"
732	  }},
733	  "output_var": "number_of_golf_balls"
734	}}
735	
736	Step #4: {{
737	  "description": "Provide the final answer",
738	  "tool": "final_answer",
739	  "params": {{
740	      "answer": "{{number_of_golf_balls}}"
741	  }},
742	  "output_var": ""
743	}}
744	------
745	Above example were using tools that might not exist for you.
746	Find below the record of what has been tried so far to solve it. Your goal is to create an updated plan to solve the task."""
747	
748	USER_PROMPT_PLAN_UPDATE_STRUCTURED = """

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/search.py:63
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
62	            # Send a GET request to the URL
63	            response = requests.get(url)
64	            response.raise_for_status()  # Raise an exception for bad status codes

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/tools.py:138
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
137	        version="4.51.0",
138	        message="Switch to smolagents instead, with the same functionalities and similar API (https://huggingface.co/docs/smolagents/index)",
139	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/tools.py:145
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
144	        version="4.51.0",
145	        message="Switch to smolagents instead, with the same functionalities and similar API (https://huggingface.co/docs/smolagents/index)",
146	    )

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/tools.py:246
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
245	        with open(config_file, "w", encoding="utf-8") as f:
246	            f.write(json.dumps(tool_config, indent=2, sort_keys=True) + "\n")
247	

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/tools.py:246
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
245	        with open(config_file, "w", encoding="utf-8") as f:
246	            f.write(json.dumps(tool_config, indent=2, sort_keys=True) + "\n")
247	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/tools.py:251
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
250	        with open(app_file, "w", encoding="utf-8") as f:
251	            f.write(APP_FILE_TEMPLATE.format(module_name=last_module, class_name=self.__class__.__name__))
252	

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/tools.py:251
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
250	        with open(app_file, "w", encoding="utf-8") as f:
251	            f.write(APP_FILE_TEMPLATE.format(module_name=last_module, class_name=self.__class__.__name__))
252	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/tools.py:260
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
259	        with open(requirements_file, "w", encoding="utf-8") as f:
260	            f.write("\n".join(imports) + "\n")
261	

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/tools.py:260
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
259	        with open(requirements_file, "w", encoding="utf-8") as f:
260	            f.write("\n".join(imports) + "\n")
261	

--------------------------------------------------
>> Issue: [B802:b64encode] base64.b64encode
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/tools.py:909
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b802_b64encode.html
908	        image.save(_bytes, format="PNG")
909	        b64 = base64.b64encode(_bytes.getvalue())
910	        return b64.decode("utf-8")

--------------------------------------------------
>> Issue: [B301:blacklist] base64.b64encode
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/tools.py:909
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b301-base64-b64encode
908	        image.save(_bytes, format="PNG")
909	        b64 = base64.b64encode(_bytes.getvalue())
910	        return b64.decode("utf-8")

--------------------------------------------------
>> Issue: [B801:b64decode] base64.b64decode
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/tools.py:921
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b801_b64decode.html
920	
921	        b64 = base64.b64decode(raw_image)
922	        _bytes = io.BytesIO(b64)

--------------------------------------------------
>> Issue: [B300:blacklist] base64.b64decode
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/agents/tools.py:921
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b300-base64-b64decode
920	
921	        b64 = base64.b64decode(raw_image)
922	        _bytes = io.BytesIO(b64)

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/audio_utils.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
54	        if audio.startswith("http://") or audio.startswith("https://"):
55	            audio = librosa.load(BytesIO(requests.get(audio, timeout=timeout).content), sr=sampling_rate)[0]
56	        elif os.path.isfile(audio):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/cache_utils.py:146
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
145	
146	            writer.write(json_string)
147	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_fast_image_processor.py:123
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
122	    with open(TRANSFORMERS_PATH / "__init__.py", "r", encoding="utf-8") as f:
123	        content = f.read()
124	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_fast_image_processor.py:132
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
131	    with open(TRANSFORMERS_PATH / "__init__.py", "w", encoding="utf-8") as f:
132	        f.write(content)
133	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_fast_image_processor.py:142
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
141	    with open(TRANSFORMERS_PATH / "models" / model_name / "__init__.py", "r", encoding="utf-8") as f:
142	        content = f.read()
143	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_fast_image_processor.py:248
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
247	    with open(TRANSFORMERS_PATH / "models" / model_name / "__init__.py", "w", encoding="utf-8") as f:
248	        f.write(updated_content)
249	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_fast_image_processor.py:256
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
255	    with open(TRANSFORMERS_PATH / "models" / "auto" / "image_processing_auto.py", "r", encoding="utf-8") as f:
256	        content = f.read()
257	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_fast_image_processor.py:265
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
264	    with open(TRANSFORMERS_PATH / "models" / "auto" / "image_processing_auto.py", "w", encoding="utf-8") as f:
265	        f.write(updated_content)
266	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_fast_image_processor.py:274
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
273	    with open(dummy_torchvision_objects_file, "r", encoding="utf-8") as f:
274	        content = f.read()
275	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_fast_image_processor.py:299
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
298	        with open(dummy_torchvision_objects_file, "w", encoding="utf-8") as f:
299	            f.write(updated_content)
300	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_fast_image_processor.py:322
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
321	        with open(doc_file, "r", encoding="utf-8") as f:
322	            content = f.read()
323	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_fast_image_processor.py:333
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
332	            with open(doc_file, "w", encoding="utf-8") as f:
333	                f.write(updated_content)
334	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_fast_image_processor.py:347
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
346	    with open(test_file, "r", encoding="utf-8") as f:
347	        content = f.read()
348	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_fast_image_processor.py:408
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
407	    with open(test_file, "w", encoding="utf-8") as f:
408	        f.write(updated_content)
409	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_fast_image_processor.py:420
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
419	        return (
420	            f"# coding=utf-8\n"
421	            f"# Copyright {CURRENT_YEAR} The HuggingFace Team. All rights reserved.\n"
422	            f"#\n"

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_fast_image_processor.py:475
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
474	    with open(fast_image_processing_module_file, "w", encoding="utf-8") as f:
475	        f.write(content)
476	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_fast_image_processor.py:586
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
585	    with open(fast_image_processing_module_file, "w", encoding="utf-8") as f:
586	        f.write(content)
587	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_fast_image_processor.py:607
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
606	    with open(image_processing_module_file, "r", encoding="utf-8") as f:
607	        content_base_file = f.read()
608	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_new_model_like.py:320
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
319	    with open(file_name, "r", encoding="utf-8") as f:
320	        old_content = f.read()
321	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_new_model_like.py:327
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
326	    with open(file_name, "w", encoding="utf-8") as f:
327	        f.write(new_content)
328	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_new_model_like.py:534
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
533	    with open(module_file, "r", encoding="utf-8") as f:
534	        content = f.read()
535	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_new_model_like.py:577
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
576	    with open(dest_file, "w", encoding="utf-8") as f:
577	        f.write(content)
578	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_new_model_like.py:680
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
679	        with open(fname, "r", encoding="utf-8") as f:
680	            content = f.read()
681	            if _re_checkpoint_for_doc.search(content) is not None:

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_new_model_like.py:862
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
861	    with open(init_file, "r", encoding="utf-8") as f:
862	        content = f.read()
863	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_new_model_like.py:904
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
903	    with open(init_file, "w", encoding="utf-8") as f:
904	        f.write("\n".join(new_lines))
905	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_new_model_like.py:925
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
924	    with open(TRANSFORMERS_PATH / "__init__.py", "r", encoding="utf-8") as f:
925	        content = f.read()
926	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_new_model_like.py:994
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
993	    with open(TRANSFORMERS_PATH / "__init__.py", "w", encoding="utf-8") as f:
994	        f.write("\n".join(new_lines))
995	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_new_model_like.py:1009
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
1008	    with open(TRANSFORMERS_PATH / "models" / "auto" / "tokenization_auto.py", "r", encoding="utf-8") as f:
1009	        content = f.read()
1010	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_new_model_like.py:1041
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1040	    with open(TRANSFORMERS_PATH / "models" / "auto" / "tokenization_auto.py", "w", encoding="utf-8") as f:
1041	        f.write("\n".join(new_lines))
1042	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_new_model_like.py:1137
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1136	
1137	DOC_OVERVIEW_TEMPLATE = """## Overview
1138	
1139	The {model_name} model was proposed in [<INSERT PAPER NAME HERE>](<INSERT PAPER LINK HERE>) by <INSERT AUTHORS HERE>.
1140	<INSERT SHORT SUMMARY HERE>
1141	
1142	The abstract from the paper is the following:
1143	
1144	*<INSERT PAPER ABSTRACT HERE>*
1145	
1146	Tips:
1147	
1148	<INSERT TIPS ABOUT MODEL HERE>
1149	
1150	This model was contributed by [INSERT YOUR HF USERNAME HERE](https://huggingface.co/<INSERT YOUR HF USERNAME HERE>).
1151	The original code can be found [here](<INSERT LINK TO GITHUB REPO HERE>).
1152	
1153	"""
1154	
1155	
1156	def duplicate_doc_file(

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_new_model_like.py:1176
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
1175	    with open(doc_file, "r", encoding="utf-8") as f:
1176	        content = f.read()
1177	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_new_model_like.py:1255
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1254	    with open(dest_file, "w", encoding="utf-8") as f:
1255	        f.write("\n".join(new_blocks))
1256	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_new_model_like.py:1305
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1304	    with open(toc_file, "w", encoding="utf-8") as f:
1305	        f.write(yaml.dump(content, allow_unicode=True))
1306	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_new_model_like.py:1416
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
1415	        with open(filename) as fp:
1416	            content = fp.read()
1417	        new_content = re.sub(r"fx_compatible\s*=\s*True", "fx_compatible = False", content)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/add_new_model_like.py:1419
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1418	        with open(filename, "w") as fp:
1419	            fp.write(new_content)
1420	        return content != new_content

--------------------------------------------------
>> Issue: [B812:system] os.system
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/chat.py:34
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b812_system.html
33	
34	if platform.system() != "Windows":
35	    import pwd

--------------------------------------------------
>> Issue: [B817:system] platform.system
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/chat.py:34
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b817_system.html
33	
34	if platform.system() != "Windows":
35	    import pwd

--------------------------------------------------
>> Issue: [B308:blacklist] platform.system
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/chat.py:34
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b308-platform-system
33	
34	if platform.system() != "Windows":
35	    import pwd

--------------------------------------------------
>> Issue: [B812:system] os.system
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/chat.py:91
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b812_system.html
90	def get_username():
91	    if platform.system() == "Windows":
92	        return os.getlogin()

--------------------------------------------------
>> Issue: [B817:system] platform.system
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/chat.py:91
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b817_system.html
90	def get_username():
91	    if platform.system() == "Windows":
92	        return os.getlogin()

--------------------------------------------------
>> Issue: [B308:blacklist] platform.system
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/chat.py:91
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b308-platform-system
90	def get_username():
91	    if platform.system() == "Windows":
92	        return os.getlogin()

--------------------------------------------------
>> Issue: [B303:blacklist] pwd.getpwuid
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/chat.py:94
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b303-pwd-getpwuid
93	    else:
94	        return pwd.getpwuid(os.getuid()).pw_name
95	

--------------------------------------------------
>> Issue: [B811:getuid] os.getuid
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/chat.py:94
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b811_getuid.html
93	    else:
94	        return pwd.getpwuid(os.getuid()).pw_name
95	

--------------------------------------------------
>> Issue: [B305:blacklist] os.getuid
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/chat.py:94
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
93	    else:
94	        return pwd.getpwuid(os.getuid()).pw_name
95	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/chat.py:575
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
574	
575	                thread = Thread(target=model.generate, kwargs=generation_kwargs)
576	                thread.start()

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/commands/convert.py:32
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
31	
32	IMPORT_ERROR_MESSAGE = """
33	transformers can only be used from the commandline to convert TensorFlow models in PyTorch, In that case, it requires
34	TensorFlow to be installed. Please see https://www.tensorflow.org/install/ for installation instructions.
35	"""
36	
37	
38	class ConvertCommand(BaseTransformersCLICommand):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/configuration_utils.py:394
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
393	            warnings.warn(
394	                "Some non-default generation parameters are set in the model config. These should go into either a) "
395	                "`model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file "
396	                "(https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model)."
397	                "This warning will become an exception in the future."
398	                f"\nNon-default generation parameters: {str(non_default_generation_parameters)}",
399	                UserWarning,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/configuration_utils.py:673
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
672	                raise OSError(
673	                    f"Can't load the configuration of '{pretrained_model_name_or_path}'. If you were trying to load it"
674	                    " from 'https://huggingface.co/models', make sure you don't have a local directory with the same"
675	                    f" name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory"
676	                    f" containing a {configuration_file} file"
677	                )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/configuration_utils.py:790
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
789	        with open(json_file, encoding="utf-8") as reader:
790	            text = reader.read()
791	        return json.loads(text)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/configuration_utils.py:948
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
947	        with open(json_file_path, "w", encoding="utf-8") as writer:
948	            writer.write(self.to_json_string(use_diff=use_diff))
949	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/convert_graph_to_onnx.py:524
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
523	                print(
524	                    "\t Using TensorFlow might not provide the same optimization level compared to PyTorch.\n"
525	                    "\t For TensorFlow users you can try optimizing the model directly through onnxruntime_tools.\n"
526	                    "\t For more information, please refer to the onnxruntime documentation:\n"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/data/datasets/glue.py:89
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
88	        warnings.warn(
89	            "This dataset will be removed from the library soon, preprocessing should be handled with the  Datasets "
90	            "library. You can have a look at this example script for pointers: "
91	            "https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py",
92	            FutureWarning,
93	        )
94	        self.args = args

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/data/datasets/language_modeling.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	            DEPRECATION_WARNING.format(
55	                "https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py"
56	            ),
57	            FutureWarning,
58	        )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/data/datasets/language_modeling.py:87
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
86	                with open(file_path, encoding="utf-8") as f:
87	                    text = f.read()
88	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/data/datasets/language_modeling.py:121
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
120	            DEPRECATION_WARNING.format(
121	                "https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py"
122	            ),
123	            FutureWarning,
124	        )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/data/datasets/language_modeling.py:133
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
132	        with open(file_path, encoding="utf-8") as f:
133	            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]
134	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/data/datasets/language_modeling.py:154
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
153	            DEPRECATION_WARNING.format(
154	                "https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm_wwm.py"
155	            ),
156	            FutureWarning,
157	        )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/data/datasets/language_modeling.py:172
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
171	        with open(ref_path, encoding="utf-8") as f:
172	            ref = [json.loads(line) for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]
173	        if len(data) != len(ref):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/data/datasets/language_modeling.py:202
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
201	            DEPRECATION_WARNING.format(
202	                "https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py"
203	            ),
204	            FutureWarning,
205	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/data/datasets/language_modeling.py:362
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
361	            DEPRECATION_WARNING.format(
362	                "https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py"
363	            ),
364	            FutureWarning,
365	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/data/metrics/__init__.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	DEPRECATION_WARNING = (
24	    "This metric will be removed from the library soon, metrics should be handled with the  Evaluate "
25	    "library. You can have a look at this example script for pointers: "
26	    "https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py"
27	)
28	
29	
30	def simple_accuracy(preds, labels):
31	    warnings.warn(DEPRECATION_WARNING, FutureWarning)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/data/processors/glue.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	DEPRECATION_WARNING = (
35	    "This {0} will be removed from the library soon, preprocessing should be handled with the  Datasets "
36	    "library. You can have a look at this example script for pointers: "
37	    "https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py"
38	)
39	
40	
41	def glue_convert_examples_to_features(
42	    examples: Union[List[InputExample], "tf.data.Dataset"],

--------------------------------------------------
>> Issue: [B839:pool] multiprocessing.Pool
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/data/processors/squad.py:368
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b839_pool.html
367	    threads = min(threads, cpu_count())
368	    with Pool(threads, initializer=squad_convert_example_to_features_init, initargs=(tokenizer,)) as p:
369	        annotate_ = partial(

--------------------------------------------------
>> Issue: [B323:blacklist] multiprocessing_Pool
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/data/processors/squad.py:368
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b323-multiprocessing-pool
367	    threads = min(threads, cpu_count())
368	    with Pool(threads, initializer=squad_convert_example_to_features_init, initargs=(tokenizer,)) as p:
369	        annotate_ = partial(

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/dynamic_module_utils.py:97
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
96	    with open(module_file, encoding="utf-8") as f:
97	        content = f.read()
98	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/dynamic_module_utils.py:151
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
150	    with open(filename, encoding="utf-8") as f:
151	        content = f.read()
152	    imported_modules = set()

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/dynamic_module_utils.py:438
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
437	        repo_type_str = "" if repo_type is None else f"{repo_type}s/"
438	        url = f"https://huggingface.co/{repo_type_str}{pretrained_model_name_or_path}"
439	        logger.warning(

--------------------------------------------------
>> Issue: [B828:signal] signal.signal
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/dynamic_module_utils.py:666
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b828_signal.html
665	            try:
666	                prev_sig_handler = signal.signal(signal.SIGALRM, _raise_timeout_error)
667	                signal.alarm(TIME_OUT_REMOTE_CODE)

--------------------------------------------------
>> Issue: [B324:blacklist] signal_signal
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/dynamic_module_utils.py:666
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b324-signal-signal
665	            try:
666	                prev_sig_handler = signal.signal(signal.SIGALRM, _raise_timeout_error)
667	                signal.alarm(TIME_OUT_REMOTE_CODE)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/dynamic_module_utils.py:670
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
669	                    answer = input(
670	                        f"The repository for {model_name} contains custom code which must be executed to correctly "
671	                        f"load the model. You can inspect the repository content at https://hf.co/{model_name}.\n"
672	                        f"You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\n"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/dynamic_module_utils.py:683
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
682	                raise ValueError(
683	                    f"The repository for {model_name} contains custom code which must be executed to correctly "
684	                    f"load the model. You can inspect the repository content at https://hf.co/{model_name}.\n"
685	                    f"Please pass the argument `trust_remote_code=True` to allow custom code to be run."

--------------------------------------------------
>> Issue: [B828:signal] signal.signal
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/dynamic_module_utils.py:689
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b828_signal.html
688	                if prev_sig_handler is not None:
689	                    signal.signal(signal.SIGALRM, prev_sig_handler)
690	                    signal.alarm(0)

--------------------------------------------------
>> Issue: [B324:blacklist] signal_signal
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/dynamic_module_utils.py:689
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b324-signal-signal
688	                if prev_sig_handler is not None:
689	                    signal.signal(signal.SIGALRM, prev_sig_handler)
690	                    signal.alarm(0)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/feature_extraction_utils.py:530
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
529	                raise OSError(
530	                    f"Can't load feature extractor for '{pretrained_model_name_or_path}'. If you were trying to load"
531	                    " it from 'https://huggingface.co/models', make sure you don't have a local directory with the"
532	                    f" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a"
533	                    f" directory containing a {FEATURE_EXTRACTOR_NAME} file"
534	                )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/feature_extraction_utils.py:539
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
538	            with open(resolved_feature_extractor_file, encoding="utf-8") as reader:
539	                text = reader.read()
540	            feature_extractor_dict = json.loads(text)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/feature_extraction_utils.py:631
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
630	        with open(json_file, encoding="utf-8") as reader:
631	            text = reader.read()
632	        feature_extractor_dict = json.loads(text)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/feature_extraction_utils.py:665
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
664	        with open(json_file_path, "w", encoding="utf-8") as writer:
665	            writer.write(self.to_json_string())
666	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/generation/configuration_utils.py:1079
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1078	                raise EnvironmentError(
1079	                    f"Can't load the configuration of '{pretrained_model_name}'. If you were trying to load it"
1080	                    " from 'https://huggingface.co/models', make sure you don't have a local directory with the same"
1081	                    f" name. Otherwise, make sure '{pretrained_model_name}' is the correct path to a directory"
1082	                    f" containing a {configuration_file} file"
1083	                )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/generation/configuration_utils.py:1111
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
1110	        with open(json_file, "r", encoding="utf-8") as reader:
1111	            text = reader.read()
1112	        return json.loads(text)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/generation/configuration_utils.py:1262
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1261	        with open(json_file_path, "w", encoding="utf-8") as writer:
1262	            writer.write(self.to_json_string(use_diff=use_diff))
1263	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/generation/configuration_utils.py:1371
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1370	
1371	            writer.write(json_string)
1372	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/generation/flax_utils.py:326
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
325	                    warnings.warn(
326	                        "You have modified the pretrained model configuration to control generation. This is a"
327	                        " deprecated strategy to control generation and will be removed soon, in a future version."
328	                        " Please use and modify the model generation configuration (see"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/generation/flax_utils.py:395
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
394	                logger.warning(
395	                    f"Both `max_new_tokens` (={generation_config.max_new_tokens}) and `max_length`(="
396	                    f"{generation_config.max_length}) seem to have been set. `max_new_tokens` will take precedence. "
397	                    "Please refer to the documentation for more information. "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/generation/tf_utils.py:729
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
728	                    warnings.warn(
729	                        "You have modified the pretrained model configuration to control generation. This is a"
730	                        " deprecated strategy to control generation and will be removed soon, in a future version."
731	                        " Please use and modify the model generation configuration (see"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/generation/tf_utils.py:844
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
843	                logger.warning(
844	                    f"Both `max_new_tokens` (={generation_config.max_new_tokens}) and `max_length`(="
845	                    f"{generation_config.max_length}) seem to have been set. `max_new_tokens` will take precedence. "
846	                    "Please refer to the documentation for more information. "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/generation/tf_utils.py:1220
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1219	                    raise ValueError(
1220	                        f"You passed `inputs_embeds` to `.generate()`, but the model class {self.__class__.__name__} "
1221	                        "doesn't have its forwarding implemented. See the GPT2 implementation for an example "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/generation/utils.py:655
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
654	                    raise ValueError(
655	                        f"You passed `inputs_embeds` to `.generate()`, but the model class {self.__class__.__name__} "
656	                        "doesn't have its forwarding implemented. See the GPT2 implementation for an example "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/generation/utils.py:1472
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1471	        doc_reference = (
1472	            "(see https://huggingface.co/docs/transformers/en/generation_strategies#universal-assisted-decoding)"
1473	        )
1474	        if self.config.get_text_config().vocab_size == assistant_model.config.get_text_config().vocab_size:
1475	            if assistant_tokenizer is not None:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/generation/utils.py:1599
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1598	                logger.warning(
1599	                    f"Both `max_new_tokens` (={generation_config.max_new_tokens}) and `max_length`(="
1600	                    f"{generation_config.max_length}) seem to have been set. `max_new_tokens` will take precedence. "
1601	                    "Please refer to the documentation for more information. "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/generation/utils.py:1625
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1624	                logger.warning(
1625	                    f"Both `min_new_tokens` (={generation_config.min_new_tokens}) and `min_length`(="
1626	                    f"{generation_config.min_length}) seem to have been set. `min_new_tokens` will take precedence. "
1627	                    "Please refer to the documentation for more information. "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/generation/utils.py:1668
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1667	                    warnings.warn(
1668	                        "You have modified the pretrained model configuration to control generation. This is a"
1669	                        " deprecated strategy to control generation and will be removed in v5."
1670	                        " Please use and modify the model generation configuration (see"
1671	                        " https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )",
1672	                        UserWarning,
1673	                    )
1674	                    self.generation_config = new_generation_config

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/generation/utils.py:1973
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1972	                    raise ValueError(
1973	                        "This model does not support `cache_implementation='static'`. Please check the following "
1974	                        "issue: https://github.com/huggingface/transformers/issues/28981"
1975	                    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/hf_argparser.py:419
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
418	        with open(Path(json_file), encoding="utf-8") as open_json_file:
419	            data = json.loads(open_json_file.read())
420	        outputs = self.parse_dict(data, allow_extra_keys=allow_extra_keys)

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/image_processing_base.py:234
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
233	            )
234	            if kwargs.get("token", None) is not None:
235	                raise ValueError(

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/image_processing_base.py:268
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
267	                commit_message=commit_message,
268	                token=kwargs.get("token"),
269	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/image_processing_base.py:360
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
359	                raise OSError(
360	                    f"Can't load image processor for '{pretrained_model_name_or_path}'. If you were trying to load"
361	                    " it from 'https://huggingface.co/models', make sure you don't have a local directory with the"
362	                    f" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a"
363	                    f" directory containing a {image_processor_filename} file"
364	                )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/image_processing_base.py:369
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
368	            with open(resolved_image_processor_file, encoding="utf-8") as reader:
369	                text = reader.read()
370	            image_processor_dict = json.loads(text)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/image_processing_base.py:466
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
465	        with open(json_file, encoding="utf-8") as reader:
466	            text = reader.read()
467	        image_processor_dict = json.loads(text)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/image_processing_base.py:500
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
499	        with open(json_file_path, "w", encoding="utf-8") as writer:
500	            writer.write(self.to_json_string())
501	

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/image_processing_base.py:547
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
546	        elif isinstance(image_url_or_urls, str):
547	            response = requests.get(image_url_or_urls, stream=True, headers=headers)
548	            response.raise_for_status()

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/image_utils.py:514
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
513	            # like http_huggingface_co.png
514	            image = PIL.Image.open(BytesIO(requests.get(image, timeout=timeout).content))
515	        elif os.path.isfile(image):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/image_utils.py:527
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
526	                raise ValueError(
527	                    f"Incorrect image source. Must be a valid URL starting with `http://` or `https://`, a valid path to an image file, or a base64 encoded string. Got {image}. Failed with {e}"
528	                )

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/image_utils.py:604
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
603	    video = cv2.VideoCapture(video_path)
604	    total_num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
605	    video_fps = video.get(cv2.CAP_PROP_FPS)

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/image_utils.py:605
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
604	    total_num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
605	    video_fps = video.get(cv2.CAP_PROP_FPS)
606	    duration = total_num_frames / video_fps if video_fps else 0

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/image_utils.py:615
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
614	    while video.isOpened():
615	        success, frame = video.read()
616	        if not success:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/image_utils.py:834
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
833	
834	    if video.startswith("https://www.youtube.com") or video.startswith("http://www.youtube.com"):
835	        if not is_yt_dlp_available():

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/image_utils.py:834
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
833	
834	    if video.startswith("https://www.youtube.com") or video.startswith("http://www.youtube.com"):
835	        if not is_yt_dlp_available():

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/image_utils.py:847
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
846	    elif video.startswith("http://") or video.startswith("https://"):
847	        file_obj = BytesIO(requests.get(video).content)
848	    elif os.path.isfile(video):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/integrations/awq.py:129
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
128	        raise ValueError(
129	            "AWQ (either `autoawq` or `llmawq`) is not available. Please install it with `pip install autoawq` or check out the installation guide in https://github.com/mit-han-lab/llm-awq"
130	        )
131	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/integrations/awq.py:234
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
233	        raise ValueError(
234	            "Fusing mapping not found either on the quantization config or the supported `AWQ_FUSED_MAPPINGS`. Please pass a `fused_mapping` argument"
235	            " in the `quantization_config` or raise an issue on transformers https://github.com/huggingface/transformers to add its support."
236	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/integrations/bitsandbytes.py:512
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
511	            err_msg = (
512	                f"None of the available devices `available_devices = {available_devices or None}` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {bnb_supported_devices_with_info}`. "
513	                "Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/integrations/bitsandbytes.py:534
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
533	        log_msg = (
534	            "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. "
535	            "Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend"
536	        )
537	        if raise_exception:
538	            logger.error(log_msg)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/integrations/integration_utils.py:135
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
134	        logger.warning(
135	            "comet_ml is installed but the Comet API Key is not configured. "
136	            "Please set the `COMET_API_KEY` environment variable to enable Comet logging. "
137	            "Check out the documentation for other ways of configuring it: "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/integrations/integration_utils.py:444
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
443	
444	            logger.info(f"created experiment: https://app.sigopt.com/experiment/{experiment.id}")
445	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/integrations/integration_utils.py:481
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
480	            )
481	            logger.info(f"created experiment: https://app.sigopt.com/experiment/{experiment.id}")
482	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/integrations/integration_utils.py:910
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
909	                            with model_artifact.new_file(f.name, mode="wb") as fa:
910	                                fa.write(f.read_bytes())
911	                    self._wandb.run.log_artifact(model_artifact, aliases=["base_model"])

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/integrations/integration_utils.py:910
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
909	                            with model_artifact.new_file(f.name, mode="wb") as fa:
910	                                fa.write(f.read_bytes())
911	                    self._wandb.run.log_artifact(model_artifact, aliases=["base_model"])

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/integrations/integration_utils.py:914
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
913	                    badge_markdown = (
914	                        f'[<img src="https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge'
915	                        f'-28.svg" alt="Visualize in Weights & Biases" width="20'
916	                        f'0" height="32"/>]({self._wandb.run.get_url()})'
917	                    )

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/integrations/integration_utils.py:968
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
967	                        with artifact.new_file(f.name, mode="wb") as fa:
968	                            fa.write(f.read_bytes())
969	                self._wandb.run.log_artifact(artifact, aliases=["final_model"])

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/integrations/integration_utils.py:968
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
967	                        with artifact.new_file(f.name, mode="wb") as fa:
968	                            fa.write(f.read_bytes())
969	                self._wandb.run.log_artifact(artifact, aliases=["final_model"])

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/integrations/integration_utils.py:1662
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
1661	        if self._volatile_checkpoints_dir is not None:
1662	            shutil.rmtree(self._volatile_checkpoints_dir, ignore_errors=True)
1663	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/integrations/integration_utils.py:1715
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1714	            raise RuntimeError(
1715	                "CodeCarbonCallback requires `codecarbon` package, which is not compatible with AMD ROCm (https://github.com/mlco2/codecarbon/pull/490). When using the Trainer, please specify the `report_to` argument (https://huggingface.co/docs/transformers/v4.39.3/en/main_classes/trainer#transformers.TrainingArguments.report_to) to disable CodeCarbonCallback."
1716	            )
1717	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/integrations/integration_utils.py:2262
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2261	                badge_markdown = (
2262	                    f'[<img src="https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg"'
2263	                    f' alt="Visualize in SwanLab" height="28'
2264	                    f'0" height="32"/>]({self._swanlab.get_run().public.cloud.exp_url})'
2265	                )

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/keras_callbacks.py:389
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
388	            with (self.output_dir / "README.md").open("w") as f:
389	                f.write(model_card)
390	            _, self.last_job = self.repo.push_to_hub(

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/keras_callbacks.py:412
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
411	            with (self.output_dir / "README.md").open("w") as f:
412	                f.write(model_card)
413	            self.repo.push_to_hub(commit_message="End of training", blocking=True)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modelcard.py:228
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
227	        with open(json_file, encoding="utf-8") as reader:
228	            text = reader.read()
229	        dict_obj = json.loads(text)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modelcard.py:250
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
249	        with open(json_file_path, "w", encoding="utf-8") as writer:
250	            writer.write(self.to_json_string())
251	

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modelcard.py:426
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
425	            if ds_tag is not None:
426	                metadata = dataset_metadata_mapping.get(ds_tag, {})
427	                result["dataset"] = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modelcard.py:489
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
488	            model_card += (
489	                "This model is a fine-tuned version of"
490	                f" [{self.finetuned_from}](https://huggingface.co/{self.finetuned_from}) on "
491	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_flax_pytorch_utils.py:68
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
67	                logger.error(
68	                    "Loading a PyTorch model in Flax, requires both PyTorch and Flax to be installed. Please see"
69	                    " https://pytorch.org/ and https://flax.readthedocs.io/en/latest/installation.html for installation"
70	                    " instructions."

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_flax_pytorch_utils.py:346
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
345	            try:
346	                flax_state_dict = from_bytes(flax_cls, state_f.read())
347	            except UnpicklingError:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_flax_pytorch_utils.py:360
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
359	        logger.error(
360	            "Loading a Flax weights in PyTorch, requires both PyTorch and Flax to be installed. Please see"
361	            " https://pytorch.org/ and https://flax.readthedocs.io/en/latest/installation.html for installation"
362	            " instructions."

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_flax_utils.py:425
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
424	                with open(resolved_archive_file, "rb") as state_f:
425	                    state = from_bytes(cls, state_f.read())
426	        except (UnpicklingError, msgpack.exceptions.ExtraData) as e:

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_flax_utils.py:429
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
428	                with open(resolved_archive_file) as f:
429	                    if f.read().startswith("version"):
430	                        raise OSError(

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_flax_utils.py:467
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
466	                with open(shard_file, "rb") as state_f:
467	                    state = from_bytes(cls, state_f.read())
468	            except (UnpicklingError, msgpack.exceptions.ExtraData) as e:

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_flax_utils.py:470
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
469	                with open(shard_file) as f:
470	                    if f.read().startswith("version"):
471	                        raise OSError(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_flax_utils.py:841
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
840	                    raise EnvironmentError(
841	                        f"Can't load the model for '{pretrained_model_name_or_path}'. If you were trying to load it"
842	                        " from 'https://huggingface.co/models', make sure you don't have a local directory with the"
843	                        f" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a"
844	                        f" directory containing a file named {FLAX_WEIGHTS_NAME} or {WEIGHTS_NAME}."
845	                    )

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_flax_utils.py:1179
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1178	                    model_bytes = to_bytes(params)
1179	                    f.write(model_bytes)
1180	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_flax_utils.py:1186
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1185	                content = json.dumps(index, indent=2, sort_keys=True) + "\n"
1186	                f.write(content)
1187	            logger.info(

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_flax_utils.py:1197
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1196	                    shard_bytes = to_bytes(params)
1197	                    f.write(shard_bytes)
1198	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_gguf_pytorch_utils.py:283
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
282	        logger.error(
283	            "Loading a GGUF checkpoint in PyTorch, requires both PyTorch and GGUF>=0.10.0 to be installed. Please see "
284	            "https://pytorch.org/ and https://github.com/ggerganov/llama.cpp/tree/master/gguf-py for installation instructions."
285	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_gguf_pytorch_utils.py:302
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
301	        raise NotImplementedError(
302	            f"Unknown gguf model_type: {model_type} in gguf-py. "
303	            "This might because you're using an outdated version of gguf-py package, "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_gguf_pytorch_utils.py:357
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
356	        logger.error(
357	            "Loading a GGUF checkpoint in PyTorch, requires both PyTorch and GGUF>=0.10.0 to be installed. Please see "
358	            "https://pytorch.org/ and https://github.com/ggerganov/llama.cpp/tree/master/gguf-py for installation instructions."
359	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_tf_pytorch_utils.py:184
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
183	        logger.error(
184	            "Loading a PyTorch model in TensorFlow, requires both PyTorch and TensorFlow to be installed. Please see "
185	            "https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions."
186	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_tf_pytorch_utils.py:242
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
241	        logger.error(
242	            "Loading a PyTorch model in TensorFlow, requires both PyTorch and TensorFlow to be installed. Please see "
243	            "https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions."
244	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_tf_pytorch_utils.py:506
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
505	        logger.error(
506	            "Loading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed. Please see "
507	            "https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions."
508	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_tf_pytorch_utils.py:551
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
550	        logger.error(
551	            "Loading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed. Please see "
552	            "https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions."
553	        )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_tf_utils.py:835
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
834	            with open(resolved_archive_file) as f:
835	                if f.read().startswith("version"):
836	                    raise OSError(

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_tf_utils.py:1864
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1863	        with open(os.path.join(output_dir, "README.md"), "w") as f:
1864	            f.write(model_card)
1865	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_tf_utils.py:2485
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
2484	                content = json.dumps(index, indent=2, sort_keys=True) + "\n"
2485	                index_file.write(content)
2486	            logger.info(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_tf_utils.py:2872
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2871	                    raise EnvironmentError(
2872	                        f"Can't load the model for '{pretrained_model_name_or_path}'. If you were trying to load it"
2873	                        " from 'https://huggingface.co/models', make sure you don't have a local directory with the"
2874	                        f" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a"
2875	                        f" directory containing a file named {WEIGHTS_NAME}, {TF2_WEIGHTS_NAME} or {TF_WEIGHTS_NAME}"
2876	                    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_tf_utils.py:3021
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
3020	                with open(resolved_archive_file) as f:
3021	                    if f.read().startswith("version"):
3022	                        raise OSError(

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_utils.py:565
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
564	            with open(checkpoint_file) as f:
565	                if f.read(7) == "version":
566	                    raise OSError(

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_utils.py:1063
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
1062	                            if not has_file(pretrained_model_name_or_path, safe_weights_name, **has_file_kwargs):
1063	                                Thread(
1064	                                    target=auto_conversion,
1065	                                    args=(pretrained_model_name_or_path,),
1066	                                    kwargs={"ignore_errors_during_conversion": True, **cached_file_kwargs},
1067	                                    name="Thread-auto_conversion",
1068	                                ).start()

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_utils.py:1113
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1112	                raise EnvironmentError(
1113	                    f"Can't load the model for '{pretrained_model_name_or_path}'. If you were trying to load it"
1114	                    " from 'https://huggingface.co/models', make sure you don't have a local directory with the"
1115	                    f" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a"
1116	                    f" directory containing a file named {_add_variant(WEIGHTS_NAME, variant)},"
1117	                    f" {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME} or {FLAX_WEIGHTS_NAME}."
1118	                ) from e

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_utils.py:2203
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2202	            logger.warning_once(
2203	                f"{cls.__name__} has generative capabilities, as `prepare_inputs_for_generation` is explicitly "
2204	                "overwritten. However, it doesn't directly inherit from `GenerationMixin`. From v4.50 onwards, "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_utils.py:2234
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2233	            raise ValueError(
2234	                f"{cls.__name__} does not support Flash Attention 2.0 yet. Please request to add support where"
2235	                f" the model is hosted, on its model hub page: https://huggingface.co/{config._name_or_path}/discussions/new"
2236	                " or in the Transformers GitHub repo: https://github.com/huggingface/transformers/issues/new"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_utils.py:2234
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2233	            raise ValueError(
2234	                f"{cls.__name__} does not support Flash Attention 2.0 yet. Please request to add support where"
2235	                f" the model is hosted, on its model hub page: https://huggingface.co/{config._name_or_path}/discussions/new"
2236	                " or in the Transformers GitHub repo: https://github.com/huggingface/transformers/issues/new"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_utils.py:2241
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2240	            preface = "FlashAttention2 has been toggled on, but it cannot be used due to the following error:"
2241	            install_message = "Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
2242	
2243	            if importlib.util.find_spec("flash_attn") is None:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_utils.py:2335
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2334	                raise ValueError(
2335	                    f"{cls.__name__} does not support an attention implementation through torch.nn.functional.scaled_dot_product_attention yet."
2336	                    " Please request the support for this architecture: https://github.com/huggingface/transformers/issues/28005. If you believe"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_utils.py:2365
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2364	                raise ValueError(
2365	                    f"{cls.__name__} does not support an attention implementation through torch's flex_attention."
2366	                    " Please request the support for this architecture: https://github.com/huggingface/transformers/issues/34809."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_utils.py:2770
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2769	            logger.info(
2770	                "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding"
2771	                f" dimension will be {new_num_tokens}. This might induce some performance reduction as *Tensor Cores* will not be available."
2772	                " For more details about this, or help on choosing the correct value for resizing, refer to this guide:"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_utils.py:2818
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2817	            logger.warning_once(
2818	                "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. "
2819	                "As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. "
2820	                "To disable this, use `mean_resizing=False`"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_utils.py:2950
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2949	            logger.warning_once(
2950	                "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. "
2951	                "As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. "
2952	                "To disable this, use `mean_resizing=False`"

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_utils.py:3579
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
3578	                content = json.dumps(index, indent=2, sort_keys=True) + "\n"
3579	                f.write(content)
3580	            logger.info(

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_utils.py:3579
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
3578	                content = json.dumps(index, indent=2, sort_keys=True) + "\n"
3579	                f.write(content)
3580	            logger.info(

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_utils.py:4874
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
4873	            load_offloaded_weights(model_to_load, cpu_offload_index, cpu_offload_folder)
4874	            shutil.rmtree(cpu_offload_folder)
4875	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_utils.py:4976
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
4975	                logger.error(
4976	                    "Loading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed."
4977	                    " Please see https://pytorch.org/ and https://www.tensorflow.org/install/ for installation"
4978	                    " instructions."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_utils.py:4991
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
4990	            logger.error(
4991	                "Loading a Flax model in PyTorch, requires both PyTorch and Flax to be installed. Please see"
4992	                " https://pytorch.org/ and https://flax.readthedocs.io/en/latest/installation.html for"
4993	                " installation instructions."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/modeling_utils.py:5112
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
5111	            warn_string = (
5112	                "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See "
5113	                "https://huggingface.co/docs/transformers/troubleshooting"
5114	                "#incorrect-output-when-padding-tokens-arent-masked."
5115	            )
5116	
5117	            # If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an
5118	            # attention_mask or not. In this case, we should still show a warning because this is a rare case.
5119	            if (
5120	                (self.config.bos_token_id is not None and self.config.bos_token_id == self.config.pad_token_id)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/albert/modeling_albert.py:70
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
69	        logger.error(
70	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
71	            "https://www.tensorflow.org/install/ for installation instructions."
72	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/albert/modeling_albert.py:620
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
619	
620	ALBERT_START_DOCSTRING = r"""
621	
622	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
623	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
624	    etc.)
625	
626	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
627	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
628	    and behavior.
629	
630	    Args:
631	        config ([`AlbertConfig`]): Model configuration class with all the parameters of the model.
632	            Initializing with a config file does not load the weights associated with the model, only the
633	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
634	"""
635	
636	ALBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/albert/modeling_flax_albert.py:84
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
83	
84	ALBERT_START_DOCSTRING = r"""
85	
86	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
87	    library implements for all its model (such as downloading, saving and converting weights from PyTorch models)
88	
89	    This model is also a
90	    [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. Use it as
91	    a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
92	    behavior.
93	
94	    Finally, this model supports inherent JAX features such as:
95	
96	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
97	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
98	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
99	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
100	
101	    Parameters:
102	        config ([`AlbertConfig`]): Model configuration class with all the parameters of the model.
103	            Initializing with a config file does not load the weights associated with the model, only the
104	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
105	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
106	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
107	            `jax.numpy.bfloat16` (on TPUs).
108	
109	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
110	            specified all the computation will be performed with the given `dtype`.
111	
112	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
113	            parameters.**
114	
115	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
116	            [`~FlaxPreTrainedModel.to_bf16`].
117	"""
118	
119	ALBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/albert/modeling_tf_albert.py:759
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
758	
759	ALBERT_START_DOCSTRING = r"""
760	
761	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
762	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
763	    etc.)
764	
765	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
766	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
767	    behavior.
768	
769	    <Tip>
770	
771	    TensorFlow models and layers in `transformers` accept two formats as input:
772	
773	    - having all inputs as keyword arguments (like PyTorch models), or
774	    - having all inputs as a list, tuple or dict in the first positional argument.
775	
776	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
777	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
778	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
779	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
780	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
781	    positional argument:
782	
783	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
784	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
785	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
786	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
787	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
788	
789	    Note that when creating models and layers with
790	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
791	    about any of this, as you can just pass inputs like you would to any other Python function!
792	
793	    </Tip>
794	
795	    Args:
796	        config ([`AlbertConfig`]): Model configuration class with all the parameters of the model.
797	            Initializing with a config file does not load the weights associated with the model, only the
798	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
799	"""
800	
801	ALBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/albert/tokenization_albert.py:345
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
344	                content_spiece_model = self.sp_model.serialized_model_proto()
345	                fi.write(content_spiece_model)
346	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/align/modeling_align.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	
50	ALIGN_START_DOCSTRING = r"""
51	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
52	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
53	    etc.)
54	
55	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
56	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
57	    and behavior.
58	
59	    Parameters:
60	        config ([`AlignConfig`]): Model configuration class with all the parameters of the model.
61	            Initializing with a config file does not load the weights associated with the model, only the
62	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
63	"""
64	
65	ALIGN_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/altclip/modeling_altclip.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	
45	ALTCLIP_START_DOCSTRING = r"""
46	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
47	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
48	    etc.)
49	
50	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
51	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
52	    and behavior.
53	
54	    Parameters:
55	        config ([`CLIPConfig`]): Model configuration class with all the parameters of the model.
56	            Initializing with a config file does not load the weights associated with the model, only the
57	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
58	"""
59	
60	ALTCLIP_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/aria/modeling_aria.py:688
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
687	
688	ARIA_TEXT_START_DOCSTRING = r"""
689	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
690	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
691	    etc.)
692	
693	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
694	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
695	    and behavior.
696	
697	    Parameters:
698	        config ([`AriaTextConfig`]):
699	            Model configuration class with all the parameters of the model. Initializing with a config file does not
700	            load the weights associated with the model, only the configuration. Check out the
701	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
702	"""
703	
704	
705	@add_start_docstrings(
706	    "The bare Aria Model outputting raw hidden-states without any specific head on top.",
707	    ARIA_TEXT_START_DOCSTRING,
708	)
709	class AriaPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/aria/modeling_aria.py:771
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
770	
771	ARIA_TEXT_INPUTS_DOCSTRING = r"""
772	    Args:
773	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
774	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
775	            it.
776	
777	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
778	            [`PreTrainedTokenizer.__call__`] for details.
779	
780	            [What are input IDs?](../glossary#input-ids)
781	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
782	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
783	
784	            - 1 for tokens that are **not masked**,
785	            - 0 for tokens that are **masked**.
786	
787	            [What are attention masks?](../glossary#attention-mask)
788	
789	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
790	            [`PreTrainedTokenizer.__call__`] for details.
791	
792	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
793	            `past_key_values`).
794	
795	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
796	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
797	            information on the default strategy.
798	
799	            - 1 indicates the head is **not masked**,
800	            - 0 indicates the head is **masked**.
801	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
802	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
803	            config.n_positions - 1]`.
804	
805	            [What are position IDs?](../glossary#position-ids)
806	        past_key_values (`Cache`, *optional*):
807	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
808	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
809	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
810	
811	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
812	
813	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
814	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
815	            of shape `(batch_size, sequence_length)`.
816	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
817	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
818	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
819	            model's internal embedding lookup matrix.
820	        use_cache (`bool`, *optional*):
821	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
822	            `past_key_values`).
823	        output_attentions (`bool`, *optional*):
824	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
825	            tensors for more detail.
826	        output_hidden_states (`bool`, *optional*):
827	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
828	            more detail.
829	        return_dict (`bool`, *optional*):
830	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
831	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
832	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
833	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
834	            the complete sequence length.
835	"""
836	
837	
838	@add_start_docstrings(
839	    "The bare AriaText Model outputting raw hidden-states without any specific head on top.",
840	    ARIA_TEXT_START_DOCSTRING,
841	)
842	class AriaTextModel(AriaTextPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/aria/modeling_aria.py:1319
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1318	
1319	ARIA_START_DOCSTRING = r"""
1320	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1321	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1322	    etc.)
1323	
1324	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1325	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1326	    and behavior.
1327	
1328	    Parameters:
1329	        config (`AriaConfig`):
1330	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1331	            load the weights associated with the model, only the configuration. Check out the
1332	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1333	"""
1334	
1335	
1336	@add_start_docstrings(
1337	    """Aria model for conditional generation tasks.
1338	
1339	    This model combines a vision tower, a multi-modal projector, and a language model
1340	    to perform tasks that involve both image and text inputs.""",
1341	    ARIA_START_DOCSTRING,
1342	)
1343	class AriaForConditionalGeneration(AriaPreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/aria/modular_aria.py:1326
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1325	
1326	ARIA_START_DOCSTRING = r"""
1327	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1328	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1329	    etc.)
1330	
1331	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1332	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1333	    and behavior.
1334	
1335	    Parameters:
1336	        config (`AriaConfig`):
1337	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1338	            load the weights associated with the model, only the configuration. Check out the
1339	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1340	"""
1341	
1342	
1343	@add_start_docstrings(
1344	    """Aria model for conditional generation tasks.
1345	
1346	    This model combines a vision tower, a multi-modal projector, and a language model
1347	    to perform tasks that involve both image and text inputs.""",
1348	    ARIA_START_DOCSTRING,
1349	)
1350	class AriaForConditionalGeneration(AriaPreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:423
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
422	
423	AUDIO_SPECTROGRAM_TRANSFORMER_START_DOCSTRING = r"""
424	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
425	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
426	    behavior.
427	
428	    Parameters:
429	        config ([`ASTConfig`]):
430	            Model configuration class with all the parameters of the model. Initializing with a config file does not
431	            load the weights associated with the model, only the configuration. Check out the
432	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
433	"""
434	
435	AUDIO_SPECTROGRAM_TRANSFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/auto/auto_factory.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	
54	FROM_CONFIG_DOCSTRING = """
55	        Instantiates one of the model classes of the library from a configuration.
56	
57	        Note:
58	            Loading a model from its configuration file does **not** load the model weights. It only affects the
59	            model's configuration. Use [`~BaseAutoModelClass.from_pretrained`] to load the model weights.
60	
61	        Args:
62	            config ([`PretrainedConfig`]):
63	                The model class to instantiate is selected based on the configuration class:
64	
65	                List options
66	            attn_implementation (`str`, *optional*):
67	                The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
68	
69	        Examples:
70	
71	        ```python
72	        >>> from transformers import AutoConfig, BaseAutoModelClass
73	
74	        >>> # Download configuration from huggingface.co and cache.
75	        >>> config = AutoConfig.from_pretrained("checkpoint_placeholder")
76	        >>> model = BaseAutoModelClass.from_config(config)
77	        ```
78	"""
79	
80	FROM_PRETRAINED_TORCH_DOCSTRING = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/auto/auto_factory.py:80
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
79	
80	FROM_PRETRAINED_TORCH_DOCSTRING = """
81	        Instantiate one of the model classes of the library from a pretrained model.
82	
83	        The model class to instantiate is selected based on the `model_type` property of the config object (either
84	        passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
85	        falling back to using pattern matching on `pretrained_model_name_or_path`:
86	
87	        List options
88	
89	        The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
90	        deactivated). To train the model, you should first set it back in training mode with `model.train()`
91	
92	        Args:
93	            pretrained_model_name_or_path (`str` or `os.PathLike`):
94	                Can be either:
95	
96	                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.
97	                    - A path to a *directory* containing model weights saved using
98	                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.
99	                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In
100	                      this case, `from_tf` should be set to `True` and a configuration object should be provided as
101	                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a
102	                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.
103	            model_args (additional positional arguments, *optional*):
104	                Will be passed along to the underlying model `__init__()` method.
105	            config ([`PretrainedConfig`], *optional*):
106	                Configuration for the model to use instead of an automatically loaded configuration. Configuration can
107	                be automatically loaded when:
108	
109	                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained
110	                      model).
111	                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the
112	                      save directory.
113	                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a
114	                      configuration JSON file named *config.json* is found in the directory.
115	            state_dict (*Dict[str, torch.Tensor]*, *optional*):
116	                A state dictionary to use instead of a state dictionary loaded from saved weights file.
117	
118	                This option can be used if you want to create a model from a pretrained configuration but load your own
119	                weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and
120	                [`~PreTrainedModel.from_pretrained`] is not a simpler option.
121	            cache_dir (`str` or `os.PathLike`, *optional*):
122	                Path to a directory in which a downloaded pretrained model configuration should be cached if the
123	                standard cache should not be used.
124	            from_tf (`bool`, *optional*, defaults to `False`):
125	                Load the model weights from a TensorFlow checkpoint save file (see docstring of
126	                `pretrained_model_name_or_path` argument).
127	            force_download (`bool`, *optional*, defaults to `False`):
128	                Whether or not to force the (re-)download of the model weights and configuration files, overriding the
129	                cached versions if they exist.
130	            resume_download:
131	                Deprecated and ignored. All downloads are now resumed by default when possible.
132	                Will be removed in v5 of Transformers.
133	            proxies (`Dict[str, str]`, *optional*):
134	                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
135	                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
136	            output_loading_info(`bool`, *optional*, defaults to `False`):
137	                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.
138	            local_files_only(`bool`, *optional*, defaults to `False`):
139	                Whether or not to only look at local files (e.g., not try downloading the model).
140	            revision (`str`, *optional*, defaults to `"main"`):
141	                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
142	                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
143	                identifier allowed by git.
144	            trust_remote_code (`bool`, *optional*, defaults to `False`):
145	                Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
146	                should only be set to `True` for repositories you trust and in which you have read the code, as it will
147	                execute code present on the Hub on your local machine.
148	            code_revision (`str`, *optional*, defaults to `"main"`):
149	                The specific revision to use for the code on the Hub, if the code leaves in a different repository than
150	                the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
151	                system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier
152	                allowed by git.
153	            kwargs (additional keyword arguments, *optional*):
154	                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
155	                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or
156	                automatically loaded:
157	
158	                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the
159	                      underlying model's `__init__` method (we assume all relevant updates to the configuration have
160	                      already been done)
161	                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class
162	                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that
163	                      corresponds to a configuration attribute will be used to override said attribute with the
164	                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute
165	                      will be passed to the underlying model's `__init__` function.
166	
167	        Examples:
168	
169	        ```python
170	        >>> from transformers import AutoConfig, BaseAutoModelClass
171	
172	        >>> # Download model and configuration from huggingface.co and cache.
173	        >>> model = BaseAutoModelClass.from_pretrained("checkpoint_placeholder")
174	
175	        >>> # Update configuration during loading
176	        >>> model = BaseAutoModelClass.from_pretrained("checkpoint_placeholder", output_attentions=True)
177	        >>> model.config.output_attentions
178	        True
179	
180	        >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower)
181	        >>> config = AutoConfig.from_pretrained("./tf_model/shortcut_placeholder_tf_model_config.json")
182	        >>> model = BaseAutoModelClass.from_pretrained(
183	        ...     "./tf_model/shortcut_placeholder_tf_checkpoint.ckpt.index", from_tf=True, config=config
184	        ... )
185	        ```
186	"""
187	
188	FROM_PRETRAINED_TF_DOCSTRING = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/auto/auto_factory.py:188
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
187	
188	FROM_PRETRAINED_TF_DOCSTRING = """
189	        Instantiate one of the model classes of the library from a pretrained model.
190	
191	        The model class to instantiate is selected based on the `model_type` property of the config object (either
192	        passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
193	        falling back to using pattern matching on `pretrained_model_name_or_path`:
194	
195	        List options
196	
197	        Args:
198	            pretrained_model_name_or_path (`str` or `os.PathLike`):
199	                Can be either:
200	
201	                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.
202	                    - A path to a *directory* containing model weights saved using
203	                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.
204	                    - A path or url to a *PyTorch state_dict save file* (e.g, `./pt_model/pytorch_model.bin`). In this
205	                      case, `from_pt` should be set to `True` and a configuration object should be provided as `config`
206	                      argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
207	                      using the provided conversion scripts and loading the TensorFlow model afterwards.
208	            model_args (additional positional arguments, *optional*):
209	                Will be passed along to the underlying model `__init__()` method.
210	            config ([`PretrainedConfig`], *optional*):
211	                Configuration for the model to use instead of an automatically loaded configuration. Configuration can
212	                be automatically loaded when:
213	
214	                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained
215	                      model).
216	                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the
217	                      save directory.
218	                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a
219	                      configuration JSON file named *config.json* is found in the directory.
220	            cache_dir (`str` or `os.PathLike`, *optional*):
221	                Path to a directory in which a downloaded pretrained model configuration should be cached if the
222	                standard cache should not be used.
223	            from_pt (`bool`, *optional*, defaults to `False`):
224	                Load the model weights from a PyTorch checkpoint save file (see docstring of
225	                `pretrained_model_name_or_path` argument).
226	            force_download (`bool`, *optional*, defaults to `False`):
227	                Whether or not to force the (re-)download of the model weights and configuration files, overriding the
228	                cached versions if they exist.
229	            resume_download:
230	                Deprecated and ignored. All downloads are now resumed by default when possible.
231	                Will be removed in v5 of Transformers.
232	            proxies (`Dict[str, str]`, *optional*):
233	                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
234	                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
235	            output_loading_info(`bool`, *optional*, defaults to `False`):
236	                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.
237	            local_files_only(`bool`, *optional*, defaults to `False`):
238	                Whether or not to only look at local files (e.g., not try downloading the model).
239	            revision (`str`, *optional*, defaults to `"main"`):
240	                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
241	                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
242	                identifier allowed by git.
243	            trust_remote_code (`bool`, *optional*, defaults to `False`):
244	                Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
245	                should only be set to `True` for repositories you trust and in which you have read the code, as it will
246	                execute code present on the Hub on your local machine.
247	            code_revision (`str`, *optional*, defaults to `"main"`):
248	                The specific revision to use for the code on the Hub, if the code leaves in a different repository than
249	                the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
250	                system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier
251	                allowed by git.
252	            kwargs (additional keyword arguments, *optional*):
253	                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
254	                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or
255	                automatically loaded:
256	
257	                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the
258	                      underlying model's `__init__` method (we assume all relevant updates to the configuration have
259	                      already been done)
260	                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class
261	                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that
262	                      corresponds to a configuration attribute will be used to override said attribute with the
263	                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute
264	                      will be passed to the underlying model's `__init__` function.
265	
266	        Examples:
267	
268	        ```python
269	        >>> from transformers import AutoConfig, BaseAutoModelClass
270	
271	        >>> # Download model and configuration from huggingface.co and cache.
272	        >>> model = BaseAutoModelClass.from_pretrained("checkpoint_placeholder")
273	
274	        >>> # Update configuration during loading
275	        >>> model = BaseAutoModelClass.from_pretrained("checkpoint_placeholder", output_attentions=True)
276	        >>> model.config.output_attentions
277	        True
278	
279	        >>> # Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
280	        >>> config = AutoConfig.from_pretrained("./pt_model/shortcut_placeholder_pt_model_config.json")
281	        >>> model = BaseAutoModelClass.from_pretrained(
282	        ...     "./pt_model/shortcut_placeholder_pytorch_model.bin", from_pt=True, config=config
283	        ... )
284	        ```
285	"""
286	
287	FROM_PRETRAINED_FLAX_DOCSTRING = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/auto/auto_factory.py:287
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
286	
287	FROM_PRETRAINED_FLAX_DOCSTRING = """
288	        Instantiate one of the model classes of the library from a pretrained model.
289	
290	        The model class to instantiate is selected based on the `model_type` property of the config object (either
291	        passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
292	        falling back to using pattern matching on `pretrained_model_name_or_path`:
293	
294	        List options
295	
296	        Args:
297	            pretrained_model_name_or_path (`str` or `os.PathLike`):
298	                Can be either:
299	
300	                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.
301	                    - A path to a *directory* containing model weights saved using
302	                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.
303	                    - A path or url to a *PyTorch state_dict save file* (e.g, `./pt_model/pytorch_model.bin`). In this
304	                      case, `from_pt` should be set to `True` and a configuration object should be provided as `config`
305	                      argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
306	                      using the provided conversion scripts and loading the TensorFlow model afterwards.
307	            model_args (additional positional arguments, *optional*):
308	                Will be passed along to the underlying model `__init__()` method.
309	            config ([`PretrainedConfig`], *optional*):
310	                Configuration for the model to use instead of an automatically loaded configuration. Configuration can
311	                be automatically loaded when:
312	
313	                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained
314	                      model).
315	                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the
316	                      save directory.
317	                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a
318	                      configuration JSON file named *config.json* is found in the directory.
319	            cache_dir (`str` or `os.PathLike`, *optional*):
320	                Path to a directory in which a downloaded pretrained model configuration should be cached if the
321	                standard cache should not be used.
322	            from_pt (`bool`, *optional*, defaults to `False`):
323	                Load the model weights from a PyTorch checkpoint save file (see docstring of
324	                `pretrained_model_name_or_path` argument).
325	            force_download (`bool`, *optional*, defaults to `False`):
326	                Whether or not to force the (re-)download of the model weights and configuration files, overriding the
327	                cached versions if they exist.
328	            resume_download:
329	                Deprecated and ignored. All downloads are now resumed by default when possible.
330	                Will be removed in v5 of Transformers.
331	            proxies (`Dict[str, str]`, *optional*):
332	                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
333	                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
334	            output_loading_info(`bool`, *optional*, defaults to `False`):
335	                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.
336	            local_files_only(`bool`, *optional*, defaults to `False`):
337	                Whether or not to only look at local files (e.g., not try downloading the model).
338	            revision (`str`, *optional*, defaults to `"main"`):
339	                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
340	                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
341	                identifier allowed by git.
342	            trust_remote_code (`bool`, *optional*, defaults to `False`):
343	                Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
344	                should only be set to `True` for repositories you trust and in which you have read the code, as it will
345	                execute code present on the Hub on your local machine.
346	            code_revision (`str`, *optional*, defaults to `"main"`):
347	                The specific revision to use for the code on the Hub, if the code leaves in a different repository than
348	                the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
349	                system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier
350	                allowed by git.
351	            kwargs (additional keyword arguments, *optional*):
352	                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
353	                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or
354	                automatically loaded:
355	
356	                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the
357	                      underlying model's `__init__` method (we assume all relevant updates to the configuration have
358	                      already been done)
359	                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class
360	                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that
361	                      corresponds to a configuration attribute will be used to override said attribute with the
362	                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute
363	                      will be passed to the underlying model's `__init__` function.
364	
365	        Examples:
366	
367	        ```python
368	        >>> from transformers import AutoConfig, BaseAutoModelClass
369	
370	        >>> # Download model and configuration from huggingface.co and cache.
371	        >>> model = BaseAutoModelClass.from_pretrained("checkpoint_placeholder")
372	
373	        >>> # Update configuration during loading
374	        >>> model = BaseAutoModelClass.from_pretrained("checkpoint_placeholder", output_attentions=True)
375	        >>> model.config.output_attentions
376	        True
377	
378	        >>> # Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
379	        >>> config = AutoConfig.from_pretrained("./pt_model/shortcut_placeholder_pt_model_config.json")
380	        >>> model = BaseAutoModelClass.from_pretrained(
381	        ...     "./pt_model/shortcut_placeholder_pytorch_model.bin", from_pt=True, config=config
382	        ... )
383	        ```
384	"""
385	
386	
387	def _get_model_class(config, model_mapping):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/auto/configuration_auto.py:1132
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1131	                raise ValueError(
1132	                    f"The checkpoint you are trying to load has model type `{config_dict['model_type']}` "
1133	                    "but Transformers does not recognize this architecture. This could be because of an "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/autoformer/modeling_autoformer.py:911
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
910	
911	AUTOFORMER_START_DOCSTRING = r"""
912	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
913	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
914	    etc.)
915	
916	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
917	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
918	    and behavior.
919	
920	    Parameters:
921	        config ([`AutoformerConfig`]):
922	            Model configuration class with all the parameters of the model. Initializing with a config file does not
923	            load the weights associated with the model, only the configuration. Check out the
924	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
925	"""
926	
927	AUTOFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/aya_vision/modeling_aya_vision.py:96
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
95	
96	AYA_VISION_START_DOCSTRING = r"""
97	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
98	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
99	    etc.)
100	
101	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
102	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
103	    and behavior.
104	
105	    Parameters:
106	        config ([`AyaVisionConfig`] or [`AyaVisionVisionConfig`]):
107	            Model configuration class with all the parameters of the model. Initializing with a config file does not
108	            load the weights associated with the model, only the configuration. Check out the
109	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
110	"""
111	
112	
113	@add_start_docstrings(
114	    "The bare Aya Vision Model outputting raw hidden-states without any specific head on top.",
115	    AYA_VISION_START_DOCSTRING,
116	)
117	class AyaVisionPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/aya_vision/modeling_aya_vision.py:192
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
191	
192	AYA_VISION_INPUTS_DOCSTRING = """
193	    Args:
194	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
195	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
196	            it.
197	
198	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
199	            [`PreTrainedTokenizer.__call__`] for details.
200	
201	            [What are input IDs?](../glossary#input-ids)
202	        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):
203	            The tensors corresponding to the input images. Pixel values can be obtained using
204	            [`AutoImageProcessor`]. See [`GotOcr2ImageProcessor.__call__`] for details. [`AyaVisionProcessor`] uses
205	            [`GotOcr2ImageProcessor`] for processing images.
206	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
207	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
208	
209	            - 1 for tokens that are **not masked**,
210	            - 0 for tokens that are **masked**.
211	
212	            [What are attention masks?](../glossary#attention-mask)
213	
214	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
215	            [`PreTrainedTokenizer.__call__`] for details.
216	
217	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
218	            `past_key_values`).
219	
220	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
221	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
222	            information on the default strategy.
223	
224	            - 1 indicates the head is **not masked**,
225	            - 0 indicates the head is **masked**.
226	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
227	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
228	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
229	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
230	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
231	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
232	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
233	
234	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
235	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
236	
237	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
238	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
239	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
240	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
241	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
242	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
243	            model's internal embedding lookup matrix.
244	        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):
245	            The index of the layer to select the vision feature. If multiple indices are provided,
246	            the vision feature of the corresponding indices will be concatenated to form the
247	            vision features.
248	        vision_feature_select_strategy (`str`, *optional*, defaults to `"default"`):
249	            The feature selection strategy used to select the vision feature from the vision backbone.
250	            Can be one of `"default"` or `"full"`.
251	        use_cache (`bool`, *optional*):
252	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
253	            `past_key_values`).
254	        output_attentions (`bool`, *optional*):
255	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
256	            tensors for more detail.
257	        output_hidden_states (`bool`, *optional*):
258	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
259	            more detail.
260	        return_dict (`bool`, *optional*):
261	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
262	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
263	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
264	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
265	            the complete sequence length.
266	"""
267	
268	
269	@add_start_docstrings(
270	    """The AyaVision model which consists of a vision backbone and a language model.""",
271	    AYA_VISION_START_DOCSTRING,
272	)
273	class AyaVisionForConditionalGeneration(AyaVisionPreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/aya_vision/modular_aya_vision.py:91
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
90	
91	AYA_VISION_START_DOCSTRING = r"""
92	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
93	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
94	    etc.)
95	
96	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
97	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
98	    and behavior.
99	
100	    Parameters:
101	        config ([`AyaVisionConfig`] or [`AyaVisionVisionConfig`]):
102	            Model configuration class with all the parameters of the model. Initializing with a config file does not
103	            load the weights associated with the model, only the configuration. Check out the
104	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
105	"""
106	
107	
108	@add_start_docstrings(
109	    "The bare Aya Vision Model outputting raw hidden-states without any specific head on top.",
110	    AYA_VISION_START_DOCSTRING,
111	)
112	class AyaVisionPreTrainedModel(LlavaPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/aya_vision/modular_aya_vision.py:121
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
120	
121	AYA_VISION_INPUTS_DOCSTRING = """
122	    Args:
123	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
124	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
125	            it.
126	
127	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
128	            [`PreTrainedTokenizer.__call__`] for details.
129	
130	            [What are input IDs?](../glossary#input-ids)
131	        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):
132	            The tensors corresponding to the input images. Pixel values can be obtained using
133	            [`AutoImageProcessor`]. See [`GotOcr2ImageProcessor.__call__`] for details. [`AyaVisionProcessor`] uses
134	            [`GotOcr2ImageProcessor`] for processing images.
135	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
136	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
137	
138	            - 1 for tokens that are **not masked**,
139	            - 0 for tokens that are **masked**.
140	
141	            [What are attention masks?](../glossary#attention-mask)
142	
143	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
144	            [`PreTrainedTokenizer.__call__`] for details.
145	
146	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
147	            `past_key_values`).
148	
149	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
150	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
151	            information on the default strategy.
152	
153	            - 1 indicates the head is **not masked**,
154	            - 0 indicates the head is **masked**.
155	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
156	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
157	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
158	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
159	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
160	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
161	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
162	
163	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
164	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
165	
166	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
167	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
168	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
169	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
170	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
171	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
172	            model's internal embedding lookup matrix.
173	        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):
174	            The index of the layer to select the vision feature. If multiple indices are provided,
175	            the vision feature of the corresponding indices will be concatenated to form the
176	            vision features.
177	        vision_feature_select_strategy (`str`, *optional*, defaults to `"default"`):
178	            The feature selection strategy used to select the vision feature from the vision backbone.
179	            Can be one of `"default"` or `"full"`.
180	        use_cache (`bool`, *optional*):
181	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
182	            `past_key_values`).
183	        output_attentions (`bool`, *optional*):
184	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
185	            tensors for more detail.
186	        output_hidden_states (`bool`, *optional*):
187	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
188	            more detail.
189	        return_dict (`bool`, *optional*):
190	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
191	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
192	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
193	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
194	            the complete sequence length.
195	"""
196	
197	
198	@add_start_docstrings(
199	    """The AyaVision model which consists of a vision backbone and a language model.""",
200	    AYA_VISION_START_DOCSTRING,
201	)
202	class AyaVisionForConditionalGeneration(LlavaForConditionalGeneration):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bamba/modeling_bamba.py:484
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
483	            logger.warning_once(
484	                "The fast path is not available because on of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)`"
485	                " is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and"
486	                " https://github.com/Dao-AILab/causal-conv1d"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bamba/modeling_bamba.py:1014
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1013	
1014	BAMBA_START_DOCSTRING = r"""
1015	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1016	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1017	    etc.)
1018	
1019	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1020	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1021	    and behavior.
1022	
1023	    Parameters:
1024	        config ([`BambaConfig`]):
1025	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1026	            load the weights associated with the model, only the configuration. Check out the
1027	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1028	"""
1029	
1030	
1031	@add_start_docstrings(
1032	    "The bare BambaModel outputting raw hidden-states without any specific head on top.",
1033	    BAMBA_START_DOCSTRING,
1034	)
1035	class BambaPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bamba/modeling_bamba.py:1058
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1057	
1058	BAMBA_INPUTS_DOCSTRING = r"""
1059	    Args:
1060	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
1061	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
1062	            it.
1063	
1064	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1065	            [`PreTrainedTokenizer.__call__`] for details.
1066	
1067	            [What are input IDs?](../glossary#input-ids)
1068	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
1069	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
1070	
1071	            - 1 for tokens that are **not masked**,
1072	            - 0 for tokens that are **masked**.
1073	
1074	            [What are attention masks?](../glossary#attention-mask)
1075	
1076	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1077	            [`PreTrainedTokenizer.__call__`] for details.
1078	
1079	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
1080	            `past_key_values`).
1081	
1082	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
1083	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
1084	            information on the default strategy.
1085	
1086	            - 1 indicates the head is **not masked**,
1087	            - 0 indicates the head is **masked**.
1088	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
1089	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
1090	            config.n_positions - 1]`.
1091	
1092	            [What are position IDs?](../glossary#position-ids)
1093	        past_key_values (`HybridMambaAttentionDynamicCache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
1094	            A HybridMambaAttentionDynamicCache object containing pre-computed hidden-states (keys and values in the
1095	            self-attention blocks and convolution and ssm states in the mamba blocks) that can be used (see
1096	            `past_key_values` input) to speed up sequential decoding.
1097	            Key and value cache tensors have shape `(batch_size, num_heads, seq_len, head_dim)`.
1098	            Convolution and ssm states tensors have shape `(batch_size, d_inner, d_conv)` and
1099	            `(batch_size, d_inner, d_state)` respectively.
1100	            See the `HybridMambaAttentionDynamicCache` class for more details.
1101	
1102	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that
1103	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
1104	            `input_ids` of shape `(batch_size, sequence_length)`.
1105	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
1106	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
1107	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
1108	            model's internal embedding lookup matrix.
1109	        use_cache (`bool`, *optional*):
1110	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
1111	            `past_key_values`).
1112	        output_attentions (`bool`, *optional*):
1113	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1114	            tensors for more detail.
1115	        output_hidden_states (`bool`, *optional*):
1116	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1117	            more detail.
1118	        output_router_logits (`bool`, *optional*):
1119	            Whether or not to return the logits of all the routers. They are useful for computing the router loss, and
1120	            should not be returned during inference.
1121	        return_dict (`bool`, *optional*):
1122	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1123	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
1124	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
1125	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
1126	            the complete sequence length.
1127	"""
1128	
1129	
1130	@add_start_docstrings(
1131	    "The bare Bamba Model outputting raw hidden-states without any specific head on top.",
1132	    BAMBA_START_DOCSTRING,
1133	)
1134	# Adapted from transformers.models.jamba.modeling_jamba.JambaModel
1135	class BambaModel(BambaPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bamba/modular_bamba.py:283
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
282	            logger.warning_once(
283	                "The fast path is not available because on of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)`"
284	                " is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and"
285	                " https://github.com/Dao-AILab/causal-conv1d"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bamba/modular_bamba.py:785
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
784	
785	BAMBA_START_DOCSTRING = r"""
786	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
787	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
788	    etc.)
789	
790	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
791	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
792	    and behavior.
793	
794	    Parameters:
795	        config ([`BambaConfig`]):
796	            Model configuration class with all the parameters of the model. Initializing with a config file does not
797	            load the weights associated with the model, only the configuration. Check out the
798	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
799	"""
800	
801	
802	@add_start_docstrings(
803	    "The bare BambaModel outputting raw hidden-states without any specific head on top.",
804	    BAMBA_START_DOCSTRING,
805	)
806	class BambaPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bamba/modular_bamba.py:829
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
828	
829	BAMBA_INPUTS_DOCSTRING = r"""
830	    Args:
831	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
832	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
833	            it.
834	
835	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
836	            [`PreTrainedTokenizer.__call__`] for details.
837	
838	            [What are input IDs?](../glossary#input-ids)
839	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
840	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
841	
842	            - 1 for tokens that are **not masked**,
843	            - 0 for tokens that are **masked**.
844	
845	            [What are attention masks?](../glossary#attention-mask)
846	
847	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
848	            [`PreTrainedTokenizer.__call__`] for details.
849	
850	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
851	            `past_key_values`).
852	
853	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
854	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
855	            information on the default strategy.
856	
857	            - 1 indicates the head is **not masked**,
858	            - 0 indicates the head is **masked**.
859	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
860	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
861	            config.n_positions - 1]`.
862	
863	            [What are position IDs?](../glossary#position-ids)
864	        past_key_values (`HybridMambaAttentionDynamicCache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
865	            A HybridMambaAttentionDynamicCache object containing pre-computed hidden-states (keys and values in the
866	            self-attention blocks and convolution and ssm states in the mamba blocks) that can be used (see
867	            `past_key_values` input) to speed up sequential decoding.
868	            Key and value cache tensors have shape `(batch_size, num_heads, seq_len, head_dim)`.
869	            Convolution and ssm states tensors have shape `(batch_size, d_inner, d_conv)` and
870	            `(batch_size, d_inner, d_state)` respectively.
871	            See the `HybridMambaAttentionDynamicCache` class for more details.
872	
873	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that
874	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
875	            `input_ids` of shape `(batch_size, sequence_length)`.
876	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
877	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
878	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
879	            model's internal embedding lookup matrix.
880	        use_cache (`bool`, *optional*):
881	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
882	            `past_key_values`).
883	        output_attentions (`bool`, *optional*):
884	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
885	            tensors for more detail.
886	        output_hidden_states (`bool`, *optional*):
887	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
888	            more detail.
889	        output_router_logits (`bool`, *optional*):
890	            Whether or not to return the logits of all the routers. They are useful for computing the router loss, and
891	            should not be returned during inference.
892	        return_dict (`bool`, *optional*):
893	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
894	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
895	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
896	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
897	            the complete sequence length.
898	"""
899	
900	
901	@add_start_docstrings(
902	    "The bare Bamba Model outputting raw hidden-states without any specific head on top.",
903	    BAMBA_START_DOCSTRING,
904	)
905	# Adapted from transformers.models.jamba.modeling_jamba.JambaModel
906	class BambaModel(BambaPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bark/configuration_bark.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	BARK_SUBMODELCONFIG_START_DOCSTRING = """
28	    This is the configuration class to store the configuration of a [`{model}`]. It is used to instantiate the model
29	    according to the specified arguments, defining the model architecture. Instantiating a configuration with the
30	    defaults will yield a similar configuration to that of the Bark [suno/bark](https://huggingface.co/suno/bark)
31	    architecture.
32	
33	    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the
34	    documentation from [`PretrainedConfig`] for more information.
35	
36	    Args:
37	        block_size (`int`, *optional*, defaults to 1024):
38	            The maximum sequence length that this model might ever be used with. Typically set this to something large
39	            just in case (e.g., 512 or 1024 or 2048).
40	        input_vocab_size (`int`, *optional*, defaults to 10_048):
41	            Vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented by the
42	            `inputs_ids` passed when calling [`{model}`]. Defaults to 10_048 but should be carefully thought with
43	            regards to the chosen sub-model.
44	        output_vocab_size (`int`, *optional*, defaults to 10_048):
45	            Output vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented
46	            by the: `output_ids` when passing forward a [`{model}`]. Defaults to 10_048 but should be carefully thought
47	            with regards to the chosen sub-model.
48	        num_layers (`int`, *optional*, defaults to 12):
49	            Number of hidden layers in the given sub-model.
50	        num_heads (`int`, *optional*, defaults to 12):
51	            Number of attention heads for each attention layer in the Transformer architecture.
52	        hidden_size (`int`, *optional*, defaults to 768):
53	            Dimensionality of the "intermediate" (often named feed-forward) layer in the architecture.
54	        dropout (`float`, *optional*, defaults to 0.0):
55	            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.
56	        bias (`bool`, *optional*, defaults to `True`):
57	            Whether or not to use bias in the linear layers and layer norm layers.
58	        initializer_range (`float`, *optional*, defaults to 0.02):
59	            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
60	        use_cache (`bool`, *optional*, defaults to `True`):
61	            Whether or not the model should return the last key/values attentions (not used by all models).
62	"""
63	
64	
65	class BarkSubModelConfig(PretrainedConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bark/modeling_bark.py:419
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
418	
419	BARK_MODEL_START_DOCSTRING = """
420	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
421	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
422	    etc.)
423	
424	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
425	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
426	    and behavior.
427	
428	    Parameters:
429	        config ([`{config}`]):
430	            Model configuration class with all the parameters of the model. Initializing with a config file does not
431	            load the weights associated with the model, only the configuration. Check out the
432	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
433	"""
434	
435	
436	BARK_START_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bark/modeling_bark.py:436
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
435	
436	BARK_START_DOCSTRING = r"""
437	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
438	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
439	    etc.)
440	
441	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
442	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
443	    and behavior.
444	
445	    Parameters:
446	        config ([`BarkConfig`]):
447	            Model configuration class with all the parameters of the model. Initializing with a config file does not
448	            load the weights associated with the model, only the configuration. Check out the
449	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
450	"""
451	
452	
453	BARK_FINE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bart/modeling_bart.py:790
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
789	
790	BART_START_DOCSTRING = r"""
791	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
792	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
793	    etc.)
794	
795	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
796	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
797	    and behavior.
798	
799	    Parameters:
800	        config ([`BartConfig`]):
801	            Model configuration class with all the parameters of the model. Initializing with a config file does not
802	            load the weights associated with the model, only the configuration. Check out the
803	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
804	"""
805	
806	BART_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bart/modeling_bart.py:849
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
848	
849	BART_INPUTS_DOCSTRING = r"""
850	    Args:
851	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
852	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
853	            it.
854	
855	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
856	            [`PreTrainedTokenizer.__call__`] for details.
857	
858	            [What are input IDs?](../glossary#input-ids)
859	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
860	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
861	
862	            - 1 for tokens that are **not masked**,
863	            - 0 for tokens that are **masked**.
864	
865	            [What are attention masks?](../glossary#attention-mask)
866	        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
867	            Indices of decoder input sequence tokens in the vocabulary.
868	
869	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
870	            [`PreTrainedTokenizer.__call__`] for details.
871	
872	            [What are decoder input IDs?](../glossary#decoder-input-ids)
873	
874	            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`
875	            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).
876	
877	            For translation and summarization training, `decoder_input_ids` should be provided. If no
878	            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
879	            for denoising pre-training following the paper.
880	        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
881	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
882	            be used by default.
883	
884	            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]
885	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
886	            information on the default strategy.
887	        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
888	            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:
889	
890	            - 1 indicates the head is **not masked**,
891	            - 0 indicates the head is **masked**.
892	
893	        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
894	            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:
895	
896	            - 1 indicates the head is **not masked**,
897	            - 0 indicates the head is **masked**.
898	
899	        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
900	            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,
901	            1]`:
902	
903	            - 1 indicates the head is **not masked**,
904	            - 0 indicates the head is **masked**.
905	
906	        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):
907	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
908	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
909	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
910	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
911	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
912	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
913	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
914	
915	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
916	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
917	
918	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
919	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
920	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
921	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
922	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.
923	            This is useful if you want more control over how to convert `input_ids` indices into associated vectors
924	            than the model's internal embedding lookup matrix.
925	        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):
926	            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded
927	            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be
928	            input (see `past_key_values`). This is useful if you want more control over how to convert
929	            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.
930	
931	            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value
932	            of `inputs_embeds`.
933	        use_cache (`bool`, *optional*):
934	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
935	            `past_key_values`).
936	        output_attentions (`bool`, *optional*):
937	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
938	            tensors for more detail.
939	        output_hidden_states (`bool`, *optional*):
940	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
941	            more detail.
942	        return_dict (`bool`, *optional*):
943	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
944	"""
945	
946	
947	class BartEncoder(BartPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bart/modeling_flax_bart.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
57	
58	BART_START_DOCSTRING = r"""
59	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
60	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
61	    etc.)
62	
63	    This model is also a Flax Linen
64	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
65	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
66	
67	    Finally, this model supports inherent JAX features such as:
68	
69	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
70	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
71	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
72	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
73	
74	    Parameters:
75	        config ([`BartConfig`]): Model configuration class with all the parameters of the model.
76	            Initializing with a config file does not load the weights associated with the model, only the
77	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
78	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
79	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
80	            `jax.numpy.bfloat16` (on TPUs).
81	
82	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
83	            specified all the computation will be performed with the given `dtype`.
84	
85	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
86	            parameters.**
87	
88	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
89	            [`~FlaxPreTrainedModel.to_bf16`].
90	"""
91	
92	BART_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bart/modeling_flax_bart.py:92
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
91	
92	BART_INPUTS_DOCSTRING = r"""
93	    Args:
94	        input_ids (`jnp.ndarray` of shape `(batch_size, sequence_length)`):
95	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
96	            it.
97	
98	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
99	            [`PreTrainedTokenizer.__call__`] for details.
100	
101	            [What are input IDs?](../glossary#input-ids)
102	        attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
103	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
104	
105	            - 1 for tokens that are **not masked**,
106	            - 0 for tokens that are **masked**.
107	
108	            [What are attention masks?](../glossary#attention-mask)
109	        decoder_input_ids (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
110	            Indices of decoder input sequence tokens in the vocabulary.
111	
112	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
113	            [`PreTrainedTokenizer.__call__`] for details.
114	
115	            [What are decoder input IDs?](../glossary#decoder-input-ids)
116	
117	            For translation and summarization training, `decoder_input_ids` should be provided. If no
118	            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
119	            for denoising pre-training following the paper.
120	        decoder_attention_mask (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
121	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
122	            be used by default.
123	
124	            If you want to change padding behavior, you should modify to your needs. See diagram 1 in [the
125	            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
126	        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
127	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
128	            config.max_position_embeddings - 1]`.
129	        decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
130	            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
131	            range `[0, config.max_position_embeddings - 1]`.
132	        output_attentions (`bool`, *optional*):
133	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
134	            tensors for more detail.
135	        output_hidden_states (`bool`, *optional*):
136	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
137	            more detail.
138	        return_dict (`bool`, *optional*):
139	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
140	"""
141	
142	
143	BART_ENCODE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bart/modeling_flax_bart.py:173
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
172	
173	BART_DECODE_INPUTS_DOCSTRING = r"""
174	    Args:
175	        decoder_input_ids (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`):
176	            Indices of decoder input sequence tokens in the vocabulary.
177	
178	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
179	            [`PreTrainedTokenizer.__call__`] for details.
180	
181	            [What are decoder input IDs?](../glossary#decoder-input-ids)
182	
183	            For translation and summarization training, `decoder_input_ids` should be provided. If no
184	            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
185	            for denoising pre-training following the paper.
186	        encoder_outputs (`tuple(tuple(jnp.ndarray)`):
187	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
188	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
189	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
190	        encoder_attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
191	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
192	
193	            - 1 for tokens that are **not masked**,
194	            - 0 for tokens that are **masked**.
195	
196	            [What are attention masks?](../glossary#attention-mask)
197	        decoder_attention_mask (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
198	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
199	            be used by default.
200	
201	            If you want to change padding behavior, you should modify to your needs. See diagram 1 in [the
202	            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
203	        decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
204	            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
205	            range `[0, config.max_position_embeddings - 1]`.
206	        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):
207	            Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
208	            auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.
209	        output_attentions (`bool`, *optional*):
210	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
211	            tensors for more detail.
212	        output_hidden_states (`bool`, *optional*):
213	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
214	            more detail.
215	        return_dict (`bool`, *optional*):
216	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
217	"""
218	
219	
220	def shift_tokens_right(input_ids: jnp.ndarray, pad_token_id: int, decoder_start_token_id: int) -> jnp.ndarray:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bart/modeling_tf_bart.py:581
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
580	
581	BART_START_DOCSTRING = r"""
582	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
583	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
584	    etc.)
585	
586	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
587	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
588	    behavior.
589	
590	    <Tip>
591	
592	    TensorFlow models and layers in `transformers` accept two formats as input:
593	
594	    - having all inputs as keyword arguments (like PyTorch models), or
595	    - having all inputs as a list, tuple or dict in the first positional argument.
596	
597	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
598	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
599	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
600	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
601	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
602	    positional argument:
603	
604	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
605	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
606	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
607	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
608	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
609	
610	    Note that when creating models and layers with
611	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
612	    about any of this, as you can just pass inputs like you would to any other Python function!
613	
614	    </Tip>
615	
616	    Args:
617	        config ([`BartConfig`]): Model configuration class with all the parameters of the model.
618	            Initializing with a config file does not load the weights associated with the model, only the
619	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
620	"""
621	
622	
623	BART_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bart/tokenization_bart.py:187
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
186	        with open(merges_file, encoding="utf-8") as merges_handle:
187	            bpe_merges = merges_handle.read().split("\n")[1:-1]
188	        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bart/tokenization_bart.py:294
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
293	        with open(vocab_file, "w", encoding="utf-8") as f:
294	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
295	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bart/tokenization_bart.py:298
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
297	        with open(merge_file, "w", encoding="utf-8") as writer:
298	            writer.write("#version: 0.2\n")
299	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bart/tokenization_bart.py:306
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
305	                    index = token_index
306	                writer.write(" ".join(bpe_tokens) + "\n")
307	                index += 1

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/barthez/tokenization_barthez.py:284
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
283	                content_spiece_model = self.sp_model.serialized_model_proto()
284	                fi.write(content_spiece_model)
285	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bartpho/tokenization_bartpho.py:301
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
300	                content_spiece_model = self.sp_model.serialized_model_proto()
301	                fi.write(content_spiece_model)
302	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bartpho/tokenization_bartpho.py:311
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
310	                    if token not in self.all_special_tokens:
311	                        fp.write(f"{str(token)} \n")
312	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/beit/modeling_beit.py:787
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
786	
787	BEIT_START_DOCSTRING = r"""
788	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
789	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
790	    behavior.
791	
792	    Parameters:
793	        config ([`BeitConfig`]): Model configuration class with all the parameters of the model.
794	            Initializing with a config file does not load the weights associated with the model, only the
795	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
796	"""
797	
798	BEIT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/beit/modeling_flax_beit.py:67
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
66	
67	BEIT_START_DOCSTRING = r"""
68	
69	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
70	    library implements for all its model (such as downloading, saving and converting weights from PyTorch models)
71	
72	    This model is also a
73	    [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. Use it as
74	    a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
75	    behavior.
76	
77	    Finally, this model supports inherent JAX features such as:
78	
79	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
80	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
81	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
82	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
83	
84	    Parameters:
85	        config ([`BeitConfig`]): Model configuration class with all the parameters of the model.
86	            Initializing with a config file does not load the weights associated with the model, only the
87	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
88	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
89	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
90	            `jax.numpy.bfloat16` (on TPUs).
91	
92	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
93	            specified all the computation will be performed with the given `dtype`.
94	
95	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
96	            parameters.**
97	
98	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
99	            [`~FlaxPreTrainedModel.to_bf16`].
100	"""
101	
102	BEIT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/beit/modeling_flax_beit.py:749
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
748	
749	FLAX_BEIT_MODEL_DOCSTRING = """
750	    Returns:
751	
752	    Examples:
753	
754	    ```python
755	    >>> from transformers import AutoImageProcessor, FlaxBeitModel
756	    >>> from PIL import Image
757	    >>> import requests
758	
759	    >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
760	    >>> image = Image.open(requests.get(url, stream=True).raw)
761	
762	    >>> image_processor = AutoImageProcessor.from_pretrained("microsoft/beit-base-patch16-224-pt22k-ft22k")
763	    >>> model = FlaxBeitModel.from_pretrained("microsoft/beit-base-patch16-224-pt22k-ft22k")
764	
765	    >>> inputs = image_processor(images=image, return_tensors="np")
766	    >>> outputs = model(**inputs)
767	    >>> last_hidden_states = outputs.last_hidden_state
768	    ```
769	"""
770	
771	overwrite_call_docstring(FlaxBeitModel, FLAX_BEIT_MODEL_DOCSTRING)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/beit/modeling_flax_beit.py:833
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
832	
833	FLAX_BEIT_MLM_DOCSTRING = """
834	    bool_masked_pos (`numpy.ndarray` of shape `(batch_size, num_patches)`):
835	        Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).
836	
837	    Returns:
838	
839	    Examples:
840	
841	    ```python
842	    >>> from transformers import AutoImageProcessor, BeitForMaskedImageModeling
843	    >>> from PIL import Image
844	    >>> import requests
845	
846	    >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
847	    >>> image = Image.open(requests.get(url, stream=True).raw)
848	
849	    >>> image_processor = AutoImageProcessor.from_pretrained("microsoft/beit-base-patch16-224-pt22k")
850	    >>> model = BeitForMaskedImageModeling.from_pretrained("microsoft/beit-base-patch16-224-pt22k")
851	
852	    >>> inputs = image_processor(images=image, return_tensors="np")
853	    >>> outputs = model(**inputs)
854	    >>> logits = outputs.logits
855	    ```
856	"""
857	
858	overwrite_call_docstring(FlaxBeitForMaskedImageModeling, FLAX_BEIT_MLM_DOCSTRING)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/beit/modeling_flax_beit.py:920
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
919	
920	FLAX_BEIT_CLASSIF_DOCSTRING = """
921	    Returns:
922	
923	    Example:
924	
925	    ```python
926	    >>> from transformers import AutoImageProcessor, FlaxBeitForImageClassification
927	    >>> from PIL import Image
928	    >>> import requests
929	
930	    >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
931	    >>> image = Image.open(requests.get(url, stream=True).raw)
932	
933	    >>> image_processor = AutoImageProcessor.from_pretrained("microsoft/beit-base-patch16-224")
934	    >>> model = FlaxBeitForImageClassification.from_pretrained("microsoft/beit-base-patch16-224")
935	
936	    >>> inputs = image_processor(images=image, return_tensors="np")
937	    >>> outputs = model(**inputs)
938	    >>> logits = outputs.logits
939	    >>> # model predicts one of the 1000 ImageNet classes
940	    >>> predicted_class_idx = logits.argmax(-1).item()
941	    >>> print("Predicted class:", model.config.id2label[predicted_class_idx])
942	    ```
943	"""
944	
945	overwrite_call_docstring(FlaxBeitForImageClassification, FLAX_BEIT_CLASSIF_DOCSTRING)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bert/modeling_bert.py:95
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
94	        logger.error(
95	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
96	            "https://www.tensorflow.org/install/ for installation instructions."
97	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bert/modeling_bert.py:889
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
888	
889	BERT_START_DOCSTRING = r"""
890	
891	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
892	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
893	    etc.)
894	
895	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
896	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
897	    and behavior.
898	
899	    Parameters:
900	        config ([`BertConfig`]): Model configuration class with all the parameters of the model.
901	            Initializing with a config file does not load the weights associated with the model, only the
902	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
903	"""
904	
905	BERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bert/modeling_flax_bert.py:91
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
90	
91	BERT_START_DOCSTRING = r"""
92	
93	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
94	    library implements for all its model (such as downloading, saving and converting weights from PyTorch models)
95	
96	    This model is also a
97	    [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. Use it as
98	    a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
99	    behavior.
100	
101	    Finally, this model supports inherent JAX features such as:
102	
103	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
104	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
105	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
106	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
107	
108	    Parameters:
109	        config ([`BertConfig`]): Model configuration class with all the parameters of the model.
110	            Initializing with a config file does not load the weights associated with the model, only the
111	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
112	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
113	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
114	            `jax.numpy.bfloat16` (on TPUs).
115	
116	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
117	            specified all the computation will be performed with the given `dtype`.
118	
119	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
120	            parameters.**
121	
122	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
123	            [`~FlaxPreTrainedModel.to_bf16`].
124	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
125	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
126	            `jax.numpy.bfloat16` (on TPUs).
127	
128	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
129	            specified all the computation will be performed with the given `dtype`.
130	
131	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
132	            parameters.**
133	
134	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
135	            [`~FlaxPreTrainedModel.to_bf16`].
136	
137	"""
138	
139	BERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bert/modeling_tf_bert.py:1057
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1056	
1057	BERT_START_DOCSTRING = r"""
1058	
1059	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
1060	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1061	    etc.)
1062	
1063	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
1064	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
1065	    behavior.
1066	
1067	    <Tip>
1068	
1069	    TensorFlow models and layers in `transformers` accept two formats as input:
1070	
1071	    - having all inputs as keyword arguments (like PyTorch models), or
1072	    - having all inputs as a list, tuple or dict in the first positional argument.
1073	
1074	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
1075	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
1076	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
1077	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
1078	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
1079	    positional argument:
1080	
1081	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
1082	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
1083	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
1084	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
1085	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
1086	
1087	    Note that when creating models and layers with
1088	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
1089	    about any of this, as you can just pass inputs like you would to any other Python function!
1090	
1091	    </Tip>
1092	
1093	    Args:
1094	        config ([`BertConfig`]): Model configuration class with all the parameters of the model.
1095	            Initializing with a config file does not load the weights associated with the model, only the
1096	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
1097	"""
1098	
1099	BERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bert/tokenization_bert.py:284
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
283	                    index = token_index
284	                writer.write(token + "\n")
285	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bert_generation/modeling_bert_generation.py:476
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
475	        logger.error(
476	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
477	            "https://www.tensorflow.org/install/ for installation instructions."
478	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bert_generation/modeling_bert_generation.py:615
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
614	
615	BERT_GENERATION_START_DOCSTRING = r"""
616	
617	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
618	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
619	    etc.)
620	
621	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
622	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
623	    and behavior.
624	
625	    Parameters:
626	        config ([`BertGenerationConfig`]): Model configuration class with all the parameters of the model.
627	            Initializing with a config file does not load the weights associated with the model, only the
628	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
629	"""
630	
631	BERT_GENERATION_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bert_generation/tokenization_bert_generation.py:170
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
169	                content_spiece_model = self.sp_model.serialized_model_proto()
170	                fi.write(content_spiece_model)
171	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py:359
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
358	                content_spiece_model = self.subword_tokenizer.sp_model.serialized_model_proto()
359	                writer.write(content_spiece_model)
360	        else:

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py:370
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
369	                        index = token_index
370	                    writer.write(token + "\n")
371	                    index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py:411
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
410	            raise error.__class__(
411	                "You need to install fugashi to use MecabTokenizer. "
412	                "See https://pypi.org/project/fugashi/ for installation."
413	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py:423
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
422	                    raise error.__class__(
423	                        "The ipadic dictionary is not installed. "
424	                        "See https://github.com/polm/ipadic-py for installation."
425	                    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py:434
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
433	                    raise error.__class__(
434	                        "The unidic_lite dictionary is not installed. "
435	                        "See https://github.com/polm/unidic-lite for installation."
436	                    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py:445
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
444	                    raise error.__class__(
445	                        "The unidic dictionary is not installed. "
446	                        "See https://github.com/polm/unidic-py for installation."
447	                    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py:452
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
451	                    raise RuntimeError(
452	                        "The unidic dictionary itself is not found. "
453	                        "See https://github.com/polm/unidic-py for installation."
454	                    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py:530
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
529	            raise ImportError(
530	                "You need to install sudachipy to use SudachiTokenizer. "
531	                "See https://github.com/WorksApplications/SudachiPy for installation."
532	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py:614
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
613	            raise ImportError(
614	                "You need to install rhoknp to use JumanppTokenizer. "
615	                "See https://github.com/ku-nlp/rhoknp for installation."
616	            )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bertweet/tokenization_bertweet.py:146
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
145	        with open(merges_file, encoding="utf-8") as merges_handle:
146	            merges = merges_handle.read().split("\n")[:-1]
147	        merges = [tuple(merge.split()[:-1]) for merge in merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bertweet/tokenization_bertweet.py:389
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
388	                content_spiece_model = self.sp_model.serialized_model_proto()
389	                fi.write(content_spiece_model)
390	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/big_bird/modeling_big_bird.py:131
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
130	        logger.error(
131	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
132	            "https://www.tensorflow.org/install/ for installation instructions."
133	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/big_bird/modeling_big_bird.py:1776
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1775	
1776	BIG_BIRD_START_DOCSTRING = r"""
1777	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
1778	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
1779	    behavior.
1780	
1781	    Parameters:
1782	        config ([`BigBirdConfig`]): Model configuration class with all the parameters of the model.
1783	            Initializing with a config file does not load the weights associated with the model, only the
1784	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1785	"""
1786	
1787	BIG_BIRD_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/big_bird/modeling_flax_big_bird.py:120
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
119	
120	BIG_BIRD_START_DOCSTRING = r"""
121	
122	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
123	    library implements for all its model (such as downloading, saving and converting weights from PyTorch models)
124	
125	    This model is also a
126	    [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. Use it as
127	    a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
128	    behavior.
129	
130	    Finally, this model supports inherent JAX features such as:
131	
132	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
133	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
134	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
135	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
136	
137	    Parameters:
138	        config ([`BigBirdConfig`]): Model configuration class with all the parameters of the model.
139	            Initializing with a config file does not load the weights associated with the model, only the
140	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
141	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
142	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
143	            `jax.numpy.bfloat16` (on TPUs).
144	
145	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
146	            specified all the computation will be performed with the given `dtype`.
147	
148	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
149	            parameters.**
150	
151	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
152	            [`~FlaxPreTrainedModel.to_bf16`].
153	"""
154	
155	BIG_BIRD_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/big_bird/tokenization_big_bird.py:244
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
243	                content_spiece_model = self.sp_model.serialized_model_proto()
244	                fi.write(content_spiece_model)
245	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:1597
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1596	
1597	BIGBIRD_PEGASUS_START_DOCSTRING = r"""
1598	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1599	    library implements for all its model (such as downloading or saving, resizing the input embeddings etc.)
1600	
1601	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1602	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1603	    and behavior.
1604	
1605	    Parameters:
1606	        config ([`BigBirdPegasusConfig`]):
1607	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1608	            load the weights associated with the model, only the configuration. Check out the
1609	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1610	"""
1611	
1612	BIGBIRD_PEGASUS_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:1638
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1637	
1638	BIGBIRD_PEGASUS_INPUTS_DOCSTRING = r"""
1639	    Args:
1640	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
1641	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
1642	            it.
1643	
1644	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1645	            [`PreTrainedTokenizer.__call__`] for details.
1646	
1647	            [What are input IDs?](../glossary#input-ids)
1648	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
1649	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
1650	
1651	            - 1 for tokens that are **not masked**,
1652	            - 0 for tokens that are **masked**.
1653	
1654	            [What are attention masks?](../glossary#attention-mask)
1655	        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
1656	            Provide for translation and summarization training. By default, the model will create this tensor by
1657	            shifting the `input_ids` to the right, following the paper.
1658	        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
1659	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
1660	            be used by default.
1661	
1662	            If you want to change padding behavior, you should read
1663	            [`modeling_bigbird_pegasus._prepare_decoder_attention_mask`] and modify to your needs. See diagram 1 in
1664	            [the paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
1665	
1666	        decoder_head_mask (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*):
1667	            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:
1668	
1669	            - 1 indicates the head is **not masked**,
1670	            - 0 indicates the head is **masked**.
1671	
1672	        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):
1673	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
1674	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
1675	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
1676	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
1677	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
1678	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
1679	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
1680	
1681	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
1682	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
1683	
1684	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
1685	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
1686	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
1687	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
1688	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.
1689	            This is useful if you want more control over how to convert `input_ids` indices into associated vectors
1690	            than the model's internal embedding lookup matrix.
1691	        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):
1692	            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded
1693	            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be
1694	            input (see `past_key_values`). This is useful if you want more control over how to convert
1695	            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.
1696	
1697	            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value
1698	            of `inputs_embeds`.
1699	        use_cache (`bool`, *optional*):
1700	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
1701	            `past_key_values`).
1702	        output_attentions (`bool`, *optional*):
1703	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1704	            tensors for more detail.
1705	        output_hidden_states (`bool`, *optional*):
1706	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1707	            more detail.
1708	        return_dict (`bool`, *optional*):
1709	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1710	"""
1711	
1712	BIGBIRD_PEGASUS_STANDALONE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/biogpt/modeling_biogpt.py:475
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
474	
475	BIOGPT_START_DOCSTRING = r"""
476	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
477	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
478	    behavior.
479	
480	    Parameters:
481	        config ([`~BioGptConfig`]): Model configuration class with all the parameters of the model.
482	            Initializing with a config file does not load the weights associated with the model, only the
483	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
484	"""
485	
486	BIOGPT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/biogpt/tokenization_biogpt.py:107
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
106	            raise ImportError(
107	                "You need to install sacremoses to use BioGptTokenizer. "
108	                "See https://pypi.org/project/sacremoses/ for installation."
109	            )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/biogpt/tokenization_biogpt.py:122
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
121	        with open(merges_file, encoding="utf-8") as merges_handle:
122	            merges = merges_handle.read().split("\n")[:-1]
123	        merges = [tuple(merge.split()[:2]) for merge in merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/biogpt/tokenization_biogpt.py:326
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
325	        with open(vocab_file, "w", encoding="utf-8") as f:
326	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
327	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/biogpt/tokenization_biogpt.py:337
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
336	                    index = token_index
337	                writer.write(" ".join(bpe_tokens) + "\n")
338	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/biogpt/tokenization_biogpt.py:354
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
353	            raise ImportError(
354	                "You need to install sacremoses to use XLMTokenizer. "
355	                "See https://pypi.org/project/sacremoses/ for installation."
356	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bit/modeling_bit.py:675
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
674	
675	BIT_START_DOCSTRING = r"""
676	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
677	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
678	    behavior.
679	
680	    Parameters:
681	        config ([`BitConfig`]): Model configuration class with all the parameters of the model.
682	            Initializing with a config file does not load the weights associated with the model, only the
683	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
684	"""
685	
686	BIT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blenderbot/modeling_blenderbot.py:484
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
483	
484	BLENDERBOT_START_DOCSTRING = r"""
485	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
486	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
487	    etc.)
488	
489	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
490	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
491	    and behavior.
492	
493	    Parameters:
494	        config ([`BlenderbotConfig`]):
495	            Model configuration class with all the parameters of the model. Initializing with a config file does not
496	            load the weights associated with the model, only the configuration. Check out the
497	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
498	"""
499	
500	BLENDERBOT_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blenderbot/modeling_flax_blenderbot.py:56
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
55	
56	BLENDERBOT_START_DOCSTRING = r"""
57	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
58	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
59	    etc.)
60	
61	    This model is also a Flax Linen
62	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
63	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
64	
65	    Finally, this model supports inherent JAX features such as:
66	
67	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
68	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
69	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
70	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
71	
72	    Parameters:
73	        config ([`BlenderbotConfig`]): Model configuration class with all the parameters of the model.
74	            Initializing with a config file does not load the weights associated with the model, only the
75	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
76	"""
77	
78	BLENDERBOT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blenderbot/modeling_flax_blenderbot.py:78
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
77	
78	BLENDERBOT_INPUTS_DOCSTRING = r"""
79	    Args:
80	        input_ids (`jnp.ndarray` of shape `(batch_size, sequence_length)`):
81	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
82	            it.
83	
84	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
85	            [`PreTrainedTokenizer.__call__`] for details.
86	
87	            [What are input IDs?](../glossary#input-ids)
88	        attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
89	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
90	
91	            - 1 for tokens that are **not masked**,
92	            - 0 for tokens that are **masked**.
93	
94	            [What are attention masks?](../glossary#attention-mask)
95	        decoder_input_ids (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
96	            Indices of decoder input sequence tokens in the vocabulary.
97	
98	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
99	            [`PreTrainedTokenizer.__call__`] for details.
100	
101	            [What are decoder input IDs?](../glossary#decoder-input-ids)
102	
103	            For translation and summarization training, `decoder_input_ids` should be provided. If no
104	            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
105	            for denoising pre-training following the paper.
106	        decoder_attention_mask (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
107	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
108	            be used by default.
109	
110	            If you want to change padding behavior, you should modify to your needs. See diagram 1 in [the
111	            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
112	        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
113	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
114	            config.max_position_embeddings - 1]`.
115	        decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
116	            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
117	            range `[0, config.max_position_embeddings - 1]`.
118	        output_attentions (`bool`, *optional*):
119	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
120	            tensors for more detail.
121	        output_hidden_states (`bool`, *optional*):
122	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
123	            more detail.
124	        return_dict (`bool`, *optional*):
125	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
126	"""
127	
128	
129	BLENDERBOT_ENCODE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blenderbot/modeling_flax_blenderbot.py:159
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
158	
159	BLENDERBOT_DECODE_INPUTS_DOCSTRING = r"""
160	    Args:
161	        decoder_input_ids (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`):
162	            Indices of decoder input sequence tokens in the vocabulary.
163	
164	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
165	            [`PreTrainedTokenizer.__call__`] for details.
166	
167	            [What are decoder input IDs?](../glossary#decoder-input-ids)
168	
169	            For translation and summarization training, `decoder_input_ids` should be provided. If no
170	            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
171	            for denoising pre-training following the paper.
172	        encoder_outputs (`tuple(tuple(jnp.ndarray)`):
173	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
174	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
175	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
176	        encoder_attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
177	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
178	
179	            - 1 for tokens that are **not masked**,
180	            - 0 for tokens that are **masked**.
181	
182	            [What are attention masks?](../glossary#attention-mask)
183	        decoder_attention_mask (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
184	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
185	            be used by default.
186	
187	            If you want to change padding behavior, you should modify to your needs. See diagram 1 in [the
188	            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
189	        decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
190	            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
191	            range `[0, config.max_position_embeddings - 1]`.
192	        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):
193	            Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
194	            auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.
195	        output_attentions (`bool`, *optional*):
196	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
197	            tensors for more detail.
198	        output_hidden_states (`bool`, *optional*):
199	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
200	            more detail.
201	        return_dict (`bool`, *optional*):
202	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
203	"""
204	
205	
206	# Copied from transformers.models.bart.modeling_flax_bart.shift_tokens_right
207	def shift_tokens_right(input_ids: jnp.ndarray, pad_token_id: int, decoder_start_token_id: int) -> jnp.ndarray:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blenderbot/modeling_tf_blenderbot.py:531
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
530	
531	BLENDERBOT_START_DOCSTRING = r"""
532	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
533	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
534	    etc.)
535	
536	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
537	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
538	    behavior.
539	
540	    <Tip>
541	
542	    TensorFlow models and layers in `transformers` accept two formats as input:
543	
544	    - having all inputs as keyword arguments (like PyTorch models), or
545	    - having all inputs as a list, tuple or dict in the first positional argument.
546	
547	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
548	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
549	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
550	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
551	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
552	    positional argument:
553	
554	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
555	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
556	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
557	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
558	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
559	
560	    Note that when creating models and layers with
561	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
562	    about any of this, as you can just pass inputs like you would to any other Python function!
563	
564	    </Tip>
565	
566	    Args:
567	        config ([`BlenderbotConfig`]): Model configuration class with all the parameters of the model.
568	            Initializing with a config file does not load the weights associated with the model, only the
569	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
570	"""
571	
572	BLENDERBOT_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blenderbot/tokenization_blenderbot.py:200
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
199	        with open(merges_file, encoding="utf-8") as merges_handle:
200	            bpe_merges = merges_handle.read().split("\n")[1:-1]
201	        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blenderbot/tokenization_blenderbot.py:317
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
316	        with open(vocab_file, "w", encoding="utf-8") as f:
317	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
318	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blenderbot/tokenization_blenderbot.py:321
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
320	        with open(merge_file, "w", encoding="utf-8") as writer:
321	            writer.write("#version: 0.2\n")
322	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blenderbot/tokenization_blenderbot.py:329
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
328	                    index = token_index
329	                writer.write(" ".join(bpe_tokens) + "\n")
330	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blenderbot_small/modeling_blenderbot_small.py:472
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
471	
472	BLENDERBOT_SMALL_START_DOCSTRING = r"""
473	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
474	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
475	    etc.)
476	
477	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
478	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
479	    and behavior.
480	
481	    Parameters:
482	        config ([`BlenderbotSmallConfig`]):
483	            Model configuration class with all the parameters of the model. Initializing with a config file does not
484	            load the weights associated with the model, only the configuration. Check out the
485	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
486	"""
487	
488	BLENDERBOT_SMALL_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blenderbot_small/modeling_flax_blenderbot_small.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	
55	BLENDERBOT_SMALL_START_DOCSTRING = r"""
56	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
57	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
58	    etc.)
59	
60	    This model is also a Flax Linen
61	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
62	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
63	
64	    Finally, this model supports inherent JAX features such as:
65	
66	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
67	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
68	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
69	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
70	
71	    Parameters:
72	        config ([`BlenderbotSmallConfig`]): Model configuration class with all the parameters of the model.
73	            Initializing with a config file does not load the weights associated with the model, only the
74	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
75	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
76	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
77	            `jax.numpy.bfloat16` (on TPUs).
78	
79	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
80	            specified all the computation will be performed with the given `dtype`.
81	
82	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
83	            parameters.**
84	
85	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
86	            [`~FlaxPreTrainedModel.to_bf16`].
87	"""
88	
89	BLENDERBOT_SMALL_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blenderbot_small/modeling_flax_blenderbot_small.py:89
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
88	
89	BLENDERBOT_SMALL_INPUTS_DOCSTRING = r"""
90	    Args:
91	        input_ids (`jnp.ndarray` of shape `(batch_size, sequence_length)`):
92	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
93	            it.
94	
95	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
96	            [`PreTrainedTokenizer.__call__`] for details.
97	
98	            [What are input IDs?](../glossary#input-ids)
99	        attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
100	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
101	
102	            - 1 for tokens that are **not masked**,
103	            - 0 for tokens that are **masked**.
104	
105	            [What are attention masks?](../glossary#attention-mask)
106	        decoder_input_ids (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
107	            Indices of decoder input sequence tokens in the vocabulary.
108	
109	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
110	            [`PreTrainedTokenizer.__call__`] for details.
111	
112	            [What are decoder input IDs?](../glossary#decoder-input-ids)
113	
114	            For translation and summarization training, `decoder_input_ids` should be provided. If no
115	            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
116	            for denoising pre-training following the paper.
117	        decoder_attention_mask (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
118	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
119	            be used by default.
120	
121	            If you want to change padding behavior, you should modify to your needs. See diagram 1 in [the
122	            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
123	        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
124	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
125	            config.max_position_embeddings - 1]`.
126	        decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
127	            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
128	            range `[0, config.max_position_embeddings - 1]`.
129	        output_attentions (`bool`, *optional*):
130	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
131	            tensors for more detail.
132	        output_hidden_states (`bool`, *optional*):
133	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
134	            more detail.
135	        return_dict (`bool`, *optional*):
136	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
137	"""
138	
139	
140	BLENDERBOT_SMALL_ENCODE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blenderbot_small/modeling_flax_blenderbot_small.py:170
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
169	
170	BLENDERBOT_SMALL_DECODE_INPUTS_DOCSTRING = r"""
171	    Args:
172	        decoder_input_ids (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`):
173	            Indices of decoder input sequence tokens in the vocabulary.
174	
175	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
176	            [`PreTrainedTokenizer.__call__`] for details.
177	
178	            [What are decoder input IDs?](../glossary#decoder-input-ids)
179	
180	            For translation and summarization training, `decoder_input_ids` should be provided. If no
181	            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
182	            for denoising pre-training following the paper.
183	        encoder_outputs (`tuple(tuple(jnp.ndarray)`):
184	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
185	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
186	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
187	        encoder_attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
188	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
189	
190	            - 1 for tokens that are **not masked**,
191	            - 0 for tokens that are **masked**.
192	
193	            [What are attention masks?](../glossary#attention-mask)
194	        decoder_attention_mask (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
195	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
196	            be used by default.
197	
198	            If you want to change padding behavior, you should modify to your needs. See diagram 1 in [the
199	            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
200	        decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
201	            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
202	            range `[0, config.max_position_embeddings - 1]`.
203	        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):
204	            Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
205	            auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.
206	        output_attentions (`bool`, *optional*):
207	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
208	            tensors for more detail.
209	        output_hidden_states (`bool`, *optional*):
210	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
211	            more detail.
212	        return_dict (`bool`, *optional*):
213	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
214	"""
215	
216	
217	# Copied from transformers.models.bart.modeling_flax_bart.shift_tokens_right
218	def shift_tokens_right(input_ids: jnp.ndarray, pad_token_id: int, decoder_start_token_id: int) -> jnp.ndarray:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py:531
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
530	
531	BLENDERBOT_SMALL_START_DOCSTRING = r"""
532	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
533	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
534	    etc.)
535	
536	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
537	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
538	    behavior.
539	
540	    <Tip>
541	
542	    TensorFlow models and layers in `transformers` accept two formats as input:
543	
544	    - having all inputs as keyword arguments (like PyTorch models), or
545	    - having all inputs as a list, tuple or dict in the first positional argument.
546	
547	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
548	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
549	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
550	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
551	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
552	    positional argument:
553	
554	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
555	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
556	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
557	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
558	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
559	
560	    Note that when creating models and layers with
561	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
562	    about any of this, as you can just pass inputs like you would to any other Python function!
563	
564	    </Tip>
565	
566	    Args:
567	        config ([`BlenderbotSmallConfig`]): Model configuration class with all the parameters of the model.
568	            Initializing with a config file does not load the weights associated with the model, only the
569	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
570	"""
571	
572	BLENDERBOT_SMALL_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blenderbot_small/tokenization_blenderbot_small.py:95
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
94	        with open(merges_file, encoding="utf-8") as merges_handle:
95	            merges = merges_handle.read().split("\n")[1:-1]
96	        merges = [tuple(merge.split()) for merge in merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blenderbot_small/tokenization_blenderbot_small.py:204
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
203	        with open(vocab_file, "w", encoding="utf-8") as f:
204	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
205	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blenderbot_small/tokenization_blenderbot_small.py:208
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
207	        with open(merge_file, "w", encoding="utf-8") as writer:
208	            writer.write("#version: 0.2\n")
209	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blenderbot_small/tokenization_blenderbot_small.py:216
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
215	                    index = token_index
216	                writer.write(" ".join(bpe_tokens) + "\n")
217	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blip/modeling_blip.py:507
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
506	
507	BLIP_START_DOCSTRING = r"""
508	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
509	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
510	    etc.)
511	
512	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
513	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
514	    and behavior.
515	
516	    Parameters:
517	        config ([`BlipConfig`]): Model configuration class with all the parameters of the model.
518	            Initializing with a config file does not load the weights associated with the model, only the
519	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
520	"""
521	
522	BLIP_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blip/modeling_tf_blip.py:538
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
537	
538	BLIP_START_DOCSTRING = r"""
539	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
540	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
541	    etc.)
542	
543	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
544	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
545	    behavior.
546	
547	    Parameters:
548	        config ([`BlipConfig`]): Model configuration class with all the parameters of the model.
549	            Initializing with a config file does not load the weights associated with the model, only the
550	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
551	"""
552	
553	BLIP_VISION_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blip_2/modeling_blip_2.py:444
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
443	
444	BLIP_2_START_DOCSTRING = r"""
445	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
446	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
447	    etc.)
448	
449	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
450	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
451	    and behavior.
452	
453	    Parameters:
454	        config ([`Blip2Config`]): Model configuration class with all the parameters of the model.
455	            Initializing with a config file does not load the weights associated with the model, only the
456	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
457	"""
458	
459	BLIP_2_VISION_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blip_2/modeling_blip_2.py:2091
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2090	            logger.warning(
2091	                "The `language_model` is not in the `hf_device_map` dictionary and you are running your script"
2092	                " in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`."
2093	                " Please pass a `device_map` that contains `language_model` to remove this warning."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blip_2/modeling_blip_2.py:2227
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2226	            logger.warning_once(
2227	                "Expanding inputs for image tokens in BLIP-2 should be done in processing. "
2228	                "Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. "
2229	                "Using processors without these attributes in the config is deprecated and will throw an error in v4.50."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blip_2/modeling_blip_2.py:2358
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2357	            logger.warning_once(
2358	                "Expanding inputs for image tokens in BLIP-2 should be done in processing. "
2359	                "Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. "
2360	                "Using processors without these attributes in the config is deprecated and will throw an error in v4.50."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/blip_2/processing_blip_2.py:154
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
153	                logger.warning_once(
154	                    "Expanding inputs for image tokens in BLIP-2 should be done in processing. "
155	                    "Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. "
156	                    "Using processors without these attributes in the config is deprecated and will throw an error in v4.50."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bloom/modeling_bloom.py:475
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
474	
475	BLOOM_START_DOCSTRING = r"""
476	
477	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
478	    library implements for all its model (such as downloading or saving, resizing the input embeddings etc.)
479	
480	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
481	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
482	    and behavior.
483	
484	    Parameters:
485	        config ([`BloomConfig`]): Model configuration class with all the parameters of the model.
486	            Initializing with a config file does not load the weights associated with the model, only the
487	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
488	"""
489	
490	BLOOM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bloom/modeling_bloom.py:490
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
489	
490	BLOOM_INPUTS_DOCSTRING = r"""
491	    Args:
492	        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):
493	            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`
494	            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.
495	
496	            If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as
497	            `input_ids`.
498	
499	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
500	            [`PreTrainedTokenizer.__call__`] for details.
501	
502	            [What are input IDs?](../glossary#input-ids)
503	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
504	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
505	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
506	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
507	
508	            Two formats are allowed:
509	            - a [`~cache_utils.Cache`] instance, see our
510	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
511	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
512	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
513	            cache format.
514	
515	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
516	            legacy cache format will be returned.
517	
518	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
519	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
520	            of shape `(batch_size, sequence_length)`.
521	        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):
522	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
523	
524	            - 1 for tokens that are **not masked**,
525	            - 0 for tokens that are **masked**.
526	
527	            [What are attention masks?](../glossary#attention-mask)
528	        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):
529	            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:
530	
531	            - 1 indicates the head is **not masked**,
532	            - 0 indicates the head is **masked**.
533	
534	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
535	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
536	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
537	            model's internal embedding lookup matrix.
538	
539	            If `past_key_values` is used, optionally only the last `inputs_embeds` have to be input (see
540	            `past_key_values`).
541	        use_cache (`bool`, *optional*):
542	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
543	            `past_key_values`).
544	        output_attentions (`bool`, *optional*):
545	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
546	            tensors for more detail.
547	        output_hidden_states (`bool`, *optional*):
548	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
549	            more detail.
550	        return_dict (`bool`, *optional*):
551	            Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.
552	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
553	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
554	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
555	            the complete sequence length.
556	"""
557	
558	
559	@add_start_docstrings(
560	    "The bare Bloom Model transformer outputting raw hidden-states without any specific head on top.",
561	    BLOOM_START_DOCSTRING,
562	)
563	class BloomModel(BloomPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bloom/modeling_bloom.py:652
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
651	                logger.warning_once(
652	                    "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and "
653	                    "will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class "
654	                    "(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bloom/modeling_flax_bloom.py:46
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
45	
46	BLOOM_START_DOCSTRING = r"""
47	
48	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
49	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
50	    etc.)
51	
52	    This model is also a Flax Linen
53	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
54	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
55	
56	    Finally, this model supports inherent JAX features such as:
57	
58	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
59	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
60	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
61	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
62	
63	    Parameters:
64	        config ([`BloomConfig`]): Model configuration class with all the parameters of the model.
65	            Initializing with a config file does not load the weights associated with the model, only the
66	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
67	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
68	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
69	            `jax.numpy.bfloat16` (on TPUs).
70	
71	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
72	            specified all the computation will be performed with the given `dtype`.
73	
74	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
75	            parameters.**
76	
77	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
78	            [`~FlaxPreTrainedModel.to_bf16`].
79	"""
80	
81	BLOOM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bridgetower/modeling_bridgetower.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	
54	BRIDGETOWER_START_DOCSTRING = r"""
55	    This model is a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ subclass. Use
56	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
57	    behavior.
58	
59	    Parameters:
60	        config ([`BridgeTowerConfig`]): Model configuration class with all the parameters of the model.
61	            Initializing with a config file does not load the weights associated with the model, only the
62	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
63	"""
64	
65	BRIDGETOWER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/bros/modeling_bros.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	
50	BROS_START_DOCSTRING = r"""
51	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
52	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
53	    and behavior.
54	
55	    Parameters:
56	        config ([`BrosConfig`]): Model configuration class with all the parameters of the model.
57	            Initializing with a config file does not load the weights associated with the model, only the
58	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
59	"""
60	
61	BROS_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/camembert/modeling_camembert.py:62
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
61	
62	CAMEMBERT_START_DOCSTRING = r"""
63	
64	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
65	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
66	    etc.)
67	
68	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
69	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
70	    and behavior.
71	
72	    Parameters:
73	        config ([`CamembertConfig`]): Model configuration class with all the parameters of the
74	            model. Initializing with a config file does not load the weights associated with the model, only the
75	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
76	"""
77	
78	
79	# Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings with Roberta->Camembert
80	class CamembertEmbeddings(nn.Module):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/camembert/modeling_tf_camembert.py:68
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
67	
68	CAMEMBERT_START_DOCSTRING = r"""
69	
70	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
71	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
72	    etc.)
73	
74	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
75	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
76	    behavior.
77	
78	    <Tip>
79	
80	    TensorFlow models and layers in `transformers` accept two formats as input:
81	
82	    - having all inputs as keyword arguments (like PyTorch models), or
83	    - having all inputs as a list, tuple or dict in the first positional argument.
84	
85	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
86	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
87	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
88	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
89	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
90	    positional argument:
91	
92	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
93	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
94	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
95	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
96	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
97	
98	    Note that when creating models and layers with
99	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
100	    about any of this, as you can just pass inputs like you would to any other Python function!
101	
102	    </Tip>
103	
104	    Parameters:
105	        config ([`CamembertConfig`]): Model configuration class with all the parameters of the
106	            model. Initializing with a config file does not load the weights associated with the model, only the
107	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
108	"""
109	
110	CAMEMBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/camembert/tokenization_camembert.py:240
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
239	                content_spiece_model = self.sp_model.serialized_model_proto()
240	                fi.write(content_spiece_model)
241	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/canine/modeling_canine.py:103
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
102	        logger.error(
103	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
104	            "https://www.tensorflow.org/install/ for installation instructions."
105	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/canine/modeling_canine.py:911
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
910	
911	CANINE_START_DOCSTRING = r"""
912	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
913	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
914	    behavior.
915	
916	    Parameters:
917	        config ([`CanineConfig`]): Model configuration class with all the parameters of the model.
918	            Initializing with a config file does not load the weights associated with the model, only the
919	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
920	"""
921	
922	CANINE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/chameleon/modeling_chameleon.py:402
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
401	            raise ValueError(
402	                "`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` "
403	                "make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers"
404	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/chameleon/modeling_chameleon.py:1023
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1022	
1023	CHAMELEON_START_DOCSTRING = r"""
1024	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1025	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1026	    etc.)
1027	
1028	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1029	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1030	    and behavior.
1031	
1032	    Parameters:
1033	        config ([`ChameleonConfig`]):
1034	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1035	            load the weights associated with the model, only the configuration. Check out the
1036	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1037	"""
1038	
1039	
1040	@add_start_docstrings(
1041	    "The bare chameleon Model outputting raw hidden-states without any specific head on top.",
1042	    CHAMELEON_START_DOCSTRING,
1043	)
1044	class ChameleonPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/chameleon/modeling_chameleon.py:1071
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1070	
1071	CHAMELEON_VQ_START_DOCSTRING = r"""
1072	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1073	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1074	    etc.)
1075	
1076	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1077	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1078	    and behavior.
1079	
1080	    Parameters:
1081	        config ([`ChameleonVQVAEConfig`]):
1082	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1083	            load the weights associated with the model, only the configuration. Check out the
1084	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1085	"""
1086	
1087	
1088	@add_start_docstrings(
1089	    """The VQ-VAE model used in Chameleon for encoding/decoding images into discrete tokens.
1090	    This model follows the "Make-a-scene: Scene-based text-to-image generation with human priors" paper from
1091	    [ Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman](https://arxiv.org/abs/2203.13131).
1092	    """,
1093	    CHAMELEON_VQ_START_DOCSTRING,
1094	)
1095	class ChameleonVQVAE(ChameleonPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/chameleon/modeling_chameleon.py:1089
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1088	@add_start_docstrings(
1089	    """The VQ-VAE model used in Chameleon for encoding/decoding images into discrete tokens.
1090	    This model follows the "Make-a-scene: Scene-based text-to-image generation with human priors" paper from
1091	    [ Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman](https://arxiv.org/abs/2203.13131).
1092	    """,
1093	    CHAMELEON_VQ_START_DOCSTRING,
1094	)
1095	class ChameleonVQVAE(ChameleonPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/chameleon/modeling_chameleon.py:1127
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1126	
1127	CHAMELEON_INPUTS_DOCSTRING = r"""
1128	    Args:
1129	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
1130	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
1131	            it.
1132	
1133	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1134	            [`PreTrainedTokenizer.__call__`] for details.
1135	
1136	            [What are input IDs?](../glossary#input-ids)
1137	        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):
1138	            The tensors corresponding to the input images. Pixel values can be obtained using
1139	            [`AutoImageProcessor`]. See [`ChameleonImageProcessor.__call__`] for details.
1140	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
1141	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
1142	
1143	            - 1 for tokens that are **not masked**,
1144	            - 0 for tokens that are **masked**.
1145	
1146	            [What are attention masks?](../glossary#attention-mask)
1147	
1148	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1149	            [`PreTrainedTokenizer.__call__`] for details.
1150	
1151	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
1152	            `past_key_values`).
1153	
1154	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
1155	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
1156	            information on the default strategy.
1157	
1158	            - 1 indicates the head is **not masked**,
1159	            - 0 indicates the head is **masked**.
1160	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
1161	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
1162	            config.n_positions - 1]`.
1163	
1164	            [What are position IDs?](../glossary#position-ids)
1165	        past_key_values (`Cache`, *optional*):
1166	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
1167	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
1168	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
1169	
1170	            Should always be a [`~cache_utils.Cache`] instance and the model will output the same cache instance.
1171	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
1172	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
1173	            of shape `(batch_size, sequence_length)`.
1174	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
1175	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
1176	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
1177	            model's internal embedding lookup matrix.
1178	        use_cache (`bool`, *optional*):
1179	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
1180	            `past_key_values`).
1181	        output_attentions (`bool`, *optional*):
1182	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1183	            tensors for more detail.
1184	        output_hidden_states (`bool`, *optional*):
1185	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1186	            more detail.
1187	        return_dict (`bool`, *optional*):
1188	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1189	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
1190	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
1191	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
1192	            the complete sequence length.
1193	"""
1194	
1195	
1196	@add_start_docstrings(
1197	    "The bare chameleon Model outputting raw hidden-states without any specific head on top.",
1198	    CHAMELEON_START_DOCSTRING,
1199	)
1200	class ChameleonModel(ChameleonPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/chinese_clip/modeling_chinese_clip.py:794
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
793	
794	CHINESE_CLIP_START_DOCSTRING = r"""
795	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
796	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
797	    behavior.
798	
799	    Parameters:
800	        config ([`ChineseCLIPConfig`]): Model configuration class with all the parameters of the model.
801	            Initializing with a config file does not load the weights associated with the model, only the
802	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
803	"""
804	
805	CHINESE_CLIP_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/clap/modeling_clap.py:1020
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1019	
1020	CLAP_START_DOCSTRING = r"""
1021	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1022	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1023	    etc.)
1024	
1025	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1026	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1027	    and behavior.
1028	
1029	    Parameters:
1030	        config ([`ClapConfig`]): Model configuration class with all the parameters of the model.
1031	            Initializing with a config file does not load the weights associated with the model, only the
1032	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1033	"""
1034	
1035	CLAP_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/clip/modeling_clip.py:704
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
703	
704	CLIP_START_DOCSTRING = r"""
705	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
706	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
707	    etc.)
708	
709	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
710	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
711	    and behavior.
712	
713	    Parameters:
714	        config ([`CLIPConfig`]): Model configuration class with all the parameters of the model.
715	            Initializing with a config file does not load the weights associated with the model, only the
716	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
717	"""
718	
719	CLIP_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/clip/modeling_flax_clip.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	
41	CLIP_START_DOCSTRING = r"""
42	
43	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
44	    library implements for all its model (such as downloading, saving and converting weights from PyTorch models)
45	
46	    This model is also a
47	    [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. Use it as
48	    a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
49	    behavior.
50	
51	    Finally, this model supports inherent JAX features such as:
52	
53	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
54	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
55	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
56	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
57	
58	    Parameters:
59	        config ([`CLIPConfig`]): Model configuration class with all the parameters of the model.
60	            Initializing with a config file does not load the weights associated with the model, only the
61	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
62	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
63	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
64	            `jax.numpy.bfloat16` (on TPUs).
65	
66	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
67	            specified all the computation will be performed with the given `dtype`.
68	
69	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
70	            parameters.**
71	
72	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
73	            [`~FlaxPreTrainedModel.to_bf16`].
74	"""
75	
76	CLIP_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/clip/modeling_flax_clip.py:1141
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1140	
1141	FLAX_CLIP_VISION_MODEL_DOCSTRING = """
1142	    Returns:
1143	
1144	    Example:
1145	
1146	    ```python
1147	    >>> from PIL import Image
1148	    >>> import requests
1149	    >>> from transformers import AutoProcessor, FlaxCLIPVisionModel
1150	
1151	    >>> model = FlaxCLIPVisionModel.from_pretrained("openai/clip-vit-base-patch32")
1152	    >>> processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")
1153	
1154	    >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
1155	    >>> image = Image.open(requests.get(url, stream=True).raw)
1156	
1157	    >>> inputs = processor(images=image, return_tensors="np")
1158	
1159	    >>> outputs = model(**inputs)
1160	    >>> last_hidden_state = outputs.last_hidden_state
1161	    >>> pooler_output = outputs.pooler_output  # pooled CLS states
1162	    ```
1163	"""
1164	
1165	overwrite_call_docstring(FlaxCLIPVisionModel, CLIP_VISION_INPUTS_DOCSTRING + FLAX_CLIP_VISION_MODEL_DOCSTRING)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/clip/modeling_flax_clip.py:1267
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1266	
1267	FLAX_CLIP_MODEL_DOCSTRING = """
1268	    Returns:
1269	
1270	    Example:
1271	
1272	    ```python
1273	    >>> import jax
1274	    >>> from PIL import Image
1275	    >>> import requests
1276	    >>> from transformers import AutoProcessor, FlaxCLIPModel
1277	
1278	    >>> model = FlaxCLIPModel.from_pretrained("openai/clip-vit-base-patch32")
1279	    >>> processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")
1280	
1281	    >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
1282	    >>> image = Image.open(requests.get(url, stream=True).raw)
1283	
1284	    >>> inputs = processor(
1285	    ...     text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="np", padding=True
1286	    ... )
1287	
1288	    >>> outputs = model(**inputs)
1289	    >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
1290	    >>> probs = jax.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities
1291	    ```
1292	"""
1293	
1294	overwrite_call_docstring(FlaxCLIPModel, CLIP_INPUTS_DOCSTRING + FLAX_CLIP_MODEL_DOCSTRING)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/clip/modeling_tf_clip.py:1036
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1035	
1036	CLIP_START_DOCSTRING = r"""
1037	
1038	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
1039	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1040	    etc.)
1041	
1042	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
1043	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
1044	    behavior.
1045	
1046	    <Tip>
1047	
1048	    TensorFlow models and layers in `transformers` accept two formats as input:
1049	
1050	    - having all inputs as keyword arguments (like PyTorch models), or
1051	    - having all inputs as a list, tuple or dict in the first positional argument.
1052	
1053	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
1054	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
1055	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
1056	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
1057	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
1058	    positional argument:
1059	
1060	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
1061	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
1062	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
1063	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
1064	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
1065	
1066	    Note that when creating models and layers with
1067	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
1068	    about any of this, as you can just pass inputs like you would to any other Python function!
1069	
1070	    </Tip>
1071	
1072	    Args:
1073	        config ([`CLIPConfig`]): Model configuration class with all the parameters of the model.
1074	            Initializing with a config file does not load the weights associated with the model, only the
1075	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
1076	"""
1077	
1078	CLIP_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/clip/tokenization_clip.py:313
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
312	        with open(merges_file, encoding="utf-8") as merges_handle:
313	            bpe_merges = merges_handle.read().strip().split("\n")[1 : 49152 - 256 - 2 + 1]
314	        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/clip/tokenization_clip.py:501
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
500	        with open(vocab_file, "w", encoding="utf-8") as f:
501	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
502	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/clip/tokenization_clip.py:505
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
504	        with open(merge_file, "w", encoding="utf-8") as writer:
505	            writer.write("#version: 0.2\n")
506	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/clip/tokenization_clip.py:513
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
512	                    index = token_index
513	                writer.write(" ".join(bpe_tokens) + "\n")
514	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/clipseg/modeling_clipseg.py:491
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
490	
491	CLIPSEG_START_DOCSTRING = r"""
492	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
493	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
494	    behavior.
495	
496	    Parameters:
497	        config ([`CLIPSegConfig`]): Model configuration class with all the parameters of the model.
498	            Initializing with a config file does not load the weights associated with the model, only the
499	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
500	"""
501	
502	CLIPSEG_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/clvp/modeling_clvp.py:755
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
754	
755	CLVP_START_DOCSTRING = r"""
756	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
757	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
758	    etc.)
759	
760	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
761	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
762	    and behavior.
763	
764	    Parameters:
765	        config ([`ClvpConfig`]): Model configuration class with all the parameters of the model.
766	            Initializing with a config file does not load the weights associated with the model, only the
767	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
768	"""
769	
770	
771	CLVP_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/clvp/tokenization_clvp.py:170
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
169	        with open(merges_file, encoding="utf-8") as merges_handle:
170	            bpe_merges = merges_handle.read().split("\n")[1:-1]
171	        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/clvp/tokenization_clvp.py:349
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
348	        with open(vocab_file, "w", encoding="utf-8") as f:
349	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
350	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/clvp/tokenization_clvp.py:353
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
352	        with open(merge_file, "w", encoding="utf-8") as writer:
353	            writer.write("#version: 0.2\n")
354	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/clvp/tokenization_clvp.py:361
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
360	                    index = token_index
361	                writer.write(" ".join(bpe_tokens) + "\n")
362	                index += 1

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/code_llama/tokenization_code_llama.py:187
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
186	        with open(self.vocab_file, "rb") as f:
187	            sp_model = f.read()
188	            model_pb2 = import_protobuf()

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/code_llama/tokenization_code_llama.py:352
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
351	                content_spiece_model = self.sp_model.serialized_model_proto()
352	                fi.write(content_spiece_model)
353	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/codegen/modeling_codegen.py:334
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
333	
334	CODEGEN_START_DOCSTRING = r"""
335	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
336	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
337	    behavior.
338	
339	    Parameters:
340	        config ([`CodeGenConfig`]): Model configuration class with all the parameters of the model.
341	            Initializing with a config file does not load the weights associated with the model, only the
342	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
343	"""
344	
345	CODEGEN_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/codegen/modeling_codegen.py:345
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
344	
345	CODEGEN_INPUTS_DOCSTRING = r"""
346	    Args:
347	        input_ids (`torch.LongTensor` of shape `({0})`):
348	            Indices of input sequence tokens in the vocabulary.
349	
350	            Indices can be obtained using [`AutoProcenizer`]. See [`PreTrainedTokenizer.encode`] and
351	            [`PreTrainedTokenizer.__call__`] for details.
352	
353	            [What are input IDs?](../glossary#input-ids)
354	        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):
355	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
356	
357	            - 1 for tokens that are **not masked**,
358	            - 0 for tokens that are **masked**.
359	
360	            [What are attention masks?](../glossary#attention-mask)
361	        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):
362	            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,
363	            1]`:
364	
365	            - 0 corresponds to a *sentence A* token,
366	            - 1 corresponds to a *sentence B* token.
367	
368	            [What are token type IDs?](../glossary#token-type-ids)
369	        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):
370	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
371	            config.n_positions - 1]`.
372	
373	            [What are position IDs?](../glossary#position-ids)
374	        head_mask (`torch.FloatTensor` of shape `(num_attention_heads,)` or `(n_layer, num_attention_heads)`, *optional*):
375	            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:
376	
377	            - 1 indicates the head is **not masked**,
378	            - 0 indicates the head is **masked**.
379	
380	        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_dim)`, *optional*):
381	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
382	            is useful if you want more control over how to convert *input_ids* indices into associated vectors than the
383	            model's internal embedding lookup matrix.
384	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
385	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
386	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
387	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
388	
389	            Two formats are allowed:
390	            - a [`~cache_utils.Cache`] instance, see our
391	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
392	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
393	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
394	            cache format.
395	
396	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
397	            legacy cache format will be returned.
398	
399	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
400	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
401	            of shape `(batch_size, sequence_length)`.
402	        output_attentions (`bool`, *optional*):
403	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
404	            tensors for more detail.
405	        output_hidden_states (`bool`, *optional*):
406	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
407	            more detail.
408	        return_dict (`bool`, *optional*):
409	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
410	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
411	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
412	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
413	            the complete sequence length.
414	"""
415	
416	
417	@add_start_docstrings(
418	    "The bare CodeGen Model transformer outputting raw hidden-states without any specific head on top.",
419	    CODEGEN_START_DOCSTRING,
420	)
421	class CodeGenModel(CodeGenPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/codegen/modeling_codegen.py:495
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
494	                logger.warning_once(
495	                    "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and "
496	                    "will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class "
497	                    "(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)"

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/codegen/tokenization_codegen.py:173
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
172	        with open(merges_file, encoding="utf-8") as merges_handle:
173	            bpe_merges = merges_handle.read().split("\n")[1:-1]
174	        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/codegen/tokenization_codegen.py:320
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
319	        with open(vocab_file, "w", encoding="utf-8") as f:
320	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
321	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/codegen/tokenization_codegen.py:324
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
323	        with open(merge_file, "w", encoding="utf-8") as writer:
324	            writer.write("#version: 0.2\n")
325	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/codegen/tokenization_codegen.py:332
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
331	                    index = token_index
332	                writer.write(" ".join(bpe_tokens) + "\n")
333	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/codegen/tokenization_codegen_fast.py:130
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
129	            raise ValueError(
130	                "Currenty GPT2's fast tokenizer does NOT support adding a BOS token. "
131	                "Instead you should use GPT2's slow tokenizer class `CodeGenTokenizer` as follows: \n"
132	                f"`CodeGenTokenizer.from_pretrained('{model_id}')`\nor\n"
133	                f"`AutoTokenizer.from_pretrained('{model_id}', use_fast=False)`\n"
134	                "This issue will be fixed soon, see: https://github.com/huggingface/tokenizers/pull/1005."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/cohere/modeling_cohere.py:374
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
373	
374	COHERE_START_DOCSTRING = r"""
375	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
376	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
377	    etc.)
378	
379	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
380	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
381	    and behavior.
382	
383	    Parameters:
384	        config ([`CohereConfig`]):
385	            Model configuration class with all the parameters of the model. Initializing with a config file does not
386	            load the weights associated with the model, only the configuration. Check out the
387	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
388	"""
389	
390	
391	@add_start_docstrings(
392	    "The bare Cohere Model outputting raw hidden-states without any specific head on top.",
393	    COHERE_START_DOCSTRING,
394	)
395	class CoherePreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/cohere/modeling_cohere.py:421
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
420	
421	COHERE_INPUTS_DOCSTRING = r"""
422	    Args:
423	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
424	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
425	            it.
426	
427	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
428	            [`PreTrainedTokenizer.__call__`] for details.
429	
430	            [What are input IDs?](../glossary#input-ids)
431	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
432	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
433	
434	            - 1 for tokens that are **not masked**,
435	            - 0 for tokens that are **masked**.
436	
437	            [What are attention masks?](../glossary#attention-mask)
438	
439	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
440	            [`PreTrainedTokenizer.__call__`] for details.
441	
442	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
443	            `past_key_values`).
444	
445	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
446	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
447	            information on the default strategy.
448	
449	            - 1 indicates the head is **not masked**,
450	            - 0 indicates the head is **masked**.
451	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
452	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
453	            config.n_positions - 1]`.
454	
455	            [What are position IDs?](../glossary#position-ids)
456	        past_key_values (`Cache`, *optional*):
457	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
458	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
459	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
460	
461	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
462	
463	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
464	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
465	            of shape `(batch_size, sequence_length)`.
466	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
467	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
468	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
469	            model's internal embedding lookup matrix.
470	        use_cache (`bool`, *optional*):
471	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
472	            `past_key_values`).
473	        output_attentions (`bool`, *optional*):
474	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
475	            tensors for more detail.
476	        output_hidden_states (`bool`, *optional*):
477	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
478	            more detail.
479	        return_dict (`bool`, *optional*):
480	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
481	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
482	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
483	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
484	            the complete sequence length.
485	"""
486	
487	
488	@add_start_docstrings(
489	    "The bare Cohere Model outputting raw hidden-states without any specific head on top.",
490	    COHERE_START_DOCSTRING,
491	)
492	class CohereModel(CoherePreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/cohere/tokenization_cohere_fast.py:36
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
35	    "tokenizer_file": {
36	        "Cohere/Command-nightly": "https://huggingface.co/Cohere/Command-nightly/blob/main/tokenizer.json",
37	    },
38	}

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/cohere2/modeling_cohere2.py:382
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
381	
382	COHERE2_START_DOCSTRING = r"""
383	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
384	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
385	    etc.)
386	
387	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
388	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
389	    and behavior.
390	
391	    Parameters:
392	        config ([`Cohere2Config`]):
393	            Model configuration class with all the parameters of the model. Initializing with a config file does not
394	            load the weights associated with the model, only the configuration. Check out the
395	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
396	"""
397	
398	
399	@add_start_docstrings(
400	    "The bare Cohere2 Model outputting raw hidden-states without any specific head on top.",
401	    COHERE2_START_DOCSTRING,
402	)
403	class Cohere2PreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/cohere2/modeling_cohere2.py:429
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
428	
429	COHERE2_INPUTS_DOCSTRING = r"""
430	    Args:
431	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
432	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
433	            it.
434	
435	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
436	            [`PreTrainedTokenizer.__call__`] for details.
437	
438	            [What are input IDs?](../glossary#input-ids)
439	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
440	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
441	
442	            - 1 for tokens that are **not masked**,
443	            - 0 for tokens that are **masked**.
444	
445	            [What are attention masks?](../glossary#attention-mask)
446	
447	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
448	            [`PreTrainedTokenizer.__call__`] for details.
449	
450	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
451	            `past_key_values`).
452	
453	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
454	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
455	            information on the default strategy.
456	
457	            - 1 indicates the head is **not masked**,
458	            - 0 indicates the head is **masked**.
459	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
460	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
461	            config.n_positions - 1]`.
462	
463	            [What are position IDs?](../glossary#position-ids)
464	        past_key_values (`Cache`, *optional*):
465	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
466	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
467	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
468	
469	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
470	
471	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
472	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
473	            of shape `(batch_size, sequence_length)`.
474	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
475	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
476	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
477	            model's internal embedding lookup matrix.
478	        use_cache (`bool`, *optional*):
479	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
480	            `past_key_values`).
481	        output_attentions (`bool`, *optional*):
482	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
483	            tensors for more detail.
484	        output_hidden_states (`bool`, *optional*):
485	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
486	            more detail.
487	        return_dict (`bool`, *optional*):
488	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
489	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
490	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
491	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
492	            the complete sequence length.
493	"""
494	
495	
496	@add_start_docstrings(
497	    "The bare Cohere2 Model outputting raw hidden-states without any specific head on top.",
498	    COHERE2_START_DOCSTRING,
499	)
500	class Cohere2Model(Cohere2PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/colpali/modeling_colpali.py:38
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
37	
38	COLPALI_START_DOCSTRING = r"""
39	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
40	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
41	    etc.)
42	
43	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
44	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
45	    and behavior.
46	
47	    Parameters:
48	        config ([`ColPaliConfig`]):
49	            Model configuration class with all the parameters of the model. Initializing with a config file does not
50	            load the weights associated with the model, only the configuration. Check out the
51	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
52	"""
53	
54	
55	@add_start_docstrings(
56	    "The bare ColPali model outputting raw hidden-states without any specific head on top.",
57	    COLPALI_START_DOCSTRING,
58	)
59	class ColPaliPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/colpali/modeling_colpali.py:121
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
120	
121	COLPALI_FOR_RETRIEVAL_INPUT_DOCSTRING = r"""
122	    Args:
123	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
124	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
125	            it.
126	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
127	            [`PreTrainedTokenizer.__call__`] for details.
128	            [What are input IDs?](../glossary#input-ids)
129	        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):
130	            The tensors corresponding to the input images. Pixel values can be obtained using
131	            [`AutoImageProcessor`]. See [`SiglipImageProcessor.__call__`] for details ([]`PaliGemmaProcessor`] uses
132	            [`SiglipImageProcessor`] for processing images). If none, ColPali will only process text (query embeddings).
133	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
134	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
135	            - 1 for tokens that are **not masked**,
136	            - 0 for tokens that are **masked**.
137	            [What are attention masks?](../glossary#attention-mask)
138	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
139	            [`PreTrainedTokenizer.__call__`] for details.
140	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
141	            `past_key_values`).
142	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
143	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
144	            information on the default strategy.
145	            - 1 indicates the head is **not masked**,
146	            - 0 indicates the head is **masked**.
147	        output_attentions (`bool`, *optional*):
148	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
149	            tensors for more detail.
150	        output_hidden_states (`bool`, *optional*):
151	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
152	            more detail.
153	        return_dict (`bool`, *optional*):
154	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
155	        kwargs (`Dict[str, Any]`, *optional*):
156	            Additional key word arguments passed along to the vlm backbone model.
157	"""
158	
159	
160	@add_start_docstrings(
161	    """
162	    In our proposed ColPali approach, we leverage VLMs to construct efficient multi-vector embeddings directly
163	    from document images (screenshots) for document retrieval. We train the model to maximize the similarity
164	    between these document embeddings and the corresponding query embeddings, using the late interaction method
165	    introduced in ColBERT.
166	
167	    Using ColPali removes the need for potentially complex and brittle layout recognition and OCR pipelines with a
168	    single model that can take into account both the textual and visual content (layout, charts, etc.) of a document.
169	    """
170	)
171	class ColPaliForRetrieval(ColPaliPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/conditional_detr/modeling_conditional_detr.py:1065
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1064	
1065	CONDITIONAL_DETR_START_DOCSTRING = r"""
1066	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1067	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1068	    etc.)
1069	
1070	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1071	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1072	    and behavior.
1073	
1074	    Parameters:
1075	        config ([`ConditionalDetrConfig`]):
1076	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1077	            load the weights associated with the model, only the configuration. Check out the
1078	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1079	"""
1080	
1081	CONDITIONAL_DETR_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/convbert/modeling_convbert.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	        logger.error(
54	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
55	            "https://www.tensorflow.org/install/ for installation instructions."
56	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/convbert/modeling_convbert.py:686
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
685	
686	CONVBERT_START_DOCSTRING = r"""
687	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
688	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
689	    behavior.
690	
691	    Parameters:
692	        config ([`ConvBertConfig`]): Model configuration class with all the parameters of the model.
693	            Initializing with a config file does not load the weights associated with the model, only the
694	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
695	"""
696	
697	CONVBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/convbert/modeling_tf_convbert.py:745
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
744	
745	CONVBERT_START_DOCSTRING = r"""
746	
747	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
748	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
749	    etc.)
750	
751	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
752	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
753	    behavior.
754	
755	    <Tip>
756	
757	    TensorFlow models and layers in `transformers` accept two formats as input:
758	
759	    - having all inputs as keyword arguments (like PyTorch models), or
760	    - having all inputs as a list, tuple or dict in the first positional argument.
761	
762	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
763	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
764	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
765	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
766	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
767	    positional argument:
768	
769	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
770	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
771	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
772	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
773	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
774	
775	    Note that when creating models and layers with
776	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
777	    about any of this, as you can just pass inputs like you would to any other Python function!
778	
779	    </Tip>
780	
781	    Args:
782	        config ([`ConvBertConfig`]): Model configuration class with all the parameters of the model.
783	            Initializing with a config file does not load the weights associated with the model, only the
784	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
785	"""
786	
787	CONVBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/convbert/tokenization_convbert.py:287
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
286	                    index = token_index
287	                writer.write(token + "\n")
288	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/convnext/modeling_convnext.py:297
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
296	
297	CONVNEXT_START_DOCSTRING = r"""
298	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
299	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
300	    behavior.
301	
302	    Parameters:
303	        config ([`ConvNextConfig`]): Model configuration class with all the parameters of the model.
304	            Initializing with a config file does not load the weights associated with the model, only the
305	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
306	"""
307	
308	CONVNEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/convnext/modeling_tf_convnext.py:434
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
433	
434	CONVNEXT_START_DOCSTRING = r"""
435	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
436	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
437	    etc.)
438	
439	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
440	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
441	    behavior.
442	
443	    <Tip>
444	
445	    TensorFlow models and layers in `transformers` accept two formats as input:
446	
447	    - having all inputs as keyword arguments (like PyTorch models), or
448	    - having all inputs as a list, tuple or dict in the first positional argument.
449	
450	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
451	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
452	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
453	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
454	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
455	    positional argument:
456	
457	    - a single Tensor with `pixel_values` only and nothing else: `model(pixel_values)`
458	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
459	    `model([pixel_values, attention_mask])` or `model([pixel_values, attention_mask, token_type_ids])`
460	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
461	    `model({"pixel_values": pixel_values, "token_type_ids": token_type_ids})`
462	
463	    Note that when creating models and layers with
464	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
465	    about any of this, as you can just pass inputs like you would to any other Python function!
466	
467	    </Tip>
468	
469	    Parameters:
470	        config ([`ConvNextConfig`]): Model configuration class with all the parameters of the model.
471	            Initializing with a config file does not load the weights associated with the model, only the
472	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
473	"""
474	
475	CONVNEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/convnextv2/modeling_convnextv2.py:317
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
316	
317	CONVNEXTV2_START_DOCSTRING = r"""
318	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
319	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
320	    behavior.
321	
322	    Parameters:
323	        config ([`ConvNextV2Config`]): Model configuration class with all the parameters of the model.
324	            Initializing with a config file does not load the weights associated with the model, only the
325	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
326	"""
327	
328	CONVNEXTV2_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/convnextv2/modeling_tf_convnextv2.py:482
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
481	
482	CONVNEXTV2_START_DOCSTRING = r"""
483	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
484	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
485	    etc.)
486	
487	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
488	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
489	    behavior.
490	
491	    <Tip>
492	
493	    TensorFlow models and layers in `transformers` accept two formats as input:
494	
495	    - having all inputs as keyword arguments (like PyTorch models), or
496	    - having all inputs as a list, tuple or dict in the first positional argument.
497	
498	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
499	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
500	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
501	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
502	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
503	    positional argument:
504	
505	    - a single Tensor with `pixel_values` only and nothing else: `model(pixel_values)`
506	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
507	    `model([pixel_values, attention_mask])` or `model([pixel_values, attention_mask, token_type_ids])`
508	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
509	    `model({"pixel_values": pixel_values, "token_type_ids": token_type_ids})`
510	
511	    Note that when creating models and layers with
512	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
513	    about any of this, as you can just pass inputs like you would to any other Python function!
514	
515	    </Tip>
516	
517	    Parameters:
518	        config ([`ConvNextV2Config`]): Model configuration class with all the parameters of the model.
519	            Initializing with a config file does not load the weights associated with the model, only the
520	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
521	"""
522	
523	CONVNEXTV2_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/cpm/tokenization_cpm.py:133
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
132	            raise error.__class__(
133	                "You need to install jieba to use CpmTokenizer or CpmTokenizerFast. "
134	                "See https://pypi.org/project/jieba/ for installation."
135	            )

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/cpm/tokenization_cpm.py:338
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
337	                content_spiece_model = self.sp_model.serialized_model_proto()
338	                fi.write(content_spiece_model)
339	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/cpm/tokenization_cpm_fast.py:141
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
140	            raise error.__class__(
141	                "You need to install jieba to use CpmTokenizer or CpmTokenizerFast. "
142	                "See https://pypi.org/project/jieba/ for installation."
143	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/cpmant/modeling_cpmant.py:554
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
553	
554	CPMANT_START_DOCSTRING = r"""
555	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
556	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
557	    behavior.
558	
559	    Parameters
560	        config ([`~CpmAntConfig`]): Model configuration class with all the parameters of the
561	            Initializing with a config file does not load the weights associated with the model, only the
562	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
563	"""
564	
565	CPMANT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/cpmant/tokenization_cpmant.py:221
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
220	                    index = token_index
221	                writer.write(token + "\n")
222	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/ctrl/modeling_ctrl.py:234
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
233	
234	CTRL_START_DOCSTRING = r"""
235	
236	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
237	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
238	    etc.)
239	
240	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
241	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
242	    and behavior.
243	
244	    Parameters:
245	        config ([`CTRLConfig`]): Model configuration class with all the parameters of the model.
246	            Initializing with a config file does not load the weights associated with the model, only the
247	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
248	"""
249	
250	CTRL_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/ctrl/modeling_tf_ctrl.py:469
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
468	
469	CTRL_START_DOCSTRING = r"""
470	
471	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
472	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
473	    etc.)
474	
475	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
476	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
477	    behavior.
478	
479	    <Tip>
480	
481	    TensorFlow models and layers in `transformers` accept two formats as input:
482	
483	    - having all inputs as keyword arguments (like PyTorch models), or
484	    - having all inputs as a list, tuple or dict in the first positional argument.
485	
486	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
487	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
488	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
489	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
490	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
491	    positional argument:
492	
493	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
494	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
495	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
496	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
497	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
498	
499	    Note that when creating models and layers with
500	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
501	    about any of this, as you can just pass inputs like you would to any other Python function!
502	
503	    </Tip>
504	
505	    Parameters:
506	        config ([`CTRLConfig`]): Model configuration class with all the parameters of the model.
507	            Initializing with a config file does not load the weights associated with the model, only the
508	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
509	"""
510	
511	CTRL_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/ctrl/tokenization_ctrl.py:135
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
134	        with open(merges_file, encoding="utf-8") as merges_handle:
135	            merges = merges_handle.read().split("\n")[1:-1]
136	        merges = [tuple(merge.split()) for merge in merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/ctrl/tokenization_ctrl.py:227
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
226	        with open(vocab_file, "w", encoding="utf-8") as f:
227	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
228	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/ctrl/tokenization_ctrl.py:231
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
230	        with open(merge_file, "w", encoding="utf-8") as writer:
231	            writer.write("#version: 0.2\n")
232	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/ctrl/tokenization_ctrl.py:239
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
238	                    index = token_index
239	                writer.write(" ".join(bpe_tokens) + "\n")
240	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/cvt/modeling_cvt.py:551
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
550	
551	CVT_START_DOCSTRING = r"""
552	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
553	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
554	    behavior.
555	
556	    Parameters:
557	        config ([`CvtConfig`]): Model configuration class with all the parameters of the model.
558	            Initializing with a config file does not load the weights associated with the model, only the
559	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
560	"""
561	
562	CVT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/cvt/modeling_tf_cvt.py:869
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
868	
869	TFCVT_START_DOCSTRING = r"""
870	
871	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
872	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
873	    etc.)
874	
875	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
876	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
877	    behavior.
878	
879	    <Tip>
880	
881	    TF 2.0 models accepts two formats as inputs:
882	
883	    - having all inputs as keyword arguments (like PyTorch models), or
884	    - having all inputs as a list, tuple or dict in the first positional arguments.
885	
886	    This second option is useful when using [`keras.Model.fit`] method which currently requires having all the
887	    tensors in the first argument of the model call function: `model(inputs)`.
888	
889	    </Tip>
890	
891	    Args:
892	        config ([`CvtConfig`]): Model configuration class with all the parameters of the model.
893	            Initializing with a config file does not load the weights associated with the model, only the
894	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
895	"""
896	
897	TFCVT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/dab_detr/modeling_dab_detr.py:897
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
896	
897	DAB_DETR_START_DOCSTRING = r"""
898	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
899	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
900	    etc.)
901	
902	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
903	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
904	    and behavior.
905	
906	    Parameters:
907	        config ([`DabDetrConfig`]):
908	            Model configuration class with all the parameters of the model. Initializing with a config file does not
909	            load the weights associated with the model, only the configuration. Check out the
910	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
911	"""
912	
913	DAB_DETR_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/dac/modeling_dac.py:559
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
558	
559	DAC_START_DOCSTRING = r"""
560	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
561	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
562	    etc.)
563	
564	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
565	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
566	    and behavior.
567	
568	    Parameters:
569	        config ([`DacConfig`]):
570	            Model configuration class with all the parameters of the model. Initializing with a config file does not
571	            load the weights associated with the model, only the configuration. Check out the
572	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
573	"""
574	
575	DAC_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/data2vec/modeling_data2vec_audio.py:976
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
975	
976	DATA2VEC_AUDIO_START_DOCSTRING = r"""
977	    Data2VecAudio was proposed in [data2vec: A General Framework for Self-supervised Learning in Speech, Vision and
978	    Language](https://arxiv.org/pdf/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and
979	    Michael Auli.
980	
981	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
982	    library implements for all its model (such as downloading or saving etc.).
983	
984	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
985	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
986	    behavior.
987	
988	    Parameters:
989	        config ([`Data2VecAudioConfig`]): Model configuration class with all the parameters of the model.
990	            Initializing with a config file does not load the weights associated with the model, only the
991	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
992	"""
993	
994	DATA2VEC_AUDIO_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/data2vec/modeling_data2vec_audio.py:994
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
993	
994	DATA2VEC_AUDIO_INPUTS_DOCSTRING = r"""
995	    Args:
996	        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):
997	            Float values of input raw speech waveform. Values can be obtained by loading a *.flac* or *.wav* audio file
998	            into an array of type *List[float]* or a *numpy.ndarray*, *e.g.* via the soundfile library (*pip install
999	            soundfile*). To prepare the array into *input_values*, the [`AutoProcessor`] should be used for padding and
1000	            conversion into a tensor of type *torch.FloatTensor*. See [`Wav2Vec2Processor.__call__`] for details.
1001	        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
1002	            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,
1003	            1]`:
1004	
1005	            - 1 for tokens that are **not masked**,
1006	            - 0 for tokens that are **masked**.
1007	
1008	            [What are attention masks?](../glossary#attention-mask)
1009	
1010	            <Tip warning={true}>
1011	
1012	            `attention_mask` should be passed if the corresponding processor has `config.return_attention_mask ==
1013	            True`, which is the case for all pre-trained Data2Vec Audio models. Be aware that that even with
1014	            `attention_mask`, zero-padded inputs will have slightly different outputs compared to non-padded inputs
1015	            because there are more than one convolutional layer in the positional encodings. For a more detailed
1016	            explanation, see [here](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349).
1017	
1018	            </Tip>
1019	
1020	        output_attentions (`bool`, *optional*):
1021	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1022	            tensors for more detail.
1023	        output_hidden_states (`bool`, *optional*):
1024	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1025	            more detail.
1026	        return_dict (`bool`, *optional*):
1027	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1028	"""
1029	
1030	Data2VecAudioBaseModelOutput = Wav2Vec2BaseModelOutput

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/data2vec/modeling_data2vec_text.py:614
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
613	
614	DATA2VECTEXT_START_DOCSTRING = r"""
615	    Data2VecText was proposed in [data2vec: A General Framework for Self-supervised Learning in Speech, Vision and
616	    Language](https://arxiv.org/pdf/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and
617	    Michael Auli.
618	
619	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
620	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
621	    etc.)
622	
623	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
624	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
625	    and behavior.
626	
627	    Parameters:
628	        config ([`Data2VecTextConfig`]): Model configuration class with all the parameters of the
629	            model. Initializing with a config file does not load the weights associated with the model, only the
630	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
631	"""
632	
633	DATA2VECTEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/data2vec/modeling_data2vec_vision.py:801
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
800	
801	DATA2VEC_VISION_START_DOCSTRING = r"""
802	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
803	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
804	    behavior.
805	
806	    Parameters:
807	        config ([`Data2VecVisionConfig`]): Model configuration class with all the parameters of the model.
808	            Initializing with a config file does not load the weights associated with the model, only the
809	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
810	"""
811	
812	DATA2VEC_VISION_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/data2vec/modeling_tf_data2vec_vision.py:898
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
897	
898	DATA2VEC_VISION_START_DOCSTRING = r"""
899	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
900	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
901	    etc.).
902	
903	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
904	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
905	    behavior.
906	
907	    <Tip>
908	
909	    TensorFlow models and layers in `transformers` accept two formats as input:
910	
911	    - having all inputs as keyword arguments (like PyTorch models), or
912	    - having all inputs as a list, tuple or dict in the first positional argument.
913	
914	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
915	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
916	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
917	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
918	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
919	    positional argument:
920	
921	    - a single Tensor with `pixel_values` only and nothing else: `model(pixel_values)`
922	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
923	    `model([pixel_values, attention_mask])` or `model([pixel_values, attention_mask, token_type_ids])`
924	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
925	    `model({"pixel_values": pixel_values, "token_type_ids": token_type_ids})`
926	
927	    Note that when creating models and layers with
928	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
929	    about any of this, as you can just pass inputs like you would to any other Python function!
930	
931	    </Tip>
932	
933	    Args:
934	        config ([`Data2VecVisionConfig`]): Model configuration class with all the parameters of the model.
935	            Initializing with a config file does not load the weights associated with the model, only the
936	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
937	"""
938	
939	DATA2VEC_VISION_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/data2vec/modular_data2vec_audio.py:193
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
192	
193	DATA2VEC_AUDIO_START_DOCSTRING = r"""
194	    Data2VecAudio was proposed in [data2vec: A General Framework for Self-supervised Learning in Speech, Vision and
195	    Language](https://arxiv.org/pdf/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and
196	    Michael Auli.
197	
198	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
199	    library implements for all its model (such as downloading or saving etc.).
200	
201	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
202	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
203	    behavior.
204	
205	    Parameters:
206	        config ([`Data2VecAudioConfig`]): Model configuration class with all the parameters of the model.
207	            Initializing with a config file does not load the weights associated with the model, only the
208	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
209	"""
210	
211	DATA2VEC_AUDIO_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/data2vec/modular_data2vec_audio.py:211
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
210	
211	DATA2VEC_AUDIO_INPUTS_DOCSTRING = r"""
212	    Args:
213	        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):
214	            Float values of input raw speech waveform. Values can be obtained by loading a *.flac* or *.wav* audio file
215	            into an array of type *List[float]* or a *numpy.ndarray*, *e.g.* via the soundfile library (*pip install
216	            soundfile*). To prepare the array into *input_values*, the [`AutoProcessor`] should be used for padding and
217	            conversion into a tensor of type *torch.FloatTensor*. See [`Wav2Vec2Processor.__call__`] for details.
218	        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
219	            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,
220	            1]`:
221	
222	            - 1 for tokens that are **not masked**,
223	            - 0 for tokens that are **masked**.
224	
225	            [What are attention masks?](../glossary#attention-mask)
226	
227	            <Tip warning={true}>
228	
229	            `attention_mask` should be passed if the corresponding processor has `config.return_attention_mask ==
230	            True`, which is the case for all pre-trained Data2Vec Audio models. Be aware that that even with
231	            `attention_mask`, zero-padded inputs will have slightly different outputs compared to non-padded inputs
232	            because there are more than one convolutional layer in the positional encodings. For a more detailed
233	            explanation, see [here](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349).
234	
235	            </Tip>
236	
237	        output_attentions (`bool`, *optional*):
238	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
239	            tensors for more detail.
240	        output_hidden_states (`bool`, *optional*):
241	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
242	            more detail.
243	        return_dict (`bool`, *optional*):
244	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
245	"""
246	
247	Data2VecAudioBaseModelOutput = Wav2Vec2BaseModelOutput

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/dbrx/modeling_dbrx.py:348
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
347	            raise ValueError(
348	                "`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` "
349	                "make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers"
350	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/dbrx/modeling_dbrx.py:811
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
810	
811	DBRX_START_DOCSTRING = r"""
812	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
813	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
814	    etc.)
815	
816	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
817	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
818	    and behavior.
819	
820	    Parameters:
821	        config ([`DbrxConfig`]):
822	            Model configuration class with all the parameters of the model. Initializing with a config file does not
823	            load the weights associated with the model, only the configuration. Check out the
824	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
825	"""
826	
827	
828	@add_start_docstrings(
829	    "The bare DBRX Model outputting raw hidden-states without any specific head on top.",
830	    DBRX_START_DOCSTRING,
831	)
832	class DbrxPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/dbrx/modeling_dbrx.py:864
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
863	
864	DBRX_INPUTS_DOCSTRING = r"""
865	    Args:
866	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
867	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
868	            it.
869	
870	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
871	            [`PreTrainedTokenizer.__call__`] for details.
872	
873	            [What are input IDs?](../glossary#input-ids)
874	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
875	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
876	
877	            - 1 for tokens that are **not masked**,
878	            - 0 for tokens that are **masked**.
879	
880	            [What are attention masks?](../glossary#attention-mask)
881	
882	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
883	            [`PreTrainedTokenizer.__call__`] for details.
884	
885	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
886	            `past_key_values`).
887	
888	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
889	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
890	            information on the default strategy.
891	
892	            - 1 indicates the head is **not masked**,
893	            - 0 indicates the head is **masked**.
894	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
895	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
896	            config.n_positions - 1]`.
897	
898	            [What are position IDs?](../glossary#position-ids)
899	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
900	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
901	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
902	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
903	
904	            Two formats are allowed:
905	            - a [`~cache_utils.Cache`] instance, see our
906	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
907	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
908	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
909	            cache format.
910	
911	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
912	            legacy cache format will be returned.
913	
914	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
915	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
916	            of shape `(batch_size, sequence_length)`.
917	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
918	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
919	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
920	            model's internal embedding lookup matrix.
921	        use_cache (`bool`, *optional*):
922	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
923	            `past_key_values`).
924	        output_attentions (`bool`, *optional*):
925	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
926	            tensors for more detail.
927	        output_hidden_states (`bool`, *optional*):
928	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
929	            more detail.
930	        output_router_logits (`bool`, *optional*):
931	            Whether or not to return the logits of all the routers. They are useful for computing the router loss, and
932	            should not be returned during inference.
933	        return_dict (`bool`, *optional*):
934	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
935	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
936	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
937	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
938	            the complete sequence length.
939	"""
940	
941	
942	@add_start_docstrings(
943	    "The bare DBRX Model outputting raw hidden-states without any specific head on top.",
944	    DBRX_START_DOCSTRING,
945	)
946	class DbrxModel(DbrxPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/dbrx/modeling_dbrx.py:1024
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1023	                logger.warning_once(
1024	                    "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and "
1025	                    "will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class "
1026	                    "(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deberta/modeling_deberta.py:668
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
667	
668	DEBERTA_START_DOCSTRING = r"""
669	    The DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled
670	    Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. It's build
671	    on top of BERT/RoBERTa with two improvements, i.e. disentangled attention and enhanced mask decoder. With those two
672	    improvements, it out perform BERT/RoBERTa on a majority of tasks with 80GB pretraining data.
673	
674	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
675	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
676	    and behavior.
677	
678	
679	    Parameters:
680	        config ([`DebertaConfig`]): Model configuration class with all the parameters of the model.
681	            Initializing with a config file does not load the weights associated with the model, only the
682	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
683	"""
684	
685	DEBERTA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deberta/modeling_tf_deberta.py:708
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
707	                for i in tf.range(self.num_attention_heads):
708	                    qkvw_inside = qkvw_inside.write(i, ws[i * 3 + k])
709	                qkvw = qkvw.write(k, qkvw_inside.concat())

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deberta/modeling_tf_deberta.py:709
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
708	                    qkvw_inside = qkvw_inside.write(i, ws[i * 3 + k])
709	                qkvw = qkvw.write(k, qkvw_inside.concat())
710	            qkvb = [None] * 3

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deberta/modeling_tf_deberta.py:1147
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1146	
1147	DEBERTA_START_DOCSTRING = r"""
1148	    The DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled
1149	    Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. It's build
1150	    on top of BERT/RoBERTa with two improvements, i.e. disentangled attention and enhanced mask decoder. With those two
1151	    improvements, it out perform BERT/RoBERTa on a majority of tasks with 80GB pretraining data.
1152	
1153	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
1154	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
1155	    behavior.
1156	
1157	    <Tip>
1158	
1159	    TensorFlow models and layers in `transformers` accept two formats as input:
1160	
1161	    - having all inputs as keyword arguments (like PyTorch models), or
1162	    - having all inputs as a list, tuple or dict in the first positional argument.
1163	
1164	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
1165	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
1166	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
1167	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
1168	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
1169	    positional argument:
1170	
1171	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
1172	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
1173	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
1174	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
1175	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
1176	
1177	    Note that when creating models and layers with
1178	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
1179	    about any of this, as you can just pass inputs like you would to any other Python function!
1180	
1181	    </Tip>
1182	
1183	    Parameters:
1184	        config ([`DebertaConfig`]): Model configuration class with all the parameters of the model.
1185	            Initializing with a config file does not load the weights associated with the model, only the
1186	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1187	"""
1188	
1189	DEBERTA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deberta/tokenization_deberta.py:174
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
173	        with open(merges_file, encoding="utf-8") as merges_handle:
174	            bpe_merges = merges_handle.read().split("\n")[1:-1]
175	        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deberta/tokenization_deberta.py:372
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
371	        with open(vocab_file, "w", encoding="utf-8") as f:
372	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
373	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deberta/tokenization_deberta.py:376
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
375	        with open(merge_file, "w", encoding="utf-8") as writer:
376	            writer.write("#version: 0.2\n")
377	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deberta/tokenization_deberta.py:384
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
383	                    index = token_index
384	                writer.write(" ".join(bpe_tokens) + "\n")
385	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deberta_v2/modeling_deberta_v2.py:736
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
735	
736	DEBERTA_START_DOCSTRING = r"""
737	    The DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled
738	    Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. It's build
739	    on top of BERT/RoBERTa with two improvements, i.e. disentangled attention and enhanced mask decoder. With those two
740	    improvements, it out perform BERT/RoBERTa on a majority of tasks with 80GB pretraining data.
741	
742	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
743	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
744	    and behavior.
745	
746	
747	    Parameters:
748	        config ([`DebertaV2Config`]): Model configuration class with all the parameters of the model.
749	            Initializing with a config file does not load the weights associated with the model, only the
750	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
751	"""
752	
753	DEBERTA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py:1256
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1255	
1256	DEBERTA_START_DOCSTRING = r"""
1257	    The DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled
1258	    Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. It's build
1259	    on top of BERT/RoBERTa with two improvements, i.e. disentangled attention and enhanced mask decoder. With those two
1260	    improvements, it out perform BERT/RoBERTa on a majority of tasks with 80GB pretraining data.
1261	
1262	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
1263	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
1264	    behavior.
1265	
1266	    <Tip>
1267	
1268	    TensorFlow models and layers in `transformers` accept two formats as input:
1269	
1270	    - having all inputs as keyword arguments (like PyTorch models), or
1271	    - having all inputs as a list, tuple or dict in the first positional argument.
1272	
1273	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
1274	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
1275	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
1276	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
1277	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
1278	    positional argument:
1279	
1280	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
1281	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
1282	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
1283	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
1284	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
1285	
1286	    Note that when creating models and layers with
1287	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
1288	    about any of this, as you can just pass inputs like you would to any other Python function!
1289	
1290	    </Tip>
1291	
1292	    Parameters:
1293	        config ([`DebertaV2Config`]): Model configuration class with all the parameters of the model.
1294	            Initializing with a config file does not load the weights associated with the model, only the
1295	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1296	"""
1297	
1298	DEBERTA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deberta_v2/tokenization_deberta_v2.py:471
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
470	        with open(full_path, "wb") as fs:
471	            fs.write(self.spm.serialized_model_proto())
472	        return (full_path,)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/decision_transformer/modeling_decision_transformer.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	        logger.error(
55	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
56	            "https://www.tensorflow.org/install/ for installation instructions."
57	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/decision_transformer/modeling_decision_transformer.py:771
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
770	
771	DECISION_TRANSFORMER_START_DOCSTRING = r"""
772	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
773	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
774	    behavior.
775	
776	    Parameters:
777	        config ([`~DecisionTransformerConfig`]): Model configuration class with all the parameters of the model.
778	            Initializing with a config file does not load the weights associated with the model, only the
779	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
780	"""
781	
782	DECISION_TRANSFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deepseek_v3/modeling_deepseek_v3.py:514
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
513	
514	DEEPSEEK_V3_START_DOCSTRING = r"""
515	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
516	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
517	    etc.)
518	
519	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
520	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
521	    and behavior.
522	
523	    Parameters:
524	        config ([`DeepseekV3Config`]):
525	            Model configuration class with all the parameters of the model. Initializing with a config file does not
526	            load the weights associated with the model, only the configuration. Check out the
527	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
528	"""
529	
530	
531	@add_start_docstrings(
532	    "The bare DeepseekV3 Model outputting raw hidden-states without any specific head on top.",
533	    DEEPSEEK_V3_START_DOCSTRING,
534	)
535	class DeepseekV3PreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deepseek_v3/modeling_deepseek_v3.py:565
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
564	
565	DEEPSEEK_V3_INPUTS_DOCSTRING = r"""
566	    Args:
567	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
568	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
569	            it.
570	
571	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
572	            [`PreTrainedTokenizer.__call__`] for details.
573	
574	            [What are input IDs?](../glossary#input-ids)
575	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
576	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
577	
578	            - 1 for tokens that are **not masked**,
579	            - 0 for tokens that are **masked**.
580	
581	            [What are attention masks?](../glossary#attention-mask)
582	
583	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
584	            [`PreTrainedTokenizer.__call__`] for details.
585	
586	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
587	            `past_key_values`).
588	
589	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
590	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
591	            information on the default strategy.
592	
593	            - 1 indicates the head is **not masked**,
594	            - 0 indicates the head is **masked**.
595	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
596	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
597	            config.n_positions - 1]`.
598	
599	            [What are position IDs?](../glossary#position-ids)
600	        past_key_values (`Cache`, *optional*):
601	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
602	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
603	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
604	
605	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
606	
607	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
608	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
609	            of shape `(batch_size, sequence_length)`.
610	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
611	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
612	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
613	            model's internal embedding lookup matrix.
614	        use_cache (`bool`, *optional*):
615	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
616	            `past_key_values`).
617	        output_attentions (`bool`, *optional*):
618	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
619	            tensors for more detail.
620	        output_hidden_states (`bool`, *optional*):
621	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
622	            more detail.
623	        return_dict (`bool`, *optional*):
624	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
625	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
626	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
627	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
628	            the complete sequence length.
629	"""
630	
631	
632	@add_start_docstrings(
633	    "The bare DeepseekV3 Model outputting raw hidden-states without any specific head on top.",
634	    DEEPSEEK_V3_START_DOCSTRING,
635	)
636	class DeepseekV3Model(DeepseekV3PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deformable_detr/modeling_deformable_detr.py:1025
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1024	
1025	DEFORMABLE_DETR_START_DOCSTRING = r"""
1026	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1027	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1028	    etc.)
1029	
1030	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1031	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1032	    and behavior.
1033	
1034	    Parameters:
1035	        config ([`DeformableDetrConfig`]):
1036	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1037	            load the weights associated with the model, only the configuration. Check out the
1038	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1039	"""
1040	
1041	DEFORMABLE_DETR_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deit/modeling_deit.py:498
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
497	
498	DEIT_START_DOCSTRING = r"""
499	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
500	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
501	    behavior.
502	
503	    Parameters:
504	        config ([`DeiTConfig`]): Model configuration class with all the parameters of the model.
505	            Initializing with a config file does not load the weights associated with the model, only the
506	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
507	"""
508	
509	DEIT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deit/modeling_deit.py:650
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
649	@add_start_docstrings(
650	    """DeiT Model with a decoder on top for masked image modeling, as proposed in [SimMIM](https://arxiv.org/abs/2111.09886).
651	
652	    <Tip>
653	
654	    Note that we provide a script to pre-train this model on custom data in our [examples
655	    directory](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining).
656	
657	    </Tip>
658	    """,
659	    DEIT_START_DOCSTRING,
660	)
661	class DeiTForMaskedImageModeling(DeiTPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deit/modeling_tf_deit.py:719
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
718	
719	DEIT_START_DOCSTRING = r"""
720	    This model is a TensorFlow
721	    [keras.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer). Use it as a regular
722	    TensorFlow Module and refer to the TensorFlow documentation for all matter related to general usage and behavior.
723	
724	    Parameters:
725	        config ([`DeiTConfig`]): Model configuration class with all the parameters of the model.
726	            Initializing with a config file does not load the weights associated with the model, only the
727	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
728	"""
729	
730	DEIT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deit/modeling_tf_deit.py:894
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
893	@add_start_docstrings(
894	    "DeiT Model with a decoder on top for masked image modeling, as proposed in"
895	    " [SimMIM](https://arxiv.org/abs/2111.09886).",
896	    DEIT_START_DOCSTRING,
897	)
898	class TFDeiTForMaskedImageModeling(TFDeiTPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/deta/modeling_deta.py:1055
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1054	
1055	DETA_START_DOCSTRING = r"""
1056	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1057	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1058	    etc.)
1059	
1060	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1061	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1062	    and behavior.
1063	
1064	    Parameters:
1065	        config ([`DetaConfig`]):
1066	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1067	            load the weights associated with the model, only the configuration. Check out the
1068	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1069	"""
1070	
1071	DETA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/efficientformer/modeling_efficientformer.py:519
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
518	
519	EFFICIENTFORMER_START_DOCSTRING = r"""
520	    This model is a PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#nn.Module) subclass. Use it as a
521	    regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.
522	
523	    Parameters:
524	        config ([`EfficientFormerConfig`]): Model configuration class with all the parameters of the model.
525	            Initializing with a config file does not load the weights associated with the model, only the
526	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
527	"""
528	
529	EFFICIENTFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/efficientformer/modeling_tf_efficientformer.py:910
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
909	
910	EFFICIENTFORMER_START_DOCSTRING = r"""
911	    This model is a TensorFlow
912	    [keras.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer). Use it as a regular
913	    TensorFlow Module and refer to the TensorFlow documentation for all matter related to general usage and behavior.
914	
915	
916	    Parameters:
917	        config ([`EfficientFormerConfig`]): Model configuration class with all the parameters of the model.
918	            Initializing with a config file does not load the weights associated with the model, only the
919	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
920	"""
921	
922	EFFICIENTFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/ernie_m/modeling_ernie_m.py:423
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
422	
423	ERNIE_M_START_DOCSTRING = r"""
424	
425	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
426	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
427	    etc.)
428	
429	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
430	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
431	    behavior.
432	
433	    Parameters:
434	        config ([`ErnieMConfig`]): Model configuration class with all the parameters of the model.
435	            Initializing with a config file does not load the weights associated with the model, only the
436	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
437	"""
438	
439	ERNIE_M_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/ernie_m/tokenization_ernie_m.py:397
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
396	                    index = token_index
397	                writer.write(token + "\n")
398	                index += 1

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/ernie_m/tokenization_ernie_m.py:403
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
402	            content_spiece_model = self.sp_model.serialized_model_proto()
403	            fi.write(content_spiece_model)
404	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py:775
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
774	
775	GPTSAN_JAPANESE_START_DOCSTRING = r"""
776	
777	    The [GPTSAN-japanese](https://github.com/tanreinama/GPTSAN) model was proposed in General-purpose Swich transformer
778	    based Japanese language model
779	
780	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
781	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
782	    and behavior.
783	
784	    Parameters:
785	        config ([`GPTSanJapaneseConfig`]): Model configuration class with all the parameters of the model.
786	            Initializing with a config file does not load the weights associated with the model, only the
787	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
788	"""
789	
790	GPTSAN_JAPANESE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/gptsan_japanese/tokenization_gptsan_japanese.py:46
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
45	    with open(emoji_file, "r", encoding="utf-8") as f:
46	        emoji = json.loads(f.read())
47	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/gptsan_japanese/tokenization_gptsan_japanese.py:264
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
263	                    index = token_index
264	                writer.write(",".join(token) + "\n")
265	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/jukebox/modeling_jukebox.py:576
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
575	
576	JUKEBOX_START_DOCSTRING = r"""
577	
578	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
579	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
580	    etc.)
581	
582	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
583	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
584	    and behavior.
585	
586	    Parameters:
587	        config (`JukeboxConfig`): Model configuration class with all the parameters of the model.
588	            Initializing with a config file does not load the weights associated with the model, only the
589	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
590	"""
591	
592	
593	@add_start_docstrings(
594	    """The Hierarchical VQ-VAE model used in Jukebox. This model follows the Hierarchical VQVAE paper from [Will Williams, Sam
595	Ringer, Tom Ash, John Hughes, David MacLeod, Jamie Dougherty](https://arxiv.org/abs/2002.08111).
596	
597	    """,
598	    JUKEBOX_START_DOCSTRING,
599	)
600	class JukeboxVQVAE(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/jukebox/modeling_jukebox.py:594
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
593	@add_start_docstrings(
594	    """The Hierarchical VQ-VAE model used in Jukebox. This model follows the Hierarchical VQVAE paper from [Will Williams, Sam
595	Ringer, Tom Ash, John Hughes, David MacLeod, Jamie Dougherty](https://arxiv.org/abs/2002.08111).
596	
597	    """,
598	    JUKEBOX_START_DOCSTRING,
599	)
600	class JukeboxVQVAE(PreTrainedModel):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/jukebox/tokenization_jukebox.py:373
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
372	        with open(artists_file, "w", encoding="utf-8") as f:
373	            f.write(json.dumps(self.artists_encoder, ensure_ascii=False))
374	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/jukebox/tokenization_jukebox.py:379
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
378	        with open(genres_file, "w", encoding="utf-8") as f:
379	            f.write(json.dumps(self.genres_encoder, ensure_ascii=False))
380	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/jukebox/tokenization_jukebox.py:385
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
384	        with open(lyrics_file, "w", encoding="utf-8") as f:
385	            f.write(json.dumps(self.lyrics_encoder, ensure_ascii=False))
386	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/mctct/modeling_mctct.py:488
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
487	
488	MCTCT_START_DOCSTRING = r"""
489	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
490	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
491	    behavior.
492	
493	    Parameters:
494	        config ([`MCTCTConfig`]): Model configuration class with all the parameters of the model.
495	            Initializing with a config file does not load the weights associated with the model, only the
496	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
497	"""
498	
499	MCTCT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/mega/modeling_mega.py:1387
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1386	
1387	MEGA_START_DOCSTRING = r"""
1388	
1389	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1390	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1391	    etc.)
1392	
1393	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1394	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1395	    and behavior.
1396	
1397	    Parameters:
1398	        config ([`MegaConfig`]): Model configuration class with all the parameters of the
1399	            model. Initializing with a config file does not load the weights associated with the model, only the
1400	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1401	"""
1402	
1403	MEGA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/mmbt/modeling_mmbt.py:77
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
76	
77	MMBT_START_DOCSTRING = r"""
78	    MMBT model was proposed in [Supervised Multimodal Bitransformers for Classifying Images and
79	    Text](https://github.com/facebookresearch/mmbt) by Douwe Kiela, Suvrat Bhooshan, Hamed Firooz, Davide Testuggine.
80	    It's a supervised multimodal bitransformer model that fuses information from text and other image encoders, and
81	    obtain state-of-the-art performance on various multimodal classification benchmark tasks.
82	
83	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
84	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
85	    etc.)
86	
87	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
88	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
89	    and behavior.
90	
91	    Parameters:
92	        config ([`MMBTConfig`]): Model configuration class with all the parameters of the model.
93	            Initializing with a config file does not load the weights associated with the model, only the
94	            configuration.
95	        transformer (`nn.Module`): A text transformer that is used by MMBT.
96	            It should have embeddings, encoder, and pooler attributes.
97	        encoder (`nn.Module`): Encoder for the second modality.
98	            It should take in a batch of modal inputs and return k, n dimension embeddings.
99	"""
100	
101	MMBT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/nat/modeling_nat.py:635
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
634	
635	NAT_START_DOCSTRING = r"""
636	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
637	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
638	    behavior.
639	
640	    Parameters:
641	        config ([`NatConfig`]): Model configuration class with all the parameters of the model.
642	            Initializing with a config file does not load the weights associated with the model, only the
643	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
644	"""
645	
646	
647	NAT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/nezha/modeling_nezha.py:67
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
66	        logger.error(
67	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
68	            "https://www.tensorflow.org/install/ for installation instructions."
69	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/nezha/modeling_nezha.py:769
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
768	
769	NEZHA_START_DOCSTRING = r"""
770	
771	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
772	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
773	    etc.)
774	
775	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
776	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
777	    and behavior.
778	
779	    Parameters:
780	        config ([`NezhaConfig`]): Model configuration class with all the parameters of the model.
781	            Initializing with a config file does not load the weights associated with the model, only the
782	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
783	"""
784	
785	NEZHA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/open_llama/modeling_open_llama.py:411
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
410	
411	OPEN_LLAMA_START_DOCSTRING = r"""
412	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
413	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
414	    etc.)
415	
416	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
417	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
418	    and behavior.
419	
420	    Parameters:
421	        config ([`OpenLlamaConfig`]):
422	            Model configuration class with all the parameters of the model. Initializing with a config file does not
423	            load the weights associated with the model, only the configuration. Check out the
424	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
425	"""
426	
427	
428	@add_start_docstrings(
429	    "The bare Open-Llama Model outputting raw hidden-states without any specific head on top.",
430	    OPEN_LLAMA_START_DOCSTRING,
431	)
432	class OpenLlamaPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/open_llama/modeling_open_llama.py:453
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
452	
453	OPEN_LLAMA_INPUTS_DOCSTRING = r"""
454	    Args:
455	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
456	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
457	            it.
458	
459	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
460	            [`PreTrainedTokenizer.__call__`] for details.
461	
462	            [What are input IDs?](../glossary#input-ids)
463	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
464	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
465	
466	            - 1 for tokens that are **not masked**,
467	            - 0 for tokens that are **masked**.
468	
469	            [What are attention masks?](../glossary#attention-mask)
470	
471	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
472	            [`PreTrainedTokenizer.__call__`] for details.
473	
474	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
475	            `past_key_values`).
476	
477	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
478	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
479	            information on the default strategy.
480	
481	            - 1 indicates the head is **not masked**,
482	            - 0 indicates the head is **masked**.
483	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
484	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
485	            config.n_positions - 1]`.
486	
487	            [What are position IDs?](../glossary#position-ids)
488	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
489	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
490	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
491	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
492	
493	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
494	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
495	
496	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
497	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
498	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
499	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
500	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
501	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
502	            model's internal embedding lookup matrix.
503	        use_cache (`bool`, *optional*):
504	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
505	            `past_key_values`).
506	        output_attentions (`bool`, *optional*):
507	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
508	            tensors for more detail.
509	        output_hidden_states (`bool`, *optional*):
510	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
511	            more detail.
512	        return_dict (`bool`, *optional*):
513	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
514	"""
515	
516	
517	@add_start_docstrings(
518	    "The bare Open-Llama Model outputting raw hidden-states without any specific head on top.",
519	    OPEN_LLAMA_START_DOCSTRING,
520	)
521	class OpenLlamaModel(OpenLlamaPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/qdqbert/modeling_qdqbert.py:63
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
62	        logger.error(
63	            "QDQBERT model are not usable since `pytorch_quantization` can't be loaded. Please try to reinstall it"
64	            " following the instructions here:"
65	            " https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/qdqbert/modeling_qdqbert.py:81
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
80	        logger.error(
81	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
82	            "https://www.tensorflow.org/install/ for installation instructions."
83	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/qdqbert/modeling_qdqbert.py:749
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
748	
749	QDQBERT_START_DOCSTRING = r"""
750	
751	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
752	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
753	    etc.)
754	
755	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
756	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
757	    and behavior.
758	
759	    Parameters:
760	        config ([`QDQBertConfig`]): Model configuration class with all the parameters of the model.
761	            Initializing with a config file does not load the weights associated with the model, only the
762	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
763	"""
764	
765	QDQBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/realm/modeling_realm.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	        logger.error(
55	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
56	            "https://www.tensorflow.org/install/ for installation instructions."
57	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/realm/modeling_realm.py:887
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
886	
887	REALM_START_DOCSTRING = r"""
888	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
889	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
890	    behavior.
891	
892	    Parameters:
893	        config ([`RealmConfig`]): Model configuration class with all the parameters of the model.
894	            Initializing with a config file does not load the weights associated with the model, only the
895	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
896	"""
897	
898	REALM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/realm/tokenization_realm.py:352
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
351	                    index = token_index
352	                writer.write(token + "\n")
353	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/retribert/modeling_retribert.py:61
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
60	
61	RETRIBERT_START_DOCSTRING = r"""
62	
63	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
64	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
65	    etc.)
66	
67	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
68	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
69	    and behavior.
70	
71	    Parameters:
72	        config ([`RetriBertConfig`]): Model configuration class with all the parameters of the model.
73	            Initializing with a config file does not load the weights associated with the model, only the
74	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
75	"""
76	
77	
78	@add_start_docstrings(
79	    """Bert Based model to embed queries or document for document retrieval.""",
80	    RETRIBERT_START_DOCSTRING,
81	)
82	class RetriBertModel(RetriBertPreTrainedModel):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/retribert/tokenization_retribert.py:281
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
280	                    index = token_index
281	                writer.write(token + "\n")
282	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/speech_to_text_2/modeling_speech_to_text_2.py:408
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
407	
408	SPEECH_TO_TEXT_2_START_DOCSTRING = r"""
409	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
410	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
411	    etc.)
412	
413	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
414	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
415	    and behavior.
416	
417	    Parameters:
418	        config ([`Speech2Text2Config`]):
419	            Model configuration class with all the parameters of the model. Initializing with a config file does not
420	            load the weights associated with the model, only the configuration. Check out the
421	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
422	"""
423	
424	
425	class Speech2Text2Decoder(Speech2Text2PreTrainedModel):

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/speech_to_text_2/tokenization_speech_to_text_2.py:106
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
105	            with open(merges_file, encoding="utf-8") as merges_handle:
106	                merges = merges_handle.read().split("\n")[:-1]
107	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/speech_to_text_2/tokenization_speech_to_text_2.py:232
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
231	        with open(vocab_file, "w", encoding="utf-8") as f:
232	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
233	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/speech_to_text_2/tokenization_speech_to_text_2.py:246
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
245	                    index = token_index
246	                writer.write(" ".join(bpe_tokens) + "\n")
247	                index += 1

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/tapex/tokenization_tapex.py:286
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
285	        with open(merges_file, encoding="utf-8") as merges_handle:
286	            bpe_merges = merges_handle.read().split("\n")[1:-1]
287	        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/tapex/tokenization_tapex.py:478
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
477	        with open(vocab_file, "w", encoding="utf-8") as f:
478	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
479	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/tapex/tokenization_tapex.py:482
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
481	        with open(merge_file, "w", encoding="utf-8") as writer:
482	            writer.write("#version: 0.2\n")
483	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/tapex/tokenization_tapex.py:490
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
489	                    index = token_index
490	                writer.write(" ".join(bpe_tokens) + "\n")
491	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/tapex/tokenization_tapex.py:951
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
950	            raise NotImplementedError(
951	                "return_offset_mapping is not available when using Python tokenizers. "
952	                "To use this feature, change your tokenizer to one deriving from "
953	                "transformers.PreTrainedTokenizerFast. "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/tapex/tokenization_tapex.py:1267
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1266	            raise NotImplementedError(
1267	                "return_offset_mapping is not available when using Python tokenizers. "
1268	                "To use this feature, change your tokenizer to one deriving from "
1269	                "transformers.PreTrainedTokenizerFast. "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/trajectory_transformer/modeling_trajectory_transformer.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	        logger.error(
54	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
55	            "https://www.tensorflow.org/install/ for installation instructions."
56	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/trajectory_transformer/modeling_trajectory_transformer.py:178
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
177	
178	TRAJECTORY_TRANSFORMER_START_DOCSTRING = r"""
179	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
180	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
181	    behavior.
182	
183	    Parameters:
184	        config ([`TrajectoryTransformerConfig`]): Model configuration class with all the parameters of the model.
185	            Initializing with a config file does not load the weights associated with the model, only the
186	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
187	"""
188	
189	TRAJECTORY_TRANSFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py:766
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
765	
766	TRANSFO_XL_START_DOCSTRING = r"""
767	
768	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
769	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
770	    etc.)
771	
772	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
773	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
774	    behavior.
775	
776	    <Tip>
777	
778	    TensorFlow models and layers in `transformers` accept two formats as input:
779	
780	    - having all inputs as keyword arguments (like PyTorch models), or
781	    - having all inputs as a list, tuple or dict in the first positional argument.
782	
783	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
784	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
785	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
786	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
787	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
788	    positional argument:
789	
790	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
791	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
792	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
793	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
794	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
795	
796	    Note that when creating models and layers with
797	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
798	    about any of this, as you can just pass inputs like you would to any other Python function!
799	
800	    </Tip>
801	
802	    Parameters:
803	        config ([`TransfoXLConfig`]): Model configuration class with all the parameters of the model.
804	            Initializing with a config file does not load the weights associated with the model, only the
805	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
806	"""
807	
808	TRANSFO_XL_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py:901
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
900	        assert self.sample_softmax <= 0, (
901	            "Sampling from the softmax is not implemented yet. Please look at issue: #3310:"
902	            " https://github.com/huggingface/transformers/issues/3310"
903	        )
904	
905	        self.crit = TFAdaptiveSoftmaxMask(
906	            config.vocab_size, config.d_embed, config.d_model, config.cutoffs, div_val=config.div_val, name="crit"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py:122
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
121	        logger.error(
122	            "Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see "
123	            "https://www.tensorflow.org/install/ for installation instructions."
124	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py:706
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
705	
706	TRANSFO_XL_START_DOCSTRING = r"""
707	
708	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
709	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
710	    etc.)
711	
712	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
713	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
714	    and behavior.
715	
716	    Parameters:
717	        config ([`TransfoXLConfig`]): Model configuration class with all the parameters of the model.
718	            Initializing with a config file does not load the weights associated with the model, only the
719	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
720	"""
721	
722	TRANSFO_XL_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py:1020
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1019	        assert self.sample_softmax <= 0, (
1020	            "Sampling from the softmax is not implemented yet. Please look at issue: #3310:"
1021	            " https://github.com/huggingface/transformers/issues/3310"
1022	        )
1023	
1024	        self.crit = ProjectedAdaptiveLogSoftmax(
1025	            config.vocab_size, config.d_embed, config.d_model, config.cutoffs, div_val=config.div_val

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/transfo_xl/tokenization_transfo_xl.py:59
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
58	PRETRAINED_CORPUS_ARCHIVE_MAP = {
59	    "transfo-xl/transfo-xl-wt103": "https://huggingface.co/transfo-xl/transfo-xl-wt103/resolve/main/corpus.bin",
60	}
61	CORPUS_NAME = "corpus.bin"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/transfo_xl/tokenization_transfo_xl.py:174
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
173	        logger.error(
174	            "`TransfoXL` was deprecated due to security issues linked to `pickle.load` in `TransfoXLTokenizer`. "
175	            "See more details on this model's documentation page: "
176	            "`https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/transfo-xl.md`."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/tvlt/modeling_tvlt.py:601
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
600	
601	TVLT_START_DOCSTRING = r"""
602	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
603	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
604	    behavior.
605	
606	    Parameters:
607	        config ([`TvltConfig`]): Model configuration class with all the parameters of the model.
608	            Initializing with a config file does not load the weights associated with the model, only the
609	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
610	"""
611	
612	TVLT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/van/modeling_van.py:384
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
383	
384	VAN_START_DOCSTRING = r"""
385	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
386	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
387	    behavior.
388	
389	    Parameters:
390	        config ([`VanConfig`]): Model configuration class with all the parameters of the model.
391	            Initializing with a config file does not load the weights associated with the model, only the
392	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
393	"""
394	
395	VAN_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/vit_hybrid/modeling_vit_hybrid.py:527
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
526	
527	VIT_START_DOCSTRING = r"""
528	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
529	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
530	    behavior.
531	
532	    Parameters:
533	        config ([`ViTHybridConfig`]): Model configuration class with all the parameters of the model.
534	            Initializing with a config file does not load the weights associated with the model, only the
535	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
536	"""
537	
538	VIT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/xlm_prophetnet/modeling_xlm_prophetnet.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	
47	XLM_PROPHETNET_START_DOCSTRING = r"""
48	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
49	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
50	    etc.)
51	
52	    Original ProphetNet code can be found [here](https://github.com/microsoft/ProphetNet). Checkpoints were converted
53	    from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
54	    file `convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py`.
55	
56	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
57	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
58	    behavior.
59	
60	    Parameters:
61	        config ([`XLMProphetNetConfig`]): Model configuration class with all the parameters of the model.
62	            Initializing with a config file does not load the weights associated with the model, only the
63	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
64	"""
65	
66	XLM_PROPHETNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/xlm_prophetnet/tokenization_xlm_prophetnet.py:132
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
131	            logger.warning(
132	                "You need to install SentencePiece to use XLMRobertaTokenizer: https://github.com/google/sentencepiece"
133	                " pip install sentencepiece"
134	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/xlm_prophetnet/tokenization_xlm_prophetnet.py:187
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
186	            logger.warning(
187	                "You need to install SentencePiece to use XLMRobertaTokenizer: https://github.com/google/sentencepiece"
188	                " pip install sentencepiece"
189	            )

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/deprecated/xlm_prophetnet/tokenization_xlm_prophetnet.py:296
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
295	                content_spiece_model = self.sp_model.serialized_model_proto()
296	                fi.write(content_spiece_model)
297	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/depth_anything/modeling_depth_anything.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	
41	DEPTH_ANYTHING_START_DOCSTRING = r"""
42	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
43	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
44	    behavior.
45	
46	    Parameters:
47	        config ([`DepthAnythingConfig`]): Model configuration class with all the parameters of the model.
48	            Initializing with a config file does not load the weights associated with the model, only the
49	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
50	"""
51	
52	DEPTH_ANYTHING_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/depth_pro/modeling_depth_pro.py:637
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
636	
637	DEPTH_PRO_START_DOCSTRING = r"""
638	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
639	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
640	    behavior.
641	
642	    Parameters:
643	        config ([`DepthProConfig`]): Model configuration class with all the parameters of the model.
644	            Initializing with a config file does not load the weights associated with the model, only the
645	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
646	"""
647	
648	DEPTH_PRO_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/depth_pro/modeling_depth_pro.py:670
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
669	
670	DEPTH_PRO_FOR_DEPTH_ESTIMATION_START_DOCSTRING = r"""
671	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
672	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
673	    behavior.
674	
675	    Parameters:
676	        config ([`DepthProConfig`]): Model configuration class with all the parameters of the model.
677	            Initializing with a config file does not load the weights associated with the model, only the
678	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
679	        use_fov_model (`bool`, *optional*, defaults to `True`):
680	            Whether to use `DepthProFovModel` to generate the field of view.
681	"""
682	
683	
684	class DepthProPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/detr/modeling_detr.py:820
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
819	
820	DETR_START_DOCSTRING = r"""
821	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
822	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
823	    etc.)
824	
825	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
826	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
827	    and behavior.
828	
829	    Parameters:
830	        config ([`DetrConfig`]):
831	            Model configuration class with all the parameters of the model. Initializing with a config file does not
832	            load the weights associated with the model, only the configuration. Check out the
833	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
834	"""
835	
836	DETR_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/diffllama/modeling_diffllama.py:276
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
275	            raise ValueError(
276	                "`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` "
277	                "make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers"
278	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/diffllama/modeling_diffllama.py:582
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
581	
582	DIFFLLAMA_START_DOCSTRING = r"""
583	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
584	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
585	    etc.)
586	
587	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
588	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
589	    and behavior.
590	
591	    Parameters:
592	        config ([`DiffLlamaConfig`]):
593	            Model configuration class with all the parameters of the model. Initializing with a config file does not
594	            load the weights associated with the model, only the configuration. Check out the
595	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
596	"""
597	
598	
599	@add_start_docstrings(
600	    "The bare DiffLlama Model outputting raw hidden-states without any specific head on top.",
601	    DIFFLLAMA_START_DOCSTRING,
602	)
603	class DiffLlamaPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/diffllama/modeling_diffllama.py:663
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
662	
663	DIFFLLAMA_INPUTS_DOCSTRING = r"""
664	    Args:
665	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
666	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
667	            it.
668	
669	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
670	            [`PreTrainedTokenizer.__call__`] for details.
671	
672	            [What are input IDs?](../glossary#input-ids)
673	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
674	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
675	
676	            - 1 for tokens that are **not masked**,
677	            - 0 for tokens that are **masked**.
678	
679	            [What are attention masks?](../glossary#attention-mask)
680	
681	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
682	            [`PreTrainedTokenizer.__call__`] for details.
683	
684	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
685	            `past_key_values`).
686	
687	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
688	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
689	            information on the default strategy.
690	
691	            - 1 indicates the head is **not masked**,
692	            - 0 indicates the head is **masked**.
693	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
694	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
695	            config.n_positions - 1]`.
696	
697	            [What are position IDs?](../glossary#position-ids)
698	        past_key_values (`Cache`, *optional*):
699	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
700	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
701	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
702	
703	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
704	
705	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
706	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
707	            of shape `(batch_size, sequence_length)`.
708	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
709	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
710	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
711	            model's internal embedding lookup matrix.
712	        use_cache (`bool`, *optional*):
713	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
714	            `past_key_values`).
715	        output_attentions (`bool`, *optional*):
716	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
717	            tensors for more detail.
718	        output_hidden_states (`bool`, *optional*):
719	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
720	            more detail.
721	        return_dict (`bool`, *optional*):
722	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
723	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
724	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
725	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
726	            the complete sequence length.
727	"""
728	
729	
730	@add_start_docstrings(
731	    "The bare DiffLlama Model outputting raw hidden-states without any specific head on top.",
732	    DIFFLLAMA_START_DOCSTRING,
733	)
734	class DiffLlamaModel(DiffLlamaPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/diffllama/modular_diffllama.py:191
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
190	            raise ValueError(
191	                "`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` "
192	                "make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers"
193	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/dinat/modeling_dinat.py:644
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
643	
644	DINAT_START_DOCSTRING = r"""
645	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
646	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
647	    behavior.
648	
649	    Parameters:
650	        config ([`DinatConfig`]): Model configuration class with all the parameters of the model.
651	            Initializing with a config file does not load the weights associated with the model, only the
652	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
653	"""
654	
655	DINAT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/dinov2/modeling_dinov2.py:557
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
556	
557	DINOV2_START_DOCSTRING = r"""
558	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
559	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
560	    behavior.
561	
562	    Parameters:
563	        config ([`Dinov2Config`]): Model configuration class with all the parameters of the model.
564	            Initializing with a config file does not load the weights associated with the model, only the
565	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
566	"""
567	
568	DINOV2_BASE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/dinov2/modeling_flax_dinov2.py:39
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
38	
39	DINOV2_START_DOCSTRING = r"""
40	
41	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
42	    library implements for all its model (such as downloading, saving and converting weights from PyTorch models)
43	
44	    This model is also a
45	    [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. Use it as
46	    a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
47	    behavior.
48	
49	    Finally, this model supports inherent JAX features such as:
50	
51	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
52	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
53	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
54	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
55	
56	    Parameters:
57	        config ([`Dinov2Config`]): Model configuration class with all the parameters of the model.
58	            Initializing with a config file does not load the weights associated with the model, only the
59	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
60	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
61	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
62	            `jax.numpy.bfloat16` (on TPUs).
63	
64	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
65	            specified all the computation will be performed with the given `dtype`.
66	
67	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
68	            parameters.**
69	
70	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
71	            [`~FlaxPreTrainedModel.to_bf16`].
72	"""
73	
74	DINOV2_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/dinov2/modeling_flax_dinov2.py:678
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
677	
678	FLAX_VISION_MODEL_DOCSTRING = """
679	    Returns:
680	
681	    Examples:
682	
683	    ```python
684	    >>> from transformers import AutoImageProcessor, FlaxDinov2Model
685	    >>> from PIL import Image
686	    >>> import requests
687	
688	    >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
689	    >>> image = Image.open(requests.get(url, stream=True).raw)
690	
691	    >>> image_processor = AutoImageProcessor.from_pretrained("facebook/dinov2-base")
692	    >>> model = FlaxDinov2Model.from_pretrained("facebook/dinov2-base")
693	
694	    >>> inputs = image_processor(images=image, return_tensors="np")
695	    >>> outputs = model(**inputs)
696	    >>> last_hidden_states = outputs.last_hidden_state
697	    ```
698	"""
699	
700	overwrite_call_docstring(FlaxDinov2Model, FLAX_VISION_MODEL_DOCSTRING)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/dinov2/modeling_flax_dinov2.py:768
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
767	
768	FLAX_VISION_CLASSIFICATION_DOCSTRING = """
769	    Returns:
770	
771	    Example:
772	
773	    ```python
774	    >>> from transformers import AutoImageProcessor, FlaxDinov2ForImageClassification
775	    >>> from PIL import Image
776	    >>> import jax
777	    >>> import requests
778	
779	    >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
780	    >>> image = Image.open(requests.get(url, stream=True).raw)
781	
782	    >>> image_processor = AutoImageProcessor.from_pretrained("facebook/dinov2-base-imagenet1k-1-layer")
783	    >>> model = FlaxDinov2ForImageClassification.from_pretrained("facebook/dinov2-base-imagenet1k-1-layer", from_pt=True)
784	
785	    >>> inputs = image_processor(images=image, return_tensors="np")
786	    >>> outputs = model(**inputs)
787	    >>> logits = outputs.logits
788	
789	    >>> # model predicts one of the 1000 ImageNet classes
790	    >>> predicted_class_idx = jax.numpy.argmax(logits, axis=-1)
791	    >>> print("Predicted class:", model.config.id2label[predicted_class_idx.item()])
792	    ```
793	"""
794	
795	overwrite_call_docstring(FlaxDinov2ForImageClassification, FLAX_VISION_CLASSIFICATION_DOCSTRING)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py:568
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
567	
568	DINOV2_WITH_REGISTERS_START_DOCSTRING = r"""
569	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
570	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
571	    behavior.
572	
573	    Parameters:
574	        config ([`Dinov2WithRegistersConfig`]): Model configuration class with all the parameters of the model.
575	            Initializing with a config file does not load the weights associated with the model, only the
576	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
577	"""
578	
579	DINOV2_WITH_REGISTERS_BASE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/distilbert/modeling_distilbert.py:614
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
613	
614	DISTILBERT_START_DOCSTRING = r"""
615	
616	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
617	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
618	    etc.)
619	
620	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
621	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
622	    and behavior.
623	
624	    Parameters:
625	        config ([`DistilBertConfig`]): Model configuration class with all the parameters of the model.
626	            Initializing with a config file does not load the weights associated with the model, only the
627	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
628	"""
629	
630	DISTILBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/distilbert/modeling_flax_distilbert.py:46
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
45	
46	FLAX_DISTILBERT_START_DOCSTRING = r"""
47	
48	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
49	    library implements for all its model (such as downloading, saving and converting weights from PyTorch models)
50	
51	    This model is also a
52	    [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. Use it as
53	    a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
54	    behavior.
55	
56	    Finally, this model supports inherent JAX features such as:
57	
58	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
59	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
60	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
61	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
62	
63	    Parameters:
64	        config ([`DistilBertConfig`]): Model configuration class with all the parameters of the model.
65	            Initializing with a config file does not load the weights associated with the model, only the
66	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
67	"""
68	
69	DISTILBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/distilbert/modeling_tf_distilbert.py:490
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
489	
490	DISTILBERT_START_DOCSTRING = r"""
491	
492	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
493	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
494	    etc.)
495	
496	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
497	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
498	    behavior.
499	
500	    <Tip>
501	
502	    TensorFlow models and layers in `transformers` accept two formats as input:
503	
504	    - having all inputs as keyword arguments (like PyTorch models), or
505	    - having all inputs as a list, tuple or dict in the first positional argument.
506	
507	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
508	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
509	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
510	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
511	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
512	    positional argument:
513	
514	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
515	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
516	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
517	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
518	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
519	
520	    Note that when creating models and layers with
521	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
522	    about any of this, as you can just pass inputs like you would to any other Python function!
523	
524	    </Tip>
525	
526	    Parameters:
527	        config ([`DistilBertConfig`]): Model configuration class with all the parameters of the model.
528	            Initializing with a config file does not load the weights associated with the model, only the
529	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
530	"""
531	
532	DISTILBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/distilbert/tokenization_distilbert.py:297
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
296	                    index = token_index
297	                writer.write(token + "\n")
298	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/donut/modeling_donut_swin.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
48	# Base docstring
49	_CHECKPOINT_FOR_DOC = "https://huggingface.co/naver-clova-ix/donut-base"
50	_EXPECTED_OUTPUT_SHAPE = [1, 49, 768]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/donut/modeling_donut_swin.py:881
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
880	
881	SWIN_START_DOCSTRING = r"""
882	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
883	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
884	    behavior.
885	
886	    Parameters:
887	        config ([`DonutSwinConfig`]): Model configuration class with all the parameters of the model.
888	            Initializing with a config file does not load the weights associated with the model, only the
889	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
890	"""
891	
892	SWIN_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/dpr/modeling_dpr.py:312
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
311	
312	DPR_START_DOCSTRING = r"""
313	
314	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
315	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
316	    etc.)
317	
318	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
319	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
320	    and behavior.
321	
322	    Parameters:
323	        config ([`DPRConfig`]): Model configuration class with all the parameters of the model.
324	            Initializing with a config file does not load the weights associated with the model, only the
325	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
326	"""
327	
328	DPR_ENCODERS_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/dpr/modeling_tf_dpr.py:393
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
392	
393	TF_DPR_START_DOCSTRING = r"""
394	
395	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
396	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
397	    etc.)
398	
399	    This model is also a Tensorflow [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
400	    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
401	    general usage and behavior.
402	
403	    <Tip>
404	
405	    TensorFlow models and layers in `transformers` accept two formats as input:
406	
407	    - having all inputs as keyword arguments (like PyTorch models), or
408	    - having all inputs as a list, tuple or dict in the first positional argument.
409	
410	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
411	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
412	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
413	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
414	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
415	    positional argument:
416	
417	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
418	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
419	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
420	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
421	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
422	
423	    Note that when creating models and layers with
424	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
425	    about any of this, as you can just pass inputs like you would to any other Python function!
426	
427	    </Tip>
428	
429	    Parameters:
430	        config ([`DPRConfig`]): Model configuration class with all the parameters of the model.
431	            Initializing with a config file does not load the weights associated with the model, only the
432	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
433	"""
434	
435	TF_DPR_ENCODERS_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/dpt/modeling_dpt.py:863
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
862	
863	DPT_START_DOCSTRING = r"""
864	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
865	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
866	    behavior.
867	
868	    Parameters:
869	        config ([`ViTConfig`]): Model configuration class with all the parameters of the model.
870	            Initializing with a config file does not load the weights associated with the model, only the
871	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
872	"""
873	
874	DPT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/efficientnet/modeling_efficientnet.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	
55	EFFICIENTNET_START_DOCSTRING = r"""
56	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
57	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
58	    behavior.
59	
60	    Parameters:
61	        config ([`EfficientNetConfig`]): Model configuration class with all the parameters of the model.
62	            Initializing with a config file does not load the weights associated with the model, only the
63	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
64	"""
65	
66	EFFICIENTNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/electra/modeling_electra.py:67
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
66	        logger.error(
67	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
68	            "https://www.tensorflow.org/install/ for installation instructions."
69	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/electra/modeling_electra.py:719
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
718	
719	ELECTRA_START_DOCSTRING = r"""
720	
721	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
722	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
723	    etc.)
724	
725	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
726	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
727	    and behavior.
728	
729	    Parameters:
730	        config ([`ElectraConfig`]): Model configuration class with all the parameters of the model.
731	            Initializing with a config file does not load the weights associated with the model, only the
732	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
733	"""
734	
735	ELECTRA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/electra/modeling_flax_electra.py:85
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
84	
85	ELECTRA_START_DOCSTRING = r"""
86	
87	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
88	    library implements for all its model (such as downloading, saving and converting weights from PyTorch models)
89	
90	    This model is also a Flax Linen
91	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
92	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
93	
94	    Finally, this model supports inherent JAX features such as:
95	
96	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
97	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
98	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
99	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
100	
101	    Parameters:
102	        config ([`ElectraConfig`]): Model configuration class with all the parameters of the model.
103	            Initializing with a config file does not load the weights associated with the model, only the
104	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
105	"""
106	
107	ELECTRA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/electra/modeling_tf_electra.py:939
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
938	
939	ELECTRA_START_DOCSTRING = r"""
940	
941	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
942	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
943	    etc.)
944	
945	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
946	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
947	    behavior.
948	
949	    <Tip>
950	
951	    TensorFlow models and layers in `transformers` accept two formats as input:
952	
953	    - having all inputs as keyword arguments (like PyTorch models), or
954	    - having all inputs as a list, tuple or dict in the first positional argument.
955	
956	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
957	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
958	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
959	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
960	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
961	    positional argument:
962	
963	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
964	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
965	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
966	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
967	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
968	
969	    Note that when creating models and layers with
970	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
971	    about any of this, as you can just pass inputs like you would to any other Python function!
972	
973	    </Tip>
974	
975	    Parameters:
976	        config ([`ElectraConfig`]): Model configuration class with all the parameters of the model.
977	            Initializing with a config file does not load the weights associated with the model, only the
978	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
979	"""
980	
981	ELECTRA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/electra/tokenization_electra.py:286
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
285	                    index = token_index
286	                writer.write(token + "\n")
287	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/emu3/modeling_emu3.py:979
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
978	
979	EMU3_VQ_START_DOCSTRING = r"""
980	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
981	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
982	    etc.)
983	
984	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
985	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
986	    and behavior.
987	
988	    Parameters:
989	        config ([`Emu3VQVAEConfig`]):
990	            Model configuration class with all the parameters of the model. Initializing with a config file does not
991	            load the weights associated with the model, only the configuration. Check out the
992	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
993	"""
994	
995	
996	@add_start_docstrings(
997	    """The VQ-VAE model used in Emu3 for encoding/decoding images into discrete tokens.
998	    This model follows the "Make-a-scene: Scene-based text-to-image generation with human priors" paper from
999	    [ Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman](https://arxiv.org/abs/2203.13131).
1000	    """,
1001	    EMU3_VQ_START_DOCSTRING,
1002	)
1003	class Emu3VQVAE(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/emu3/modeling_emu3.py:997
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
996	@add_start_docstrings(
997	    """The VQ-VAE model used in Emu3 for encoding/decoding images into discrete tokens.
998	    This model follows the "Make-a-scene: Scene-based text-to-image generation with human priors" paper from
999	    [ Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman](https://arxiv.org/abs/2203.13131).
1000	    """,
1001	    EMU3_VQ_START_DOCSTRING,
1002	)
1003	class Emu3VQVAE(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/emu3/modeling_emu3.py:1159
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1158	
1159	EMU3_START_DOCSTRING = r"""
1160	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1161	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1162	    etc.)
1163	
1164	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1165	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1166	    and behavior.
1167	
1168	    Parameters:
1169	        config ([`Emu3Config`]):
1170	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1171	            load the weights associated with the model, only the configuration. Check out the
1172	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1173	"""
1174	
1175	
1176	@add_start_docstrings(
1177	    "The bare emu3 Model outputting raw hidden-states without any specific head on top.",
1178	    EMU3_START_DOCSTRING,
1179	)
1180	class Emu3PreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/emu3/modeling_emu3.py:1244
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1243	
1244	EMU3_TEXT_INPUTS_DOCSTRING = r"""
1245	    Args:
1246	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
1247	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
1248	            it.
1249	
1250	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1251	            [`PreTrainedTokenizer.__call__`] for details.
1252	
1253	            [What are input IDs?](../glossary#input-ids)
1254	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
1255	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
1256	
1257	            - 1 for tokens that are **not masked**,
1258	            - 0 for tokens that are **masked**.
1259	
1260	            [What are attention masks?](../glossary#attention-mask)
1261	
1262	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1263	            [`PreTrainedTokenizer.__call__`] for details.
1264	
1265	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
1266	            `past_key_values`).
1267	
1268	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
1269	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
1270	            information on the default strategy.
1271	
1272	            - 1 indicates the head is **not masked**,
1273	            - 0 indicates the head is **masked**.
1274	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
1275	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
1276	            config.n_positions - 1]`.
1277	
1278	            [What are position IDs?](../glossary#position-ids)
1279	        past_key_values (`Cache`, *optional*):
1280	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
1281	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
1282	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
1283	
1284	            Has to be an instance of [`~cache_utils.Cache`] instance, see our
1285	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
1286	
1287	            The model will output the same cache type that is fed as input. If no `past_key_values` are passed, the
1288	            legacy cache format will be returned.
1289	
1290	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
1291	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
1292	            of shape `(batch_size, sequence_length)`.
1293	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
1294	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
1295	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
1296	            model's internal embedding lookup matrix.
1297	        use_cache (`bool`, *optional*):
1298	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
1299	            `past_key_values`).
1300	        output_attentions (`bool`, *optional*):
1301	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1302	            tensors for more detail.
1303	        output_hidden_states (`bool`, *optional*):
1304	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1305	            more detail.
1306	        return_dict (`bool`, *optional*):
1307	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1308	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
1309	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
1310	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
1311	            the complete sequence length.
1312	"""
1313	
1314	
1315	@add_start_docstrings(
1316	    "The bare Emu3Text Model outputting raw hidden-states without any specific head on top.",
1317	    EMU3_START_DOCSTRING,
1318	)
1319	class Emu3TextModel(Emu3PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/emu3/modeling_emu3.py:1710
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1709	
1710	EMU3_INPUTS_DOCSTRING = r"""
1711	    Args:
1712	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
1713	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
1714	            it.
1715	
1716	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1717	            [`PreTrainedTokenizer.__call__`] for details.
1718	
1719	            [What are input IDs?](../glossary#input-ids)
1720	        pixel_values (`torch.FloatTensor` of shape `(batch_size, max_num_images, max_num_tiles, channels, image_size, image_size)):
1721	            The tensors corresponding to the input images. Pixel values can be obtained using
1722	            [`AutoImageProcessor`]. See [`Emu3ImageProcessor.__call__`] for details ([]`Emu3Processor`] uses
1723	            [`Emu3ImageProcessor`] for processing images).
1724	        image_sizes (`torch.LongTensor` of shape `(batch_size, 2)`):
1725	                The sizes of the images in the batch, being (height, width) for each image. Image sizes can be obtained using
1726	            [`AutoImageProcessor`]. See [`Emu3ImageProcessor.__call__`] for details ([]`Emu3Processor`] uses
1727	            [`Emu3ImageProcessor`] for processing images).
1728	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
1729	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
1730	
1731	            - 1 for tokens that are **not masked**,
1732	            - 0 for tokens that are **masked**.
1733	
1734	            [What are attention masks?](../glossary#attention-mask)
1735	
1736	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1737	            [`PreTrainedTokenizer.__call__`] for details.
1738	
1739	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
1740	            `past_key_values`).
1741	
1742	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
1743	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
1744	            information on the default strategy.
1745	
1746	            - 1 indicates the head is **not masked**,
1747	            - 0 indicates the head is **masked**.
1748	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
1749	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
1750	            config.n_positions - 1]`.
1751	
1752	            [What are position IDs?](../glossary#position-ids)
1753	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
1754	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
1755	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
1756	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
1757	
1758	            Has to be an instance of [`~cache_utils.Cache`] instance, see our
1759	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
1760	
1761	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
1762	            legacy cache format will be returned.
1763	
1764	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
1765	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
1766	            of shape `(batch_size, sequence_length)`.
1767	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
1768	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
1769	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
1770	            model's internal embedding lookup matrix.
1771	        use_cache (`bool`, *optional*):
1772	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
1773	            `past_key_values`).
1774	        output_attentions (`bool`, *optional*):
1775	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1776	            tensors for more detail.
1777	        output_hidden_states (`bool`, *optional*):
1778	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1779	            more detail.
1780	        return_dict (`bool`, *optional*):
1781	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1782	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
1783	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
1784	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
1785	            the complete sequence length.
1786	"""
1787	
1788	
1789	class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/emu3/modular_emu3.py:709
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
708	
709	EMU3_VQ_START_DOCSTRING = r"""
710	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
711	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
712	    etc.)
713	
714	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
715	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
716	    and behavior.
717	
718	    Parameters:
719	        config ([`Emu3VQVAEConfig`]):
720	            Model configuration class with all the parameters of the model. Initializing with a config file does not
721	            load the weights associated with the model, only the configuration. Check out the
722	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
723	"""
724	
725	
726	@add_start_docstrings(
727	    """The VQ-VAE model used in Emu3 for encoding/decoding images into discrete tokens.
728	    This model follows the "Make-a-scene: Scene-based text-to-image generation with human priors" paper from
729	    [ Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman](https://arxiv.org/abs/2203.13131).
730	    """,
731	    EMU3_VQ_START_DOCSTRING,
732	)
733	class Emu3VQVAE(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/emu3/modular_emu3.py:727
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
726	@add_start_docstrings(
727	    """The VQ-VAE model used in Emu3 for encoding/decoding images into discrete tokens.
728	    This model follows the "Make-a-scene: Scene-based text-to-image generation with human priors" paper from
729	    [ Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman](https://arxiv.org/abs/2203.13131).
730	    """,
731	    EMU3_VQ_START_DOCSTRING,
732	)
733	class Emu3VQVAE(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/emu3/modular_emu3.py:909
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
908	
909	EMU3_TEXT_INPUTS_DOCSTRING = r"""
910	    Args:
911	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
912	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
913	            it.
914	
915	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
916	            [`PreTrainedTokenizer.__call__`] for details.
917	
918	            [What are input IDs?](../glossary#input-ids)
919	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
920	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
921	
922	            - 1 for tokens that are **not masked**,
923	            - 0 for tokens that are **masked**.
924	
925	            [What are attention masks?](../glossary#attention-mask)
926	
927	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
928	            [`PreTrainedTokenizer.__call__`] for details.
929	
930	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
931	            `past_key_values`).
932	
933	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
934	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
935	            information on the default strategy.
936	
937	            - 1 indicates the head is **not masked**,
938	            - 0 indicates the head is **masked**.
939	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
940	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
941	            config.n_positions - 1]`.
942	
943	            [What are position IDs?](../glossary#position-ids)
944	        past_key_values (`Cache`, *optional*):
945	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
946	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
947	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
948	
949	            Has to be an instance of [`~cache_utils.Cache`] instance, see our
950	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
951	
952	            The model will output the same cache type that is fed as input. If no `past_key_values` are passed, the
953	            legacy cache format will be returned.
954	
955	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
956	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
957	            of shape `(batch_size, sequence_length)`.
958	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
959	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
960	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
961	            model's internal embedding lookup matrix.
962	        use_cache (`bool`, *optional*):
963	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
964	            `past_key_values`).
965	        output_attentions (`bool`, *optional*):
966	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
967	            tensors for more detail.
968	        output_hidden_states (`bool`, *optional*):
969	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
970	            more detail.
971	        return_dict (`bool`, *optional*):
972	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
973	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
974	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
975	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
976	            the complete sequence length.
977	"""
978	
979	
980	EMU3_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/emu3/modular_emu3.py:980
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
979	
980	EMU3_INPUTS_DOCSTRING = r"""
981	    Args:
982	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
983	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
984	            it.
985	
986	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
987	            [`PreTrainedTokenizer.__call__`] for details.
988	
989	            [What are input IDs?](../glossary#input-ids)
990	        pixel_values (`torch.FloatTensor` of shape `(batch_size, max_num_images, max_num_tiles, channels, image_size, image_size)):
991	            The tensors corresponding to the input images. Pixel values can be obtained using
992	            [`AutoImageProcessor`]. See [`Emu3ImageProcessor.__call__`] for details ([]`Emu3Processor`] uses
993	            [`Emu3ImageProcessor`] for processing images).
994	        image_sizes (`torch.LongTensor` of shape `(batch_size, 2)`):
995	                The sizes of the images in the batch, being (height, width) for each image. Image sizes can be obtained using
996	            [`AutoImageProcessor`]. See [`Emu3ImageProcessor.__call__`] for details ([]`Emu3Processor`] uses
997	            [`Emu3ImageProcessor`] for processing images).
998	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
999	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
1000	
1001	            - 1 for tokens that are **not masked**,
1002	            - 0 for tokens that are **masked**.
1003	
1004	            [What are attention masks?](../glossary#attention-mask)
1005	
1006	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1007	            [`PreTrainedTokenizer.__call__`] for details.
1008	
1009	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
1010	            `past_key_values`).
1011	
1012	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
1013	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
1014	            information on the default strategy.
1015	
1016	            - 1 indicates the head is **not masked**,
1017	            - 0 indicates the head is **masked**.
1018	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
1019	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
1020	            config.n_positions - 1]`.
1021	
1022	            [What are position IDs?](../glossary#position-ids)
1023	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
1024	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
1025	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
1026	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
1027	
1028	            Has to be an instance of [`~cache_utils.Cache`] instance, see our
1029	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
1030	
1031	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
1032	            legacy cache format will be returned.
1033	
1034	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
1035	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
1036	            of shape `(batch_size, sequence_length)`.
1037	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
1038	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
1039	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
1040	            model's internal embedding lookup matrix.
1041	        use_cache (`bool`, *optional*):
1042	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
1043	            `past_key_values`).
1044	        output_attentions (`bool`, *optional*):
1045	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1046	            tensors for more detail.
1047	        output_hidden_states (`bool`, *optional*):
1048	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1049	            more detail.
1050	        return_dict (`bool`, *optional*):
1051	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1052	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
1053	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
1054	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
1055	            the complete sequence length.
1056	"""
1057	
1058	
1059	class Emu3TextModel(LlamaModel, Emu3PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/encodec/modeling_encodec.py:486
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
485	
486	ENCODEC_START_DOCSTRING = r"""
487	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
488	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
489	    etc.)
490	
491	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
492	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
493	    and behavior.
494	
495	    Parameters:
496	        config ([`EncodecConfig`]):
497	            Model configuration class with all the parameters of the model. Initializing with a config file does not
498	            load the weights associated with the model, only the configuration. Check out the
499	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
500	"""
501	
502	
503	ENCODEC_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
48	
49	ENCODER_DECODER_START_DOCSTRING = r"""
50	    This class can be used to initialize a sequence-to-sequence model with any pretrained autoencoding model as the
51	    encoder and any pretrained autoregressive model as the decoder. The encoder is loaded via
52	    [`~AutoModel.from_pretrained`] function and the decoder is loaded via [`~AutoModelForCausalLM.from_pretrained`]
53	    function. Cross-attention layers are automatically added to the decoder and should be fine-tuned on a downstream
54	    generative task, like summarization.
55	
56	    The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
57	    tasks was shown in [Leveraging Pre-trained Checkpoints for Sequence Generation
58	    Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
59	    Zhou, Wei Li, Peter J. Liu.
60	
61	    After such an Encoder Decoder model has been trained/fine-tuned, it can be saved/loaded just like any other models
62	    (see the examples for more information).
63	
64	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
65	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
66	    etc.)
67	
68	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
69	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
70	    and behavior.
71	
72	    Parameters:
73	        config ([`EncoderDecoderConfig`]): Model configuration class with all the parameters of the model.
74	            Initializing with a config file does not load the weights associated with the model, only the
75	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
76	"""
77	
78	ENCODER_DECODER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py:259
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
258	            raise ValueError(
259	                "The selected decoder is not prepared for the encoder hidden states to be passed. Please see the "
260	                "following discussion on GitHub: https://github.com/huggingface/transformers/issues/23350"
261	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py:40
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
39	
40	ENCODER_DECODER_START_DOCSTRING = r"""
41	    This class can be used to initialize a sequence-to-sequence model with any pretrained autoencoding model as the
42	    encoder and any pretrained autoregressive model as the decoder. The encoder is loaded via
43	    [`~AutoModel.from_pretrained`] function and the decoder is loaded via [`~AutoModelForCausalLM.from_pretrained`]
44	    function. Cross-attention layers are automatically added to the decoder and should be fine-tuned on a downstream
45	    generative task, like summarization.
46	
47	    The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
48	    tasks was shown in [Leveraging Pre-trained Checkpoints for Sequence Generation
49	    Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
50	    Zhou, Wei Li, Peter J. Liu.
51	
52	    After such an Encoder Decoder model has been trained/fine-tuned, it can be saved/loaded just like any other models
53	    (see the examples for more information).
54	
55	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
56	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
57	    etc.)
58	
59	    This model is also a Flax Linen
60	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
61	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
62	
63	    Parameters:
64	        config ([`EncoderDecoderConfig`]): Model configuration class with all the parameters of the model.
65	            Initializing with a config file does not load the weights associated with the model, only the
66	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
67	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
68	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
69	            `jax.numpy.bfloat16` (on TPUs).
70	
71	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
72	            specified all the computation will be performed with the given `dtype`.
73	
74	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
75	            parameters.**
76	
77	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
78	            [`~FlaxPreTrainedModel.to_bf16`].
79	"""
80	
81	ENCODER_DECODER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py:61
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
60	
61	ENCODER_DECODER_START_DOCSTRING = r"""
62	    This class can be used to initialize a sequence-to-sequence model with any pretrained autoencoding model as the
63	    encoder and any pretrained autoregressive model as the decoder. The encoder is loaded via
64	    [`~TFAutoModel.from_pretrained`] function and the decoder is loaded via [`~TFAutoModelForCausalLM.from_pretrained`]
65	    function. Cross-attention layers are automatically added to the decoder and should be fine-tuned on a downstream
66	    generative task, like summarization.
67	
68	    The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
69	    tasks was shown in [Leveraging Pre-trained Checkpoints for Sequence Generation
70	    Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
71	    Zhou, Wei Li, Peter J. Liu.
72	
73	    After such an Encoder Decoder model has been trained/fine-tuned, it can be saved/loaded just like any other models
74	    (see the examples for more information).
75	
76	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
77	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
78	    etc.)
79	
80	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
81	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
82	    behavior.
83	
84	    Parameters:
85	        config ([`EncoderDecoderConfig`]): Model configuration class with all the parameters of the model.
86	            Initializing with a config file does not load the weights associated with the model, only the
87	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
88	"""
89	
90	ENCODER_DECODER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py:275
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
274	            raise ValueError(
275	                "The selected decoder is not prepared for the encoder hidden states to be passed. Please see the "
276	                "following discussion on GitHub: https://github.com/huggingface/transformers/issues/23350"
277	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/ernie/modeling_ernie.py:709
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
708	
709	ERNIE_START_DOCSTRING = r"""
710	
711	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
712	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
713	    etc.)
714	
715	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
716	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
717	    and behavior.
718	
719	    Parameters:
720	        config ([`ErnieConfig`]): Model configuration class with all the parameters of the model.
721	            Initializing with a config file does not load the weights associated with the model, only the
722	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
723	"""
724	
725	ERNIE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/esm/modeling_esm.py:704
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
703	
704	ESM_START_DOCSTRING = r"""
705	
706	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
707	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
708	    etc.)
709	
710	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
711	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
712	    and behavior.
713	
714	    Parameters:
715	        config ([`EsmConfig`]): Model configuration class with all the parameters of the
716	            model. Initializing with a config file does not load the weights associated with the model, only the
717	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
718	"""
719	
720	ESM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/esm/modeling_tf_esm.py:811
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
810	
811	ESM_START_DOCSTRING = r"""
812	
813	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
814	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
815	    etc.)
816	
817	    This model is also a Keras [Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it as a
818	    regular Keras model and refer to the TF/Keras documentation for all matters related to general usage and behavior.
819	
820	    Parameters:
821	        config ([`EsmConfig`]): Model configuration class with all the parameters of the
822	            model. Initializing with a config file does not load the weights associated with the model, only the
823	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
824	"""
825	
826	ESM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/esm/tokenization_esm.py:31
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
30	    with open(vocab_file, "r") as f:
31	        lines = f.read().splitlines()
32	        return [l.strip() for l in lines]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/esm/tokenization_esm.py:139
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
138	        with open(vocab_file, "w") as f:
139	            f.write("\n".join(self.all_tokens))
140	        return (vocab_file,)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/falcon/modeling_falcon.py:659
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
658	
659	FALCON_START_DOCSTRING = r"""
660	
661	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
662	    library implements for all its model (such as downloading or saving, resizing the input embeddings etc.)
663	
664	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
665	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
666	    and behavior.
667	
668	    Parameters:
669	        config ([`FalconConfig`]): Model configuration class with all the parameters of the model.
670	            Initializing with a config file does not load the weights associated with the model, only the
671	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
672	"""
673	
674	FALCON_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/falcon/modeling_falcon.py:674
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
673	
674	FALCON_INPUTS_DOCSTRING = r"""
675	    Args:
676	        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):
677	            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`
678	            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.
679	
680	            If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as
681	            `input_ids`.
682	
683	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
684	            [`PreTrainedTokenizer.__call__`] for details.
685	
686	            [What are input IDs?](../glossary#input-ids)
687	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
688	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
689	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
690	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
691	
692	            Two formats are allowed:
693	            - a [`~cache_utils.Cache`] instance, see our
694	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
695	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
696	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
697	            cache format.
698	
699	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
700	            legacy cache format will be returned.
701	
702	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
703	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
704	            of shape `(batch_size, sequence_length)`.
705	        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):
706	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
707	
708	            - 1 for tokens that are **not masked**,
709	            - 0 for tokens that are **masked**.
710	
711	            [What are attention masks?](../glossary#attention-mask)
712	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
713	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
714	            config.n_positions - 1]`.
715	
716	            [What are position IDs?](../glossary#position-ids)
717	        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):
718	            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:
719	
720	            - 1 indicates the head is **not masked**,
721	            - 0 indicates the head is **masked**.
722	
723	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
724	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
725	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
726	            model's internal embedding lookup matrix.
727	
728	            If `past_key_values` is used, optionally only the last `inputs_embeds` have to be input (see
729	            `past_key_values`).
730	        use_cache (`bool`, *optional*):
731	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
732	            `past_key_values`).
733	        output_attentions (`bool`, *optional*):
734	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
735	            tensors for more detail.
736	        output_hidden_states (`bool`, *optional*):
737	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
738	            more detail.
739	        return_dict (`bool`, *optional*):
740	            Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.
741	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
742	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
743	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
744	            the complete sequence length.
745	"""
746	
747	
748	class FalconPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/falcon/modeling_falcon.py:880
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
879	                logger.warning_once(
880	                    "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and "
881	                    "will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class "
882	                    "(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/falcon_mamba/modeling_falcon_mamba.py:150
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
149	                    logger.warning_once(
150	                        "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`"
151	                        " is None. Falling back to the mamba.py backend. To install follow https://github.com/state-spaces/mamba/#installation and"
152	                        " https://github.com/Dao-AILab/causal-conv1d"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/falcon_mamba/modeling_falcon_mamba.py:156
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
155	                    raise ImportError(
156	                        "use_mambapy is set to True but the mambapy package is not installed. To install it follow https://github.com/alxndrTL/mamba.py."
157	                    )
158	            else:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/falcon_mamba/modeling_falcon_mamba.py:160
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
159	                logger.warning_once(
160	                    "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`"
161	                    " is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation and"
162	                    " https://github.com/Dao-AILab/causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/falcon_mamba/modeling_falcon_mamba.py:561
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
560	
561	FALCONMAMBA_START_DOCSTRING = r"""
562	
563	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
564	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
565	    etc.)
566	
567	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
568	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
569	    and behavior.
570	
571	    Parameters:
572	        config ([`FalconMambaConfig`]): Model configuration class with all the parameters of the model.
573	            Initializing with a config file does not load the weights associated with the model, only the
574	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
575	"""
576	
577	FALCONMAMBA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py:141
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
140	
141	FASTSPEECH2_CONFORMER_START_DOCSTRING = r"""
142	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
143	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
144	    etc.)
145	
146	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
147	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
148	    and behavior.
149	
150	    Parameters:
151	        config ([`FastSpeech2ConformerConfig`]):
152	            Model configuration class with all the parameters of the model. Initializing with a config file does not
153	            load the weights associated with the model, only the configuration. Check out the
154	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
155	"""
156	
157	
158	HIFIGAN_START_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py:158
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
157	
158	HIFIGAN_START_DOCSTRING = r"""
159	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
160	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
161	    etc.)
162	
163	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
164	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
165	    and behavior.
166	
167	    Parameters:
168	        config ([`FastSpeech2ConformerConfig`]):
169	            Model configuration class with all the parameters of the model. Initializing with a config file does not
170	            load the weights associated with the model, only the configuration. Check out the
171	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
172	"""
173	
174	FASTSPEECH2_CONFORMER_WITH_HIFIGAN_START_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py:174
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
173	
174	FASTSPEECH2_CONFORMER_WITH_HIFIGAN_START_DOCSTRING = r"""
175	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
176	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
177	    etc.)
178	
179	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
180	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
181	    and behavior.
182	
183	    Parameters:
184	        config ([`FastSpeech2ConformerWithHifiGanConfig`]):
185	            Model configuration class with all the parameters of the model. Initializing with a config file does not
186	            load the weights associated with the model, only the configuration. Check out the
187	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
188	"""
189	
190	
191	def length_regulator(encoded_embeddings, duration_labels, speaking_speed=1.0):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/fastspeech2_conformer/tokenization_fastspeech2_conformer.py:165
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
164	        with open(vocab_file, "w", encoding="utf-8") as f:
165	            f.write(json.dumps(self.get_vocab(), ensure_ascii=False))
166	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/fastspeech2_conformer/tokenization_fastspeech2_conformer.py:183
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
182	            raise ImportError(
183	                "You need to install g2p-en to use FastSpeech2ConformerTokenizer. "
184	                "See https://pypi.org/project/g2p-en/ for installation."
185	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/flaubert/modeling_flaubert.py:210
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
209	
210	FLAUBERT_START_DOCSTRING = r"""
211	
212	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
213	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
214	    etc.)
215	
216	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
217	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
218	    and behavior.
219	
220	    Parameters:
221	        config ([`FlaubertConfig`]): Model configuration class with all the parameters of the model.
222	            Initializing with a config file does not load the weights associated with the model, only the
223	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
224	"""
225	
226	FLAUBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/flaubert/modeling_tf_flaubert.py:70
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
69	
70	FLAUBERT_START_DOCSTRING = r"""
71	
72	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
73	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
74	    etc.)
75	
76	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
77	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
78	    behavior.
79	
80	    <Tip>
81	
82	    TensorFlow models and layers in `transformers` accept two formats as input:
83	
84	    - having all inputs as keyword arguments (like PyTorch models), or
85	    - having all inputs as a list, tuple or dict in the first positional argument.
86	
87	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
88	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
89	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
90	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
91	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
92	    positional argument:
93	
94	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
95	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
96	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
97	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
98	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
99	
100	    Note that when creating models and layers with
101	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
102	    about any of this, as you can just pass inputs like you would to any other Python function!
103	
104	    </Tip>
105	
106	    Parameters:
107	        config ([`FlaubertConfig`]): Model configuration class with all the parameters of the model.
108	            Initializing with a config file does not load the weights associated with the model, only the
109	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
110	"""
111	
112	FLAUBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/flaubert/tokenization_flaubert.py:220
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
219	            raise ImportError(
220	                "You need to install sacremoses to use FlaubertTokenizer. "
221	                "See https://pypi.org/project/sacremoses/ for installation."
222	            )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/flaubert/tokenization_flaubert.py:243
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
242	        with open(merges_file, encoding="utf-8") as merges_handle:
243	            merges = merges_handle.read().split("\n")[:-1]
244	        merges = [tuple(merge.split()[:2]) for merge in merges]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/flaubert/tokenization_flaubert.py:303
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
302	                logger.error(
303	                    "Make sure you install KyTea (https://github.com/neubig/kytea) and it's python wrapper"
304	                    " (https://github.com/chezou/Mykytea-python) with the following steps"
305	                )

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/flaubert/tokenization_flaubert.py:531
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
530	        with open(vocab_file, "w", encoding="utf-8") as f:
531	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
532	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/flaubert/tokenization_flaubert.py:542
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
541	                    index = token_index
542	                writer.write(" ".join(bpe_tokens) + "\n")
543	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/flaubert/tokenization_flaubert.py:561
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
560	            raise ImportError(
561	                "You need to install sacremoses to use XLMTokenizer. "
562	                "See https://pypi.org/project/sacremoses/ for installation."
563	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/flava/modeling_flava.py:708
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
707	
708	FLAVA_START_DOCSTRING = r"""
709	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
710	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
711	    behavior.
712	
713	    Parameters:
714	        config ([`{config}`]): Model configuration class with all the parameters of the model.
715	            Initializing with a config file does not load the weights associated with the model, only the
716	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
717	"""
718	
719	FLAVA_INPUTS_DOCSTRING_COMMON = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/flava/modeling_flava.py:1299
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1298	    ) -> torch.FloatTensor:
1299	        r"""
1300	        Returns:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/flava/modeling_flava.py:1583
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1582	    def get_codebook_indices(self, pixel_values: torch.Tensor) -> torch.Tensor:
1583	        """
1584	        Args:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/flava/modeling_flava.py:1615
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1614	    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:
1615	        """
1616	        Args:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/fnet/modeling_fnet.py:452
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
451	
452	FNET_START_DOCSTRING = r"""
453	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
454	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
455	    behavior.
456	
457	    Parameters:
458	        config ([`FNetConfig`]): Model configuration class with all the parameters of the model.
459	            Initializing with a config file does not load the weights associated with the model, only the
460	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
461	"""
462	
463	FNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/fnet/tokenization_fnet.py:336
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
335	                content_spiece_model = self.sp_model.serialized_model_proto()
336	                fi.write(content_spiece_model)
337	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/focalnet/modeling_focalnet.py:656
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
655	
656	FOCALNET_START_DOCSTRING = r"""
657	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
658	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
659	    behavior.
660	
661	    Parameters:
662	        config ([`FocalNetConfig`]): Model configuration class with all the parameters of the model.
663	            Initializing with a config file does not load the weights associated with the model, only the
664	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
665	"""
666	
667	FOCALNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/focalnet/modeling_focalnet.py:762
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
761	@add_start_docstrings(
762	    """FocalNet Model with a decoder on top for masked image modeling.
763	
764	    This follows the same implementation as in [SimMIM](https://arxiv.org/abs/2111.09886).
765	
766	    <Tip>
767	
768	    Note that we provide a script to pre-train this model on custom data in our [examples
769	    directory](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining).
770	
771	    </Tip>
772	    """,
773	    FOCALNET_START_DOCSTRING,
774	)
775	class FocalNetForMaskedImageModeling(FocalNetPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/fsmt/modeling_fsmt.py:182
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
181	
182	FSMT_START_DOCSTRING = r"""
183	
184	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
185	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
186	    etc.)
187	
188	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
189	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
190	    and behavior.
191	
192	    Parameters:
193	        config ([`FSMTConfig`]): Model configuration class with all the parameters of the model.
194	            Initializing with a config file does not load the weights associated with the model, only the
195	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
196	
197	"""
198	FSMT_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/fsmt/tokenization_fsmt.py:180
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
179	            raise ImportError(
180	                "You need to install sacremoses to use XLMTokenizer. "
181	                "See https://pypi.org/project/sacremoses/ for installation."
182	            )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/fsmt/tokenization_fsmt.py:212
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
211	        with open(merges_file, encoding="utf-8") as merges_handle:
212	            merges = merges_handle.read().split("\n")[:-1]
213	        merges = [tuple(merge.split()[:2]) for merge in merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/fsmt/tokenization_fsmt.py:482
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
481	        with open(src_vocab_file, "w", encoding="utf-8") as f:
482	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
483	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/fsmt/tokenization_fsmt.py:486
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
485	            tgt_vocab = {v: k for k, v in self.decoder.items()}
486	            f.write(json.dumps(tgt_vocab, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
487	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/fsmt/tokenization_fsmt.py:497
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
496	                    index = token_index
497	                writer.write(" ".join(bpe_tokens) + "\n")
498	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/fsmt/tokenization_fsmt.py:514
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
513	            raise ImportError(
514	                "You need to install sacremoses to use XLMTokenizer. "
515	                "See https://pypi.org/project/sacremoses/ for installation."
516	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/funnel/modeling_funnel.py:65
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
64	        logger.error(
65	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
66	            "https://www.tensorflow.org/install/ for installation instructions."
67	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/funnel/modeling_funnel.py:849
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
848	
849	FUNNEL_START_DOCSTRING = r"""
850	
851	    The Funnel Transformer model was proposed in [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
852	    Language Processing](https://arxiv.org/abs/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.
853	
854	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
855	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
856	    etc.)
857	
858	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
859	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
860	    and behavior.
861	
862	    Parameters:
863	        config ([`FunnelConfig`]): Model configuration class with all the parameters of the model.
864	            Initializing with a config file does not load the weights associated with the model, only the
865	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
866	"""
867	
868	FUNNEL_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/funnel/modeling_tf_funnel.py:1112
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1111	
1112	FUNNEL_START_DOCSTRING = r"""
1113	
1114	    The Funnel Transformer model was proposed in [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
1115	    Language Processing](https://arxiv.org/abs/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.
1116	
1117	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
1118	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1119	    etc.)
1120	
1121	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
1122	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
1123	    behavior.
1124	
1125	    <Tip>
1126	
1127	    TensorFlow models and layers in `transformers` accept two formats as input:
1128	
1129	    - having all inputs as keyword arguments (like PyTorch models), or
1130	    - having all inputs as a list, tuple or dict in the first positional argument.
1131	
1132	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
1133	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
1134	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
1135	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
1136	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
1137	    positional argument:
1138	
1139	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
1140	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
1141	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
1142	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
1143	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
1144	
1145	    Note that when creating models and layers with
1146	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
1147	    about any of this, as you can just pass inputs like you would to any other Python function!
1148	
1149	    </Tip>
1150	
1151	    Parameters:
1152	        config ([`XxxConfig`]): Model configuration class with all the parameters of the model.
1153	            Initializing with a config file does not load the weights associated with the model, only the
1154	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1155	"""
1156	
1157	FUNNEL_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/funnel/tokenization_funnel.py:317
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
316	                    index = token_index
317	                writer.write(token + "\n")
318	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/fuyu/modeling_fuyu.py:36
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
35	
36	FUYU_START_DOCSTRING = r"""
37	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
38	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
39	    etc.)
40	
41	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
42	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
43	    and behavior.
44	
45	    Parameters:
46	        config ([`FuyuConfig`]):
47	            Model configuration class with all the parameters of the model. Initializing with a config file does not
48	            load the weights associated with the model, only the configuration. Check out the
49	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
50	"""
51	
52	
53	@add_start_docstrings(
54	    "The bare Fuyu Model outputting raw hidden-states without any specific head on top.",
55	    FUYU_START_DOCSTRING,
56	)
57	class FuyuPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/fuyu/modeling_fuyu.py:76
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
75	
76	FUYU_INPUTS_DOCSTRING = r"""
77	    Args:
78	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
79	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
80	            it.
81	
82	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
83	            [`PreTrainedTokenizer.__call__`] for details.
84	
85	            [What are input IDs?](../glossary#input-ids)
86	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
87	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
88	
89	            - 1 for tokens that are **not masked**,
90	            - 0 for tokens that are **masked**.
91	
92	            [What are attention masks?](../glossary#attention-mask)
93	
94	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
95	            [`PreTrainedTokenizer.__call__`] for details.
96	
97	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
98	            `past_key_values`).
99	
100	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
101	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
102	            information on the default strategy.
103	
104	            - 1 indicates the head is **not masked**,
105	            - 0 indicates the head is **masked**.
106	        image_patches (`torch.FloatTensor` of shape `(batch_size, num_total_patches, patch_size_ x patch_size x num_channels)`, *optional*):
107	            Image patches to be used as continuous embeddings. The patches are flattened and then projected to the
108	            hidden size of the model.
109	        image_patches_indices (`torch.LongTensor` of shape `(batch_size, num_total_patches + number_of_newline_tokens + number_of_text_tokens, patch_size_ x patch_size x num_channels )`, *optional*):
110	            Indices indicating at which position the image_patches have to be inserted in input_embeds.
111	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
112	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
113	            config.n_positions - 1]`.
114	
115	            [What are position IDs?](../glossary#position-ids)
116	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
117	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
118	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
119	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
120	
121	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
122	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
123	
124	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
125	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
126	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
127	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
128	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
129	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
130	            model's internal embedding lookup matrix.
131	        use_cache (`bool`, *optional*):
132	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
133	            `past_key_values`).
134	        output_attentions (`bool`, *optional*):
135	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
136	            tensors for more detail.
137	        output_hidden_states (`bool`, *optional*):
138	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
139	            more detail.
140	        return_dict (`bool`, *optional*):
141	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
142	"""
143	
144	
145	@add_start_docstrings(
146	    "Fuyu Model with a language modeling head on top for causal language model conditioned on image patches and text.",
147	    FUYU_START_DOCSTRING,
148	)
149	class FuyuForCausalLM(FuyuPreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gemma/modeling_flax_gemma.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	
41	GEMMA_START_DOCSTRING = r"""
42	
43	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
44	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
45	    etc.)
46	
47	    This model is also a Flax Linen
48	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
49	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
50	
51	    Finally, this model supports inherent JAX features such as:
52	
53	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
54	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
55	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
56	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
57	
58	    Parameters:
59	        config ([`GemmaConfig`]): Model configuration class with all the parameters of the model.
60	            Initializing with a config file does not load the weights associated with the model, only the
61	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
62	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
63	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`, or
64	            `jax.numpy.bfloat16`.
65	
66	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
67	            specified all the computation will be performed with the given `dtype`.
68	
69	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
70	            parameters.**
71	
72	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
73	            [`~FlaxPreTrainedModel.to_bf16`].
74	"""
75	
76	GEMMA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gemma/modeling_flax_gemma.py:76
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
75	
76	GEMMA_INPUTS_DOCSTRING = r"""
77	    Args:
78	        input_ids (`numpy.ndarray` of shape `(batch_size, input_ids_length)`):
79	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
80	            it.
81	
82	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
83	            [`PreTrainedTokenizer.__call__`] for details.
84	
85	            [What are input IDs?](../glossary#input-ids)
86	        attention_mask (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
87	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
88	
89	            - 1 for tokens that are **not masked**,
90	            - 0 for tokens that are **masked**.
91	
92	            [What are attention masks?](../glossary#attention-mask)
93	
94	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
95	            [`PreTrainedTokenizer.__call__`] for details.
96	
97	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
98	            `past_key_values`).
99	
100	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
101	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
102	            information on the default strategy.
103	
104	            - 1 indicates the head is **not masked**,
105	            - 0 indicates the head is **masked**.
106	        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
107	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
108	            config.n_positions - 1]`.
109	
110	            [What are position IDs?](../glossary#position-ids)
111	        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):
112	            Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
113	            auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.
114	        output_attentions (`bool`, *optional*):
115	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
116	            tensors for more detail.
117	        output_hidden_states (`bool`, *optional*):
118	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
119	            more detail.
120	        return_dict (`bool`, *optional*):
121	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
122	"""
123	
124	
125	def create_sinusoidal_positions(num_pos, dim):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gemma/modeling_flax_gemma.py:354
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
353	            logger.warning_once(
354	                "Gemma's activation function should be approximate GeLU and not exact GeLU. "
355	                "Changing the activation function to `gelu_pytorch_tanh`."
356	                f"if you want to use the legacy `{self.config.hidden_act}`, "
357	                f"edit the `model.config` to set `hidden_activation={self.config.hidden_act}` "
358	                "  instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gemma/modeling_gemma.py:339
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
338	
339	GEMMA_START_DOCSTRING = r"""
340	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
341	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
342	    etc.)
343	
344	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
345	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
346	    and behavior.
347	
348	    Parameters:
349	        config ([`GemmaConfig`]):
350	            Model configuration class with all the parameters of the model. Initializing with a config file does not
351	            load the weights associated with the model, only the configuration. Check out the
352	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
353	"""
354	
355	
356	@add_start_docstrings(
357	    "The bare Gemma Model outputting raw hidden-states without any specific head on top.",
358	    GEMMA_START_DOCSTRING,
359	)
360	class GemmaPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gemma/modeling_gemma.py:386
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
385	
386	GEMMA_INPUTS_DOCSTRING = r"""
387	    Args:
388	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
389	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
390	            it.
391	
392	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
393	            [`PreTrainedTokenizer.__call__`] for details.
394	
395	            [What are input IDs?](../glossary#input-ids)
396	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
397	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
398	
399	            - 1 for tokens that are **not masked**,
400	            - 0 for tokens that are **masked**.
401	
402	            [What are attention masks?](../glossary#attention-mask)
403	
404	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
405	            [`PreTrainedTokenizer.__call__`] for details.
406	
407	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
408	            `past_key_values`).
409	
410	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
411	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
412	            information on the default strategy.
413	
414	            - 1 indicates the head is **not masked**,
415	            - 0 indicates the head is **masked**.
416	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
417	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
418	            config.n_positions - 1]`.
419	
420	            [What are position IDs?](../glossary#position-ids)
421	        past_key_values (`Cache`, *optional*):
422	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
423	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
424	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
425	
426	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
427	
428	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
429	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
430	            of shape `(batch_size, sequence_length)`.
431	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
432	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
433	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
434	            model's internal embedding lookup matrix.
435	        use_cache (`bool`, *optional*):
436	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
437	            `past_key_values`).
438	        output_attentions (`bool`, *optional*):
439	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
440	            tensors for more detail.
441	        output_hidden_states (`bool`, *optional*):
442	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
443	            more detail.
444	        return_dict (`bool`, *optional*):
445	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
446	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
447	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
448	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
449	            the complete sequence length.
450	"""
451	
452	
453	@add_start_docstrings(
454	    "The bare Gemma Model outputting raw hidden-states without any specific head on top.",
455	    GEMMA_START_DOCSTRING,
456	)
457	class GemmaModel(GemmaPreTrainedModel):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gemma/tokenization_gemma.py:218
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
217	                content_spiece_model = self.sp_model.serialized_model_proto()
218	                fi.write(content_spiece_model)
219	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gemma2/modeling_gemma2.py:384
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
383	
384	GEMMA2_START_DOCSTRING = r"""
385	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
386	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
387	    etc.)
388	
389	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
390	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
391	    and behavior.
392	
393	    Parameters:
394	        config ([`Gemma2Config`]):
395	            Model configuration class with all the parameters of the model. Initializing with a config file does not
396	            load the weights associated with the model, only the configuration. Check out the
397	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
398	"""
399	
400	
401	@add_start_docstrings(
402	    "The bare Gemma2 Model outputting raw hidden-states without any specific head on top.",
403	    GEMMA2_START_DOCSTRING,
404	)
405	class Gemma2PreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gemma2/modeling_gemma2.py:431
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
430	
431	GEMMA2_INPUTS_DOCSTRING = r"""
432	    Args:
433	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
434	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
435	            it.
436	
437	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
438	            [`PreTrainedTokenizer.__call__`] for details.
439	
440	            [What are input IDs?](../glossary#input-ids)
441	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
442	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
443	
444	            - 1 for tokens that are **not masked**,
445	            - 0 for tokens that are **masked**.
446	
447	            [What are attention masks?](../glossary#attention-mask)
448	
449	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
450	            [`PreTrainedTokenizer.__call__`] for details.
451	
452	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
453	            `past_key_values`).
454	
455	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
456	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
457	            information on the default strategy.
458	
459	            - 1 indicates the head is **not masked**,
460	            - 0 indicates the head is **masked**.
461	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
462	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
463	            config.n_positions - 1]`.
464	
465	            [What are position IDs?](../glossary#position-ids)
466	        past_key_values (`Cache`, *optional*):
467	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
468	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
469	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
470	
471	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
472	
473	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
474	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
475	            of shape `(batch_size, sequence_length)`.
476	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
477	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
478	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
479	            model's internal embedding lookup matrix.
480	        use_cache (`bool`, *optional*):
481	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
482	            `past_key_values`).
483	        output_attentions (`bool`, *optional*):
484	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
485	            tensors for more detail.
486	        output_hidden_states (`bool`, *optional*):
487	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
488	            more detail.
489	        return_dict (`bool`, *optional*):
490	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
491	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
492	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
493	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
494	            the complete sequence length.
495	"""
496	
497	
498	@add_start_docstrings(
499	    "The bare Gemma2 Model outputting raw hidden-states without any specific head on top.",
500	    GEMMA2_START_DOCSTRING,
501	)
502	class Gemma2Model(Gemma2PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gemma3/modeling_gemma3.py:448
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
447	
448	GEMMA3_START_DOCSTRING = r"""
449	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
450	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
451	    etc.)
452	
453	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
454	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
455	    and behavior.
456	
457	    Parameters:
458	        config ([`Gemma3Config`]):
459	            Model configuration class with all the parameters of the model. Initializing with a config file does not
460	            load the weights associated with the model, only the configuration. Check out the
461	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
462	"""
463	
464	
465	@add_start_docstrings(
466	    "The bare Gemma3 Model outputting raw hidden-states without any specific head on top.",
467	    GEMMA3_START_DOCSTRING,
468	)
469	class Gemma3PreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gemma3/modeling_gemma3.py:507
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
506	
507	GEMMA3_INPUTS_DOCSTRING = r"""
508	    Args:
509	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
510	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
511	            it.
512	
513	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
514	            [`PreTrainedTokenizer.__call__`] for details.
515	
516	            [What are input IDs?](../glossary#input-ids)
517	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
518	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
519	
520	            - 1 for tokens that are **not masked**,
521	            - 0 for tokens that are **masked**.
522	
523	            [What are attention masks?](../glossary#attention-mask)
524	
525	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
526	            [`PreTrainedTokenizer.__call__`] for details.
527	
528	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
529	            `past_key_values`).
530	
531	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
532	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
533	            information on the default strategy.
534	
535	            - 1 indicates the head is **not masked**,
536	            - 0 indicates the head is **masked**.
537	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
538	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
539	            config.n_positions - 1]`.
540	
541	            [What are position IDs?](../glossary#position-ids)
542	        past_key_values (`Cache`, *optional*):
543	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
544	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
545	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
546	
547	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
548	
549	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
550	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
551	            of shape `(batch_size, sequence_length)`.
552	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
553	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
554	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
555	            model's internal embedding lookup matrix.
556	        use_cache (`bool`, *optional*):
557	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
558	            `past_key_values`).
559	        output_attentions (`bool`, *optional*):
560	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
561	            tensors for more detail.
562	        output_hidden_states (`bool`, *optional*):
563	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
564	            more detail.
565	        return_dict (`bool`, *optional*):
566	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
567	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
568	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
569	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
570	            the complete sequence length.
571	"""
572	
573	
574	@add_start_docstrings(
575	    "The bare Gemma3Text Model outputting raw hidden-states without any specific head on top.",
576	    GEMMA3_START_DOCSTRING,
577	)
578	class Gemma3TextModel(Gemma3PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/git/modeling_git.py:435
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
434	                logger.warning_once(
435	                    "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and "
436	                    "will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class "
437	                    "(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/git/modeling_git.py:533
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
532	
533	GIT_START_DOCSTRING = r"""
534	
535	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
536	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
537	    etc.)
538	
539	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
540	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
541	    and behavior.
542	
543	    Parameters:
544	        config ([`GitConfig`]): Model configuration class with all the parameters of the model.
545	            Initializing with a config file does not load the weights associated with the model, only the
546	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
547	"""
548	
549	GIT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/git/modeling_git.py:549
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
548	
549	GIT_INPUTS_DOCSTRING = r"""
550	    Args:
551	        input_ids (`torch.LongTensor` of shape `({0})`):
552	            Indices of input sequence tokens in the vocabulary.
553	
554	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
555	            [`PreTrainedTokenizer.__call__`] for details.
556	
557	            [What are input IDs?](../glossary#input-ids)
558	        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):
559	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
560	
561	            - 1 for tokens that are **not masked**,
562	            - 0 for tokens that are **masked**.
563	
564	            [What are attention masks?](../glossary#attention-mask)
565	
566	        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):
567	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
568	            config.max_position_embeddings - 1]`.
569	
570	            [What are position IDs?](../glossary#position-ids)
571	
572	        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):
573	            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See
574	            [`CLIPImageProcessor.__call__`] for details.
575	
576	        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):
577	            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:
578	
579	            - 1 indicates the head is **not masked**,
580	            - 0 indicates the head is **masked**.
581	
582	        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):
583	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
584	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
585	            model's internal embedding lookup matrix.
586	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
587	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
588	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
589	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
590	
591	            Two formats are allowed:
592	            - a [`~cache_utils.Cache`] instance, see our
593	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
594	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
595	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
596	            cache format.
597	
598	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
599	            legacy cache format will be returned.
600	
601	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
602	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
603	            of shape `(batch_size, sequence_length)`.
604	        output_attentions (`bool`, *optional*):
605	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
606	            tensors for more detail.
607	        output_hidden_states (`bool`, *optional*):
608	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
609	            more detail.
610	        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):
611	            Whether to interpolate the pre-trained position encodings.
612	        return_dict (`bool`, *optional*):
613	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
614	"""
615	
616	
617	# Copied from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings with CLIP->Git
618	class GitVisionEmbeddings(nn.Module):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/glm/modeling_glm.py:355
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
354	
355	GLM_START_DOCSTRING = r"""
356	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
357	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
358	    etc.)
359	
360	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
361	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
362	    and behavior.
363	
364	    Parameters:
365	        config ([`GlmConfig`]):
366	            Model configuration class with all the parameters of the model. Initializing with a config file does not
367	            load the weights associated with the model, only the configuration. Check out the
368	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
369	"""
370	
371	
372	@add_start_docstrings(
373	    "The bare Glm Model outputting raw hidden-states without any specific head on top.",
374	    GLM_START_DOCSTRING,
375	)
376	class GlmPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/glm/modeling_glm.py:402
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
401	
402	GLM_INPUTS_DOCSTRING = r"""
403	    Args:
404	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
405	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
406	            it.
407	
408	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
409	            [`PreTrainedTokenizer.__call__`] for details.
410	
411	            [What are input IDs?](../glossary#input-ids)
412	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
413	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
414	
415	            - 1 for tokens that are **not masked**,
416	            - 0 for tokens that are **masked**.
417	
418	            [What are attention masks?](../glossary#attention-mask)
419	
420	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
421	            [`PreTrainedTokenizer.__call__`] for details.
422	
423	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
424	            `past_key_values`).
425	
426	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
427	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
428	            information on the default strategy.
429	
430	            - 1 indicates the head is **not masked**,
431	            - 0 indicates the head is **masked**.
432	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
433	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
434	            config.n_positions - 1]`.
435	
436	            [What are position IDs?](../glossary#position-ids)
437	        past_key_values (`Cache`, *optional*):
438	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
439	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
440	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
441	
442	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
443	
444	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
445	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
446	            of shape `(batch_size, sequence_length)`.
447	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
448	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
449	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
450	            model's internal embedding lookup matrix.
451	        use_cache (`bool`, *optional*):
452	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
453	            `past_key_values`).
454	        output_attentions (`bool`, *optional*):
455	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
456	            tensors for more detail.
457	        output_hidden_states (`bool`, *optional*):
458	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
459	            more detail.
460	        return_dict (`bool`, *optional*):
461	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
462	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
463	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
464	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
465	            the complete sequence length.
466	"""
467	
468	
469	@add_start_docstrings(
470	    "The bare Glm Model outputting raw hidden-states without any specific head on top.",
471	    GLM_START_DOCSTRING,
472	)
473	class GlmModel(GlmPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/glpn/modeling_glpn.py:445
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
444	
445	GLPN_START_DOCSTRING = r"""
446	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
447	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
448	    behavior.
449	
450	    Parameters:
451	        config ([`GLPNConfig`]): Model configuration class with all the parameters of the model.
452	            Initializing with a config file does not load the weights associated with the model, only the
453	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
454	"""
455	
456	GLPN_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/got_ocr2/modeling_got_ocr2.py:560
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
559	
560	GOT_OCR2_START_DOCSTRING = r"""
561	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
562	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
563	    etc.)
564	
565	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
566	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
567	    and behavior.
568	
569	    Parameters:
570	        config ([`GotOcr2Config`] or [`GotOcr2VisionConfig`]):
571	            Model configuration class with all the parameters of the model. Initializing with a config file does not
572	            load the weights associated with the model, only the configuration. Check out the
573	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
574	"""
575	
576	
577	@add_start_docstrings(
578	    "The bare LLaMA Model outputting raw hidden-states without any specific head on top.",
579	    GOT_OCR2_START_DOCSTRING,
580	)
581	class GotOcr2PreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/got_ocr2/modeling_got_ocr2.py:616
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
615	
616	GOT_OCR2_INPUTS_DOCSTRING = r"""
617	    Args:
618	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
619	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
620	            it.
621	
622	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
623	            [`PreTrainedTokenizer.__call__`] for details.
624	
625	            [What are input IDs?](../glossary#input-ids)
626	        pixel_values (`torch.FloatTensor` of shape `(seq_length, num_channels * image_size * image_size)):
627	            The tensors corresponding to the input images. Pixel values can be obtained using
628	            [`AutoImageProcessor`]. See [`GotOcr2ImageProcessor.__call__`] for details. [`GotOcr2Processor`] uses
629	            [`GotOcr2ImageProcessor`] for processing images.
630	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
631	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
632	
633	            - 1 for tokens that are **not masked**,
634	            - 0 for tokens that are **masked**.
635	
636	            [What are attention masks?](../glossary#attention-mask)
637	
638	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
639	            [`PreTrainedTokenizer.__call__`] for details.
640	
641	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
642	            `past_key_values`).
643	
644	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
645	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
646	            information on the default strategy.
647	
648	            - 1 indicates the head is **not masked**,
649	            - 0 indicates the head is **masked**.
650	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
651	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
652	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
653	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
654	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
655	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
656	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
657	
658	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
659	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
660	
661	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
662	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
663	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
664	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
665	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
666	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
667	            model's internal embedding lookup matrix.
668	        use_cache (`bool`, *optional*):
669	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
670	            `past_key_values`).
671	        output_attentions (`bool`, *optional*):
672	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
673	            tensors for more detail.
674	        output_hidden_states (`bool`, *optional*):
675	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
676	            more detail.
677	        return_dict (`bool`, *optional*):
678	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
679	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
680	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
681	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
682	            the complete sequence length.
683	"""
684	
685	
686	@add_start_docstrings(
687	    """The GOT_OCR2 model which consists of a vision backbone and a language model.""",
688	    GOT_OCR2_START_DOCSTRING,
689	)
690	class GotOcr2ForConditionalGeneration(GotOcr2PreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/got_ocr2/modular_got_ocr2.py:282
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
281	
282	GOT_OCR2_INPUTS_DOCSTRING = r"""
283	    Args:
284	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
285	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
286	            it.
287	
288	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
289	            [`PreTrainedTokenizer.__call__`] for details.
290	
291	            [What are input IDs?](../glossary#input-ids)
292	        pixel_values (`torch.FloatTensor` of shape `(seq_length, num_channels * image_size * image_size)):
293	            The tensors corresponding to the input images. Pixel values can be obtained using
294	            [`AutoImageProcessor`]. See [`GotOcr2ImageProcessor.__call__`] for details. [`GotOcr2Processor`] uses
295	            [`GotOcr2ImageProcessor`] for processing images.
296	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
297	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
298	
299	            - 1 for tokens that are **not masked**,
300	            - 0 for tokens that are **masked**.
301	
302	            [What are attention masks?](../glossary#attention-mask)
303	
304	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
305	            [`PreTrainedTokenizer.__call__`] for details.
306	
307	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
308	            `past_key_values`).
309	
310	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
311	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
312	            information on the default strategy.
313	
314	            - 1 indicates the head is **not masked**,
315	            - 0 indicates the head is **masked**.
316	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
317	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
318	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
319	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
320	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
321	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
322	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
323	
324	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
325	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
326	
327	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
328	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
329	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
330	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
331	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
332	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
333	            model's internal embedding lookup matrix.
334	        use_cache (`bool`, *optional*):
335	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
336	            `past_key_values`).
337	        output_attentions (`bool`, *optional*):
338	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
339	            tensors for more detail.
340	        output_hidden_states (`bool`, *optional*):
341	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
342	            more detail.
343	        return_dict (`bool`, *optional*):
344	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
345	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
346	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
347	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
348	            the complete sequence length.
349	"""
350	
351	
352	class GotOcr2ForConditionalGeneration(LlavaForConditionalGeneration):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt2/modeling_flax_gpt2.py:42
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
41	
42	GPT2_START_DOCSTRING = r"""
43	
44	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
45	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
46	    etc.)
47	
48	    This model is also a Flax Linen
49	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
50	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
51	
52	    Finally, this model supports inherent JAX features such as:
53	
54	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
55	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
56	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
57	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
58	
59	    Parameters:
60	        config ([`GPT2Config`]): Model configuration class with all the parameters of the model.
61	            Initializing with a config file does not load the weights associated with the model, only the
62	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
63	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
64	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
65	            `jax.numpy.bfloat16` (on TPUs).
66	
67	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
68	            specified all the computation will be performed with the given `dtype`.
69	
70	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
71	            parameters.**
72	
73	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
74	            [`~FlaxPreTrainedModel.to_bf16`].
75	"""
76	
77	GPT2_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt2/modeling_gpt2.py:67
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
66	        logger.error(
67	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
68	            "https://www.tensorflow.org/install/ for installation instructions."
69	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt2/modeling_gpt2.py:542
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
541	
542	GPT2_START_DOCSTRING = r"""
543	
544	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
545	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
546	    etc.)
547	
548	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
549	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
550	    and behavior.
551	
552	    Parameters:
553	        config ([`GPT2Config`]): Model configuration class with all the parameters of the model.
554	            Initializing with a config file does not load the weights associated with the model, only the
555	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
556	"""
557	
558	GPT2_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt2/modeling_tf_gpt2.py:638
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
637	
638	GPT2_START_DOCSTRING = r"""
639	
640	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
641	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
642	    etc.)
643	
644	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
645	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
646	    behavior.
647	
648	    <Tip>
649	
650	    TensorFlow models and layers in `transformers` accept two formats as input:
651	
652	    - having all inputs as keyword arguments (like PyTorch models), or
653	    - having all inputs as a list, tuple or dict in the first positional argument.
654	
655	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
656	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
657	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
658	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
659	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
660	    positional argument:
661	
662	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
663	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
664	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
665	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
666	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
667	
668	    Note that when creating models and layers with
669	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
670	    about any of this, as you can just pass inputs like you would to any other Python function!
671	
672	    </Tip>
673	
674	    Parameters:
675	        config ([`GPT2Config`]): Model configuration class with all the parameters of the model.
676	            Initializing with a config file does not load the weights associated with the model, only the
677	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
678	"""
679	
680	GPT2_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt2/tokenization_gpt2.py:160
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
159	        with open(merges_file, encoding="utf-8") as merges_handle:
160	            bpe_merges = merges_handle.read().split("\n")[1:-1]
161	        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt2/tokenization_gpt2.py:310
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
309	        with open(vocab_file, "w", encoding="utf-8") as f:
310	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
311	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt2/tokenization_gpt2.py:314
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
313	        with open(merge_file, "w", encoding="utf-8") as writer:
314	            writer.write("#version: 0.2\n")
315	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt2/tokenization_gpt2.py:322
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
321	                    index = token_index
322	                writer.write(" ".join(bpe_tokens) + "\n")
323	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:701
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
700	
701	GPT_BIGCODE_START_DOCSTRING = r"""
702	
703	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
704	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
705	    etc.)
706	
707	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
708	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
709	    and behavior.
710	
711	    Parameters:
712	        config ([`GPTBigCodeConfig`]): Model configuration class with all the parameters of the model.
713	            Initializing with a config file does not load the weights associated with the model, only the
714	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
715	"""
716	
717	GPT_BIGCODE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py:40
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
39	
40	GPT_NEO_START_DOCSTRING = r"""
41	
42	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
43	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
44	    etc.)
45	
46	    This model is also a Flax Linen
47	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
48	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
49	
50	    Finally, this model supports inherent JAX features such as:
51	
52	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
53	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
54	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
55	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
56	
57	    Parameters:
58	        config ([`GPTNeoConfig`]): Model configuration class with all the parameters of the model.
59	            Initializing with a config file does not load the weights associated with the model, only the
60	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
61	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
62	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
63	            `jax.numpy.bfloat16` (on TPUs).
64	
65	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
66	            specified all the computation will be performed with the given `dtype`.
67	
68	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
69	            parameters.**
70	
71	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
72	            [`~FlaxPreTrainedModel.to_bf16`].
73	"""
74	
75	GPT_NEO_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt_neo/modeling_gpt_neo.py:83
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
82	        logger.error(
83	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
84	            "https://www.tensorflow.org/install/ for installation instructions."
85	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt_neo/modeling_gpt_neo.py:525
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
524	
525	GPT_NEO_START_DOCSTRING = r"""
526	
527	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
528	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
529	    etc.)
530	
531	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
532	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
533	    and behavior.
534	
535	    Parameters:
536	        config ([`GPTNeoConfig`]): Model configuration class with all the parameters of the model.
537	            Initializing with a config file does not load the weights associated with the model, only the
538	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
539	"""
540	
541	GPT_NEO_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt_neo/modeling_gpt_neo.py:541
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
540	
541	GPT_NEO_INPUTS_DOCSTRING = r"""
542	    Args:
543	        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):
544	            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else
545	            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input
546	            sequence tokens in the vocabulary.
547	
548	            If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as
549	            `input_ids`.
550	
551	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
552	            [`PreTrainedTokenizer.__call__`] for details.
553	
554	            [What are input IDs?](../glossary#input-ids)
555	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
556	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
557	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
558	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
559	
560	            Two formats are allowed:
561	            - a [`~cache_utils.Cache`] instance, see our
562	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
563	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
564	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
565	            cache format.
566	
567	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
568	            legacy cache format will be returned.
569	
570	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
571	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
572	            of shape `(batch_size, sequence_length)`.
573	        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):
574	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
575	
576	            - 1 for tokens that are **not masked**,
577	            - 0 for tokens that are **masked**.
578	
579	            [What are attention masks?](../glossary#attention-mask)
580	        token_type_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):
581	            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,
582	            1]`:
583	
584	            - 0 corresponds to a *sentence A* token,
585	            - 1 corresponds to a *sentence B* token.
586	
587	            [What are token type IDs?](../glossary#token-type-ids)
588	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
589	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
590	            config.max_position_embeddings - 1]`.
591	
592	            [What are position IDs?](../glossary#position-ids)
593	        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):
594	            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:
595	
596	            - 1 indicates the head is **not masked**,
597	            - 0 indicates the head is **masked**.
598	
599	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
600	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
601	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
602	            model's internal embedding lookup matrix.
603	
604	            If `past_key_values` is used, optionally only the last `inputs_embeds` have to be input (see
605	            `past_key_values`).
606	        use_cache (`bool`, *optional*):
607	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
608	            `past_key_values`).
609	        output_attentions (`bool`, *optional*):
610	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
611	            tensors for more detail.
612	        output_hidden_states (`bool`, *optional*):
613	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
614	            more detail.
615	        return_dict (`bool`, *optional*):
616	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
617	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
618	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
619	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
620	            the complete sequence length.
621	"""
622	
623	
624	@add_start_docstrings(
625	    "The bare GPT Neo Model transformer outputting raw hidden-states without any specific head on top.",
626	    GPT_NEO_START_DOCSTRING,
627	)
628	class GPTNeoModel(GPTNeoPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt_neo/modeling_gpt_neo.py:699
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
698	                logger.warning_once(
699	                    "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and "
700	                    "will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class "
701	                    "(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt_neox/modeling_gpt_neox.py:325
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
324	
325	GPT_NEOX_START_DOCSTRING = r"""
326	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
327	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
328	    etc.)
329	
330	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
331	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
332	    and behavior.
333	
334	    Parameters:
335	        config ([`GPTNeoXConfig`]):
336	            Model configuration class with all the parameters of the model. Initializing with a config file does not
337	            load the weights associated with the model, only the configuration. Check out the
338	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
339	"""
340	
341	
342	@add_start_docstrings(
343	    "The bare GPTNeoX Model outputting raw hidden-states without any specific head on top.",
344	    GPT_NEOX_START_DOCSTRING,
345	)
346	class GPTNeoXPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt_neox/modeling_gpt_neox.py:379
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
378	
379	GPT_NEOX_INPUTS_DOCSTRING = r"""
380	    Args:
381	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
382	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
383	            it.
384	
385	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
386	            [`PreTrainedTokenizer.__call__`] for details.
387	
388	            [What are input IDs?](../glossary#input-ids)
389	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
390	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
391	
392	            - 1 for tokens that are **not masked**,
393	            - 0 for tokens that are **masked**.
394	
395	            [What are attention masks?](../glossary#attention-mask)
396	
397	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
398	            [`PreTrainedTokenizer.__call__`] for details.
399	
400	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
401	            `past_key_values`).
402	
403	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
404	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
405	            information on the default strategy.
406	
407	            - 1 indicates the head is **not masked**,
408	            - 0 indicates the head is **masked**.
409	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
410	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
411	            config.n_positions - 1]`.
412	
413	            [What are position IDs?](../glossary#position-ids)
414	        past_key_values (`Cache`, *optional*):
415	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
416	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
417	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
418	
419	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
420	
421	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
422	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
423	            of shape `(batch_size, sequence_length)`.
424	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
425	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
426	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
427	            model's internal embedding lookup matrix.
428	        use_cache (`bool`, *optional*):
429	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
430	            `past_key_values`).
431	        output_attentions (`bool`, *optional*):
432	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
433	            tensors for more detail.
434	        output_hidden_states (`bool`, *optional*):
435	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
436	            more detail.
437	        return_dict (`bool`, *optional*):
438	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
439	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
440	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
441	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
442	            the complete sequence length.
443	"""
444	
445	
446	@add_start_docstrings(
447	    "The bare GPTNeoX Model outputting raw hidden-states without any specific head on top.",
448	    GPT_NEOX_START_DOCSTRING,
449	)
450	class GPTNeoXModel(GPTNeoXPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py:405
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
404	
405	GPT_NEOX_JAPANESE_START_DOCSTRING = r"""
406	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
407	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
408	    behavior.
409	
410	    Parameters:
411	        config ([`~GPTNeoXJapaneseConfig`]): Model configuration class with all the parameters of the model.
412	            Initializing with a config file does not load the weights associated with the model, only the
413	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
414	"""
415	
416	GPT_NEOX_JAPANESE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py:566
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
565	                logger.warning_once(
566	                    "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and "
567	                    "will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class "
568	                    "(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)"

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py:38
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
37	    with open(emoji_file, "r", encoding="utf-8") as f:
38	        emoji = json.loads(f.read())
39	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py:189
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
188	                    index = token_index
189	                writer.write(",".join(token) + "\n")
190	                index += 1

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gpt_sw3/tokenization_gpt_sw3.py:247
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
246	                content_spiece_model = self.sp_model.serialized_model_proto()
247	                fi.write(content_spiece_model)
248	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gptj/modeling_flax_gptj.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	
41	GPTJ_START_DOCSTRING = r"""
42	
43	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
44	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
45	    etc.)
46	
47	    This model is also a Flax Linen
48	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
49	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
50	
51	    Finally, this model supports inherent JAX features such as:
52	
53	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
54	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
55	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
56	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
57	
58	    Parameters:
59	        config ([`GPTJConfig`]): Model configuration class with all the parameters of the model.
60	            Initializing with a config file does not load the weights associated with the model, only the
61	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
62	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
63	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
64	            `jax.numpy.bfloat16` (on TPUs).
65	
66	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
67	            specified all the computation will be performed with the given `dtype`.
68	
69	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
70	            parameters.**
71	
72	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
73	            [`~FlaxPreTrainedModel.to_bf16`].
74	"""
75	
76	GPTJ_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gptj/modeling_gptj.py:520
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
519	
520	GPTJ_START_DOCSTRING = r"""
521	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
522	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
523	    behavior.
524	
525	    Parameters:
526	        config ([`GPTJConfig`]): Model configuration class with all the parameters of the model.
527	            Initializing with a config file does not load the weights associated with the model, only the
528	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
529	"""
530	
531	GPTJ_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gptj/modeling_gptj.py:531
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
530	
531	GPTJ_INPUTS_DOCSTRING = r"""
532	    Args:
533	        input_ids (`torch.LongTensor` of shape `({0})`):
534	            Indices of input sequence tokens in the vocabulary.
535	
536	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
537	            [`PreTrainedTokenizer.__call__`] for details.
538	
539	            [What are input IDs?](../glossary#input-ids)
540	        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):
541	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
542	
543	            - 1 for tokens that are **not masked**,
544	            - 0 for tokens that are **masked**.
545	
546	            [What are attention masks?](../glossary#attention-mask)
547	        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):
548	            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,
549	            1]`:
550	
551	            - 0 corresponds to a *sentence A* token,
552	            - 1 corresponds to a *sentence B* token.
553	
554	            [What are token type IDs?](../glossary#token-type-ids)
555	        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):
556	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
557	            config.n_positions - 1]`.
558	
559	            [What are position IDs?](../glossary#position-ids)
560	        head_mask (`torch.FloatTensor` of shape `(num_attention_heads,)` or `(n_layer, num_attention_heads)`, *optional*):
561	            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:
562	
563	            - 1 indicates the head is **not masked**,
564	            - 0 indicates the head is **masked**.
565	
566	        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_dim)`, *optional*):
567	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
568	            is useful if you want more control over how to convert *input_ids* indices into associated vectors than the
569	            model's internal embedding lookup matrix.
570	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
571	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
572	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
573	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
574	
575	            Two formats are allowed:
576	            - a [`~cache_utils.Cache`] instance, see our
577	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
578	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
579	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
580	            cache format.
581	
582	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
583	            legacy cache format will be returned.
584	
585	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
586	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
587	            of shape `(batch_size, sequence_length)`.
588	        output_attentions (`bool`, *optional*):
589	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
590	            tensors for more detail.
591	        output_hidden_states (`bool`, *optional*):
592	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
593	            more detail.
594	        return_dict (`bool`, *optional*):
595	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
596	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
597	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
598	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
599	            the complete sequence length.
600	"""
601	
602	PARALLELIZE_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gptj/modeling_gptj.py:775
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
774	                logger.warning_once(
775	                    "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and "
776	                    "will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class "
777	                    "(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/gptj/modeling_tf_gptj.py:573
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
572	
573	GPTJ_START_DOCSTRING = r"""
574	
575	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
576	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
577	    etc.)
578	
579	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
580	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
581	    behavior.
582	
583	    <Tip>
584	
585	    TensorFlow models and layers in `transformers` accept two formats as input:
586	
587	    - having all inputs as keyword arguments (like PyTorch models), or
588	    - having all inputs as a list, tuple or dict in the first positional argument.
589	
590	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
591	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
592	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
593	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
594	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
595	    positional argument:
596	
597	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
598	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
599	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
600	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
601	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
602	
603	    Note that when creating models and layers with
604	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
605	    about any of this, as you can just pass inputs like you would to any other Python function!
606	
607	    </Tip>
608	
609	    Parameters:
610	        config ([`GPTJConfig`]): Model configuration class with all the parameters of the model.
611	            Initializing with a config file does not load the weights associated with the model, only the
612	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
613	"""
614	
615	GPTJ_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/granite/modeling_granite.py:355
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
354	
355	GRANITE_START_DOCSTRING = r"""
356	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
357	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
358	    etc.)
359	
360	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
361	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
362	    and behavior.
363	
364	    Parameters:
365	        config ([`GraniteConfig`]):
366	            Model configuration class with all the parameters of the model. Initializing with a config file does not
367	            load the weights associated with the model, only the configuration. Check out the
368	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
369	"""
370	
371	
372	@add_start_docstrings(
373	    "The bare Granite Model outputting raw hidden-states without any specific head on top.",
374	    GRANITE_START_DOCSTRING,
375	)
376	class GranitePreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/granite/modeling_granite.py:402
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
401	
402	GRANITE_INPUTS_DOCSTRING = r"""
403	    Args:
404	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
405	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
406	            it.
407	
408	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
409	            [`PreTrainedTokenizer.__call__`] for details.
410	
411	            [What are input IDs?](../glossary#input-ids)
412	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
413	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
414	
415	            - 1 for tokens that are **not masked**,
416	            - 0 for tokens that are **masked**.
417	
418	            [What are attention masks?](../glossary#attention-mask)
419	
420	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
421	            [`PreTrainedTokenizer.__call__`] for details.
422	
423	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
424	            `past_key_values`).
425	
426	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
427	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
428	            information on the default strategy.
429	
430	            - 1 indicates the head is **not masked**,
431	            - 0 indicates the head is **masked**.
432	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
433	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
434	            config.n_positions - 1]`.
435	
436	            [What are position IDs?](../glossary#position-ids)
437	        past_key_values (`Cache`, *optional*):
438	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
439	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
440	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
441	
442	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
443	
444	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
445	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
446	            of shape `(batch_size, sequence_length)`.
447	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
448	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
449	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
450	            model's internal embedding lookup matrix.
451	        use_cache (`bool`, *optional*):
452	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
453	            `past_key_values`).
454	        output_attentions (`bool`, *optional*):
455	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
456	            tensors for more detail.
457	        output_hidden_states (`bool`, *optional*):
458	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
459	            more detail.
460	        return_dict (`bool`, *optional*):
461	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
462	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
463	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
464	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
465	            the complete sequence length.
466	"""
467	
468	
469	@add_start_docstrings(
470	    "The bare Granite Model outputting raw hidden-states without any specific head on top.",
471	    GRANITE_START_DOCSTRING,
472	)
473	class GraniteModel(GranitePreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/granitemoe/modeling_granitemoe.py:793
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
792	
793	GRANITEMOE_START_DOCSTRING = r"""
794	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
795	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
796	    etc.)
797	
798	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
799	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
800	    and behavior.
801	
802	    Parameters:
803	        config ([`GraniteMoeConfig`]):
804	            Model configuration class with all the parameters of the model. Initializing with a config file does not
805	            load the weights associated with the model, only the configuration. Check out the
806	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
807	"""
808	
809	
810	@add_start_docstrings(
811	    "The bare GraniteMoe Model outputting raw hidden-states without any specific head on top.",
812	    GRANITEMOE_START_DOCSTRING,
813	)
814	class GraniteMoePreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/granitemoe/modeling_granitemoe.py:843
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
842	
843	GRANITEMOE_INPUTS_DOCSTRING = r"""
844	    Args:
845	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
846	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
847	            it.
848	
849	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
850	            [`PreTrainedTokenizer.__call__`] for details.
851	
852	            [What are input IDs?](../glossary#input-ids)
853	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
854	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
855	
856	            - 1 for tokens that are **not masked**,
857	            - 0 for tokens that are **masked**.
858	
859	            [What are attention masks?](../glossary#attention-mask)
860	
861	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
862	            [`PreTrainedTokenizer.__call__`] for details.
863	
864	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
865	            `past_key_values`).
866	
867	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
868	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
869	            information on the default strategy.
870	
871	            - 1 indicates the head is **not masked**,
872	            - 0 indicates the head is **masked**.
873	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
874	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
875	            config.n_positions - 1]`.
876	
877	            [What are position IDs?](../glossary#position-ids)
878	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
879	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
880	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
881	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
882	
883	            Two formats are allowed:
884	            - a [`~cache_utils.Cache`] instance;
885	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
886	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
887	            cache format.
888	
889	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
890	            legacy cache format will be returned.
891	
892	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
893	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
894	            of shape `(batch_size, sequence_length)`.
895	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
896	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
897	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
898	            model's internal embedding lookup matrix.
899	        use_cache (`bool`, *optional*):
900	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
901	            `past_key_values`).
902	        output_attentions (`bool`, *optional*):
903	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
904	            tensors for more detail.
905	        output_hidden_states (`bool`, *optional*):
906	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
907	            more detail.
908	        return_dict (`bool`, *optional*):
909	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
910	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
911	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
912	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
913	            the complete sequence length.
914	"""
915	
916	
917	@add_start_docstrings(
918	    "The bare GraniteMoe Model outputting raw hidden-states without any specific head on top.",
919	    GRANITEMOE_START_DOCSTRING,
920	)
921	class GraniteMoeModel(GraniteMoePreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/granitemoe/modeling_granitemoe.py:1001
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1000	            logger.warning_once(
1001	                "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. "
1002	                "Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)"
1003	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/granitemoeshared/modeling_granitemoeshared.py:705
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
704	
705	GRANITEMOESHARED_START_DOCSTRING = r"""
706	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
707	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
708	    etc.)
709	
710	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
711	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
712	    and behavior.
713	
714	    Parameters:
715	        config ([`GraniteMoeSharedConfig`]):
716	            Model configuration class with all the parameters of the model. Initializing with a config file does not
717	            load the weights associated with the model, only the configuration. Check out the
718	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
719	"""
720	
721	
722	@add_start_docstrings(
723	    "The bare GraniteMoeShared Model outputting raw hidden-states without any specific head on top.",
724	    GRANITEMOESHARED_START_DOCSTRING,
725	)
726	class GraniteMoeSharedPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/granitemoeshared/modeling_granitemoeshared.py:789
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
788	
789	GRANITEMOESHARED_INPUTS_DOCSTRING = r"""
790	    Args:
791	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
792	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
793	            it.
794	
795	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
796	            [`PreTrainedTokenizer.__call__`] for details.
797	
798	            [What are input IDs?](../glossary#input-ids)
799	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
800	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
801	
802	            - 1 for tokens that are **not masked**,
803	            - 0 for tokens that are **masked**.
804	
805	            [What are attention masks?](../glossary#attention-mask)
806	
807	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
808	            [`PreTrainedTokenizer.__call__`] for details.
809	
810	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
811	            `past_key_values`).
812	
813	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
814	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
815	            information on the default strategy.
816	
817	            - 1 indicates the head is **not masked**,
818	            - 0 indicates the head is **masked**.
819	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
820	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
821	            config.n_positions - 1]`.
822	
823	            [What are position IDs?](../glossary#position-ids)
824	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
825	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
826	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
827	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
828	
829	            Two formats are allowed:
830	            - a [`~cache_utils.Cache`] instance;
831	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
832	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
833	            cache format.
834	
835	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
836	            legacy cache format will be returned.
837	
838	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
839	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
840	            of shape `(batch_size, sequence_length)`.
841	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
842	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
843	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
844	            model's internal embedding lookup matrix.
845	        use_cache (`bool`, *optional*):
846	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
847	            `past_key_values`).
848	        output_attentions (`bool`, *optional*):
849	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
850	            tensors for more detail.
851	        output_hidden_states (`bool`, *optional*):
852	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
853	            more detail.
854	        return_dict (`bool`, *optional*):
855	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
856	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
857	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
858	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
859	            the complete sequence length.
860	"""
861	
862	
863	@add_start_docstrings(
864	    "The bare GraniteMoeShared Model outputting raw hidden-states without any specific head on top.",
865	    GRANITEMOESHARED_START_DOCSTRING,
866	)
867	class GraniteMoeSharedModel(GraniteMoeSharedPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/granitemoeshared/modeling_granitemoeshared.py:947
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
946	            logger.warning_once(
947	                "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. "
948	                "Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)"
949	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/granitemoeshared/modular_granitemoeshared.py:156
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
155	
156	GRANITEMOESHARED_START_DOCSTRING = r"""
157	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
158	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
159	    etc.)
160	
161	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
162	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
163	    and behavior.
164	
165	    Parameters:
166	        config ([`GraniteMoeSharedConfig`]):
167	            Model configuration class with all the parameters of the model. Initializing with a config file does not
168	            load the weights associated with the model, only the configuration. Check out the
169	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
170	"""
171	
172	
173	@add_start_docstrings(
174	    "The bare GraniteMoeShared Model outputting raw hidden-states without any specific head on top.",
175	    GRANITEMOESHARED_START_DOCSTRING,
176	)
177	class GraniteMoeSharedPreTrainedModel(GraniteMoePreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/granitemoeshared/modular_granitemoeshared.py:182
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
181	
182	GRANITEMOESHARED_INPUTS_DOCSTRING = r"""
183	    Args:
184	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
185	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
186	            it.
187	
188	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
189	            [`PreTrainedTokenizer.__call__`] for details.
190	
191	            [What are input IDs?](../glossary#input-ids)
192	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
193	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
194	
195	            - 1 for tokens that are **not masked**,
196	            - 0 for tokens that are **masked**.
197	
198	            [What are attention masks?](../glossary#attention-mask)
199	
200	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
201	            [`PreTrainedTokenizer.__call__`] for details.
202	
203	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
204	            `past_key_values`).
205	
206	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
207	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
208	            information on the default strategy.
209	
210	            - 1 indicates the head is **not masked**,
211	            - 0 indicates the head is **masked**.
212	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
213	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
214	            config.n_positions - 1]`.
215	
216	            [What are position IDs?](../glossary#position-ids)
217	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
218	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
219	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
220	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
221	
222	            Two formats are allowed:
223	            - a [`~cache_utils.Cache`] instance;
224	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
225	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
226	            cache format.
227	
228	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
229	            legacy cache format will be returned.
230	
231	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
232	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
233	            of shape `(batch_size, sequence_length)`.
234	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
235	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
236	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
237	            model's internal embedding lookup matrix.
238	        use_cache (`bool`, *optional*):
239	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
240	            `past_key_values`).
241	        output_attentions (`bool`, *optional*):
242	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
243	            tensors for more detail.
244	        output_hidden_states (`bool`, *optional*):
245	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
246	            more detail.
247	        return_dict (`bool`, *optional*):
248	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
249	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
250	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
251	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
252	            the complete sequence length.
253	"""
254	
255	
256	@add_start_docstrings(
257	    "The bare GraniteMoeShared Model outputting raw hidden-states without any specific head on top.",
258	    GRANITEMOESHARED_START_DOCSTRING,
259	)
260	class GraniteMoeSharedModel(GraniteMoeModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/grounding_dino/modeling_grounding_dino.py:1479
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1478	
1479	GROUNDING_DINO_START_DOCSTRING = r"""
1480	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1481	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1482	    etc.)
1483	
1484	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1485	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1486	    and behavior.
1487	
1488	    Parameters:
1489	        config ([`GroundingDinoConfig`]):
1490	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1491	            load the weights associated with the model, only the configuration. Check out the
1492	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1493	"""
1494	
1495	GROUNDING_DINO_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/groupvit/modeling_groupvit.py:798
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
797	
798	GROUPVIT_START_DOCSTRING = r"""
799	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
800	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
801	    behavior.
802	
803	    Parameters:
804	        config ([`GroupViTConfig`]): Model configuration class with all the parameters of the model.
805	            Initializing with a config file does not load the weights associated with the model, only the
806	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
807	"""
808	
809	GROUPVIT_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/groupvit/modeling_tf_groupvit.py:61
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
60	        logger.error(
61	            "GroupViT models are not usable since `tensorflow_probability` can't be loaded. "
62	            "It seems you have `tensorflow_probability` installed with the wrong tensorflow version."
63	            "Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/groupvit/modeling_tf_groupvit.py:1716
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1715	
1716	GROUPVIT_START_DOCSTRING = r"""
1717	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
1718	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1719	    etc.)
1720	
1721	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
1722	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
1723	    behavior.
1724	
1725	    <Tip>
1726	
1727	    TF 2.0 models accepts two formats as inputs:
1728	
1729	    - having all inputs as keyword arguments (like PyTorch models), or
1730	    - having all inputs as a list, tuple or dict in the first positional arguments.
1731	
1732	    This second option is useful when using [`keras.Model.fit`] method which currently requires having all the
1733	    tensors in the first argument of the model call function: `model(inputs)`.
1734	
1735	    If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
1736	    first positional argument :
1737	
1738	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
1739	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
1740	      `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
1741	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
1742	      `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
1743	
1744	    </Tip>
1745	
1746	    Args:
1747	        config ([`GroupViTConfig`]): Model configuration class with all the parameters of the model.
1748	            Initializing with a config file does not load the weights associated with the model, only the
1749	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1750	"""
1751	
1752	GROUPVIT_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/helium/modeling_helium.py:342
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
341	
342	HELIUM_START_DOCSTRING = r"""
343	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
344	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
345	    etc.)
346	
347	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
348	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
349	    and behavior.
350	
351	    Parameters:
352	        config ([`HeliumConfig`]):
353	            Model configuration class with all the parameters of the model. Initializing with a config file does not
354	            load the weights associated with the model, only the configuration. Check out the
355	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
356	"""
357	
358	
359	@add_start_docstrings(
360	    "The bare Helium Model outputting raw hidden-states without any specific head on top.",
361	    HELIUM_START_DOCSTRING,
362	)
363	class HeliumPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/helium/modeling_helium.py:389
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
388	
389	HELIUM_INPUTS_DOCSTRING = r"""
390	    Args:
391	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
392	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
393	            it.
394	
395	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
396	            [`PreTrainedTokenizer.__call__`] for details.
397	
398	            [What are input IDs?](../glossary#input-ids)
399	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
400	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
401	
402	            - 1 for tokens that are **not masked**,
403	            - 0 for tokens that are **masked**.
404	
405	            [What are attention masks?](../glossary#attention-mask)
406	
407	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
408	            [`PreTrainedTokenizer.__call__`] for details.
409	
410	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
411	            `past_key_values`).
412	
413	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
414	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
415	            information on the default strategy.
416	
417	            - 1 indicates the head is **not masked**,
418	            - 0 indicates the head is **masked**.
419	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
420	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
421	            config.n_positions - 1]`.
422	
423	            [What are position IDs?](../glossary#position-ids)
424	        past_key_values (`Cache`, *optional*):
425	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
426	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
427	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
428	
429	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
430	
431	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
432	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
433	            of shape `(batch_size, sequence_length)`.
434	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
435	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
436	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
437	            model's internal embedding lookup matrix.
438	        use_cache (`bool`, *optional*):
439	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
440	            `past_key_values`).
441	        output_attentions (`bool`, *optional*):
442	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
443	            tensors for more detail.
444	        output_hidden_states (`bool`, *optional*):
445	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
446	            more detail.
447	        return_dict (`bool`, *optional*):
448	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
449	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
450	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
451	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
452	            the complete sequence length.
453	"""
454	
455	
456	@add_start_docstrings(
457	    "The bare Helium Model outputting raw hidden-states without any specific head on top.",
458	    HELIUM_START_DOCSTRING,
459	)
460	class HeliumModel(HeliumPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/herbert/tokenization_herbert.py:326
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
325	            raise ImportError(
326	                "You need to install sacremoses to use HerbertTokenizer. "
327	                "See https://pypi.org/project/sacremoses/ for installation."
328	            )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/herbert/tokenization_herbert.py:351
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
350	        with open(merges_file, encoding="utf-8") as merges_handle:
351	            merges = merges_handle.read().split("\n")[:-1]
352	        merges = [tuple(merge.split()[:2]) for merge in merges]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/herbert/tokenization_herbert.py:419
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
418	                logger.error(
419	                    "Make sure you install KyTea (https://github.com/neubig/kytea) and it's python wrapper"
420	                    " (https://github.com/chezou/Mykytea-python) with the following steps"
421	                )

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/herbert/tokenization_herbert.py:610
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
609	        with open(vocab_file, "w", encoding="utf-8") as f:
610	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
611	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/herbert/tokenization_herbert.py:621
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
620	                    index = token_index
621	                writer.write(" ".join(bpe_tokens) + "\n")
622	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/herbert/tokenization_herbert.py:640
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
639	            raise ImportError(
640	                "You need to install sacremoses to use XLMTokenizer. "
641	                "See https://pypi.org/project/sacremoses/ for installation."
642	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/hiera/modeling_hiera.py:872
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
871	
872	HIERA_START_DOCSTRING = r"""
873	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
874	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
875	    behavior.
876	
877	    Parameters:
878	        config ([`HieraConfig`]): Model configuration class with all the parameters of the model.
879	            Initializing with a config file does not load the weights associated with the model, only the
880	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
881	"""
882	
883	HIERA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/hiera/modeling_hiera.py:1212
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1211	@add_start_docstrings(
1212	    """The Hiera Model transformer with the decoder on top for self-supervised pre-training.
1213	
1214	    <Tip>
1215	
1216	    Note that we provide a script to pre-train this model on custom data in our [examples
1217	    directory](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining).
1218	
1219	    </Tip>
1220	    """,
1221	    HIERA_START_DOCSTRING,
1222	)
1223	class HieraForPreTraining(HieraPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/hubert/modeling_hubert.py:1132
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1131	
1132	HUBERT_START_DOCSTRING = r"""
1133	    Hubert was proposed in [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden
1134	    Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia,
1135	    Ruslan Salakhutdinov, Abdelrahman Mohamed.
1136	
1137	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1138	    library implements for all its model (such as downloading or saving etc.).
1139	
1140	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
1141	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
1142	    behavior.
1143	
1144	    Parameters:
1145	        config ([`HubertConfig`]): Model configuration class with all the parameters of the model.
1146	            Initializing with a config file does not load the weights associated with the model, only the
1147	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1148	"""
1149	
1150	
1151	HUBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/hubert/modeling_hubert.py:1151
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1150	
1151	HUBERT_INPUTS_DOCSTRING = r"""
1152	    Args:
1153	        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):
1154	            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file
1155	            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install
1156	            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and
1157	            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2Processor.__call__`] for details.
1158	        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
1159	            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,
1160	            1]`:
1161	
1162	            - 1 for tokens that are **not masked**,
1163	            - 0 for tokens that are **masked**.
1164	
1165	            [What are attention masks?](../glossary#attention-mask)
1166	
1167	            <Tip warning={true}>
1168	
1169	            `attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask ==
1170	            True`. For all models whose processor has `config.return_attention_mask == False`, such as
1171	            [hubert-base](https://huggingface.co/facebook/hubert-base-ls960), `attention_mask` should **not** be passed
1172	            to avoid degraded performance when doing batched inference. For such models `input_values` should simply be
1173	            padded with 0 and passed without `attention_mask`. Be aware that these models also yield slightly different
1174	            results depending on whether `input_values` is padded or not.
1175	
1176	            </Tip>
1177	
1178	        output_attentions (`bool`, *optional*):
1179	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1180	            tensors for more detail.
1181	        output_hidden_states (`bool`, *optional*):
1182	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1183	            more detail.
1184	        return_dict (`bool`, *optional*):
1185	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1186	"""
1187	
1188	
1189	@add_start_docstrings(
1190	    "The bare Hubert Model transformer outputting raw hidden-states without any specific head on top.",
1191	    HUBERT_START_DOCSTRING,
1192	)
1193	class HubertModel(HubertPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/hubert/modeling_tf_hubert.py:1329
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1328	
1329	HUBERT_START_DOCSTRING = r"""
1330	
1331	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
1332	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1333	    etc.)
1334	
1335	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
1336	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
1337	    behavior.
1338	
1339	    <Tip>
1340	
1341	    TensorFlow models and layers in `transformers` accept two formats as input:
1342	
1343	    - having all inputs as keyword arguments (like PyTorch models), or
1344	    - having all inputs as a list, tuple or dict in the first positional argument.
1345	
1346	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
1347	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
1348	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
1349	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
1350	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
1351	    positional argument:
1352	
1353	    - a single Tensor with `input_values` only and nothing else: `model(input_values)`
1354	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
1355	    `model([input_values, attention_mask])` or `model([input_values, attention_mask, token_type_ids])`
1356	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
1357	    `model({"input_values": input_values, "token_type_ids": token_type_ids})`
1358	
1359	    Note that when creating models and layers with
1360	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
1361	    about any of this, as you can just pass inputs like you would to any other Python function!
1362	
1363	    </Tip>
1364	
1365	    Args:
1366	        config ([`HubertConfig`]): Model configuration class with all the parameters of the model.
1367	            Initializing with a config file does not load the weights associated with the model, only the
1368	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1369	"""
1370	
1371	HUBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/hubert/modular_hubert.py:205
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
204	
205	HUBERT_START_DOCSTRING = r"""
206	    Hubert was proposed in [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden
207	    Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia,
208	    Ruslan Salakhutdinov, Abdelrahman Mohamed.
209	
210	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
211	    library implements for all its model (such as downloading or saving etc.).
212	
213	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
214	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
215	    behavior.
216	
217	    Parameters:
218	        config ([`HubertConfig`]): Model configuration class with all the parameters of the model.
219	            Initializing with a config file does not load the weights associated with the model, only the
220	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
221	"""
222	
223	
224	HUBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/hubert/modular_hubert.py:224
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
223	
224	HUBERT_INPUTS_DOCSTRING = r"""
225	    Args:
226	        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):
227	            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file
228	            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install
229	            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and
230	            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2Processor.__call__`] for details.
231	        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
232	            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,
233	            1]`:
234	
235	            - 1 for tokens that are **not masked**,
236	            - 0 for tokens that are **masked**.
237	
238	            [What are attention masks?](../glossary#attention-mask)
239	
240	            <Tip warning={true}>
241	
242	            `attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask ==
243	            True`. For all models whose processor has `config.return_attention_mask == False`, such as
244	            [hubert-base](https://huggingface.co/facebook/hubert-base-ls960), `attention_mask` should **not** be passed
245	            to avoid degraded performance when doing batched inference. For such models `input_values` should simply be
246	            padded with 0 and passed without `attention_mask`. Be aware that these models also yield slightly different
247	            results depending on whether `input_values` is padded or not.
248	
249	            </Tip>
250	
251	        output_attentions (`bool`, *optional*):
252	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
253	            tensors for more detail.
254	        output_hidden_states (`bool`, *optional*):
255	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
256	            more detail.
257	        return_dict (`bool`, *optional*):
258	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
259	"""
260	
261	
262	@add_start_docstrings(
263	    "The bare Hubert Model transformer outputting raw hidden-states without any specific head on top.",
264	    HUBERT_START_DOCSTRING,
265	)
266	class HubertModel(Wav2Vec2Model, HubertPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/ibert/modeling_ibert.py:661
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
660	
661	IBERT_START_DOCSTRING = r"""
662	
663	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
664	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
665	    etc.)
666	
667	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
668	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
669	    and behavior.
670	
671	    Parameters:
672	        config ([`IBertConfig`]): Model configuration class with all the parameters of the
673	            model. Initializing with a config file does not load the weights associated with the model, only the
674	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
675	"""
676	
677	IBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/idefics/modeling_idefics.py:902
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
901	
902	LLAMA_START_DOCSTRING = r"""
903	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
904	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
905	    etc.)
906	
907	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
908	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
909	    and behavior.
910	
911	    Parameters:
912	        config ([`IdeficsConfig`]):
913	            Model configuration class with all the parameters of the model. Initializing with a config file does not
914	            load the weights associated with the model, only the configuration. Check out the
915	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
916	"""
917	
918	
919	@add_start_docstrings(
920	    "The bare LLaMA Model outputting raw hidden-states without any specific head on top.",
921	    LLAMA_START_DOCSTRING,
922	)
923	class IdeficsPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/idefics/modeling_idefics.py:947
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
946	
947	LLAMA_INPUTS_DOCSTRING = r"""
948	    Args:
949	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
950	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
951	            it.
952	
953	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
954	            [`PreTrainedTokenizer.__call__`] for details.
955	
956	            [What are input IDs?](../glossary#input-ids)
957	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
958	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
959	
960	            - 1 for tokens that are **not masked**,
961	            - 0 for tokens that are **masked**.
962	
963	            [What are attention masks?](../glossary#attention-mask)
964	
965	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
966	            [`PreTrainedTokenizer.__call__`] for details.
967	
968	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
969	            `past_key_values`).
970	
971	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
972	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
973	            information on the default strategy.
974	
975	            - 1 indicates the head is **not masked**,
976	            - 0 indicates the head is **masked**.
977	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
978	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
979	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
980	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
981	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
982	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
983	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
984	
985	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
986	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
987	
988	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
989	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
990	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
991	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
992	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
993	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
994	            model's internal embedding lookup matrix.
995	        use_cache (`bool`, *optional*):
996	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
997	            `past_key_values`).
998	        output_attentions (`bool`, *optional*):
999	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1000	            tensors for more detail.
1001	        output_hidden_states (`bool`, *optional*):
1002	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1003	            more detail.
1004	        return_dict (`bool`, *optional*):
1005	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1006	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
1007	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
1008	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
1009	            the complete sequence length.
1010	"""
1011	
1012	
1013	@add_start_docstrings(
1014	    "The bare LLaMA Model outputting raw hidden-states without any specific head on top.",
1015	    LLAMA_START_DOCSTRING,
1016	)
1017	class IdeficsModel(IdeficsPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/idefics/modeling_idefics.py:1146
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1145	                logger.warning_once(
1146	                    "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and "
1147	                    "will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class "
1148	                    "(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/idefics/modeling_tf_idefics.py:1068
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1067	
1068	LLAMA_START_DOCSTRING = r"""
1069	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
1070	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1071	    etc.)
1072	
1073	    This model is also a TensorFlow [tf.keras.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) subclass.
1074	    Use it as a regular TensorFlow Layer and refer to the TensorFlow documentation for all matter related to general usage
1075	    and behavior.
1076	
1077	    Parameters:
1078	        config ([`IdeficsConfig`]):
1079	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1080	            load the weights associated with the model, only the configuration. Check out the
1081	            [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
1082	"""
1083	
1084	
1085	@add_start_docstrings(
1086	    "The bare LLaMA Model outputting raw hidden-states without any specific head on top.",
1087	    LLAMA_START_DOCSTRING,
1088	)
1089	class TFIdeficsPreTrainedModel(TFPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/idefics/modeling_tf_idefics.py:1096
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1095	
1096	LLAMA_INPUTS_DOCSTRING = r"""
1097	    Args:
1098	        input_ids (`tf.Tensor` of shape `(batch_size, sequence_length)`):
1099	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
1100	            it.
1101	
1102	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1103	            [`PreTrainedTokenizer.__call__`] for details.
1104	
1105	            [What are input IDs?](../glossary#input-ids)
1106	        attention_mask (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
1107	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
1108	
1109	            - 1 for tokens that are **not masked**,
1110	            - 0 for tokens that are **masked**.
1111	
1112	            [What are attention masks?](../glossary#attention-mask)
1113	
1114	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1115	            [`PreTrainedTokenizer.__call__`] for details.
1116	
1117	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
1118	            `past_key_values`).
1119	
1120	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
1121	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
1122	            information on the default strategy.
1123	
1124	            - 1 indicates the head is **not masked**,
1125	            - 0 indicates the head is **masked**.
1126	        position_ids (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
1127	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
1128	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
1129	        past_key_values (`tuple(tuple(tf.Tensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
1130	            Tuple of `tuple(tf.Tensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
1131	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
1132	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
1133	
1134	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
1135	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
1136	
1137	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
1138	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
1139	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
1140	        inputs_embeds (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
1141	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
1142	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
1143	            model's internal embedding lookup matrix.
1144	        use_cache (`bool`, *optional*):
1145	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
1146	            `past_key_values`).
1147	        output_attentions (`bool`, *optional*):
1148	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1149	            tensors for more detail.
1150	        output_hidden_states (`bool`, *optional*):
1151	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1152	            more detail.
1153	        return_dict (`bool`, *optional*):
1154	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1155	"""
1156	
1157	
1158	@add_start_docstrings(
1159	    "The bare LLaMA Model outputting raw hidden-states without any specific head on top.",
1160	    LLAMA_START_DOCSTRING,
1161	)
1162	@keras_serializable
1163	class TFIdeficsMainLayer(tf.keras.layers.Layer):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/idefics2/modeling_idefics2.py:487
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
486	
487	IDEFICS2_START_DOCSTRING = r"""
488	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
489	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
490	    etc.)
491	
492	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
493	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
494	    and behavior.
495	
496	    Parameters:
497	        config ([`Idefics2Config`] or [`Idefics2VisionConfig`]):
498	            Model configuration class with all the parameters of the model. Initializing with a config file does not
499	            load the weights associated with the model, only the configuration. Check out the
500	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
501	"""
502	
503	
504	@add_start_docstrings(
505	    "The bare Idefics2 Model outputting raw hidden-states without any specific head on top.",
506	    IDEFICS2_START_DOCSTRING,
507	)
508	class Idefics2PreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/idefics2/modeling_idefics2.py:856
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
855	    "`n_latents` inputs to decrease embedding sequence length. The Resampler acts as a form of learned pooling and ",
856	    "is derived from [Perceiver: General Perception with Iterative Attention](https://arxiv.org/abs/2103.03206)",
857	    IDEFICS2_START_DOCSTRING,
858	)
859	class Idefics2PerceiverResampler(Idefics2PreTrainedModel):
860	    config_class = Idefics2PerceiverConfig
861	    _supports_sdpa = True

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/idefics2/modeling_idefics2.py:936
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
935	
936	IDEFICS2_INPUTS_DOCSTRING = r"""
937	    Args:
938	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
939	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
940	            it.
941	
942	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
943	            [`PreTrainedTokenizer.__call__`] for details.
944	
945	            [What are input IDs?](../glossary#input-ids)
946	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
947	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
948	
949	            - 1 for tokens that are **not masked**,
950	            - 0 for tokens that are **masked**.
951	
952	            [What are attention masks?](../glossary#attention-mask)
953	
954	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
955	            [`PreTrainedTokenizer.__call__`] for details.
956	
957	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
958	            `past_key_values`).
959	
960	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
961	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
962	            information on the default strategy.
963	
964	            - 1 indicates the head is **not masked**,
965	            - 0 indicates the head is **masked**.
966	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
967	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
968	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
969	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
970	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
971	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
972	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
973	
974	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
975	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
976	
977	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
978	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
979	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
980	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
981	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
982	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
983	            model's internal embedding lookup matrix.
984	        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):
985	            The tensors corresponding to the input images. Pixel values can be obtained using
986	            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`LlavaProcessor`] uses
987	            [`CLIPImageProcessor`] for processing images).
988	        pixel_attention_mask (`torch.Tensor` of shape `(batch_size, image_size, image_size)`, *optional*):
989	            Mask to avoid performing attention on padding pixel indices.
990	        image_hidden_states (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):
991	            The hidden states of the image encoder after modality projection and perceiver resampling.
992	        use_cache (`bool`, *optional*):
993	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
994	            `past_key_values`).
995	        output_attentions (`bool`, *optional*):
996	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
997	            tensors for more detail.
998	        output_hidden_states (`bool`, *optional*):
999	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1000	            more detail.
1001	        return_dict (`bool`, *optional*):
1002	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1003	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
1004	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
1005	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
1006	            the complete sequence length.
1007	"""
1008	
1009	
1010	@add_start_docstrings(
1011	    """Idefics2 model consisting of a SIGLIP vision encoder and Mistral language decoder""",
1012	    IDEFICS2_START_DOCSTRING,
1013	)
1014	class Idefics2Model(Idefics2PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/idefics2/modeling_idefics2.py:1150
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1149	                    logger.warning_once(
1150	                        "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and "
1151	                        "will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class "
1152	                        "(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/idefics3/modeling_idefics3.py:504
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
503	
504	IDEFICS3_START_DOCSTRING = r"""
505	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
506	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
507	    etc.)
508	
509	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
510	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
511	    and behavior.
512	
513	    Parameters:
514	        config ([`Idefics3Config`] or [`Idefics3VisionConfig`]):
515	            Model configuration class with all the parameters of the model. Initializing with a config file does not
516	            load the weights associated with the model, only the configuration. Check out the
517	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
518	"""
519	
520	
521	@add_start_docstrings(
522	    "The bare Idefics3 Model outputting raw hidden-states without any specific head on top.",
523	    IDEFICS3_START_DOCSTRING,
524	)
525	class Idefics3PreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/idefics3/modeling_idefics3.py:557
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
556	
557	IDEFICS3_VISION_START_DOCSTRING = r"""
558	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
559	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
560	    etc.)
561	
562	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
563	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
564	    and behavior.
565	
566	    Parameters:
567	        config ([`Idefics3VisionConfig`]):
568	            Model configuration class with all the parameters of the model. Initializing with a config file does not
569	            load the weights associated with the model, only the configuration. Check out the
570	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
571	"""
572	
573	
574	@add_start_docstrings(
575	    "The Idefics3 Vision Transformer Model outputting raw image embedding.",
576	    IDEFICS3_VISION_START_DOCSTRING,
577	)
578	class Idefics3VisionTransformer(Idefics3PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/idefics3/modeling_idefics3.py:660
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
659	
660	IDEFICS3_INPUTS_DOCSTRING = r"""
661	    Args:
662	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
663	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
664	            it.
665	
666	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
667	            [`PreTrainedTokenizer.__call__`] for details.
668	
669	            [What are input IDs?](../glossary#input-ids)
670	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
671	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
672	
673	            - 1 for tokens that are **not masked**,
674	            - 0 for tokens that are **masked**.
675	
676	            [What are attention masks?](../glossary#attention-mask)
677	
678	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
679	            [`PreTrainedTokenizer.__call__`] for details.
680	
681	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
682	            `past_key_values`).
683	
684	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
685	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
686	            information on the default strategy.
687	
688	            - 1 indicates the head is **not masked**,
689	            - 0 indicates the head is **masked**.
690	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
691	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
692	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
693	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
694	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
695	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
696	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
697	
698	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
699	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
700	
701	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
702	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
703	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
704	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
705	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
706	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
707	            model's internal embedding lookup matrix.
708	        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):
709	            The tensors corresponding to the input images. Pixel values can be obtained using
710	            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`LlavaProcessor`] uses
711	            [`CLIPImageProcessor`] for processing images).
712	        pixel_attention_mask (`torch.Tensor` of shape `(batch_size, image_size, image_size)`, *optional*):
713	            Mask to avoid performing attention on padding pixel indices.
714	        image_hidden_states (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):
715	            The hidden states of the image encoder after modality projection.
716	        use_cache (`bool`, *optional*):
717	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
718	            `past_key_values`).
719	        output_attentions (`bool`, *optional*):
720	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
721	            tensors for more detail.
722	        output_hidden_states (`bool`, *optional*):
723	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
724	            more detail.
725	        return_dict (`bool`, *optional*):
726	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
727	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
728	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
729	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
730	            the complete sequence length.
731	"""
732	
733	
734	@add_start_docstrings(
735	    """Idefics3 model consisting of a SIGLIP vision encoder and Llama3 language decoder""",
736	    IDEFICS3_START_DOCSTRING,
737	)
738	class Idefics3Model(Idefics3PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/ijepa/modeling_ijepa.py:509
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
508	
509	IJEPA_START_DOCSTRING = r"""
510	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
511	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
512	    behavior.
513	
514	    Parameters:
515	        config ([`IJepaConfig`]): Model configuration class with all the parameters of the model.
516	            Initializing with a config file does not load the weights associated with the model, only the
517	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
518	"""
519	
520	
521	@add_start_docstrings(
522	    "The bare IJepa Model transformer outputting raw hidden-states without any specific head on top.",
523	    IJEPA_START_DOCSTRING,
524	)
525	class IJepaModel(IJepaPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/ijepa/modular_ijepa.py:138
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
137	
138	IJEPA_START_DOCSTRING = r"""
139	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
140	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
141	    behavior.
142	
143	    Parameters:
144	        config ([`IJepaConfig`]): Model configuration class with all the parameters of the model.
145	            Initializing with a config file does not load the weights associated with the model, only the
146	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
147	"""
148	
149	
150	@add_start_docstrings(
151	    "The bare IJepa Model transformer outputting raw hidden-states without any specific head on top.",
152	    IJEPA_START_DOCSTRING,
153	)
154	class IJepaModel(IJepaPreTrainedModel, ViTModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/imagegpt/modeling_imagegpt.py:63
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
62	        logger.error(
63	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
64	            "https://www.tensorflow.org/install/ for installation instructions."
65	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/imagegpt/modeling_imagegpt.py:528
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
527	
528	IMAGEGPT_START_DOCSTRING = r"""
529	
530	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
531	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
532	    etc.)
533	
534	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
535	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
536	    and behavior.
537	
538	    Parameters:
539	        config ([`ImageGPTConfig`]): Model configuration class with all the parameters of the model.
540	            Initializing with a config file does not load the weights associated with the model, only the
541	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
542	"""
543	
544	IMAGEGPT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/informer/modeling_informer.py:895
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
894	
895	INFORMER_START_DOCSTRING = r"""
896	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
897	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
898	    etc.)
899	
900	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
901	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
902	    and behavior.
903	
904	    Parameters:
905	        config ([`TimeSeriesTransformerConfig`]):
906	            Model configuration class with all the parameters of the model. Initializing with a config file does not
907	            load the weights associated with the model, only the configuration. Check out the
908	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
909	"""
910	
911	INFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/instructblip/modeling_instructblip.py:348
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
347	
348	INSTRUCTBLIP_START_DOCSTRING = r"""
349	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
350	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
351	    etc.)
352	
353	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
354	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
355	    and behavior.
356	
357	    Parameters:
358	        config ([`InstructBlipConfig`]): Model configuration class with all the parameters of the model.
359	            Initializing with a config file does not load the weights associated with the model, only the
360	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
361	"""
362	
363	INSTRUCTBLIP_VISION_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/instructblip/modeling_instructblip.py:1356
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1355	            logger.warning(
1356	                "The `language_model` is not in the `hf_device_map` dictionary and you are running your script"
1357	                " in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`."
1358	                " Please pass a `device_map` that contains `language_model` to remove this warning."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/instructblip/modeling_instructblip.py:1479
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1478	            logger.warning_once(
1479	                "Expanding inputs for image tokens in InstructBLIP should be done in processing. "
1480	                "Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your InstructBLIP model. "
1481	                "Using processors without these attributes in the config is deprecated and will throw an error in v4.50."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/instructblip/modeling_instructblip.py:1621
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1620	            logger.warning_once(
1621	                "Expanding inputs for image tokens in InstructBLIP should be done in processing. "
1622	                "Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your InstructBLIP model. "
1623	                "Using processors without these attributes in the config is deprecated and will throw an error in v4.50."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/instructblip/processing_instructblip.py:149
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
148	                    logger.warning_once(
149	                        "Expanding inputs for image tokens in InstructBLIP should be done in processing. "
150	                        "Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your InstructBLIP model. "
151	                        "Using processors without these attributes in the config is deprecated and will throw an error in v4.50."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/instructblipvideo/modeling_instructblipvideo.py:1189
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1188	
1189	INSTRUCTBLIPVIDEO_START_DOCSTRING = r"""
1190	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1191	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1192	    etc.)
1193	
1194	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1195	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1196	    and behavior.
1197	
1198	    Parameters:
1199	        config ([`InstructBlipVideoConfig`]): Model configuration class with all the parameters of the model.
1200	            Initializing with a config file does not load the weights associated with the model, only the
1201	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1202	"""
1203	
1204	INSTRUCTBLIPVIDEO_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/instructblipvideo/modeling_instructblipvideo.py:1350
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1349	            logger.warning(
1350	                "The `language_model` is not in the `hf_device_map` dictionary and you are running your script"
1351	                " in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`."
1352	                " Please pass a `device_map` that contains `language_model` to remove this warning."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/instructblipvideo/modeling_instructblipvideo.py:1506
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1505	            logger.warning_once(
1506	                "Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. "
1507	                "Please follow instruction here (https://gist.github.com/zucchini-nlp/65f22892b054dc0d68228af56fbeaac2) to update your InstructBLIPVideo model. "
1508	                "Using processors without these attributes in the config is deprecated and will throw an error in v4.47."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/instructblipvideo/modeling_instructblipvideo.py:1656
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1655	            logger.warning_once(
1656	                "Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. "
1657	                "Please follow instruction here (https://gist.github.com/zucchini-nlp/65f22892b054dc0d68228af56fbeaac2) to update your InstructBLIPVideo model. "
1658	                "Using processors without these attributes in the config is deprecated and will throw an error in v4.47."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/instructblipvideo/modular_instructblipvideo.py:310
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
309	            logger.warning_once(
310	                "Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. "
311	                "Please follow instruction here (https://gist.github.com/zucchini-nlp/65f22892b054dc0d68228af56fbeaac2) to update your InstructBLIPVideo model. "
312	                "Using processors without these attributes in the config is deprecated and will throw an error in v4.47."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/instructblipvideo/modular_instructblipvideo.py:460
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
459	            logger.warning_once(
460	                "Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. "
461	                "Please follow instruction here (https://gist.github.com/zucchini-nlp/65f22892b054dc0d68228af56fbeaac2) to update your InstructBLIPVideo model. "
462	                "Using processors without these attributes in the config is deprecated and will throw an error in v4.47."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/instructblipvideo/processing_instructblipvideo.py:149
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
148	                    logger.warning_once(
149	                        "Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. "
150	                        "Please follow instruction here (https://gist.github.com/zucchini-nlp/65f22892b054dc0d68228af56fbeaac2) to update your InstructBLIPVideo model. "
151	                        "Using processors without these attributes in the config is deprecated and will throw an error in v4.47."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/jamba/modeling_jamba.py:622
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
621	            logger.warning_once(
622	                "The fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`"
623	                " is None. To install follow https://github.com/state-spaces/mamba/#installation and"
624	                " https://github.com/Dao-AILab/causal-conv1d. If you want to use the naive implementation, set `use_mamba_kernels=False` in the model config"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/jamba/modeling_jamba.py:1076
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1075	
1076	JAMBA_START_DOCSTRING = r"""
1077	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1078	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1079	    etc.)
1080	
1081	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1082	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1083	    and behavior.
1084	
1085	    Parameters:
1086	        config ([`JambaConfig`]):
1087	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1088	            load the weights associated with the model, only the configuration. Check out the
1089	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1090	"""
1091	
1092	
1093	@add_start_docstrings(
1094	    "The bare Jamba Model outputting raw hidden-states without any specific head on top.",
1095	    JAMBA_START_DOCSTRING,
1096	)
1097	class JambaPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/jamba/modeling_jamba.py:1120
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1119	
1120	JAMBA_INPUTS_DOCSTRING = r"""
1121	    Args:
1122	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
1123	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
1124	            it.
1125	
1126	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1127	            [`PreTrainedTokenizer.__call__`] for details.
1128	
1129	            [What are input IDs?](../glossary#input-ids)
1130	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
1131	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
1132	
1133	            - 1 for tokens that are **not masked**,
1134	            - 0 for tokens that are **masked**.
1135	
1136	            [What are attention masks?](../glossary#attention-mask)
1137	
1138	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1139	            [`PreTrainedTokenizer.__call__`] for details.
1140	
1141	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
1142	            `past_key_values`).
1143	
1144	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
1145	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
1146	            information on the default strategy.
1147	
1148	            - 1 indicates the head is **not masked**,
1149	            - 0 indicates the head is **masked**.
1150	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
1151	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
1152	            config.n_positions - 1]`.
1153	
1154	            [What are position IDs?](../glossary#position-ids)
1155	        past_key_values (`HybridMambaAttentionDynamicCache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
1156	            A HybridMambaAttentionDynamicCache object containing pre-computed hidden-states (keys and values in the
1157	            self-attention blocks and convolution and ssm states in the mamba blocks) that can be used (see
1158	            `past_key_values` input) to speed up sequential decoding.
1159	            Key and value cache tensors have shape `(batch_size, num_heads, seq_len, head_dim)`.
1160	            Convolution and ssm states tensors have shape `(batch_size, d_inner, d_conv)` and
1161	            `(batch_size, d_inner, d_state)` respectively.
1162	            See the `HybridMambaAttentionDynamicCache` class for more details.
1163	
1164	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that
1165	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
1166	            `input_ids` of shape `(batch_size, sequence_length)`.
1167	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
1168	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
1169	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
1170	            model's internal embedding lookup matrix.
1171	        use_cache (`bool`, *optional*):
1172	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
1173	            `past_key_values`).
1174	        output_attentions (`bool`, *optional*):
1175	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1176	            tensors for more detail.
1177	        output_hidden_states (`bool`, *optional*):
1178	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1179	            more detail.
1180	        output_router_logits (`bool`, *optional*):
1181	            Whether or not to return the logits of all the routers. They are useful for computing the router loss, and
1182	            should not be returned during inference.
1183	        return_dict (`bool`, *optional*):
1184	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1185	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
1186	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
1187	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
1188	            the complete sequence length.
1189	"""
1190	
1191	ALL_DECODER_LAYER_TYPES = {"attention": JambaAttentionDecoderLayer, "mamba": JambaMambaDecoderLayer}

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/jetmoe/modeling_jetmoe.py:870
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
869	
870	JETMOE_START_DOCSTRING = r"""
871	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
872	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
873	    behavior.
874	
875	    Parameters:
876	        config ([`JetMoeConfig`]): Model configuration class with all the parameters of the model.
877	            Initializing with a config file does not load the weights associated with the model, only the
878	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
879	"""
880	
881	JETMOE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/jetmoe/modeling_jetmoe.py:1004
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1003	                logger.warning_once(
1004	                    "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and "
1005	                    "will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class "
1006	                    "(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/kosmos2/modeling_kosmos2.py:99
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
98	
99	KOSMOS2_START_DOCSTRING = r"""
100	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
101	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
102	    etc.)
103	
104	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
105	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
106	    and behavior.
107	
108	    Parameters:
109	        config ([`Kosmos2Config`]): Model configuration class with all the parameters of the model.
110	            Initializing with a config file does not load the weights associated with the model, only the
111	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
112	"""
113	
114	KOSMOS2_VISION_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlm/modeling_layoutlm.py:642
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
641	
642	LAYOUTLM_START_DOCSTRING = r"""
643	    The LayoutLM model was proposed in [LayoutLM: Pre-training of Text and Layout for Document Image
644	    Understanding](https://arxiv.org/abs/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei and
645	    Ming Zhou.
646	
647	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
648	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
649	    behavior.
650	
651	    Parameters:
652	        config ([`LayoutLMConfig`]): Model configuration class with all the parameters of the model.
653	            Initializing with a config file does not load the weights associated with the model, only the
654	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
655	"""
656	
657	LAYOUTLM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlm/modeling_layoutlm.py:983
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
982	@add_start_docstrings(
983	    """
984	    LayoutLM Model with a sequence classification head on top (a linear layer on top of the pooled output) e.g. for
985	    document image classification tasks such as the [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/) dataset.
986	    """,
987	    LAYOUTLM_START_DOCSTRING,
988	)
989	class LayoutLMForSequenceClassification(LayoutLMPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlm/modeling_layoutlm.py:1119
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1118	@add_start_docstrings(
1119	    """
1120	    LayoutLM Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
1121	    sequence labeling (information extraction) tasks such as the [FUNSD](https://guillaumejaume.github.io/FUNSD/)
1122	    dataset and the [SROIE](https://rrc.cvc.uab.es/?ch=13) dataset.
1123	    """,
1124	    LAYOUTLM_START_DOCSTRING,
1125	)
1126	class LayoutLMForTokenClassification(LayoutLMPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlm/modeling_layoutlm.py:1237
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1236	@add_start_docstrings(
1237	    """
1238	    LayoutLM Model with a span classification head on top for extractive question-answering tasks such as
1239	    [DocVQA](https://rrc.cvc.uab.es/?ch=17) (a linear layer on top of the final hidden-states output to compute `span
1240	    start logits` and `span end logits`).
1241	    """,
1242	    LAYOUTLM_START_DOCSTRING,
1243	)
1244	class LayoutLMForQuestionAnswering(LayoutLMPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlm/modeling_tf_layoutlm.py:949
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
948	
949	LAYOUTLM_START_DOCSTRING = r"""
950	
951	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
952	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
953	    etc.)
954	
955	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
956	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
957	    behavior.
958	
959	    <Tip>
960	
961	    TensorFlow models and layers in `transformers` accept two formats as input:
962	
963	    - having all inputs as keyword arguments (like PyTorch models), or
964	    - having all inputs as a list, tuple or dict in the first positional argument.
965	
966	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
967	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
968	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
969	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
970	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
971	    positional argument:
972	
973	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
974	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
975	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
976	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
977	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
978	
979	    Note that when creating models and layers with
980	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
981	    about any of this, as you can just pass inputs like you would to any other Python function!
982	
983	    </Tip>
984	
985	    Args:
986	        config ([`LayoutLMConfig`]): Model configuration class with all the parameters of the model.
987	            Initializing with a config file does not load the weights associated with the model, only the
988	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
989	"""
990	
991	LAYOUTLM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlm/modeling_tf_layoutlm.py:1534
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1533	@add_start_docstrings(
1534	    """
1535	    LayoutLM Model with a span classification head on top for extractive question-answering tasks such as
1536	    [DocVQA](https://rrc.cvc.uab.es/?ch=17) (a linear layer on top of the final hidden-states output to compute `span
1537	    start logits` and `span end logits`).
1538	    """,
1539	    LAYOUTLM_START_DOCSTRING,
1540	)
1541	class TFLayoutLMForQuestionAnswering(TFLayoutLMPreTrainedModel, TFQuestionAnsweringLoss):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlm/tokenization_layoutlm.py:287
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
286	                    index = token_index
287	                writer.write(token + "\n")
288	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py:616
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
615	
616	LAYOUTLMV2_START_DOCSTRING = r"""
617	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
618	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
619	    behavior.
620	
621	    Parameters:
622	        config ([`LayoutLMv2Config`]): Model configuration class with all the parameters of the model.
623	            Initializing with a config file does not load the weights associated with the model, only the
624	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
625	"""
626	
627	LAYOUTLMV2_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py:959
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
958	@add_start_docstrings(
959	    """
960	    LayoutLMv2 Model with a sequence classification head on top (a linear layer on top of the concatenation of the
961	    final hidden state of the [CLS] token, average-pooled initial visual embeddings and average-pooled final visual
962	    embeddings, e.g. for document image classification tasks such as the
963	    [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/) dataset.
964	    """,
965	    LAYOUTLMV2_START_DOCSTRING,
966	)
967	class LayoutLMv2ForSequenceClassification(LayoutLMv2PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py:1141
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1140	@add_start_docstrings(
1141	    """
1142	    LayoutLMv2 Model with a token classification head on top (a linear layer on top of the text part of the hidden
1143	    states) e.g. for sequence labeling (information extraction) tasks such as
1144	    [FUNSD](https://guillaumejaume.github.io/FUNSD/), [SROIE](https://rrc.cvc.uab.es/?ch=13),
1145	    [CORD](https://github.com/clovaai/cord) and [Kleister-NDA](https://github.com/applicaai/kleister-nda).
1146	    """,
1147	    LAYOUTLMV2_START_DOCSTRING,
1148	)
1149	class LayoutLMv2ForTokenClassification(LayoutLMv2PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py:1273
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1272	@add_start_docstrings(
1273	    """
1274	    LayoutLMv2 Model with a span classification head on top for extractive question-answering tasks such as
1275	    [DocVQA](https://rrc.cvc.uab.es/?ch=17) (a linear layer on top of the text part of the hidden-states output to
1276	    compute `span start logits` and `span end logits`).
1277	    """,
1278	    LAYOUTLMV2_START_DOCSTRING,
1279	)
1280	class LayoutLMv2ForQuestionAnswering(LayoutLMv2PreTrainedModel):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py:400
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
399	                    index = token_index
400	                writer.write(token + "\n")
401	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py:891
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
890	            raise NotImplementedError(
891	                "return_offset_mapping is not available when using Python tokenizers. "
892	                "To use this feature, change your tokenizer to one deriving from "
893	                "transformers.PreTrainedTokenizerFast. "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py:51
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
50	
51	LAYOUTLMV3_START_DOCSTRING = r"""
52	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
53	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
54	    behavior.
55	
56	    Parameters:
57	        config ([`LayoutLMv3Config`]): Model configuration class with all the parameters of the model.
58	            Initializing with a config file does not load the weights associated with the model, only the
59	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
60	"""
61	
62	LAYOUTLMV3_MODEL_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py:1032
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1031	@add_start_docstrings(
1032	    """
1033	    LayoutLMv3 Model with a token classification head on top (a linear layer on top of the final hidden states) e.g.
1034	    for sequence labeling (information extraction) tasks such as [FUNSD](https://guillaumejaume.github.io/FUNSD/),
1035	    [SROIE](https://rrc.cvc.uab.es/?ch=13), [CORD](https://github.com/clovaai/cord) and
1036	    [Kleister-NDA](https://github.com/applicaai/kleister-nda).
1037	    """,
1038	    LAYOUTLMV3_START_DOCSTRING,
1039	)
1040	class LayoutLMv3ForTokenClassification(LayoutLMv3PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py:1145
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1144	@add_start_docstrings(
1145	    """
1146	    LayoutLMv3 Model with a span classification head on top for extractive question-answering tasks such as
1147	    [DocVQA](https://rrc.cvc.uab.es/?ch=17) (a linear layer on top of the text part of the hidden-states output to
1148	    compute `span start logits` and `span end logits`).
1149	    """,
1150	    LAYOUTLMV3_START_DOCSTRING,
1151	)
1152	class LayoutLMv3ForQuestionAnswering(LayoutLMv3PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py:1275
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1274	@add_start_docstrings(
1275	    """
1276	    LayoutLMv3 Model with a sequence classification head on top (a linear layer on top of the final hidden state of the
1277	    [CLS] token) e.g. for document image classification tasks such as the
1278	    [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/) dataset.
1279	    """,
1280	    LAYOUTLMV3_START_DOCSTRING,
1281	)
1282	class LayoutLMv3ForSequenceClassification(LayoutLMv3PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py:1134
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1133	
1134	LAYOUTLMV3_START_DOCSTRING = r"""
1135	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
1136	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1137	    etc.)
1138	
1139	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
1140	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
1141	    behavior.
1142	
1143	    <Tip>
1144	
1145	    TensorFlow models and layers in `transformers` accept two formats as input:
1146	
1147	    - having all inputs as keyword arguments (like PyTorch models), or
1148	    - having all inputs as a list, tuple or dict in the first positional argument.
1149	
1150	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
1151	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
1152	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
1153	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
1154	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
1155	    positional argument:
1156	
1157	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
1158	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
1159	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
1160	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
1161	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
1162	
1163	    Note that when creating models and layers with
1164	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
1165	    about any of this, as you can just pass inputs like you would to any other Python function!
1166	
1167	    </Tip>
1168	
1169	    Parameters:
1170	        config ([`LayoutLMv3Config`]): Model configuration class with all the parameters of the model.
1171	            Initializing with a config file does not load the weights associated with the model, only the
1172	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
1173	"""
1174	
1175	LAYOUTLMV3_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py:1387
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1386	@add_start_docstrings(
1387	    """
1388	    LayoutLMv3 Model with a sequence classification head on top (a linear layer on top of the final hidden state of the
1389	    [CLS] token) e.g. for document image classification tasks such as the
1390	    [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/) dataset.
1391	    """,
1392	    LAYOUTLMV3_START_DOCSTRING,
1393	)
1394	class TFLayoutLMv3ForSequenceClassification(TFLayoutLMv3PreTrainedModel, TFSequenceClassificationLoss):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py:1501
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1500	@add_start_docstrings(
1501	    """
1502	    LayoutLMv3 Model with a token classification head on top (a linear layer on top of the final hidden states) e.g.
1503	    for sequence labeling (information extraction) tasks such as [FUNSD](https://guillaumejaume.github.io/FUNSD/),
1504	    [SROIE](https://rrc.cvc.uab.es/?ch=13), [CORD](https://github.com/clovaai/cord) and
1505	    [Kleister-NDA](https://github.com/applicaai/kleister-nda).
1506	    """,
1507	    LAYOUTLMV3_START_DOCSTRING,
1508	)
1509	class TFLayoutLMv3ForTokenClassification(TFLayoutLMv3PreTrainedModel, TFTokenClassificationLoss):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py:1639
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1638	@add_start_docstrings(
1639	    """
1640	    LayoutLMv3 Model with a span classification head on top for extractive question-answering tasks such as
1641	    [DocVQA](https://rrc.cvc.uab.es/?ch=17) (a linear layer on top of the text part of the hidden-states output to
1642	    compute `span start logits` and `span end logits`).
1643	    """,
1644	    LAYOUTLMV3_START_DOCSTRING,
1645	)
1646	class TFLayoutLMv3ForQuestionAnswering(TFLayoutLMv3PreTrainedModel, TFQuestionAnsweringLoss):

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py:296
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
295	        with open(merges_file, encoding="utf-8") as merges_handle:
296	            bpe_merges = merges_handle.read().split("\n")[1:-1]
297	        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py:425
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
424	        with open(vocab_file, "w", encoding="utf-8") as f:
425	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
426	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py:429
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
428	        with open(merge_file, "w", encoding="utf-8") as writer:
429	            writer.write("#version: 0.2\n")
430	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py:437
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
436	                    index = token_index
437	                writer.write(" ".join(bpe_tokens) + "\n")
438	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py:1026
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1025	            raise NotImplementedError(
1026	                "return_offset_mapping is not available when using Python tokenizers. "
1027	                "To use this feature, change your tokenizer to one deriving from "
1028	                "transformers.PreTrainedTokenizerFast. "

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py:433
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
432	                content_spiece_model = self.sp_model.serialized_model_proto()
433	                fi.write(content_spiece_model)
434	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py:745
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
744	            raise NotImplementedError(
745	                "return_offset_mapping is not available when using Python tokenizers. "
746	                "To use this feature, change your tokenizer to one deriving from "
747	                "transformers.PreTrainedTokenizerFast. "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/led/modeling_led.py:1465
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1464	
1465	LED_START_DOCSTRING = r"""
1466	    This model inherits from [`PreTrainedModel`]. See the superclass documentation for the generic methods the library
1467	    implements for all its models (such as downloading or saving, resizing the input embeddings, pruning heads etc.)
1468	
1469	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1470	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for general usage and behavior.
1471	
1472	    Parameters:
1473	        config ([`LEDConfig`]):
1474	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1475	            load the weights associated with the model, only the configuration. Check out the
1476	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1477	"""
1478	
1479	LED_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/led/modeling_led.py:1517
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1516	
1517	LED_INPUTS_DOCSTRING = r"""
1518	    Args:
1519	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
1520	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
1521	            it.
1522	
1523	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1524	            [`PreTrainedTokenizer.__call__`] for details.
1525	
1526	            [What are input IDs?](../glossary#input-ids)
1527	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
1528	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
1529	
1530	            - 1 for tokens that are **not masked**,
1531	            - 0 for tokens that are **masked**.
1532	
1533	            [What are attention masks?](../glossary#attention-mask)
1534	        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
1535	            Indices of decoder input sequence tokens in the vocabulary.
1536	
1537	            Indices can be obtained using [`LedTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1538	            [`PreTrainedTokenizer.__call__`] for details.
1539	
1540	            [What are input IDs?](../glossary#input-ids)
1541	
1542	            LED uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`
1543	            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).
1544	        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
1545	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
1546	            be used by default.
1547	
1548	            If you want to change padding behavior, you should read [`modeling_led._prepare_decoder_inputs`] and modify
1549	            to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more information on the
1550	            default strategy.
1551	        global_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):
1552	            Mask to decide the attention given on each token, local attention or global attention for the encoder.
1553	            Tokens with global attention attends to all other tokens, and all other tokens attend to them. This is
1554	            important for task-specific finetuning because it makes the model more flexible at representing the task.
1555	            For example, for classification, the <s> token should be given global attention. For QA, all question
1556	            tokens should also have global attention. Please refer to the [Longformer
1557	            paper](https://arxiv.org/abs/2004.05150) for more details. Mask values selected in `[0, 1]`:
1558	
1559	            - 0 for local attention (a sliding window attention),
1560	            - 1 for global attention (tokens that attend to all other tokens, and all other tokens attend to them).
1561	        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
1562	            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:
1563	
1564	            - 1 indicates the head is **not masked**,
1565	            - 0 indicates the head is **masked**.
1566	
1567	        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
1568	            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:
1569	
1570	            - 1 indicates the head is **not masked**,
1571	            - 0 indicates the head is **masked**.
1572	        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
1573	            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,
1574	            1]`:
1575	
1576	            - 1 indicates the head is **not masked**,
1577	            - 0 indicates the head is **masked**.
1578	
1579	        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):
1580	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
1581	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
1582	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
1583	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
1584	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
1585	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
1586	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
1587	
1588	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
1589	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
1590	
1591	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
1592	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
1593	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
1594	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
1595	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
1596	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
1597	            model's internal embedding lookup matrix.
1598	        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):
1599	            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded
1600	            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be
1601	            input (see `past_key_values`). This is useful if you want more control over how to convert
1602	            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.
1603	
1604	            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value
1605	            of `inputs_embeds`.
1606	        use_cache (`bool`, *optional*):
1607	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
1608	            `past_key_values`).
1609	        output_attentions (`bool`, *optional*):
1610	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1611	            tensors for more detail.
1612	        output_hidden_states (`bool`, *optional*):
1613	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1614	            more detail.
1615	        return_dict (`bool`, *optional*):
1616	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1617	"""
1618	
1619	
1620	class LEDEncoder(LEDPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/led/modeling_tf_led.py:1614
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1613	
1614	LED_START_DOCSTRING = r"""
1615	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
1616	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1617	    etc.)
1618	
1619	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
1620	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
1621	    behavior.
1622	
1623	    <Tip>
1624	
1625	    TensorFlow models and layers in `transformers` accept two formats as input:
1626	
1627	    - having all inputs as keyword arguments (like PyTorch models), or
1628	    - having all inputs as a list, tuple or dict in the first positional argument.
1629	
1630	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
1631	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
1632	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
1633	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
1634	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
1635	    positional argument:
1636	
1637	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
1638	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
1639	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
1640	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
1641	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
1642	
1643	    Note that when creating models and layers with
1644	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
1645	    about any of this, as you can just pass inputs like you would to any other Python function!
1646	
1647	    </Tip>
1648	
1649	    Args:
1650	        config ([`LEDConfig`]): Model configuration class with all the parameters of the model.
1651	            Initializing with a config file does not load the weights associated with the model, only the
1652	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
1653	"""
1654	
1655	LED_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/led/tokenization_led.py:192
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
191	        with open(merges_file, encoding="utf-8") as merges_handle:
192	            bpe_merges = merges_handle.read().split("\n")[1:-1]
193	        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/led/tokenization_led.py:307
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
306	        with open(vocab_file, "w", encoding="utf-8") as f:
307	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
308	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/led/tokenization_led.py:311
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
310	        with open(merge_file, "w", encoding="utf-8") as writer:
311	            writer.write("#version: 0.2\n")
312	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/led/tokenization_led.py:319
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
318	                    index = token_index
319	                writer.write(" ".join(bpe_tokens) + "\n")
320	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/levit/modeling_levit.py:506
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
505	
506	LEVIT_START_DOCSTRING = r"""
507	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
508	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
509	    behavior.
510	
511	    Parameters:
512	        config ([`LevitConfig`]): Model configuration class with all the parameters of the model.
513	            Initializing with a config file does not load the weights associated with the model, only the
514	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
515	"""
516	
517	LEVIT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/lilt/modeling_lilt.py:599
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
598	
599	LILT_START_DOCSTRING = r"""
600	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
601	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
602	    etc.)
603	
604	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
605	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
606	    and behavior.
607	
608	    Parameters:
609	        config ([`LiltConfig`]): Model configuration class with all the parameters of the
610	            model. Initializing with a config file does not load the weights associated with the model, only the
611	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
612	"""
613	
614	LILT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llama/modeling_flax_llama.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	
47	LLAMA_START_DOCSTRING = r"""
48	
49	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
50	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
51	    etc.)
52	
53	    This model is also a Flax Linen
54	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
55	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
56	
57	    Finally, this model supports inherent JAX features such as:
58	
59	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
60	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
61	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
62	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
63	
64	    Parameters:
65	        config ([`LlamaConfig`]): Model configuration class with all the parameters of the model.
66	            Initializing with a config file does not load the weights associated with the model, only the
67	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
68	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
69	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`, or
70	            `jax.numpy.bfloat16`.
71	
72	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
73	            specified all the computation will be performed with the given `dtype`.
74	
75	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
76	            parameters.**
77	
78	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
79	            [`~FlaxPreTrainedModel.to_bf16`].
80	"""
81	
82	LLAMA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llama/modeling_flax_llama.py:82
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
81	
82	LLAMA_INPUTS_DOCSTRING = r"""
83	    Args:
84	        input_ids (`numpy.ndarray` of shape `(batch_size, input_ids_length)`):
85	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
86	            it.
87	
88	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
89	            [`PreTrainedTokenizer.__call__`] for details.
90	
91	            [What are input IDs?](../glossary#input-ids)
92	        attention_mask (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
93	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
94	
95	            - 1 for tokens that are **not masked**,
96	            - 0 for tokens that are **masked**.
97	
98	            [What are attention masks?](../glossary#attention-mask)
99	
100	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
101	            [`PreTrainedTokenizer.__call__`] for details.
102	
103	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
104	            `past_key_values`).
105	
106	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
107	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
108	            information on the default strategy.
109	
110	            - 1 indicates the head is **not masked**,
111	            - 0 indicates the head is **masked**.
112	        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
113	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
114	            config.n_positions - 1]`.
115	
116	            [What are position IDs?](../glossary#position-ids)
117	        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):
118	            Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
119	            auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.
120	        output_attentions (`bool`, *optional*):
121	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
122	            tensors for more detail.
123	        output_hidden_states (`bool`, *optional*):
124	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
125	            more detail.
126	        return_dict (`bool`, *optional*):
127	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
128	"""
129	
130	
131	def create_sinusoidal_positions(num_pos, dim):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llama/modeling_llama.py:344
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
343	
344	LLAMA_START_DOCSTRING = r"""
345	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
346	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
347	    etc.)
348	
349	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
350	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
351	    and behavior.
352	
353	    Parameters:
354	        config ([`LlamaConfig`]):
355	            Model configuration class with all the parameters of the model. Initializing with a config file does not
356	            load the weights associated with the model, only the configuration. Check out the
357	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
358	"""
359	
360	
361	@add_start_docstrings(
362	    "The bare LLaMA Model outputting raw hidden-states without any specific head on top.",
363	    LLAMA_START_DOCSTRING,
364	)
365	class LlamaPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llama/modeling_llama.py:391
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
390	
391	LLAMA_INPUTS_DOCSTRING = r"""
392	    Args:
393	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
394	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
395	            it.
396	
397	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
398	            [`PreTrainedTokenizer.__call__`] for details.
399	
400	            [What are input IDs?](../glossary#input-ids)
401	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
402	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
403	
404	            - 1 for tokens that are **not masked**,
405	            - 0 for tokens that are **masked**.
406	
407	            [What are attention masks?](../glossary#attention-mask)
408	
409	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
410	            [`PreTrainedTokenizer.__call__`] for details.
411	
412	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
413	            `past_key_values`).
414	
415	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
416	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
417	            information on the default strategy.
418	
419	            - 1 indicates the head is **not masked**,
420	            - 0 indicates the head is **masked**.
421	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
422	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
423	            config.n_positions - 1]`.
424	
425	            [What are position IDs?](../glossary#position-ids)
426	        past_key_values (`Cache`, *optional*):
427	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
428	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
429	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
430	
431	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
432	
433	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
434	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
435	            of shape `(batch_size, sequence_length)`.
436	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
437	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
438	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
439	            model's internal embedding lookup matrix.
440	        use_cache (`bool`, *optional*):
441	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
442	            `past_key_values`).
443	        output_attentions (`bool`, *optional*):
444	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
445	            tensors for more detail.
446	        output_hidden_states (`bool`, *optional*):
447	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
448	            more detail.
449	        return_dict (`bool`, *optional*):
450	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
451	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
452	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
453	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
454	            the complete sequence length.
455	"""
456	
457	
458	@add_start_docstrings(
459	    "The bare LLaMA Model outputting raw hidden-states without any specific head on top.",
460	    LLAMA_START_DOCSTRING,
461	)
462	class LlamaModel(LlamaPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llama/tokenization_llama.py:155
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
154	            logger.warning_once(
155	                f"You are using the default legacy behaviour of the {self.__class__}. This is"
156	                " expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you."

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llama/tokenization_llama.py:200
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
199	        with open(self.vocab_file, "rb") as f:
200	            sp_model = f.read()
201	            model_pb2 = import_protobuf(f"The new behaviour of {self.__class__.__name__} (with `self.legacy = False`)")

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llama/tokenization_llama.py:327
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
326	                content_spiece_model = self.sp_model.serialized_model_proto()
327	                fi.write(content_spiece_model)
328	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llama/tokenization_llama_fast.py:144
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
143	            logger.warning_once(
144	                f"You are using the default legacy behaviour of the {self.__class__}. This is"
145	                " expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llama4/modeling_llama4.py:468
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
467	
468	LLAMA4_START_DOCSTRING = r"""
469	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
470	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
471	    etc.)
472	
473	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
474	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
475	    and behavior.
476	
477	    Parameters:
478	        config ([`Llama4Config`]):
479	            Model configuration class with all the parameters of the model. Initializing with a config file does not
480	            load the weights associated with the model, only the configuration. Check out the
481	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
482	"""
483	
484	
485	@add_start_docstrings(
486	    "The bare Llama4 Model outputting raw hidden-states without any specific head on top.",
487	    LLAMA4_START_DOCSTRING,
488	)
489	class Llama4PreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llama4/modeling_llama4.py:517
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
516	
517	LLAMA4_INPUTS_DOCSTRING = r"""
518	    Args:
519	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
520	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
521	            it.
522	
523	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
524	            [`PreTrainedTokenizer.__call__`] for details.
525	
526	            [What are input IDs?](../glossary#input-ids)
527	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
528	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
529	
530	            - 1 for tokens that are **not masked**,
531	            - 0 for tokens that are **masked**.
532	
533	            [What are attention masks?](../glossary#attention-mask)
534	
535	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
536	            [`PreTrainedTokenizer.__call__`] for details.
537	
538	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
539	            `past_key_values`).
540	
541	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
542	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
543	            information on the default strategy.
544	
545	            - 1 indicates the head is **not masked**,
546	            - 0 indicates the head is **masked**.
547	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
548	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
549	            config.n_positions - 1]`.
550	
551	            [What are position IDs?](../glossary#position-ids)
552	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
553	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
554	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
555	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
556	
557	            Two formats are allowed:
558	            - a [`~cache_utils.Cache`] instance, see our
559	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
560	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
561	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
562	            cache format.
563	
564	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
565	            legacy cache format will be returned.
566	
567	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
568	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
569	            of shape `(batch_size, sequence_length)`.
570	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
571	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
572	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
573	            model's internal embedding lookup matrix.
574	        use_cache (`bool`, *optional*):
575	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
576	            `past_key_values`).
577	        output_attentions (`bool`, *optional*):
578	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
579	            tensors for more detail.
580	        output_hidden_states (`bool`, *optional*):
581	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
582	            more detail.
583	        return_dict (`bool`, *optional*):
584	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
585	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
586	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
587	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
588	            the complete sequence length.
589	"""
590	
591	
592	@add_start_docstrings(
593	    "The bare Llama4 Model outputting raw hidden-states without any specific head on top.",
594	    LLAMA4_START_DOCSTRING,
595	)
596	class Llama4TextModel(Llama4PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llama4/modeling_llama4.py:1169
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1168	
1169	LLAVA_START_DOCSTRING = r"""
1170	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1171	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1172	    etc.)
1173	
1174	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1175	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1176	    and behavior.
1177	
1178	    Parameters:
1179	        config ([`LlavaConfig`] or [`LlavaVisionConfig`]):
1180	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1181	            load the weights associated with the model, only the configuration. Check out the
1182	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1183	"""
1184	
1185	
1186	# TODO there is a different RoPE for vision encoder, defined as below
1187	def reshape_for_broadcast(freqs_ci: torch.Tensor, query: torch.Tensor):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llava/modeling_llava.py:110
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
109	
110	LLAVA_START_DOCSTRING = r"""
111	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
112	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
113	    etc.)
114	
115	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
116	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
117	    and behavior.
118	
119	    Parameters:
120	        config ([`LlavaConfig`] or [`LlavaVisionConfig`]):
121	            Model configuration class with all the parameters of the model. Initializing with a config file does not
122	            load the weights associated with the model, only the configuration. Check out the
123	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
124	"""
125	
126	
127	@add_start_docstrings(
128	    "The bare LLaMA Model outputting raw hidden-states without any specific head on top.",
129	    LLAVA_START_DOCSTRING,
130	)
131	class LlavaPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llava/modeling_llava.py:166
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
165	
166	LLAVA_INPUTS_DOCSTRING = r"""
167	    Args:
168	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
169	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
170	            it.
171	
172	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
173	            [`PreTrainedTokenizer.__call__`] for details.
174	
175	            [What are input IDs?](../glossary#input-ids)
176	        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):
177	            The tensors corresponding to the input images. Pixel values can be obtained using
178	            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`LlavaProcessor`] uses
179	            [`CLIPImageProcessor`] for processing images).
180	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
181	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
182	
183	            - 1 for tokens that are **not masked**,
184	            - 0 for tokens that are **masked**.
185	
186	            [What are attention masks?](../glossary#attention-mask)
187	
188	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
189	            [`PreTrainedTokenizer.__call__`] for details.
190	
191	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
192	            `past_key_values`).
193	
194	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
195	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
196	            information on the default strategy.
197	
198	            - 1 indicates the head is **not masked**,
199	            - 0 indicates the head is **masked**.
200	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
201	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
202	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
203	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
204	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
205	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
206	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
207	
208	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
209	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
210	
211	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
212	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
213	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
214	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
215	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
216	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
217	            model's internal embedding lookup matrix.
218	        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):
219	            The index of the layer to select the vision feature. If multiple indices are provided,
220	            the vision feature of the corresponding indices will be concatenated to form the
221	            vision features.
222	        vision_feature_select_strategy (`str`, *optional*, defaults to `"default"`):
223	            The feature selection strategy used to select the vision feature from the vision backbone.
224	            Can be one of `"default"` or `"full"`.
225	        use_cache (`bool`, *optional*):
226	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
227	            `past_key_values`).
228	        output_attentions (`bool`, *optional*):
229	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
230	            tensors for more detail.
231	        output_hidden_states (`bool`, *optional*):
232	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
233	            more detail.
234	        return_dict (`bool`, *optional*):
235	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
236	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
237	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
238	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
239	            the complete sequence length.
240	"""
241	
242	
243	@add_start_docstrings(
244	    """The LLAVA model which consists of a vision backbone and a language model.""",
245	    LLAVA_START_DOCSTRING,
246	)
247	class LlavaForConditionalGeneration(LlavaPreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llava_next/modeling_llava_next.py:218
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
217	
218	LLAVA_NEXT_START_DOCSTRING = r"""
219	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
220	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
221	    etc.)
222	
223	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
224	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
225	    and behavior.
226	
227	    Parameters:
228	        config ([`LlavaNextConfig`] or [`LlavaNextVisionConfig`]):
229	            Model configuration class with all the parameters of the model. Initializing with a config file does not
230	            load the weights associated with the model, only the configuration. Check out the
231	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
232	"""
233	
234	
235	@add_start_docstrings(
236	    "The bare LLaMA Model outputting raw hidden-states without any specific head on top.",
237	    LLAVA_NEXT_START_DOCSTRING,
238	)
239	# Copied from transformers.models.llava.modeling_llava.LlavaPreTrainedModel with Llava->LlavaNext,llava->llava_next
240	class LlavaNextPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llava_next/modeling_llava_next.py:275
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
274	
275	LLAVA_NEXT_INPUTS_DOCSTRING = r"""
276	    Args:
277	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
278	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
279	            it.
280	
281	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
282	            [`PreTrainedTokenizer.__call__`] for details.
283	
284	            [What are input IDs?](../glossary#input-ids)
285	        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):
286	            The tensors corresponding to the input images. Pixel values can be obtained using
287	            [`AutoImageProcessor`]. See [`LlavaNextImageProcessor.__call__`] for details. [`LlavaProcessor`] uses
288	            [`LlavaNextImageProcessor`] for processing images.
289	        image_sizes (`torch.LongTensor` of shape `(batch_size, 2)`, *optional*):
290	            The sizes of the images in the batch, being (height, width) for each image.
291	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
292	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
293	
294	            - 1 for tokens that are **not masked**,
295	            - 0 for tokens that are **masked**.
296	
297	            [What are attention masks?](../glossary#attention-mask)
298	
299	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
300	            [`PreTrainedTokenizer.__call__`] for details.
301	
302	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
303	            `past_key_values`).
304	
305	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
306	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
307	            information on the default strategy.
308	
309	            - 1 indicates the head is **not masked**,
310	            - 0 indicates the head is **masked**.
311	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
312	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
313	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
314	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
315	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
316	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
317	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
318	
319	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
320	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
321	
322	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
323	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
324	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
325	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
326	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
327	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
328	            model's internal embedding lookup matrix.
329	        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):
330	            The index of the layer to select the vision feature. If multiple indices are provided,
331	            the vision feature of the corresponding indices will be concatenated to form the
332	            vision features.
333	        vision_feature_select_strategy (`str`, *optional*, defaults to `"default"`):
334	            The feature selection strategy used to select the vision feature from the vision backbone.
335	            Can be one of `"default"` or `"full"`. If `"default"`, the CLS token is removed from the vision features.
336	            If `"full"`, the full vision features are used.
337	        use_cache (`bool`, *optional*):
338	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
339	            `past_key_values`).
340	        output_attentions (`bool`, *optional*):
341	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
342	            tensors for more detail.
343	        output_hidden_states (`bool`, *optional*):
344	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
345	            more detail.
346	        return_dict (`bool`, *optional*):
347	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
348	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
349	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
350	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
351	            the complete sequence length.
352	"""
353	
354	
355	@add_start_docstrings(
356	    """The LLAVA-NeXT model which consists of a vision backbone and a language model.""",
357	    LLAVA_NEXT_START_DOCSTRING,
358	)
359	class LlavaNextForConditionalGeneration(LlavaNextPreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llava_next_video/modeling_llava_next_video.py:132
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
131	
132	LLAVA_NEXT_VIDEO_START_DOCSTRING = r"""
133	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
134	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
135	    etc.)
136	
137	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
138	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
139	    and behavior.
140	
141	    Parameters:
142	        config ([`LlavaNextVideoConfig`] or [`LlavaNextVideoVisionConfig`]):
143	            Model configuration class with all the parameters of the model. Initializing with a config file does not
144	            load the weights associated with the model, only the configuration. Check out the
145	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
146	"""
147	
148	
149	@add_start_docstrings(
150	    "The bare LLaMA Model outputting raw hidden-states without any specific head on top.",
151	    LLAVA_NEXT_VIDEO_START_DOCSTRING,
152	)
153	class LlavaNextVideoPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llava_next_video/modeling_llava_next_video.py:317
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
316	
317	LLAVA_NEXT_VIDEO_INPUTS_DOCSTRING = r"""
318	    Args:
319	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
320	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
321	            it.
322	
323	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
324	            [`PreTrainedTokenizer.__call__`] for details.
325	
326	            [What are input IDs?](../glossary#input-ids)
327	        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):
328	            The tensors corresponding to the input images. Pixel values can be obtained using
329	            [`AutoImageProcessor`]. See [`LlavaNextVideoImageProcessor.__call__`] for details. [`LlavaProcessor`] uses
330	            [`LlavaNextVideoImageProcessor`] for processing images.
331	        image_sizes (`torch.LongTensor` of shape `(batch_size, 2)`, *optional*):
332	            The sizes of the images in the batch, being (height, width) for each image.
333	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
334	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
335	
336	            - 1 for tokens that are **not masked**,
337	            - 0 for tokens that are **masked**.
338	
339	            [What are attention masks?](../glossary#attention-mask)
340	
341	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
342	            [`PreTrainedTokenizer.__call__`] for details.
343	
344	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
345	            `past_key_values`).
346	
347	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
348	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
349	            information on the default strategy.
350	
351	            - 1 indicates the head is **not masked**,
352	            - 0 indicates the head is **masked**.
353	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
354	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
355	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
356	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
357	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
358	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
359	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
360	
361	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
362	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
363	
364	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
365	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
366	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
367	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
368	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
369	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
370	            model's internal embedding lookup matrix.
371	        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):
372	            The index of the layer to select the vision feature. If multiple indices are provided,
373	            the vision feature of the corresponding indices will be concatenated to form the
374	            vision features.
375	        vision_feature_select_strategy (`str`, *optional*, defaults to `"default"`):
376	            The feature selection strategy used to select the vision feature from the vision backbone.
377	            Can be one of `"default"` or `"full"`. If `"default"`, the CLS token is removed from the vision features.
378	            If `"full"`, the full vision features are used.
379	        use_cache (`bool`, *optional*):
380	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
381	            `past_key_values`).
382	        output_attentions (`bool`, *optional*):
383	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
384	            tensors for more detail.
385	        output_hidden_states (`bool`, *optional*):
386	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
387	            more detail.
388	        return_dict (`bool`, *optional*):
389	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
390	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
391	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
392	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
393	            the complete sequence length.
394	"""
395	
396	
397	@add_start_docstrings(
398	    """The LLAVA-NeXT model which consists of a vision backbone and a language model.""",
399	    LLAVA_NEXT_VIDEO_START_DOCSTRING,
400	)
401	class LlavaNextVideoForConditionalGeneration(LlavaNextVideoPreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llava_onevision/modeling_llava_onevision.py:225
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
224	
225	LLAVA_ONEVISION_START_DOCSTRING = r"""
226	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
227	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
228	    etc.)
229	
230	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
231	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
232	    and behavior.
233	
234	    Parameters:
235	        config ([`LlavaNextConfig`] or [`LlavaNextVisionConfig`]):
236	            Model configuration class with all the parameters of the model. Initializing with a config file does not
237	            load the weights associated with the model, only the configuration. Check out the
238	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
239	"""
240	
241	
242	@add_start_docstrings(
243	    "The bare LLaVA-Onevision Model outputting raw hidden-states without any specific head on top.",
244	    LLAVA_ONEVISION_START_DOCSTRING,
245	)
246	class LlavaOnevisionPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/llava_onevision/modeling_llava_onevision.py:282
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
281	
282	LLAVA_ONEVISION_INPUTS_DOCSTRING = r"""
283	    Args:
284	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
285	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
286	            it.
287	
288	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
289	            [`PreTrainedTokenizer.__call__`] for details.
290	
291	            [What are input IDs?](../glossary#input-ids)
292	        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):
293	            The tensors corresponding to the input images. Pixel values can be obtained using
294	            [`AutoImageProcessor`]. See [`LlavaNextImageProcessor.__call__`] for details. [`LlavaProcessor`] uses
295	            [`LlavaNextImageProcessor`] for processing images.
296	        image_sizes (`torch.LongTensor` of shape `(batch_size, 2)`, *optional*):
297	            The sizes of the images in the batch, being (height, width) for each image.
298	        pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, frames, num_channels, image_size, image_size)):
299	            The tensors corresponding to the input videos. Pixel values can be obtained using
300	            [`LlavaNextVideoProcessor`]. See [`LlavaNextVideoProcessor.__call__`] for details. [`LlavaProcessor`] uses
301	            [`LlavaNextVideoProcessor`] for processing videos.
302	        image_sizes_videos (`torch.LongTensor` of shape `(batch_size, frames, 2)`, *optional*):
303	            The sizes of the videos in the batch, being (height, width) for each frame in the video.
304	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
305	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
306	
307	            - 1 for tokens that are **not masked**,
308	            - 0 for tokens that are **masked**.
309	
310	            [What are attention masks?](../glossary#attention-mask)
311	
312	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
313	            [`PreTrainedTokenizer.__call__`] for details.
314	
315	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
316	            `past_key_values`).
317	
318	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
319	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
320	            information on the default strategy.
321	
322	            - 1 indicates the head is **not masked**,
323	            - 0 indicates the head is **masked**.
324	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
325	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
326	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
327	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
328	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
329	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
330	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
331	
332	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
333	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
334	
335	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
336	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
337	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
338	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
339	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
340	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
341	            model's internal embedding lookup matrix.
342	        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):
343	            The index of the layer to select the vision feature. If multiple indices are provided,
344	            the vision feature of the corresponding indices will be concatenated to form the
345	            vision features.
346	        vision_feature_select_strategy (`str`, *optional*, defaults to `"default"`):
347	            The feature selection strategy used to select the vision feature from the vision backbone.
348	            Can be one of `"default"` or `"full"`. If `"default"`, the CLS token is removed from the vision features.
349	            If `"full"`, the full vision features are used.
350	        vision_aspect_ratio (`str`, *optional*, defaults to `"anyres_max_9"`):
351	            Aspect ratio used when processong image features. The default value is "anyres_max_9".
352	        use_cache (`bool`, *optional*):
353	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
354	            `past_key_values`).
355	        output_attentions (`bool`, *optional*):
356	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
357	            tensors for more detail.
358	        output_hidden_states (`bool`, *optional*):
359	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
360	            more detail.
361	        return_dict (`bool`, *optional*):
362	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
363	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
364	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
365	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
366	            the complete sequence length.
367	"""
368	
369	
370	@add_start_docstrings(
371	    """The LLaVA-Onevision model which consists of a vision backbone and a language model.""",
372	    LLAVA_ONEVISION_START_DOCSTRING,
373	)
374	class LlavaOnevisionForConditionalGeneration(LlavaOnevisionPreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/longformer/modeling_longformer.py:1429
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1428	
1429	LONGFORMER_START_DOCSTRING = r"""
1430	
1431	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1432	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1433	    etc.)
1434	
1435	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1436	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1437	    and behavior.
1438	
1439	    Parameters:
1440	        config ([`LongformerConfig`]): Model configuration class with all the parameters of the
1441	            model. Initializing with a config file does not load the weights associated with the model, only the
1442	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1443	"""
1444	
1445	LONGFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/longformer/modeling_longformer.py:1445
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1444	
1445	LONGFORMER_INPUTS_DOCSTRING = r"""
1446	    Args:
1447	        input_ids (`torch.LongTensor` of shape `({0})`):
1448	            Indices of input sequence tokens in the vocabulary.
1449	
1450	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1451	            [`PreTrainedTokenizer.__call__`] for details.
1452	
1453	            [What are input IDs?](../glossary#input-ids)
1454	        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):
1455	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
1456	
1457	            - 1 for tokens that are **not masked**,
1458	            - 0 for tokens that are **masked**.
1459	
1460	            [What are attention masks?](../glossary#attention-mask)
1461	        global_attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):
1462	            Mask to decide the attention given on each token, local attention or global attention. Tokens with global
1463	            attention attends to all other tokens, and all other tokens attend to them. This is important for
1464	            task-specific finetuning because it makes the model more flexible at representing the task. For example,
1465	            for classification, the <s> token should be given global attention. For QA, all question tokens should also
1466	            have global attention. Please refer to the [Longformer paper](https://arxiv.org/abs/2004.05150) for more
1467	            details. Mask values selected in `[0, 1]`:
1468	
1469	            - 0 for local attention (a sliding window attention),
1470	            - 1 for global attention (tokens that attend to all other tokens, and all other tokens attend to them).
1471	
1472	        head_mask (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*):
1473	            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:
1474	
1475	            - 1 indicates the head is **not masked**,
1476	            - 0 indicates the head is **masked**.
1477	
1478	        decoder_head_mask (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*):
1479	            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:
1480	
1481	            - 1 indicates the head is **not masked**,
1482	            - 0 indicates the head is **masked**.
1483	
1484	        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):
1485	            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,
1486	            1]`:
1487	
1488	            - 0 corresponds to a *sentence A* token,
1489	            - 1 corresponds to a *sentence B* token.
1490	
1491	            [What are token type IDs?](../glossary#token-type-ids)
1492	        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):
1493	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
1494	            config.max_position_embeddings - 1]`.
1495	
1496	            [What are position IDs?](../glossary#position-ids)
1497	        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):
1498	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
1499	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
1500	            model's internal embedding lookup matrix.
1501	        output_attentions (`bool`, *optional*):
1502	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1503	            tensors for more detail.
1504	        output_hidden_states (`bool`, *optional*):
1505	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1506	            more detail.
1507	        return_dict (`bool`, *optional*):
1508	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1509	"""
1510	
1511	
1512	@add_start_docstrings(
1513	    "The bare Longformer Model outputting raw hidden-states without any specific head on top.",
1514	    LONGFORMER_START_DOCSTRING,
1515	)
1516	class LongformerModel(LongformerPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/longformer/modeling_tf_longformer.py:1994
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1993	
1994	LONGFORMER_START_DOCSTRING = r"""
1995	
1996	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
1997	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1998	    etc.)
1999	
2000	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
2001	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
2002	    behavior.
2003	
2004	    <Tip>
2005	
2006	    TensorFlow models and layers in `transformers` accept two formats as input:
2007	
2008	    - having all inputs as keyword arguments (like PyTorch models), or
2009	    - having all inputs as a list, tuple or dict in the first positional argument.
2010	
2011	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
2012	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
2013	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
2014	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
2015	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
2016	    positional argument:
2017	
2018	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
2019	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
2020	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
2021	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
2022	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
2023	
2024	    Note that when creating models and layers with
2025	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
2026	    about any of this, as you can just pass inputs like you would to any other Python function!
2027	
2028	    </Tip>
2029	
2030	    Parameters:
2031	        config ([`LongformerConfig`]): Model configuration class with all the parameters of the model.
2032	            Initializing with a config file does not load the weights associated with the model, only the
2033	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
2034	"""
2035	
2036	
2037	LONGFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/longformer/modeling_tf_longformer.py:2037
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2036	
2037	LONGFORMER_INPUTS_DOCSTRING = r"""
2038	    Args:
2039	        input_ids (`np.ndarray` or `tf.Tensor` of shape `({0})`):
2040	            Indices of input sequence tokens in the vocabulary.
2041	
2042	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and
2043	            [`PreTrainedTokenizer.encode`] for details.
2044	
2045	            [What are input IDs?](../glossary#input-ids)
2046	        attention_mask (`np.ndarray` or `tf.Tensor` of shape `({0})`, *optional*):
2047	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
2048	
2049	            - 1 for tokens that are **not masked**,
2050	            - 0 for tokens that are **masked**.
2051	
2052	            [What are attention masks?](../glossary#attention-mask)
2053	        head_mask (`np.ndarray` or `tf.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
2054	            Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:
2055	
2056	            - 1 indicates the head is **not masked**,
2057	            - 0 indicates the head is **masked**.
2058	
2059	        global_attention_mask (`np.ndarray` or `tf.Tensor` of shape `({0})`, *optional*):
2060	            Mask to decide the attention given on each token, local attention or global attention. Tokens with global
2061	            attention attends to all other tokens, and all other tokens attend to them. This is important for
2062	            task-specific finetuning because it makes the model more flexible at representing the task. For example,
2063	            for classification, the <s> token should be given global attention. For QA, all question tokens should also
2064	            have global attention. Please refer to the [Longformer paper](https://arxiv.org/abs/2004.05150) for more
2065	            details. Mask values selected in `[0, 1]`:
2066	
2067	            - 0 for local attention (a sliding window attention),
2068	            - 1 for global attention (tokens that attend to all other tokens, and all other tokens attend to them).
2069	
2070	        token_type_ids (`np.ndarray` or `tf.Tensor` of shape `({0})`, *optional*):
2071	            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,
2072	            1]`:
2073	
2074	            - 0 corresponds to a *sentence A* token,
2075	            - 1 corresponds to a *sentence B* token.
2076	
2077	            [What are token type IDs?](../glossary#token-type-ids)
2078	        position_ids (`np.ndarray` or `tf.Tensor` of shape `({0})`, *optional*):
2079	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
2080	            config.max_position_embeddings - 1]`.
2081	
2082	            [What are position IDs?](../glossary#position-ids)
2083	        inputs_embeds (`np.ndarray` or `tf.Tensor` of shape `({0}, hidden_size)`, *optional*):
2084	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
2085	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
2086	            model's internal embedding lookup matrix.
2087	        output_attentions (`bool`, *optional*):
2088	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
2089	            tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
2090	            config will be used instead.
2091	        output_hidden_states (`bool`, *optional*):
2092	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
2093	            more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
2094	            used instead.
2095	        return_dict (`bool`, *optional*):
2096	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple. This argument can be used in
2097	            eager mode, in graph mode the value will always be set to True.
2098	        training (`bool`, *optional*, defaults to `False`):
2099	            Whether or not to use the model in training mode (some modules like dropout modules have different
2100	            behaviors between training and evaluation).
2101	"""
2102	
2103	
2104	@add_start_docstrings(
2105	    "The bare Longformer Model outputting raw hidden-states without any specific head on top.",
2106	    LONGFORMER_START_DOCSTRING,
2107	)
2108	class TFLongformerModel(TFLongformerPreTrainedModel):

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/longformer/tokenization_longformer.py:194
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
193	        with open(merges_file, encoding="utf-8") as merges_handle:
194	            bpe_merges = merges_handle.read().split("\n")[1:-1]
195	        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/longformer/tokenization_longformer.py:303
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
302	        with open(vocab_file, "w", encoding="utf-8") as f:
303	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
304	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/longformer/tokenization_longformer.py:307
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
306	        with open(merge_file, "w", encoding="utf-8") as writer:
307	            writer.write("#version: 0.2\n")
308	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/longformer/tokenization_longformer.py:315
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
314	                    index = token_index
315	                writer.write(" ".join(bpe_tokens) + "\n")
316	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/longt5/modeling_flax_longt5.py:1563
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1562	
1563	LONGT5_DECODE_INPUTS_DOCSTRING = r"""
1564	    Args:
1565	        decoder_input_ids (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`):
1566	            Indices of decoder input sequence tokens in the vocabulary.
1567	
1568	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1569	            [`PreTrainedTokenizer.__call__`] for details.
1570	
1571	            [What are decoder input IDs?](../glossary#decoder-input-ids)
1572	
1573	            For training, `decoder_input_ids` should be provided.
1574	        encoder_outputs (`tuple(tuple(jnp.ndarray)`):
1575	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
1576	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
1577	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
1578	        encoder_attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
1579	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
1580	
1581	            - 1 for tokens that are **not masked**,
1582	            - 0 for tokens that are **masked**.
1583	
1584	            [What are attention masks?](../glossary#attention-mask)
1585	        decoder_attention_mask (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
1586	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
1587	            be used by default.
1588	
1589	            If you want to change padding behavior, you should modify to your needs. See diagram 1 in [the
1590	            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
1591	        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):
1592	            Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
1593	            auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.
1594	        output_attentions (`bool`, *optional*):
1595	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1596	            tensors for more detail.
1597	        output_hidden_states (`bool`, *optional*):
1598	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1599	            more detail.
1600	        return_dict (`bool`, *optional*):
1601	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1602	"""
1603	
1604	
1605	LONGT5_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/longt5/modeling_flax_longt5.py:1971
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1970	
1971	LONGT5_START_DOCSTRING = r"""
1972	    The LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long
1973	    Sequences](https://arxiv.org/abs/2112.07916) by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo
1974	    Ni, Yun-Hsuan Sung and Yinfei Yang. It's an encoder-decoder transformer pre-trained in a text-to-text denoising
1975	    generative setting. LongT5 model is an extension of T5 model, and it enables using one of the two different
1976	    efficient attention mechanisms - (1) Local attention, or (2) Transient-Global attention.
1977	
1978	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
1979	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1980	    etc.)
1981	
1982	    This model is also a Flax Linen
1983	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
1984	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
1985	
1986	    Finally, this model supports inherent JAX features such as:
1987	
1988	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
1989	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
1990	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
1991	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
1992	
1993	    Parameters:
1994	        config ([`LongT5Config`]): Model configuration class with all the parameters of the model.
1995	            Initializing with a config file does not load the weights associated with the model, only the
1996	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
1997	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
1998	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
1999	            `jax.numpy.bfloat16` (on TPUs).
2000	
2001	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
2002	            specified all the computation will be performed with the given `dtype`.
2003	
2004	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
2005	            parameters.**
2006	
2007	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
2008	            [`~FlaxPreTrainedModel.to_bf16`].
2009	"""
2010	
2011	
2012	@add_start_docstrings(
2013	    "The bare LONGT5 Model transformer outputting raw hidden-stateswithout any specific head on top.",
2014	    LONGT5_START_DOCSTRING,
2015	)
2016	# Copied from transformers.models.t5.modeling_flax_t5.FlaxT5Module with T5->LongT5
2017	class FlaxLongT5Module(nn.Module):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/longt5/modeling_longt5.py:1731
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1730	
1731	LONGT5_START_DOCSTRING = r"""
1732	
1733	    The LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long
1734	    Sequences](https://arxiv.org/abs/2112.07916) by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo
1735	    Ni, Yun-Hsuan Sung and Yinfei Yang. It's an encoder-decoder transformer pre-trained in a text-to-text denoising
1736	    generative setting. LongT5 model is an extension of T5 model, and it enables using one of the two different
1737	    efficient attention mechanisms - (1) Local attention, or (2) Transient-Global attention.
1738	
1739	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1740	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1741	    etc.)
1742	
1743	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1744	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1745	    and behavior.
1746	
1747	    Parameters:
1748	        config ([`LongT5Config`]): Model configuration class with all the parameters of the model.
1749	            Initializing with a config file does not load the weights associated with the model, only the
1750	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1751	"""
1752	
1753	LONGT5_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/luke/modeling_luke.py:915
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
914	
915	LUKE_START_DOCSTRING = r"""
916	
917	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
918	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
919	    etc.)
920	
921	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
922	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
923	    and behavior.
924	
925	    Parameters:
926	        config ([`LukeConfig`]): Model configuration class with all the parameters of the
927	            model. Initializing with a config file does not load the weights associated with the model, only the
928	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
929	"""
930	
931	LUKE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/luke/tokenization_luke.py:315
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
314	        with open(merges_file, encoding="utf-8") as merges_handle:
315	            bpe_merges = merges_handle.read().split("\n")[1:-1]
316	        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/luke/tokenization_luke.py:734
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
733	            raise NotImplementedError(
734	                "return_offset_mapping is not available when using Python tokenizers. "
735	                "To use this feature, change your tokenizer to one deriving from "
736	                "transformers.PreTrainedTokenizerFast. "

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/luke/tokenization_luke.py:1706
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1705	        with open(vocab_file, "w", encoding="utf-8") as f:
1706	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
1707	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/luke/tokenization_luke.py:1710
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1709	        with open(merge_file, "w", encoding="utf-8") as writer:
1710	            writer.write("#version: 0.2\n")
1711	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/luke/tokenization_luke.py:1718
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1717	                    index = token_index
1718	                writer.write(" ".join(bpe_tokens) + "\n")
1719	                index += 1

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/luke/tokenization_luke.py:1726
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1725	        with open(entity_vocab_file, "w", encoding="utf-8") as f:
1726	            f.write(json.dumps(self.entity_vocab, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
1727	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/lxmert/modeling_lxmert.py:197
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
196	        logger.error(
197	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
198	            "https://www.tensorflow.org/install/ for installation instructions."
199	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/lxmert/modeling_lxmert.py:797
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
796	
797	LXMERT_START_DOCSTRING = r"""
798	
799	    The LXMERT model was proposed in [LXMERT: Learning Cross-Modality Encoder Representations from
800	    Transformers](https://arxiv.org/abs/1908.07490) by Hao Tan and Mohit Bansal. It's a vision and language transformer
801	    model, pretrained on a variety of multi-modal datasets comprising of GQA, VQAv2.0, MSCOCO captions, and Visual
802	    genome, using a combination of masked language modeling, region of interest feature regression, cross entropy loss
803	    for question answering attribute prediction, and object tag prediction.
804	
805	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
806	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
807	    etc.)
808	
809	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
810	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
811	    and behavior.
812	
813	    Parameters:
814	        config ([`LxmertConfig`]): Model configuration class with all the parameters of the model.
815	            Initializing with a config file does not load the weights associated with the model, only the
816	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
817	"""
818	
819	LXMERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/lxmert/modeling_tf_lxmert.py:980
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
979	
980	LXMERT_START_DOCSTRING = r"""
981	
982	    The LXMERT model was proposed in [LXMERT: Learning Cross-Modality Encoder Representations from
983	    Transformers](https://arxiv.org/abs/1908.07490) by Hao Tan and Mohit Bansal. It's a vision and language transformer
984	    model, pre-trained on a variety of multi-modal datasets comprising of GQA, VQAv2.0, MCSCOCO captions, and Visual
985	    genome, using a combination of masked language modeling, region of interest feature regression, cross entropy loss
986	    for question answering attribute prediction, and object tag prediction.
987	
988	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
989	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
990	    behavior.
991	
992	    <Tip>
993	
994	    TensorFlow models and layers in `transformers` accept two formats as input:
995	
996	    - having all inputs as keyword arguments (like PyTorch models), or
997	    - having all inputs as a list, tuple or dict in the first positional argument.
998	
999	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
1000	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
1001	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
1002	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
1003	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
1004	    positional argument:
1005	
1006	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
1007	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
1008	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
1009	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
1010	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
1011	
1012	    Note that when creating models and layers with
1013	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
1014	    about any of this, as you can just pass inputs like you would to any other Python function!
1015	
1016	    </Tip>
1017	
1018	    Parameters:
1019	        config ([`LxmertConfig`]): Model configuration class with all the parameters of the model.
1020	            Initializing with a config file does not load the weights associated with the model, only the
1021	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1022	"""
1023	
1024	LXMERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/lxmert/tokenization_lxmert.py:286
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
285	                    index = token_index
286	                writer.write(token + "\n")
287	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/m2m_100/modeling_m2m_100.py:796
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
795	
796	M2M_100_START_DOCSTRING = r"""
797	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
798	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
799	    etc.)
800	
801	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
802	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
803	    and behavior.
804	
805	    Parameters:
806	        config ([`M2M100Config`]):
807	            Model configuration class with all the parameters of the model. Initializing with a config file does not
808	            load the weights associated with the model, only the configuration. Check out the
809	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
810	"""
811	
812	M2M_100_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/m2m_100/tokenization_m2m_100.py:311
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
310	                content_spiece_model = self.sp_model.serialized_model_proto()
311	                fi.write(content_spiece_model)
312	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mamba/modeling_mamba.py:120
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
119	                    logger.warning_once(
120	                        "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`"
121	                        " is None. Falling back to the mamba.py backend. To install follow https://github.com/state-spaces/mamba/#installation and"
122	                        " https://github.com/Dao-AILab/causal-conv1d"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mamba/modeling_mamba.py:126
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
125	                    raise ImportError(
126	                        "use_mambapy is set to True but the mambapy package is not installed. To install it follow https://github.com/alxndrTL/mamba.py."
127	                    )
128	            else:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mamba/modeling_mamba.py:130
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
129	                logger.warning_once(
130	                    "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`"
131	                    " is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation and"
132	                    " https://github.com/Dao-AILab/causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mamba/modeling_mamba.py:492
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
491	
492	MAMBA_START_DOCSTRING = r"""
493	
494	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
495	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
496	    etc.)
497	
498	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
499	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
500	    and behavior.
501	
502	    Parameters:
503	        config ([`MambaConfig`]): Model configuration class with all the parameters of the model.
504	            Initializing with a config file does not load the weights associated with the model, only the
505	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
506	"""
507	
508	MAMBA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mamba2/modeling_mamba2.py:297
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
296	            logger.warning_once(
297	                "The fast path is not available because on of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)`"
298	                " is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and"
299	                " https://github.com/Dao-AILab/causal-conv1d"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mamba2/modeling_mamba2.py:828
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
827	
828	MAMBA2_START_DOCSTRING = r"""
829	
830	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
831	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
832	    etc.)
833	
834	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
835	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
836	    and behavior.
837	
838	    Parameters:
839	        config ([`Mamba2Config`]): Model configuration class with all the parameters of the model.
840	            Initializing with a config file does not load the weights associated with the model, only the
841	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
842	"""
843	
844	MAMBA2_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/marian/modeling_flax_marian.py:57
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
56	
57	MARIAN_START_DOCSTRING = r"""
58	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
59	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
60	    etc.)
61	
62	    This model is also a Flax Linen
63	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
64	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
65	
66	    Finally, this model supports inherent JAX features such as:
67	
68	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
69	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
70	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
71	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
72	
73	    Parameters:
74	        config ([`MarianConfig`]): Model configuration class with all the parameters of the model.
75	            Initializing with a config file does not load the weights associated with the model, only the
76	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
77	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
78	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
79	            `jax.numpy.bfloat16` (on TPUs).
80	
81	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
82	            specified all the computation will be performed with the given `dtype`.
83	
84	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
85	            parameters.**
86	
87	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
88	            [`~FlaxPreTrainedModel.to_bf16`].
89	"""
90	
91	MARIAN_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/marian/modeling_flax_marian.py:91
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
90	
91	MARIAN_INPUTS_DOCSTRING = r"""
92	    Args:
93	        input_ids (`jnp.ndarray` of shape `(batch_size, sequence_length)`):
94	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
95	            it.
96	
97	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
98	            [`PreTrainedTokenizer.__call__`] for details.
99	
100	            [What are input IDs?](../glossary#input-ids)
101	        attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
102	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
103	
104	            - 1 for tokens that are **not masked**,
105	            - 0 for tokens that are **masked**.
106	
107	            [What are attention masks?](../glossary#attention-mask)
108	        decoder_input_ids (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
109	            Indices of decoder input sequence tokens in the vocabulary.
110	
111	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
112	            [`PreTrainedTokenizer.__call__`] for details.
113	
114	            [What are decoder input IDs?](../glossary#decoder-input-ids)
115	
116	            For translation and summarization training, `decoder_input_ids` should be provided. If no
117	            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
118	            for denoising pre-training following the paper.
119	        decoder_attention_mask (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
120	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
121	            be used by default.
122	
123	            If you want to change padding behavior, you should modify to your needs. See diagram 1 in [the
124	            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
125	        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
126	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
127	            config.max_position_embeddings - 1]`.
128	        decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
129	            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
130	            range `[0, config.max_position_embeddings - 1]`.
131	        output_attentions (`bool`, *optional*):
132	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
133	            tensors for more detail.
134	        output_hidden_states (`bool`, *optional*):
135	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
136	            more detail.
137	        return_dict (`bool`, *optional*):
138	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
139	"""
140	
141	
142	MARIAN_ENCODE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/marian/modeling_flax_marian.py:172
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
171	
172	MARIAN_DECODE_INPUTS_DOCSTRING = r"""
173	    Args:
174	        decoder_input_ids (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`):
175	            Indices of decoder input sequence tokens in the vocabulary.
176	
177	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
178	            [`PreTrainedTokenizer.__call__`] for details.
179	
180	            [What are decoder input IDs?](../glossary#decoder-input-ids)
181	
182	            For translation and summarization training, `decoder_input_ids` should be provided. If no
183	            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
184	            for denoising pre-training following the paper.
185	        encoder_outputs (`tuple(tuple(jnp.ndarray)`):
186	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
187	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
188	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
189	        encoder_attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
190	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
191	
192	            - 1 for tokens that are **not masked**,
193	            - 0 for tokens that are **masked**.
194	
195	            [What are attention masks?](../glossary#attention-mask)
196	        decoder_attention_mask (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
197	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
198	            be used by default.
199	
200	            If you want to change padding behavior, you should modify to your needs. See diagram 1 in [the
201	            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
202	        decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
203	            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
204	            range `[0, config.max_position_embeddings - 1]`.
205	        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):
206	            Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
207	            auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.
208	        output_attentions (`bool`, *optional*):
209	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
210	            tensors for more detail.
211	        output_hidden_states (`bool`, *optional*):
212	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
213	            more detail.
214	        return_dict (`bool`, *optional*):
215	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
216	"""
217	
218	
219	def create_sinusoidal_positions(n_pos, dim):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/marian/modeling_marian.py:486
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
485	
486	MARIAN_START_DOCSTRING = r"""
487	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
488	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
489	    etc.)
490	
491	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
492	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
493	    and behavior.
494	
495	    Parameters:
496	        config ([`MarianConfig`]):
497	            Model configuration class with all the parameters of the model. Initializing with a config file does not
498	            load the weights associated with the model, only the configuration. Check out the
499	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
500	"""
501	
502	MARIAN_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/marian/modeling_marian.py:502
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
501	
502	MARIAN_GENERATION_EXAMPLE = r"""
503	    Pytorch version of marian-nmt's transformer.h (c++). Designed for the OPUS-NMT translation checkpoints. Available
504	    models are listed [here](https://huggingface.co/models?search=Helsinki-NLP).
505	
506	    Examples:
507	
508	    ```python
509	    >>> from transformers import AutoTokenizer, MarianMTModel
510	
511	    >>> src = "fr"  # source language
512	    >>> trg = "en"  # target language
513	
514	    >>> model_name = f"Helsinki-NLP/opus-mt-{src}-{trg}"
515	    >>> model = MarianMTModel.from_pretrained(model_name)
516	    >>> tokenizer = AutoTokenizer.from_pretrained(model_name)
517	
518	    >>> sample_text = "o est l'arrt de bus ?"
519	    >>> batch = tokenizer([sample_text], return_tensors="pt")
520	
521	    >>> generated_ids = model.generate(**batch)
522	    >>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
523	    "Where's the bus stop?"
524	    ```
525	"""
526	
527	MARIAN_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/marian/modeling_tf_marian.py:568
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
567	
568	MARIAN_START_DOCSTRING = r"""
569	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
570	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
571	    etc.)
572	
573	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
574	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
575	    behavior.
576	
577	    <Tip>
578	
579	    TensorFlow models and layers in `transformers` accept two formats as input:
580	
581	    - having all inputs as keyword arguments (like PyTorch models), or
582	    - having all inputs as a list, tuple or dict in the first positional argument.
583	
584	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
585	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
586	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
587	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
588	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
589	    positional argument:
590	
591	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
592	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
593	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
594	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
595	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
596	
597	    Note that when creating models and layers with
598	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
599	    about any of this, as you can just pass inputs like you would to any other Python function!
600	
601	    </Tip>
602	
603	    Args:
604	        config ([`MarianConfig`]): Model configuration class with all the parameters of the model.
605	            Initializing with a config file does not load the weights associated with the model, only the
606	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
607	"""
608	
609	MARIAN_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/marian/modeling_tf_marian.py:609
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
608	
609	MARIAN_GENERATION_EXAMPLE = r"""
610	        TF version of marian-nmt's transformer.h (c++). Designed for the OPUS-NMT translation checkpoints. Available
611	        models are listed [here](https://huggingface.co/models?search=Helsinki-NLP).
612	
613	        Examples:
614	
615	        ```python
616	        >>> from transformers import AutoTokenizer, TFMarianMTModel
617	        >>> from typing import List
618	
619	        >>> src = "fr"  # source language
620	        >>> trg = "en"  # target language
621	        >>> sample_text = "o est l'arrt de bus ?"
622	        >>> model_name = f"Helsinki-NLP/opus-mt-{src}-{trg}"
623	
624	        >>> model = TFMarianMTModel.from_pretrained(model_name)
625	        >>> tokenizer = AutoTokenizer.from_pretrained(model_name)
626	        >>> batch = tokenizer([sample_text], return_tensors="tf")
627	        >>> gen = model.generate(**batch)
628	        >>> tokenizer.batch_decode(gen, skip_special_tokens=True)
629	        "Where is the bus stop ?"
630	        ```
631	"""
632	
633	MARIAN_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/marian/tokenization_marian.py:325
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
324	                    content_spiece_model = spm_model.serialized_model_proto()
325	                    fi.write(content_spiece_model)
326	                saved_files.append(spm_save_path)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/markuplm/modeling_markuplm.py:744
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
743	
744	MARKUPLM_START_DOCSTRING = r"""
745	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
746	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
747	    behavior.
748	
749	    Parameters:
750	        config ([`MarkupLMConfig`]): Model configuration class with all the parameters of the model.
751	            Initializing with a config file does not load the weights associated with the model, only the
752	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
753	"""
754	
755	MARKUPLM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/markuplm/tokenization_markuplm.py:225
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
224	        with open(merges_file, encoding="utf-8") as merges_handle:
225	            bpe_merges = merges_handle.read().split("\n")[1:-1]
226	        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/markuplm/tokenization_markuplm.py:383
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
382	        with open(vocab_file, "w", encoding="utf-8") as f:
383	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
384	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/markuplm/tokenization_markuplm.py:388
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
387	        with open(merge_file, "w", encoding="utf-8") as writer:
388	            writer.write("#version: 0.2\n")
389	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/markuplm/tokenization_markuplm.py:396
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
395	                    index = token_index
396	                writer.write(" ".join(bpe_tokens) + "\n")
397	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/markuplm/tokenization_markuplm.py:976
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
975	            raise NotImplementedError(
976	                "return_offset_mapping is not available when using Python tokenizers. "
977	                "To use this feature, change your tokenizer to one deriving from "
978	                "transformers.PreTrainedTokenizerFast. "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mask2former/modeling_mask2former.py:2112
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2111	
2112	MASK2FORMER_START_DOCSTRING = r"""
2113	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
2114	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
2115	    behavior.
2116	
2117	    Parameters:
2118	        config ([`Mask2FormerConfig`]): Model configuration class with all the parameters of the model.
2119	            Initializing with a config file does not load the weights associated with the model, only the
2120	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
2121	"""
2122	
2123	MASK2FORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/maskformer/modeling_maskformer.py:1452
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1451	
1452	MASKFORMER_START_DOCSTRING = r"""
1453	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
1454	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
1455	    behavior.
1456	
1457	    Parameters:
1458	        config ([`MaskFormerConfig`]): Model configuration class with all the parameters of the model.
1459	            Initializing with a config file does not load the weights associated with the model, only the
1460	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1461	"""
1462	
1463	MASKFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mbart/modeling_flax_mbart.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
57	
58	MBART_START_DOCSTRING = r"""
59	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
60	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
61	    etc.)
62	
63	    This model is also a Flax Linen
64	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
65	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
66	
67	    Finally, this model supports inherent JAX features such as:
68	
69	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
70	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
71	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
72	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
73	
74	    Parameters:
75	        config ([`MBartConfig`]): Model configuration class with all the parameters of the model.
76	            Initializing with a config file does not load the weights associated with the model, only the
77	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
78	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
79	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
80	            `jax.numpy.bfloat16` (on TPUs).
81	
82	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
83	            specified all the computation will be performed with the given `dtype`.
84	
85	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
86	            parameters.**
87	
88	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
89	            [`~FlaxPreTrainedModel.to_bf16`].
90	"""
91	
92	MBART_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mbart/modeling_flax_mbart.py:92
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
91	
92	MBART_INPUTS_DOCSTRING = r"""
93	    Args:
94	        input_ids (`jnp.ndarray` of shape `(batch_size, sequence_length)`):
95	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
96	            it.
97	
98	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
99	            [`PreTrainedTokenizer.__call__`] for details.
100	
101	            [What are input IDs?](../glossary#input-ids)
102	        attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
103	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
104	
105	            - 1 for tokens that are **not masked**,
106	            - 0 for tokens that are **masked**.
107	
108	            [What are attention masks?](../glossary#attention-mask)
109	        decoder_input_ids (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
110	            Indices of decoder input sequence tokens in the vocabulary.
111	
112	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
113	            [`PreTrainedTokenizer.__call__`] for details.
114	
115	            [What are decoder input IDs?](../glossary#decoder-input-ids)
116	
117	            For translation and summarization training, `decoder_input_ids` should be provided. If no
118	            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
119	            for denoising pre-training following the paper.
120	        decoder_attention_mask (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
121	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
122	            be used by default.
123	
124	            If you want to change padding behavior, you should modify to your needs. See diagram 1 in [the
125	            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
126	        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
127	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
128	            config.max_position_embeddings - 1]`.
129	        decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
130	            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
131	            range `[0, config.max_position_embeddings - 1]`.
132	        output_attentions (`bool`, *optional*):
133	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
134	            tensors for more detail.
135	        output_hidden_states (`bool`, *optional*):
136	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
137	            more detail.
138	        return_dict (`bool`, *optional*):
139	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
140	"""
141	
142	
143	MBART_ENCODE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mbart/modeling_flax_mbart.py:173
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
172	
173	MBART_DECODE_INPUTS_DOCSTRING = r"""
174	    Args:
175	        decoder_input_ids (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`):
176	            Indices of decoder input sequence tokens in the vocabulary.
177	
178	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
179	            [`PreTrainedTokenizer.__call__`] for details.
180	
181	            [What are decoder input IDs?](../glossary#decoder-input-ids)
182	
183	            For translation and summarization training, `decoder_input_ids` should be provided. If no
184	            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
185	            for denoising pre-training following the paper.
186	        encoder_outputs (`tuple(tuple(jnp.ndarray)`):
187	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
188	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
189	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
190	        encoder_attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
191	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
192	
193	            - 1 for tokens that are **not masked**,
194	            - 0 for tokens that are **masked**.
195	
196	            [What are attention masks?](../glossary#attention-mask)
197	        decoder_attention_mask (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
198	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
199	            be used by default.
200	
201	            If you want to change padding behavior, you should modify to your needs. See diagram 1 in [the
202	            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
203	        decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
204	            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
205	            range `[0, config.max_position_embeddings - 1]`.
206	        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):
207	            Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
208	            auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.
209	        output_attentions (`bool`, *optional*):
210	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
211	            tensors for more detail.
212	        output_hidden_states (`bool`, *optional*):
213	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
214	            more detail.
215	        return_dict (`bool`, *optional*):
216	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
217	"""
218	
219	
220	def shift_tokens_right(input_ids: jnp.ndarray, pad_token_id: int) -> jnp.ndarray:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mbart/modeling_mbart.py:769
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
768	
769	MBART_START_DOCSTRING = r"""
770	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
771	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
772	    etc.)
773	
774	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
775	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
776	    and behavior.
777	
778	    Parameters:
779	        config ([`MBartConfig`]):
780	            Model configuration class with all the parameters of the model. Initializing with a config file does not
781	            load the weights associated with the model, only the configuration. Check out the
782	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
783	"""
784	
785	MBART_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mbart/modeling_tf_mbart.py:535
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
534	
535	MBART_START_DOCSTRING = r"""
536	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
537	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
538	    etc.)
539	
540	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
541	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
542	    behavior.
543	
544	    <Tip>
545	
546	    TensorFlow models and layers in `transformers` accept two formats as input:
547	
548	    - having all inputs as keyword arguments (like PyTorch models), or
549	    - having all inputs as a list, tuple or dict in the first positional argument.
550	
551	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
552	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
553	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
554	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
555	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
556	    positional argument:
557	
558	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
559	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
560	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
561	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
562	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
563	
564	    Note that when creating models and layers with
565	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
566	    about any of this, as you can just pass inputs like you would to any other Python function!
567	
568	    </Tip>
569	
570	    Args:
571	        config ([`MBartConfig`]): Model configuration class with all the parameters of the model.
572	            Initializing with a config file does not load the weights associated with the model, only the
573	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
574	"""
575	
576	MBART_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mbart/tokenization_mbart.py:305
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
304	                content_spiece_model = self.sp_model.serialized_model_proto()
305	                fi.write(content_spiece_model)
306	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mbart50/tokenization_mbart50.py:253
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
252	                content_spiece_model = self.sp_model.serialized_model_proto()
253	                fi.write(content_spiece_model)
254	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/megatron_bert/modeling_megatron_bert.py:70
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
69	        logger.error(
70	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
71	            "https://www.tensorflow.org/install/ for installation instructions."
72	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/megatron_bert/modeling_megatron_bert.py:762
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
761	
762	MEGATRON_BERT_START_DOCSTRING = r"""
763	
764	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
765	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
766	    etc.)
767	
768	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
769	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
770	    and behavior.
771	
772	    Parameters:
773	        config ([`MegatronBertConfig`]): Model configuration class with all the parameters of the model.
774	            Initializing with a config file does not load the weights associated with the model, only the
775	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
776	"""
777	
778	MEGATRON_BERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py:596
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
595	            content = json.dumps(index, indent=2, sort_keys=True) + "\n"
596	            f.write(content)
597	        print(

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py:647
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
646	    with open(tracker_filepath, "w") as f:
647	        f.write("release")
648	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mgp_str/modeling_mgp_str.py:333
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
332	
333	MGP_STR_START_DOCSTRING = r"""
334	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
335	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
336	    behavior.
337	
338	    Parameters:
339	        config ([`MgpstrConfig`]): Model configuration class with all the parameters of the model.
340	            Initializing with a config file does not load the weights associated with the model, only the
341	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
342	"""
343	
344	MGP_STR_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mgp_str/tokenization_mgp_str.py:99
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
98	        with open(vocab_file, "w", encoding="utf-8") as f:
99	            f.write(json.dumps(self.vocab, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
100	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mimi/modeling_mimi.py:593
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
592	            raise ValueError(
593	                "`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` "
594	                "make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers"
595	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mimi/modeling_mimi.py:959
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
958	                logger.warning_once(
959	                    "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and "
960	                    "will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class "
961	                    "(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mimi/modeling_mimi.py:1442
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1441	
1442	MIMI_START_DOCSTRING = r"""
1443	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1444	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1445	    etc.)
1446	
1447	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1448	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1449	    and behavior.
1450	
1451	    Parameters:
1452	        config ([`MimiConfig`]):
1453	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1454	            load the weights associated with the model, only the configuration. Check out the
1455	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1456	"""
1457	
1458	
1459	MIMI_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mistral/modeling_flax_mistral.py:46
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
45	
46	MISTRAL_START_DOCSTRING = r"""
47	
48	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
49	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
50	    etc.)
51	
52	    This model is also a Flax Linen
53	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
54	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
55	
56	    Finally, this model supports inherent JAX features such as:
57	
58	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
59	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
60	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
61	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
62	
63	    Parameters:
64	        config ([`MistralConfig`]): Model configuration class with all the parameters of the model.
65	            Initializing with a config file does not load the weights associated with the model, only the
66	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
67	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
68	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`, or
69	            `jax.numpy.bfloat16`.
70	
71	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
72	            specified all the computation will be performed with the given `dtype`.
73	
74	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
75	            parameters.**
76	
77	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
78	            [`~FlaxPreTrainedModel.to_bf16`].
79	"""
80	
81	MISTRAL_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mistral/modeling_flax_mistral.py:81
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
80	
81	MISTRAL_INPUTS_DOCSTRING = r"""
82	    Args:
83	        input_ids (`numpy.ndarray` of shape `(batch_size, input_ids_length)`):
84	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
85	            it.
86	
87	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
88	            [`PreTrainedTokenizer.__call__`] for details.
89	
90	            [What are input IDs?](../glossary#input-ids)
91	        attention_mask (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
92	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
93	
94	            - 1 for tokens that are **not masked**,
95	            - 0 for tokens that are **masked**.
96	
97	            [What are attention masks?](../glossary#attention-mask)
98	
99	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
100	            [`PreTrainedTokenizer.__call__`] for details.
101	
102	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
103	            `past_key_values`).
104	
105	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
106	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
107	            information on the default strategy.
108	
109	            - 1 indicates the head is **not masked**,
110	            - 0 indicates the head is **masked**.
111	        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
112	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
113	            config.n_positions - 1]`.
114	
115	            [What are position IDs?](../glossary#position-ids)
116	        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):
117	            Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
118	            auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.
119	        output_attentions (`bool`, *optional*):
120	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
121	            tensors for more detail.
122	        output_hidden_states (`bool`, *optional*):
123	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
124	            more detail.
125	        return_dict (`bool`, *optional*):
126	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
127	"""
128	
129	
130	# Copied from transformers.models.llama.modeling_flax_llama.FlaxLlamaRMSNorm with Llama->Mistral
131	class FlaxMistralRMSNorm(nn.Module):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mistral/modeling_mistral.py:309
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
308	
309	MISTRAL_START_DOCSTRING = r"""
310	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
311	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
312	    etc.)
313	
314	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
315	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
316	    and behavior.
317	
318	    Parameters:
319	        config ([`MistralConfig`]):
320	            Model configuration class with all the parameters of the model. Initializing with a config file does not
321	            load the weights associated with the model, only the configuration. Check out the
322	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
323	"""
324	
325	
326	@add_start_docstrings(
327	    "The bare Mistral Model outputting raw hidden-states without any specific head on top.",
328	    MISTRAL_START_DOCSTRING,
329	)
330	class MistralPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mistral/modeling_mistral.py:356
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
355	
356	MISTRAL_INPUTS_DOCSTRING = r"""
357	    Args:
358	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
359	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
360	            it.
361	
362	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
363	            [`PreTrainedTokenizer.__call__`] for details.
364	
365	            [What are input IDs?](../glossary#input-ids)
366	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
367	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
368	
369	            - 1 for tokens that are **not masked**,
370	            - 0 for tokens that are **masked**.
371	
372	            [What are attention masks?](../glossary#attention-mask)
373	
374	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
375	            [`PreTrainedTokenizer.__call__`] for details.
376	
377	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
378	            `past_key_values`).
379	
380	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
381	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
382	            information on the default strategy.
383	
384	            - 1 indicates the head is **not masked**,
385	            - 0 indicates the head is **masked**.
386	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
387	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
388	            config.n_positions - 1]`.
389	
390	            [What are position IDs?](../glossary#position-ids)
391	        past_key_values (`Cache`, *optional*):
392	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
393	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
394	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
395	
396	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
397	
398	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
399	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
400	            of shape `(batch_size, sequence_length)`.
401	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
402	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
403	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
404	            model's internal embedding lookup matrix.
405	        use_cache (`bool`, *optional*):
406	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
407	            `past_key_values`).
408	        output_attentions (`bool`, *optional*):
409	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
410	            tensors for more detail.
411	        output_hidden_states (`bool`, *optional*):
412	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
413	            more detail.
414	        return_dict (`bool`, *optional*):
415	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
416	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
417	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
418	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
419	            the complete sequence length.
420	"""
421	
422	
423	@add_start_docstrings(
424	    "The bare Mistral Model outputting raw hidden-states without any specific head on top.",
425	    MISTRAL_START_DOCSTRING,
426	)
427	class MistralModel(MistralPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mistral/modeling_tf_mistral.py:639
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
638	
639	MISTRAL_START_DOCSTRING = r"""
640	
641	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
642	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
643	    etc.)
644	
645	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
646	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
647	    behavior.
648	
649	    <Tip>
650	
651	    TensorFlow models and layers in `model` accept two formats as input:
652	
653	    - having all inputs as keyword arguments (like PyTorch models), or
654	    - having all inputs as a list, tuple or dict in the first positional argument.
655	
656	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
657	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
658	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
659	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
660	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
661	    positional argument:
662	
663	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
664	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
665	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
666	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
667	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
668	
669	    Note that when creating models and layers with
670	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
671	    about any of this, as you can just pass inputs like you would to any other Python function!
672	
673	    </Tip>
674	
675	    Parameters:
676	        config ([`MistralConfig`]): Model configuration class with all the parameters of the model.
677	            Initializing with a config file does not load the weights associated with the model, only the
678	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
679	"""
680	
681	
682	@add_start_docstrings(
683	    "The bare Mistral Model outputting raw hidden-states without any specific head on top.",
684	    MISTRAL_START_DOCSTRING,
685	)
686	class TFMistralPreTrainedModel(TFPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mistral/modeling_tf_mistral.py:691
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
690	
691	MISTRAL_INPUTS_DOCSTRING = r"""
692	    Args:
693	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
694	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
695	            it.
696	
697	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
698	            [`PreTrainedTokenizer.__call__`] for details.
699	
700	            [What are input IDs?](../glossary#input-ids)
701	        attention_mask (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
702	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
703	
704	            - 1 for tokens that are **not masked**,
705	            - 0 for tokens that are **masked**.
706	
707	            [What are attention masks?](../glossary#attention-mask)
708	
709	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
710	            [`PreTrainedTokenizer.__call__`] for details.
711	
712	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
713	            `past_key_values`).
714	
715	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
716	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
717	            information on the default strategy.
718	
719	            - 1 indicates the head is **not masked**,
720	            - 0 indicates the head is **masked**.
721	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
722	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
723	            config.n_positions - 1]`.
724	
725	            [What are position IDs?](../glossary#position-ids)
726	        past_key_values (`Cache` or `tuple(tuple(tf.Tensor))`, *optional*):
727	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
728	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
729	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
730	
731	            One formats is allowed:
732	            - Tuple of `tuple(tf.Tensor)` of length `config.n_layers`, with each tuple having 2 tensors of
733	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
734	            cache format.
735	
736	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
737	            legacy cache format will be returned.
738	
739	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
740	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
741	            of shape `(batch_size, sequence_length)`.
742	        inputs_embeds (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
743	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
744	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
745	            model's internal embedding lookup matrix.
746	        use_cache (`bool`, *optional*):
747	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
748	            `past_key_values`).
749	        output_attentions (`bool`, *optional*):
750	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
751	            tensors for more detail.
752	        output_hidden_states (`bool`, *optional*):
753	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
754	            more detail.
755	        return_dict (`bool`, *optional*):
756	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
757	"""
758	
759	
760	@add_start_docstrings(
761	    "The bare Mistral Model outputting raw hidden-states without any specific head on top.",
762	    MISTRAL_START_DOCSTRING,
763	)
764	class TFMistralModel(TFMistralPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mistral3/modeling_mistral3.py:170
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
169	
170	MISTRAL3_START_DOCSTRING = r"""
171	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
172	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
173	    etc.)
174	
175	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
176	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
177	    and behavior.
178	
179	    Parameters:
180	        config ([`Mistral3Config`] or [`Mistral3VisionConfig`]):
181	            Model configuration class with all the parameters of the model. Initializing with a config file does not
182	            load the weights associated with the model, only the configuration. Check out the
183	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
184	"""
185	
186	
187	@add_start_docstrings(
188	    "The bare LLaMA Model outputting raw hidden-states without any specific head on top.",
189	    MISTRAL3_START_DOCSTRING,
190	)
191	class Mistral3PreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mistral3/modeling_mistral3.py:226
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
225	
226	MISTRAL3_INPUTS_DOCSTRING = r"""
227	    Args:
228	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
229	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
230	            it.
231	
232	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
233	            [`PreTrainedTokenizer.__call__`] for details.
234	
235	            [What are input IDs?](../glossary#input-ids)
236	        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):
237	            The tensors corresponding to the input images. Pixel values can be obtained using
238	            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`Mistral3Processor`] uses
239	            [`CLIPImageProcessor`] for processing images).
240	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
241	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
242	
243	            - 1 for tokens that are **not masked**,
244	            - 0 for tokens that are **masked**.
245	
246	            [What are attention masks?](../glossary#attention-mask)
247	
248	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
249	            [`PreTrainedTokenizer.__call__`] for details.
250	
251	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
252	            `past_key_values`).
253	
254	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
255	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
256	            information on the default strategy.
257	
258	            - 1 indicates the head is **not masked**,
259	            - 0 indicates the head is **masked**.
260	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
261	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
262	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
263	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
264	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
265	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
266	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
267	
268	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
269	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
270	
271	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
272	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
273	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
274	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
275	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
276	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
277	            model's internal embedding lookup matrix.
278	        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):
279	            The index of the layer to select the vision feature. If multiple indices are provided,
280	            the vision feature of the corresponding indices will be concatenated to form the
281	            vision features.
282	        vision_feature_select_strategy (`str`, *optional*, defaults to `"default"`):
283	            The feature selection strategy used to select the vision feature from the vision backbone.
284	            Can be one of `"default"` or `"full"`.
285	        use_cache (`bool`, *optional*):
286	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
287	            `past_key_values`).
288	        output_attentions (`bool`, *optional*):
289	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
290	            tensors for more detail.
291	        output_hidden_states (`bool`, *optional*):
292	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
293	            more detail.
294	        return_dict (`bool`, *optional*):
295	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
296	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
297	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
298	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
299	            the complete sequence length.
300	"""
301	
302	
303	@add_start_docstrings(
304	    """The MISTRAL3 model which consists of a vision backbone and a language model.""",
305	    MISTRAL3_START_DOCSTRING,
306	)
307	class Mistral3ForConditionalGeneration(Mistral3PreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mixtral/modeling_mixtral.py:431
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
430	
431	MIXTRAL_START_DOCSTRING = r"""
432	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
433	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
434	    etc.)
435	
436	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
437	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
438	    and behavior.
439	
440	    Parameters:
441	        config ([`MixtralConfig`]):
442	            Model configuration class with all the parameters of the model. Initializing with a config file does not
443	            load the weights associated with the model, only the configuration. Check out the
444	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
445	"""
446	
447	
448	@add_start_docstrings(
449	    "The bare Mixtral Model outputting raw hidden-states without any specific head on top.",
450	    MIXTRAL_START_DOCSTRING,
451	)
452	class MixtralPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mixtral/modeling_mixtral.py:478
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
477	
478	MIXTRAL_INPUTS_DOCSTRING = r"""
479	    Args:
480	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
481	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
482	            it.
483	
484	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
485	            [`PreTrainedTokenizer.__call__`] for details.
486	
487	            [What are input IDs?](../glossary#input-ids)
488	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
489	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
490	
491	            - 1 for tokens that are **not masked**,
492	            - 0 for tokens that are **masked**.
493	
494	            [What are attention masks?](../glossary#attention-mask)
495	
496	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
497	            [`PreTrainedTokenizer.__call__`] for details.
498	
499	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
500	            `past_key_values`).
501	
502	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
503	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
504	            information on the default strategy.
505	
506	            - 1 indicates the head is **not masked**,
507	            - 0 indicates the head is **masked**.
508	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
509	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
510	            config.n_positions - 1]`.
511	
512	            [What are position IDs?](../glossary#position-ids)
513	        past_key_values (`Cache`, *optional*):
514	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
515	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
516	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
517	
518	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
519	
520	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
521	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
522	            of shape `(batch_size, sequence_length)`.
523	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
524	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
525	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
526	            model's internal embedding lookup matrix.
527	        use_cache (`bool`, *optional*):
528	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
529	            `past_key_values`).
530	        output_attentions (`bool`, *optional*):
531	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
532	            tensors for more detail.
533	        output_hidden_states (`bool`, *optional*):
534	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
535	            more detail.
536	        return_dict (`bool`, *optional*):
537	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
538	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
539	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
540	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
541	            the complete sequence length.
542	"""
543	
544	
545	@add_start_docstrings(
546	    "The bare Mixtral Model outputting raw hidden-states without any specific head on top.",
547	    MIXTRAL_START_DOCSTRING,
548	)
549	class MixtralModel(MixtralPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mllama/modeling_mllama.py:1182
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1181	
1182	MLLAMA_START_DOCSTRING = r"""
1183	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1184	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1185	    etc.)
1186	
1187	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1188	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1189	    and behavior.
1190	
1191	    Parameters:
1192	        config ([`MllamaConfig`]):
1193	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1194	            load the weights associated with the model, only the configuration. Check out the
1195	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1196	"""
1197	
1198	
1199	MLLAMA_VISION_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mllama/modeling_mllama.py:1233
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1232	
1233	MLLAMA_TEXT_INPUTS_DOCSTRING = r"""
1234	    Args:
1235	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
1236	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
1237	            it.
1238	
1239	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1240	            [`PreTrainedTokenizer.__call__`] for details.
1241	
1242	            [What are input IDs?](../glossary#input-ids)
1243	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
1244	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
1245	
1246	            - 1 for tokens that are **not masked**,
1247	            - 0 for tokens that are **masked**.
1248	
1249	            [What are attention masks?](../glossary#attention-mask)
1250	
1251	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1252	            [`PreTrainedTokenizer.__call__`] for details.
1253	
1254	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
1255	            `past_key_values`).
1256	
1257	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
1258	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
1259	            information on the default strategy.
1260	
1261	            - 1 indicates the head is **not masked**,
1262	            - 0 indicates the head is **masked**.
1263	        cross_attention_mask (`torch.Tensor` of shape `(batch_size, seq_length, max_num_images, max_num_tiles)`, *optional*):
1264	            Cross-attention mask to control the interaction between text tokens and image tiles.
1265	            This 4D tensor defines which image tiles each text token should attend to.
1266	
1267	            For each text token (in seq_length):
1268	            - 1 indicates the token **should attend** to the corresponding image tile
1269	            - 0 indicates the token **should not attend** to the corresponding image tile
1270	        cross_attention_states (`torch.FloatTensor`, *optional*):
1271	            Output of the vision model, used for cross-attention. This tensor contains the processed image features that
1272	            the language model will attend to.
1273	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
1274	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
1275	            config.n_positions - 1]`.
1276	
1277	            [What are position IDs?](../glossary#position-ids)
1278	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
1279	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
1280	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
1281	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
1282	
1283	            Two formats are allowed:
1284	            - a [`~cache_utils.Cache`] instance, see our
1285	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
1286	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
1287	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
1288	            cache format.
1289	
1290	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
1291	            legacy cache format will be returned.
1292	
1293	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
1294	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
1295	            of shape `(batch_size, sequence_length)`.
1296	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
1297	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
1298	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
1299	            model's internal embedding lookup matrix.
1300	        use_cache (`bool`, *optional*):
1301	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
1302	            `past_key_values`).
1303	        output_attentions (`bool`, *optional*):
1304	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1305	            tensors for more detail.
1306	        output_hidden_states (`bool`, *optional*):
1307	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1308	            more detail.
1309	        return_dict (`bool`, *optional*):
1310	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1311	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
1312	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
1313	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
1314	            the complete sequence length.
1315	"""
1316	
1317	
1318	MLLAMA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mllama/modeling_mllama.py:1318
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1317	
1318	MLLAMA_INPUTS_DOCSTRING = r"""
1319	    Args:
1320	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
1321	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
1322	            it.
1323	
1324	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1325	            [`PreTrainedTokenizer.__call__`] for details.
1326	
1327	            [What are input IDs?](../glossary#input-ids)
1328	        pixel_values (`torch.FloatTensor` of shape `(batch_size, max_num_images, max_num_tiles, channels, image_size, image_size)):
1329	            The tensors corresponding to the input images. Pixel values can be obtained using
1330	            [`AutoImageProcessor`]. See [`MllamaImageProcessor.__call__`] for details ([]`MllamaProcessor`] uses
1331	            [`MllamaImageProcessor`] for processing images).
1332	        aspect_ratio_mask (`torch.Tensor` of shape `(batch_size, max_num_images, max_num_tiles)`, *optional*):
1333	            Mask to avoid performing attention on padding tiles. Mask values selected in `[0, 1]`:
1334	
1335	            - 1 for tiles that are **not masked**,
1336	            - 0 for tiles that are **masked**.
1337	        aspect_ratio_ids (`torch.Tensor` of shape `(batch_size, max_num_images)`, *optional*):
1338	            Aspect ratio ids used to select the appropriate precomputed tile embeddings based on the aspect ratio of each input image.
1339	            These ids correspond to indices in the model's list of supported aspect ratios, offset by 1.
1340	
1341	            For example, if the model supports aspect ratios [[1, 1], [1, 2], [2, 1]]:
1342	            - An image with aspect ratio [1, 1] would have ID 1
1343	            - An image with aspect ratio [1, 2] would have ID 2
1344	            - An image with aspect ratio [2, 1] would have ID 3
1345	
1346	            The id 0 is reserved for padding (i.e., no image).
1347	
1348	            If an image has aspect ratio [1, 2], that means it was split into 2 tiles horizontally, and its `aspect_ratio_id` would be 2.
1349	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
1350	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
1351	
1352	            - 1 for tokens that are **not masked**,
1353	            - 0 for tokens that are **masked**.
1354	
1355	            [What are attention masks?](../glossary#attention-mask)
1356	
1357	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1358	            [`PreTrainedTokenizer.__call__`] for details.
1359	
1360	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
1361	            `past_key_values`).
1362	
1363	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
1364	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
1365	            information on the default strategy.
1366	
1367	            - 1 indicates the head is **not masked**,
1368	            - 0 indicates the head is **masked**.
1369	        cross_attention_mask (`torch.Tensor` of shape `(batch_size, seq_length, max_num_images, max_num_tiles)`, *optional*):
1370	            Cross-attention mask to control the interaction between text tokens and image tiles.
1371	            This 4D tensor defines which image tiles each text token should attend to.
1372	
1373	            For each text token (in seq_length):
1374	            - 1 indicates the token **should attend** to the corresponding image tile
1375	            - 0 indicates the token **should not attend** to the corresponding image tile
1376	        cross_attention_states (`torch.FloatTensor`, *optional*):
1377	            Output of the vision model, used for cross-attention. This tensor contains the processed image features that
1378	            the language model will attend to.
1379	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
1380	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
1381	            config.n_positions - 1]`.
1382	
1383	            [What are position IDs?](../glossary#position-ids)
1384	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
1385	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
1386	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
1387	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
1388	
1389	            Two formats are allowed:
1390	            - a [`~cache_utils.Cache`] instance, see our
1391	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
1392	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
1393	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
1394	            cache format.
1395	
1396	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
1397	            legacy cache format will be returned.
1398	
1399	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
1400	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
1401	            of shape `(batch_size, sequence_length)`.
1402	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
1403	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
1404	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
1405	            model's internal embedding lookup matrix.
1406	        use_cache (`bool`, *optional*):
1407	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
1408	            `past_key_values`).
1409	        output_attentions (`bool`, *optional*):
1410	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1411	            tensors for more detail.
1412	        output_hidden_states (`bool`, *optional*):
1413	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1414	            more detail.
1415	        return_dict (`bool`, *optional*):
1416	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1417	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
1418	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
1419	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
1420	            the complete sequence length.
1421	"""
1422	
1423	
1424	@add_start_docstrings(
1425	    """The Mllama Vision Model which consists of two vision encoders.""",
1426	    MLLAMA_START_DOCSTRING,
1427	)
1428	class MllamaVisionModel(MllamaPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mluke/tokenization_mluke.py:564
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
563	            raise NotImplementedError(
564	                "return_offset_mapping is not available when using Python tokenizers. "
565	                "To use this feature, change your tokenizer to one deriving from "
566	                "transformers.PreTrainedTokenizerFast. "

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mluke/tokenization_mluke.py:1545
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1544	                content_spiece_model = self.sp_model.serialized_model_proto()
1545	                fi.write(content_spiece_model)
1546	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mluke/tokenization_mluke.py:1552
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1551	        with open(entity_vocab_file, "w", encoding="utf-8") as f:
1552	            f.write(json.dumps(self.entity_vocab, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
1553	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mobilebert/modeling_mobilebert.py:89
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
88	        logger.error(
89	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
90	            "https://www.tensorflow.org/install/ for installation instructions."
91	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mobilebert/modeling_mobilebert.py:745
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
744	
745	MOBILEBERT_START_DOCSTRING = r"""
746	
747	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
748	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
749	    etc.)
750	
751	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
752	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
753	    and behavior.
754	
755	    Parameters:
756	        config ([`MobileBertConfig`]): Model configuration class with all the parameters of the model.
757	            Initializing with a config file does not load the weights associated with the model, only the
758	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
759	"""
760	
761	MOBILEBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mobilebert/modeling_tf_mobilebert.py:1072
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1071	
1072	MOBILEBERT_START_DOCSTRING = r"""
1073	
1074	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
1075	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1076	    etc.)
1077	
1078	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
1079	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
1080	    behavior.
1081	
1082	    <Tip>
1083	
1084	    TensorFlow models and layers in `transformers` accept two formats as input:
1085	
1086	    - having all inputs as keyword arguments (like PyTorch models), or
1087	    - having all inputs as a list, tuple or dict in the first positional argument.
1088	
1089	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
1090	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
1091	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
1092	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
1093	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
1094	    positional argument:
1095	
1096	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
1097	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
1098	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
1099	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
1100	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
1101	
1102	    Note that when creating models and layers with
1103	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
1104	    about any of this, as you can just pass inputs like you would to any other Python function!
1105	
1106	    </Tip>
1107	
1108	    Parameters:
1109	        config ([`MobileBertConfig`]): Model configuration class with all the parameters of the model.
1110	            Initializing with a config file does not load the weights associated with the model, only the
1111	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1112	"""
1113	
1114	MOBILEBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mobilebert/tokenization_mobilebert.py:288
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
287	                    index = token_index
288	                writer.write(token + "\n")
289	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mobilenet_v1/modeling_mobilenet_v1.py:99
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
98	        logger.error(
99	            "Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see "
100	            "https://www.tensorflow.org/install/ for installation instructions."
101	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mobilenet_v1/modeling_mobilenet_v1.py:266
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
265	
266	MOBILENET_V1_START_DOCSTRING = r"""
267	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
268	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
269	    behavior.
270	
271	    Parameters:
272	        config ([`MobileNetV1Config`]): Model configuration class with all the parameters of the model.
273	            Initializing with a config file does not load the weights associated with the model, only the
274	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
275	"""
276	
277	MOBILENET_V1_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py:174
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
173	        logger.error(
174	            "Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see "
175	            "https://www.tensorflow.org/install/ for installation instructions."
176	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py:465
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
464	
465	MOBILENET_V2_START_DOCSTRING = r"""
466	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
467	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
468	    behavior.
469	
470	    Parameters:
471	        config ([`MobileNetV2Config`]): Model configuration class with all the parameters of the model.
472	            Initializing with a config file does not load the weights associated with the model, only the
473	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
474	"""
475	
476	MOBILENET_V2_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mobilevit/modeling_mobilevit.py:667
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
666	
667	MOBILEVIT_START_DOCSTRING = r"""
668	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
669	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
670	    behavior.
671	
672	    Parameters:
673	        config ([`MobileViTConfig`]): Model configuration class with all the parameters of the model.
674	            Initializing with a config file does not load the weights associated with the model, only the
675	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
676	"""
677	
678	MOBILEVIT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mobilevit/modeling_tf_mobilevit.py:920
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
919	
920	MOBILEVIT_START_DOCSTRING = r"""
921	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
922	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
923	    etc.)
924	
925	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
926	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
927	    behavior.
928	
929	    <Tip>
930	
931	    TensorFlow models and layers in `transformers` accept two formats as input:
932	
933	    - having all inputs as keyword arguments (like PyTorch models), or
934	    - having all inputs as a list, tuple or dict in the first positional argument.
935	
936	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
937	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
938	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
939	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
940	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
941	    positional argument:
942	
943	    - a single Tensor with `pixel_values` only and nothing else: `model(pixel_values)`
944	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
945	    `model([pixel_values, attention_mask])` or `model([pixel_values, attention_mask, token_type_ids])`
946	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
947	    `model({"pixel_values": pixel_values, "token_type_ids": token_type_ids})`
948	
949	    Note that when creating models and layers with
950	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
951	    about any of this, as you can just pass inputs like you would to any other Python function!
952	
953	    </Tip>
954	
955	    Parameters:
956	        config ([`MobileViTConfig`]): Model configuration class with all the parameters of the model.
957	            Initializing with a config file does not load the weights associated with the model, only the
958	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
959	"""
960	
961	MOBILEVIT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py:620
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
619	
620	MOBILEVITV2_START_DOCSTRING = r"""
621	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
622	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
623	    behavior.
624	
625	    Parameters:
626	        config ([`MobileViTV2Config`]): Model configuration class with all the parameters of the model.
627	            Initializing with a config file does not load the weights associated with the model, only the
628	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
629	"""
630	
631	MOBILEVITV2_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/modernbert/modeling_modernbert.py:565
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
564	
565	MODERNBERT_START_DOCSTRING = r"""
566	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
567	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
568	    etc.)
569	
570	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
571	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
572	    and behavior.
573	
574	    Parameters:
575	        config ([`ModernBertConfig`]):
576	            Model configuration class with all the parameters of the model. Initializing with a config file does not
577	            load the weights associated with the model, only the configuration. Check out the
578	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
579	"""
580	
581	
582	@add_start_docstrings(
583	    "The bare ModernBert Model outputting raw hidden-states without any specific head on top.",
584	    MODERNBERT_START_DOCSTRING,
585	)
586	class ModernBertPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/modernbert/modeling_modernbert.py:791
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
790	
791	MODERNBERT_INPUTS_DOCSTRING = r"""
792	    Args:
793	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
794	            Indices of input sequence tokens in the vocabulary. With Flash Attention 2.0, padding will be ignored
795	            by default should you provide it.
796	
797	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
798	            [`PreTrainedTokenizer.__call__`] for details.
799	
800	            [What are input IDs?](../glossary#input-ids)
801	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
802	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
803	
804	            - 1 for tokens that are **not masked**,
805	            - 0 for tokens that are **masked**.
806	
807	            [What are attention masks?](../glossary#attention-mask)
808	
809	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
810	            [`PreTrainedTokenizer.__call__`] for details.
811	
812	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
813	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
814	            information on the default strategy.
815	
816	            - 1 indicates the head is **not masked**,
817	            - 0 indicates the head is **masked**.
818	        sliding_window_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
819	            Mask to avoid performing attention on padding or far-away tokens. In ModernBert, only every few layers
820	            perform global attention, while the rest perform local attention. This mask is used to avoid attending to
821	            far-away tokens in the local attention layers when not using Flash Attention.
822	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
823	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
824	            config.n_positions - 1]`.
825	
826	            [What are position IDs?](../glossary#position-ids)
827	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
828	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
829	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
830	            model's internal embedding lookup matrix.
831	        indices (`torch.Tensor` of shape `(total_unpadded_tokens,)`, *optional*):
832	            Indices of the non-padding tokens in the input sequence. Used for unpadding the output.
833	        cu_seqlens (`torch.Tensor` of shape `(batch + 1,)`, *optional*):
834	            Cumulative sequence lengths of the input sequences. Used to index the unpadded tensors.
835	        max_seqlen (`int`, *optional*):
836	            Maximum sequence length in the batch excluding padding tokens. Used to unpad input_ids and pad output tensors.
837	        batch_size (`int`, *optional*):
838	            Batch size of the input sequences. Used to pad the output tensors.
839	        seq_len (`int`, *optional*):
840	            Sequence length of the input sequences including padding tokens. Used to pad the output tensors.
841	        output_attentions (`bool`, *optional*):
842	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
843	            tensors for more detail.
844	        output_hidden_states (`bool`, *optional*):
845	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
846	            more detail.
847	        return_dict (`bool`, *optional*):
848	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
849	"""
850	
851	
852	@add_start_docstrings(
853	    "The bare ModernBert Model outputting raw hidden-states without any specific head on top.",
854	    MODERNBERT_START_DOCSTRING,
855	)
856	class ModernBertModel(ModernBertPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/modernbert/modular_modernbert.py:767
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
766	
767	MODERNBERT_START_DOCSTRING = r"""
768	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
769	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
770	    etc.)
771	
772	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
773	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
774	    and behavior.
775	
776	    Parameters:
777	        config ([`ModernBertConfig`]):
778	            Model configuration class with all the parameters of the model. Initializing with a config file does not
779	            load the weights associated with the model, only the configuration. Check out the
780	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
781	"""
782	
783	
784	@add_start_docstrings(
785	    "The bare ModernBert Model outputting raw hidden-states without any specific head on top.",
786	    MODERNBERT_START_DOCSTRING,
787	)
788	class ModernBertPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/modernbert/modular_modernbert.py:921
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
920	
921	MODERNBERT_INPUTS_DOCSTRING = r"""
922	    Args:
923	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
924	            Indices of input sequence tokens in the vocabulary. With Flash Attention 2.0, padding will be ignored
925	            by default should you provide it.
926	
927	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
928	            [`PreTrainedTokenizer.__call__`] for details.
929	
930	            [What are input IDs?](../glossary#input-ids)
931	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
932	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
933	
934	            - 1 for tokens that are **not masked**,
935	            - 0 for tokens that are **masked**.
936	
937	            [What are attention masks?](../glossary#attention-mask)
938	
939	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
940	            [`PreTrainedTokenizer.__call__`] for details.
941	
942	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
943	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
944	            information on the default strategy.
945	
946	            - 1 indicates the head is **not masked**,
947	            - 0 indicates the head is **masked**.
948	        sliding_window_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
949	            Mask to avoid performing attention on padding or far-away tokens. In ModernBert, only every few layers
950	            perform global attention, while the rest perform local attention. This mask is used to avoid attending to
951	            far-away tokens in the local attention layers when not using Flash Attention.
952	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
953	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
954	            config.n_positions - 1]`.
955	
956	            [What are position IDs?](../glossary#position-ids)
957	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
958	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
959	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
960	            model's internal embedding lookup matrix.
961	        indices (`torch.Tensor` of shape `(total_unpadded_tokens,)`, *optional*):
962	            Indices of the non-padding tokens in the input sequence. Used for unpadding the output.
963	        cu_seqlens (`torch.Tensor` of shape `(batch + 1,)`, *optional*):
964	            Cumulative sequence lengths of the input sequences. Used to index the unpadded tensors.
965	        max_seqlen (`int`, *optional*):
966	            Maximum sequence length in the batch excluding padding tokens. Used to unpad input_ids and pad output tensors.
967	        batch_size (`int`, *optional*):
968	            Batch size of the input sequences. Used to pad the output tensors.
969	        seq_len (`int`, *optional*):
970	            Sequence length of the input sequences including padding tokens. Used to pad the output tensors.
971	        output_attentions (`bool`, *optional*):
972	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
973	            tensors for more detail.
974	        output_hidden_states (`bool`, *optional*):
975	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
976	            more detail.
977	        return_dict (`bool`, *optional*):
978	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
979	"""
980	
981	
982	@add_start_docstrings(
983	    "The bare ModernBert Model outputting raw hidden-states without any specific head on top.",
984	    MODERNBERT_START_DOCSTRING,
985	)
986	class ModernBertModel(ModernBertPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/moonshine/modeling_moonshine.py:502
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
501	
502	MOONSHINE_START_DOCSTRING = r"""
503	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
504	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
505	    etc.)
506	
507	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
508	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
509	    and behavior.
510	
511	    Parameters:
512	        config ([`MoonshineConfig`]):
513	            Model configuration class with all the parameters of the model. Initializing with a config file does not
514	            load the weights associated with the model, only the configuration. Check out the
515	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
516	"""
517	
518	
519	@add_start_docstrings(
520	    "The bare Moonshine Model outputting raw hidden-states without any specific head on top.",
521	    MOONSHINE_START_DOCSTRING,
522	)
523	class MoonshinePreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/moonshine/modeling_moonshine.py:708
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
707	
708	MOONSHINE_INPUTS_DOCSTRING = r"""
709	    Args:
710	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
711	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
712	            it.
713	
714	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
715	            [`PreTrainedTokenizer.__call__`] for details.
716	
717	            [What are input IDs?](../glossary#input-ids)
718	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
719	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
720	
721	            - 1 for tokens that are **not masked**,
722	            - 0 for tokens that are **masked**.
723	
724	            [What are attention masks?](../glossary#attention-mask)
725	
726	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
727	            [`PreTrainedTokenizer.__call__`] for details.
728	
729	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
730	            `past_key_values`).
731	
732	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
733	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
734	            information on the default strategy.
735	
736	            - 1 indicates the head is **not masked**,
737	            - 0 indicates the head is **masked**.
738	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
739	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
740	            config.n_positions - 1]`.
741	
742	            [What are position IDs?](../glossary#position-ids)
743	        past_key_values (`Cache`, *optional*):
744	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
745	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
746	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
747	
748	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
749	
750	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
751	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
752	            of shape `(batch_size, sequence_length)`.
753	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
754	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
755	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
756	            model's internal embedding lookup matrix.
757	        use_cache (`bool`, *optional*):
758	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
759	            `past_key_values`).
760	        output_attentions (`bool`, *optional*):
761	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
762	            tensors for more detail.
763	        output_hidden_states (`bool`, *optional*):
764	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
765	            more detail.
766	        return_dict (`bool`, *optional*):
767	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
768	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
769	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
770	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
771	            the complete sequence length.
772	"""
773	
774	
775	@add_start_docstrings(
776	    "The bare Moonshine Model outputting raw hidden-states without any specific head on top.",
777	    MOONSHINE_START_DOCSTRING,
778	)
779	class MoonshineDecoder(MoonshinePreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/moonshine/modeling_moonshine.py:1207
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1206	
1207	MOONSHINE_MODEL_INPUTS_DOCSTRING = r"""
1208	    Args:
1209	        input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):
1210	            Float values of the raw speech waveform. Raw speech waveform can be
1211	            obtained by loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a
1212	            `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into
1213	            `input_values`, the [`AutoFeatureExtractor`] should be used for padding
1214	            and conversion into a tensor of type `torch.FloatTensor`.
1215	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
1216	            Mask to avoid performing attention on padding indices in `input_values`. Mask values selected in `[0, 1]`:
1217	            - 1 for tokens that are **not masked**,
1218	            - 0 for tokens that are **masked**.
1219	            [What are attention masks?](../glossary#attention-mask)
1220	        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
1221	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
1222	            it.
1223	
1224	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1225	            [`PreTrainedTokenizer.__call__`] for details.
1226	
1227	            [What are input IDs?](../glossary#input-ids)
1228	        decoder_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
1229	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
1230	
1231	            - 1 for tokens that are **not masked**,
1232	            - 0 for tokens that are **masked**.
1233	
1234	            [What are attention masks?](../glossary#attention-mask)
1235	
1236	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1237	            [`PreTrainedTokenizer.__call__`] for details.
1238	
1239	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
1240	            `past_key_values`).
1241	
1242	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
1243	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
1244	            information on the default strategy.
1245	
1246	            - 1 indicates the head is **not masked**,
1247	            - 0 indicates the head is **masked**.
1248	        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):
1249	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
1250	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
1251	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
1252	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
1253	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
1254	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
1255	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
1256	
1257	            Two formats are allowed:
1258	            - a [`~cache_utils.Cache`] instance, see our
1259	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
1260	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
1261	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
1262	            cache format.
1263	
1264	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
1265	            legacy cache format will be returned.
1266	
1267	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that don't
1268	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `decoder_input_ids`
1269	            of shape `(batch_size, sequence_length)`.
1270	        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
1271	            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded representation. This
1272	            is useful if you want more control over how to convert `decoder_input_ids` indices into associated vectors than the
1273	            model's internal embedding lookup matrix.
1274	        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
1275	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
1276	            config.n_positions - 1]`.
1277	
1278	            [What are position IDs?](../glossary#position-ids)
1279	        use_cache (`bool`, *optional*):
1280	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
1281	            `past_key_values`).
1282	        output_attentions (`bool`, *optional*):
1283	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1284	            tensors for more detail.
1285	        output_hidden_states (`bool`, *optional*):
1286	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1287	            more detail.
1288	        return_dict (`bool`, *optional*):
1289	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1290	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
1291	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `decoder_position_ids`,
1292	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
1293	            the complete sequence length.
1294	"""
1295	
1296	
1297	@add_start_docstrings(
1298	    "The bare Moonshine Model outputting raw hidden-states without any specific head on top.",
1299	    MOONSHINE_START_DOCSTRING,
1300	)
1301	class MoonshineModel(MoonshinePreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/moonshine/modular_moonshine.py:519
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
518	
519	MOONSHINE_START_DOCSTRING = r"""
520	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
521	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
522	    etc.)
523	
524	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
525	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
526	    and behavior.
527	
528	    Parameters:
529	        config ([`MoonshineConfig`]):
530	            Model configuration class with all the parameters of the model. Initializing with a config file does not
531	            load the weights associated with the model, only the configuration. Check out the
532	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
533	"""
534	
535	
536	@add_start_docstrings(
537	    "The bare Moonshine Model outputting raw hidden-states without any specific head on top.",
538	    MOONSHINE_START_DOCSTRING,
539	)
540	class MoonshinePreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/moonshine/modular_moonshine.py:882
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
881	
882	MOONSHINE_MODEL_INPUTS_DOCSTRING = r"""
883	    Args:
884	        input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):
885	            Float values of the raw speech waveform. Raw speech waveform can be
886	            obtained by loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a
887	            `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into
888	            `input_values`, the [`AutoFeatureExtractor`] should be used for padding
889	            and conversion into a tensor of type `torch.FloatTensor`.
890	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
891	            Mask to avoid performing attention on padding indices in `input_values`. Mask values selected in `[0, 1]`:
892	            - 1 for tokens that are **not masked**,
893	            - 0 for tokens that are **masked**.
894	            [What are attention masks?](../glossary#attention-mask)
895	        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
896	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
897	            it.
898	
899	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
900	            [`PreTrainedTokenizer.__call__`] for details.
901	
902	            [What are input IDs?](../glossary#input-ids)
903	        decoder_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
904	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
905	
906	            - 1 for tokens that are **not masked**,
907	            - 0 for tokens that are **masked**.
908	
909	            [What are attention masks?](../glossary#attention-mask)
910	
911	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
912	            [`PreTrainedTokenizer.__call__`] for details.
913	
914	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
915	            `past_key_values`).
916	
917	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
918	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
919	            information on the default strategy.
920	
921	            - 1 indicates the head is **not masked**,
922	            - 0 indicates the head is **masked**.
923	        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):
924	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
925	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
926	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
927	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
928	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
929	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
930	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
931	
932	            Two formats are allowed:
933	            - a [`~cache_utils.Cache`] instance, see our
934	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
935	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
936	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
937	            cache format.
938	
939	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
940	            legacy cache format will be returned.
941	
942	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that don't
943	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `decoder_input_ids`
944	            of shape `(batch_size, sequence_length)`.
945	        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
946	            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded representation. This
947	            is useful if you want more control over how to convert `decoder_input_ids` indices into associated vectors than the
948	            model's internal embedding lookup matrix.
949	        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
950	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
951	            config.n_positions - 1]`.
952	
953	            [What are position IDs?](../glossary#position-ids)
954	        use_cache (`bool`, *optional*):
955	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
956	            `past_key_values`).
957	        output_attentions (`bool`, *optional*):
958	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
959	            tensors for more detail.
960	        output_hidden_states (`bool`, *optional*):
961	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
962	            more detail.
963	        return_dict (`bool`, *optional*):
964	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
965	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
966	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `decoder_position_ids`,
967	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
968	            the complete sequence length.
969	"""
970	
971	
972	@add_start_docstrings(
973	    "The bare Moonshine Model outputting raw hidden-states without any specific head on top.",
974	    MOONSHINE_START_DOCSTRING,
975	)
976	class MoonshineModel(WhisperModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/moshi/modeling_moshi.py:562
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
561	            raise ValueError(
562	                "`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` "
563	                "make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers"
564	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/moshi/modeling_moshi.py:870
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
869	
870	MOSHI_START_DOCSTRING = r"""
871	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
872	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
873	    etc.)
874	
875	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
876	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
877	    and behavior.
878	
879	    Parameters:
880	        config ([`MoshiConfig`]): Model configuration class with all the parameters of the model.
881	            Initializing with a config file does not load the weights associated with the model, only the
882	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
883	"""
884	
885	
886	MOSHI_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/moshi/modeling_moshi.py:953
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
952	
953	MOSHI_DECODER_INPUTS_DOCSTRING = r"""
954	    Args:
955	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
956	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
957	            it.
958	
959	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
960	            [`PreTrainedTokenizer.__call__`] for details.
961	
962	            [What are input IDs?](../glossary#input-ids)
963	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
964	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
965	
966	            - 1 for tokens that are **not masked**,
967	            - 0 for tokens that are **masked**.
968	
969	            [What are attention masks?](../glossary#attention-mask)
970	
971	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
972	            [`PreTrainedTokenizer.__call__`] for details.
973	
974	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
975	            `past_key_values`).
976	
977	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
978	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
979	            information on the default strategy.
980	
981	            - 1 indicates the head is **not masked**,
982	            - 0 indicates the head is **masked**.
983	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
984	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
985	            config.n_positions - 1]`.
986	
987	            [What are position IDs?](../glossary#position-ids)
988	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
989	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
990	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
991	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
992	
993	            Two formats are allowed:
994	            - a [`~cache_utils.Cache`] instance, see our
995	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
996	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
997	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
998	            cache format.
999	
1000	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
1001	            legacy cache format will be returned.
1002	
1003	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
1004	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
1005	            of shape `(batch_size, sequence_length)`.
1006	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
1007	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
1008	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
1009	            model's internal embedding lookup matrix.
1010	        use_cache (`bool`, *optional*):
1011	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
1012	            `past_key_values`).
1013	        output_attentions (`bool`, *optional*):
1014	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1015	            tensors for more detail.
1016	        output_hidden_states (`bool`, *optional*):
1017	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1018	            more detail.
1019	        return_dict (`bool`, *optional*):
1020	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1021	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
1022	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
1023	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
1024	            the complete sequence length.
1025	"""
1026	
1027	
1028	class MoshiDepthDecoder(MoshiPreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/moshi/modeling_moshi.py:1516
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1515	            logger.warning_once(
1516	                "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. "
1517	                "Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)"
1518	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mpnet/modeling_mpnet.py:416
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
415	
416	MPNET_START_DOCSTRING = r"""
417	
418	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
419	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
420	    etc.)
421	
422	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
423	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
424	    and behavior.
425	
426	    Parameters:
427	        config ([`MPNetConfig`]): Model configuration class with all the parameters of the model.
428	            Initializing with a config file does not load the weights associated with the model, only the
429	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
430	"""
431	
432	MPNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mpnet/modeling_tf_mpnet.py:659
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
658	
659	MPNET_START_DOCSTRING = r"""
660	
661	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
662	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
663	    etc.)
664	
665	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
666	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
667	    behavior.
668	
669	    <Tip>
670	
671	    TensorFlow models and layers in `transformers` accept two formats as input:
672	
673	    - having all inputs as keyword arguments (like PyTorch models), or
674	    - having all inputs as a list, tuple or dict in the first positional argument.
675	
676	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
677	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
678	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
679	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
680	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
681	    positional argument:
682	
683	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
684	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
685	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
686	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
687	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
688	
689	    Note that when creating models and layers with
690	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
691	    about any of this, as you can just pass inputs like you would to any other Python function!
692	
693	    </Tip>
694	
695	    Args:
696	        config ([`MPNetConfig`]): Model configuration class with all the parameters of the model.
697	            Initializing with a config file does not load the weights associated with the model, only the
698	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
699	"""
700	
701	MPNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mpnet/tokenization_mpnet.py:312
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
311	                    index = token_index
312	                writer.write(token + "\n")
313	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mpt/modeling_mpt.py:278
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
277	
278	MPT_START_DOCSTRING = r"""
279	
280	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
281	    library implements for all its model (such as downloading or saving, resizing the input embeddings etc.)
282	
283	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
284	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
285	    and behavior.
286	
287	    Parameters:
288	        config ([`MptConfig`]): Model configuration class with all the parameters of the model.
289	            Initializing with a config file does not load the weights associated with the model, only the
290	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
291	"""
292	
293	MPT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mra/modeling_mra.py:856
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
855	
856	MRA_START_DOCSTRING = r"""
857	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
858	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
859	    behavior.
860	
861	    Parameters:
862	        config ([`MraConfig`]): Model configuration class with all the parameters of the model.
863	            Initializing with a config file does not load the weights associated with the model, only the
864	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
865	"""
866	
867	MRA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mt5/modeling_mt5.py:639
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
638	        logger.error(
639	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
640	            "https://www.tensorflow.org/install/ for installation instructions."
641	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mt5/modeling_mt5.py:1322
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1321	
1322	MT5_START_DOCSTRING = r"""
1323	
1324	    The MT5 model was proposed in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text
1325	    Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
1326	    Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. It's an encoder decoder transformer pre-trained in a
1327	    text-to-text denoising generative setting.
1328	
1329	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1330	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1331	    etc.)
1332	
1333	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1334	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1335	    and behavior.
1336	
1337	    Parameters:
1338	        config ([`MT5Config`]): Model configuration class with all the parameters of the model.
1339	            Initializing with a config file does not load the weights associated with the model, only the
1340	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1341	"""
1342	
1343	MT5_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/musicgen/modeling_musicgen.py:476
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
475	            logger.warning_once(
476	                '`torch.nn.functional.scaled_dot_product_attention` does not support having an empty attention mask. Falling back to the manual attention implementation. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.'
477	                "Note that this probably happens because `guidance_scale>1` or because you used `get_unconditional_inputs`. See https://github.com/huggingface/transformers/issues/31189 for more information."
478	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/musicgen/modeling_musicgen.py:728
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
727	
728	MUSICGEN_START_DOCSTRING = r"""
729	
730	    The Musicgen model was proposed in [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284) by
731	    Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Dfossez. It is an
732	    encoder decoder transformer trained on the task of conditional music generation
733	
734	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
735	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
736	    etc.)
737	
738	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
739	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
740	    and behavior.
741	
742	    Parameters:
743	        config ([`MusicgenConfig`]): Model configuration class with all the parameters of the model.
744	            Initializing with a config file does not load the weights associated with the model, only the
745	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
746	"""
747	
748	MUSICGEN_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/musicgen/modeling_musicgen.py:1768
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1767	            raise ValueError(
1768	                "The selected decoder is not prepared for the encoder hidden states to be passed. Please see the "
1769	                "following discussion on GitHub: https://github.com/huggingface/transformers/issues/23350"
1770	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/musicgen_melody/feature_extraction_musicgen_melody.py:251
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
250	            logger.warning_once(
251	                "`audio` is a 4-dimensional torch tensor and has thus been recognized as the output of `Demucs`. "
252	                "If this is not the case, make sure to read Musicgen Melody docstrings and "
253	                "to correct `audio` to get the right behaviour."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/musicgen_melody/feature_extraction_musicgen_melody.py:283
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
282	            logger.warning_once(
283	                "`audio` has been detected as a batch of stereo signals. Will be convert to mono signals. "
284	                "If this is an undesired behaviour, make sure to read Musicgen Melody docstrings and "
285	                "to correct `audio` to get the right behaviour."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/musicgen_melody/modeling_musicgen_melody.py:687
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
686	
687	MUSICGEN_MELODY_START_DOCSTRING = r"""
688	
689	    The Musicgen Melody model was proposed in [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284) by
690	    Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Dfossez. It is a
691	    decoder-only transformer trained on the task of conditional music generation.
692	
693	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
694	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
695	    etc.)
696	
697	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
698	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
699	    and behavior.
700	
701	    Parameters:
702	        config ([`MusicgenMelodyConfig`]): Model configuration class with all the parameters of the model.
703	            Initializing with a config file does not load the weights associated with the model, only the
704	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
705	"""
706	
707	MUSICGEN_MELODY_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mvp/modeling_mvp.py:528
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
527	
528	MVP_START_DOCSTRING = r"""
529	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
530	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
531	    etc.)
532	
533	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
534	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
535	    and behavior.
536	
537	    Parameters:
538	        config ([`MvpConfig`]):
539	            Model configuration class with all the parameters of the model. Initializing with a config file does not
540	            load the weights associated with the model, only the configuration. Check out the
541	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
542	"""
543	
544	MVP_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mvp/modeling_mvp.py:544
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
543	
544	MVP_INPUTS_DOCSTRING = r"""
545	    Args:
546	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
547	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
548	            it.
549	
550	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
551	            [`PreTrainedTokenizer.__call__`] for details.
552	
553	            [What are input IDs?](../glossary#input-ids)
554	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
555	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
556	
557	            - 1 for tokens that are **not masked**,
558	            - 0 for tokens that are **masked**.
559	
560	            [What are attention masks?](../glossary#attention-mask)
561	        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
562	            Indices of decoder input sequence tokens in the vocabulary.
563	
564	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
565	            [`PreTrainedTokenizer.__call__`] for details.
566	
567	            [What are decoder input IDs?](../glossary#decoder-input-ids)
568	
569	            Mvp uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`
570	            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).
571	
572	            For translation and summarization training, `decoder_input_ids` should be provided. If no
573	            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
574	            for denoising pre-training following the paper.
575	        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
576	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
577	            be used by default.
578	
579	            If you want to change padding behavior, you should read [`modeling_mvp._prepare_decoder_attention_mask`]
580	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
581	            information on the default strategy.
582	        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
583	            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:
584	
585	            - 1 indicates the head is **not masked**,
586	            - 0 indicates the head is **masked**.
587	
588	        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
589	            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:
590	
591	            - 1 indicates the head is **not masked**,
592	            - 0 indicates the head is **masked**.
593	
594	        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
595	            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,
596	            1]`:
597	
598	            - 1 indicates the head is **not masked**,
599	            - 0 indicates the head is **masked**.
600	
601	        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):
602	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
603	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
604	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
605	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
606	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
607	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
608	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
609	
610	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
611	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
612	
613	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
614	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
615	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
616	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
617	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.
618	            This is useful if you want more control over how to convert `input_ids` indices into associated vectors
619	            than the model's internal embedding lookup matrix.
620	        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):
621	            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded
622	            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be
623	            input (see `past_key_values`). This is useful if you want more control over how to convert
624	            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.
625	
626	            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value
627	            of `inputs_embeds`.
628	        use_cache (`bool`, *optional*):
629	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
630	            `past_key_values`).
631	        output_attentions (`bool`, *optional*):
632	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
633	            tensors for more detail.
634	        output_hidden_states (`bool`, *optional*):
635	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
636	            more detail.
637	        return_dict (`bool`, *optional*):
638	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
639	"""
640	
641	MVP_CONDITIONAL_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mvp/tokenization_mvp.py:186
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
185	        with open(merges_file, encoding="utf-8") as merges_handle:
186	            bpe_merges = merges_handle.read().split("\n")[1:-1]
187	        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mvp/tokenization_mvp.py:295
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
294	        with open(vocab_file, "w", encoding="utf-8") as f:
295	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
296	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mvp/tokenization_mvp.py:299
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
298	        with open(merge_file, "w", encoding="utf-8") as writer:
299	            writer.write("#version: 0.2\n")
300	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/mvp/tokenization_mvp.py:307
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
306	                    index = token_index
307	                writer.write(" ".join(bpe_tokens) + "\n")
308	                index += 1

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/myt5/tokenization_myt5.py:376
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
375	        with open(vocab_file, "w", encoding="utf-8") as writer:
376	            writer.write(json.dumps(self.byte_maps, indent=2, ensure_ascii=False))
377	        return (vocab_file,)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/nemotron/modeling_nemotron.py:316
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
315	            raise ValueError(
316	                "`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` "
317	                "make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers"
318	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/nemotron/modeling_nemotron.py:583
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
582	
583	NEMOTRON_START_DOCSTRING = r"""
584	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
585	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
586	    etc.)
587	
588	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
589	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
590	    and behavior.
591	
592	    Parameters:
593	        config ([`NemotronConfig`]):
594	            Model configuration class with all the parameters of the model. Initializing with a config file does not
595	            load the weights associated with the model, only the configuration. Check out the
596	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
597	"""
598	
599	
600	@add_start_docstrings(
601	    "The bare Nemotron Model outputting raw hidden-states without any specific head on top.",
602	    NEMOTRON_START_DOCSTRING,
603	)
604	class NemotronPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/nemotron/modeling_nemotron.py:628
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
627	
628	NEMOTRON_INPUTS_DOCSTRING = r"""
629	    Args:
630	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
631	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
632	            it.
633	
634	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
635	            [`PreTrainedTokenizer.__call__`] for details.
636	
637	            [What are input IDs?](../glossary#input-ids)
638	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
639	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
640	
641	            - 1 for tokens that are **not masked**,
642	            - 0 for tokens that are **masked**.
643	
644	            [What are attention masks?](../glossary#attention-mask)
645	
646	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
647	            [`PreTrainedTokenizer.__call__`] for details.
648	
649	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
650	            `past_key_values`).
651	
652	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
653	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
654	            information on the default strategy.
655	
656	            - 1 indicates the head is **not masked**,
657	            - 0 indicates the head is **masked**.
658	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
659	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
660	            config.n_positions - 1]`.
661	
662	            [What are position IDs?](../glossary#position-ids)
663	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
664	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
665	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
666	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
667	
668	            Two formats are allowed:
669	            - a [`~cache_utils.Cache`] instance, see our
670	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
671	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
672	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
673	            cache format.
674	
675	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
676	            legacy cache format will be returned.
677	
678	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
679	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
680	            of shape `(batch_size, sequence_length)`.
681	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
682	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
683	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
684	            model's internal embedding lookup matrix.
685	        use_cache (`bool`, *optional*):
686	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
687	            `past_key_values`).
688	        output_attentions (`bool`, *optional*):
689	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
690	            tensors for more detail.
691	        output_hidden_states (`bool`, *optional*):
692	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
693	            more detail.
694	        return_dict (`bool`, *optional*):
695	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
696	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
697	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
698	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
699	            the complete sequence length.
700	"""
701	
702	
703	@add_start_docstrings(
704	    "The bare Nemotron Model outputting raw hidden-states without any specific head on top.",
705	    NEMOTRON_START_DOCSTRING,
706	)
707	class NemotronModel(NemotronPreTrainedModel):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/nllb/tokenization_nllb.py:343
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
342	                content_spiece_model = self.sp_model.serialized_model_proto()
343	                fi.write(content_spiece_model)
344	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py:863
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
862	
863	NLLB_MOE_START_DOCSTRING = r"""
864	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
865	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
866	    etc.)
867	
868	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
869	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
870	    and behavior.
871	
872	    Parameters:
873	        config ([`NllbMoeConfig`]):
874	            Model configuration class with all the parameters of the model. Initializing with a config file does not
875	            load the weights associated with the model, only the configuration. Check out the
876	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
877	"""
878	
879	NLLB_MOE_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B839:pool] multiprocessing.Pool
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/nougat/tokenization_nougat_fast.py:612
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b839_pool.html
611	            if num_workers is not None and isinstance(num_workers, int):
612	                with Pool(num_workers) as p:
613	                    return p.map(partial(self.post_process_single, fix_markdown=fix_markdown), generation)

--------------------------------------------------
>> Issue: [B323:blacklist] multiprocessing_Pool
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/nougat/tokenization_nougat_fast.py:612
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b323-multiprocessing-pool
611	            if num_workers is not None and isinstance(num_workers, int):
612	                with Pool(num_workers) as p:
613	                    return p.map(partial(self.post_process_single, fix_markdown=fix_markdown), generation)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/nystromformer/modeling_nystromformer.py:472
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
471	
472	NYSTROMFORMER_START_DOCSTRING = r"""
473	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
474	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
475	    behavior.
476	
477	    Parameters:
478	        config ([`NystromformerConfig`]): Model configuration class with all the parameters of the model.
479	            Initializing with a config file does not load the weights associated with the model, only the
480	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
481	"""
482	
483	NYSTROMFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/olmo/modeling_olmo.py:320
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
319	
320	OLMO_START_DOCSTRING = r"""
321	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
322	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
323	    etc.)
324	
325	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
326	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
327	    and behavior.
328	
329	    Parameters:
330	        config ([`OlmoConfig`]):
331	            Model configuration class with all the parameters of the model. Initializing with a config file does not
332	            load the weights associated with the model, only the configuration. Check out the
333	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
334	"""
335	
336	
337	@add_start_docstrings(
338	    "The bare Olmo Model outputting raw hidden-states without any specific head on top.",
339	    OLMO_START_DOCSTRING,
340	)
341	class OlmoPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/olmo/modeling_olmo.py:367
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
366	
367	OLMO_INPUTS_DOCSTRING = r"""
368	    Args:
369	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
370	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
371	            it.
372	
373	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
374	            [`PreTrainedTokenizer.__call__`] for details.
375	
376	            [What are input IDs?](../glossary#input-ids)
377	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
378	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
379	
380	            - 1 for tokens that are **not masked**,
381	            - 0 for tokens that are **masked**.
382	
383	            [What are attention masks?](../glossary#attention-mask)
384	
385	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
386	            [`PreTrainedTokenizer.__call__`] for details.
387	
388	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
389	            `past_key_values`).
390	
391	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
392	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
393	            information on the default strategy.
394	
395	            - 1 indicates the head is **not masked**,
396	            - 0 indicates the head is **masked**.
397	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
398	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
399	            config.n_positions - 1]`.
400	
401	            [What are position IDs?](../glossary#position-ids)
402	        past_key_values (`Cache`, *optional*):
403	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
404	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
405	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
406	
407	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
408	
409	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
410	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
411	            of shape `(batch_size, sequence_length)`.
412	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
413	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
414	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
415	            model's internal embedding lookup matrix.
416	        use_cache (`bool`, *optional*):
417	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
418	            `past_key_values`).
419	        output_attentions (`bool`, *optional*):
420	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
421	            tensors for more detail.
422	        output_hidden_states (`bool`, *optional*):
423	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
424	            more detail.
425	        return_dict (`bool`, *optional*):
426	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
427	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
428	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
429	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
430	            the complete sequence length.
431	"""
432	
433	
434	@add_start_docstrings(
435	    "The bare Olmo Model outputting raw hidden-states without any specific head on top.",
436	    OLMO_START_DOCSTRING,
437	)
438	class OlmoModel(OlmoPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/olmo2/modeling_olmo2.py:321
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
320	
321	OLMO2_START_DOCSTRING = r"""
322	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
323	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
324	    etc.)
325	
326	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
327	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
328	    and behavior.
329	
330	    Parameters:
331	        config ([`Olmo2Config`]):
332	            Model configuration class with all the parameters of the model. Initializing with a config file does not
333	            load the weights associated with the model, only the configuration. Check out the
334	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
335	"""
336	
337	
338	@add_start_docstrings(
339	    "The bare Olmo2 Model outputting raw hidden-states without any specific head on top.",
340	    OLMO2_START_DOCSTRING,
341	)
342	class Olmo2PreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/olmo2/modeling_olmo2.py:368
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
367	
368	OLMO2_INPUTS_DOCSTRING = r"""
369	    Args:
370	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
371	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
372	            it.
373	
374	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
375	            [`PreTrainedTokenizer.__call__`] for details.
376	
377	            [What are input IDs?](../glossary#input-ids)
378	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
379	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
380	
381	            - 1 for tokens that are **not masked**,
382	            - 0 for tokens that are **masked**.
383	
384	            [What are attention masks?](../glossary#attention-mask)
385	
386	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
387	            [`PreTrainedTokenizer.__call__`] for details.
388	
389	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
390	            `past_key_values`).
391	
392	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
393	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
394	            information on the default strategy.
395	
396	            - 1 indicates the head is **not masked**,
397	            - 0 indicates the head is **masked**.
398	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
399	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
400	            config.n_positions - 1]`.
401	
402	            [What are position IDs?](../glossary#position-ids)
403	        past_key_values (`Cache`, *optional*):
404	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
405	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
406	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
407	
408	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
409	
410	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
411	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
412	            of shape `(batch_size, sequence_length)`.
413	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
414	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
415	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
416	            model's internal embedding lookup matrix.
417	        use_cache (`bool`, *optional*):
418	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
419	            `past_key_values`).
420	        output_attentions (`bool`, *optional*):
421	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
422	            tensors for more detail.
423	        output_hidden_states (`bool`, *optional*):
424	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
425	            more detail.
426	        return_dict (`bool`, *optional*):
427	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
428	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
429	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
430	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
431	            the complete sequence length.
432	"""
433	
434	
435	@add_start_docstrings(
436	    "The bare Olmo2 Model outputting raw hidden-states without any specific head on top.",
437	    OLMO2_START_DOCSTRING,
438	)
439	class Olmo2Model(Olmo2PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/olmoe/modeling_olmoe.py:711
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
710	
711	OLMOE_START_DOCSTRING = r"""
712	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
713	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
714	    etc.)
715	
716	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
717	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
718	    and behavior.
719	
720	    Parameters:
721	        config ([`OlmoeConfig`]):
722	            Model configuration class with all the parameters of the model. Initializing with a config file does not
723	            load the weights associated with the model, only the configuration. Check out the
724	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
725	"""
726	
727	
728	@add_start_docstrings(
729	    "The bare Olmoe Model outputting raw hidden-states without any specific head on top.",
730	    OLMOE_START_DOCSTRING,
731	)
732	class OlmoePreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/olmoe/modeling_olmoe.py:756
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
755	
756	OLMOE_INPUTS_DOCSTRING = r"""
757	    Args:
758	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
759	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
760	            it.
761	
762	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
763	            [`PreTrainedTokenizer.__call__`] for details.
764	
765	            [What are input IDs?](../glossary#input-ids)
766	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
767	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
768	
769	            - 1 for tokens that are **not masked**,
770	            - 0 for tokens that are **masked**.
771	
772	            [What are attention masks?](../glossary#attention-mask)
773	
774	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
775	            [`PreTrainedTokenizer.__call__`] for details.
776	
777	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
778	            `past_key_values`).
779	
780	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
781	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
782	            information on the default strategy.
783	
784	            - 1 indicates the head is **not masked**,
785	            - 0 indicates the head is **masked**.
786	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
787	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
788	            config.n_positions - 1]`.
789	
790	            [What are position IDs?](../glossary#position-ids)
791	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
792	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
793	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
794	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
795	
796	            Two formats are allowed:
797	            - a [`~cache_utils.Cache`] instance;
798	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
799	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
800	            cache format.
801	
802	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
803	            legacy cache format will be returned.
804	
805	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
806	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
807	            of shape `(batch_size, sequence_length)`.
808	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
809	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
810	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
811	            model's internal embedding lookup matrix.
812	        use_cache (`bool`, *optional*):
813	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
814	            `past_key_values`).
815	        output_attentions (`bool`, *optional*):
816	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
817	            tensors for more detail.
818	        output_hidden_states (`bool`, *optional*):
819	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
820	            more detail.
821	        output_router_logits (`bool`, *optional*):
822	            Whether or not to return the logits of all the routers. They are useful for computing the router loss, and
823	            should not be returned during inference.
824	        return_dict (`bool`, *optional*):
825	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
826	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
827	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
828	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
829	            the complete sequence length.
830	"""
831	
832	
833	@add_start_docstrings(
834	    "The bare Olmoe Model outputting raw hidden-states without any specific head on top.",
835	    OLMOE_START_DOCSTRING,
836	)
837	# TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaModel with Llama->Olmoe
838	class OlmoeModel(OlmoePreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/olmoe/modeling_olmoe.py:915
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
914	                logger.warning_once(
915	                    "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and "
916	                    "will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class "
917	                    "(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/omdet_turbo/modeling_omdet_turbo.py:1133
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1132	
1133	OMDET_TURBO_START_DOCSTRING = r"""
1134	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1135	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1136	    etc.)
1137	
1138	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1139	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1140	    and behavior.
1141	
1142	    Parameters:
1143	        config ([`OmDetTurboConfig`]):
1144	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1145	            load the weights associated with the model, only the configuration. Check out the
1146	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1147	"""
1148	
1149	OMDET_TURBO_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/oneformer/modeling_oneformer.py:2756
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2755	
2756	ONEFORMER_START_DOCSTRING = r"""
2757	    This model is a PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use it as a
2758	    regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.
2759	
2760	    Parameters:
2761	        config ([`OneFormerConfig`]): Model configuration class with all the parameters of the model.
2762	            Initializing with a config file does not load the weights associated with the model, only the
2763	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
2764	"""
2765	
2766	ONEFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/openai/modeling_openai.py:327
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
326	
327	OPENAI_GPT_START_DOCSTRING = r"""
328	
329	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
330	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
331	    etc.)
332	
333	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
334	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
335	    and behavior.
336	
337	    Parameters:
338	        config ([`OpenAIGPTConfig`]): Model configuration class with all the parameters of the model.
339	            Initializing with a config file does not load the weights associated with the model, only the
340	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
341	"""
342	
343	OPENAI_GPT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/openai/modeling_tf_openai.py:438
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
437	
438	OPENAI_GPT_START_DOCSTRING = r"""
439	
440	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
441	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
442	    etc.)
443	
444	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
445	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
446	    behavior.
447	
448	    <Tip>
449	
450	    TensorFlow models and layers in `transformers` accept two formats as input:
451	
452	    - having all inputs as keyword arguments (like PyTorch models), or
453	    - having all inputs as a list, tuple or dict in the first positional argument.
454	
455	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
456	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
457	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
458	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
459	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
460	    positional argument:
461	
462	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
463	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
464	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
465	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
466	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
467	
468	    Note that when creating models and layers with
469	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
470	    about any of this, as you can just pass inputs like you would to any other Python function!
471	
472	    </Tip>
473	
474	    Parameters:
475	        config ([`OpenAIGPTConfig`]): Model configuration class with all the parameters of the model.
476	            Initializing with a config file does not load the weights associated with the model, only the
477	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
478	"""
479	
480	OPENAI_GPT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/openai/tokenization_openai.py:276
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
275	        with open(merges_file, encoding="utf-8") as merges_handle:
276	            merges = merges_handle.read().split("\n")[1:-1]
277	        merges = [tuple(merge.split()) for merge in merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/openai/tokenization_openai.py:378
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
377	        with open(vocab_file, "w", encoding="utf-8") as f:
378	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
379	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/openai/tokenization_openai.py:382
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
381	        with open(merge_file, "w", encoding="utf-8") as writer:
382	            writer.write("#version: 0.2\n")
383	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/openai/tokenization_openai.py:390
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
389	                    index = token_index
390	                writer.write(" ".join(bpe_tokens) + "\n")
391	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/opt/modeling_flax_opt.py:42
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
41	
42	OPT_START_DOCSTRING = r"""
43	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
44	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
45	    etc.)
46	
47	    This model is also a Flax Linen
48	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
49	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
50	
51	    Finally, this model supports inherent JAX features such as:
52	
53	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
54	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
55	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
56	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
57	
58	    Parameters:
59	        config ([`OPTConfig`]): Model configuration class with all the parameters of the model.
60	            Initializing with a config file does not load the weights associated with the model, only the
61	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
62	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
63	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
64	            `jax.numpy.bfloat16` (on TPUs).
65	
66	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
67	            specified all the computation will be performed with the given `dtype`.
68	
69	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
70	            parameters.**
71	
72	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
73	            [`~FlaxPreTrainedModel.to_bf16`].
74	"""
75	
76	OPT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/opt/modeling_opt.py:472
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
471	
472	OPT_START_DOCSTRING = r"""
473	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
474	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
475	    etc.)
476	
477	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
478	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
479	    and behavior.
480	
481	    Parameters:
482	        config ([`OPTConfig`]):
483	            Model configuration class with all the parameters of the model. Initializing with a config file does not
484	            load the weights associated with the model, only the configuration. Check out the
485	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
486	"""
487	
488	
489	@add_start_docstrings(
490	    "The bare OPT Model outputting raw hidden-states without any specific head on top.",
491	    OPT_START_DOCSTRING,
492	)
493	class OPTPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/opt/modeling_opt.py:516
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
515	
516	OPT_INPUTS_DOCSTRING = r"""
517	    Args:
518	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
519	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
520	            it.
521	
522	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
523	            [`PreTrainedTokenizer.__call__`] for details.
524	
525	            [What are input IDs?](../glossary#input-ids)
526	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
527	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
528	
529	            - 1 for tokens that are **not masked**,
530	            - 0 for tokens that are **masked**.
531	
532	            [What are attention masks?](../glossary#attention-mask)
533	
534	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
535	            [`PreTrainedTokenizer.__call__`] for details.
536	
537	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
538	            `past_key_values`).
539	
540	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
541	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
542	            information on the default strategy.
543	        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
544	            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:
545	
546	            - 1 indicates the head is **not masked**,
547	            - 0 indicates the head is **masked**.
548	
549	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
550	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
551	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
552	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
553	
554	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
555	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
556	
557	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
558	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
559	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
560	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
561	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
562	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
563	            model's internal embedding lookup matrix.
564	        use_cache (`bool`, *optional*):
565	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
566	            `past_key_values`).
567	        output_attentions (`bool`, *optional*):
568	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
569	            tensors for more detail.
570	        output_hidden_states (`bool`, *optional*):
571	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
572	            more detail.
573	        return_dict (`bool`, *optional*):
574	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
575	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
576	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
577	            config.n_positions - 1]`. for padding use -1.
578	
579	            [What are position IDs?](../glossary#position-ids)
580	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
581	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
582	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
583	            the complete sequence length.
584	"""
585	
586	
587	class OPTDecoder(OPTPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/opt/modeling_tf_opt.py:396
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
395	
396	OPT_START_DOCSTRING = r"""
397	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
398	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
399	    etc.)
400	
401	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
402	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
403	    behavior.
404	
405	    <Tip>
406	
407	    TensorFlow models and layers in `transformers` accept two formats as input:
408	
409	    - having all inputs as keyword arguments (like PyTorch models), or
410	    - having all inputs as a list, tuple or dict in the first positional argument.
411	
412	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
413	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
414	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
415	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
416	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
417	    positional argument:
418	
419	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
420	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
421	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
422	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
423	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
424	
425	    Note that when creating models and layers with
426	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
427	    about any of this, as you can just pass inputs like you would to any other Python function!
428	
429	    </Tip>
430	
431	    Args:
432	        config ([`OPTConfig`]): Model configuration class with all the parameters of the model.
433	            Initializing with a config file does not load the weights associated with the model, only the
434	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
435	"""
436	
437	
438	@add_start_docstrings(
439	    "The bare OPT Model outputting raw hidden-states without any specific head on top.",
440	    OPT_START_DOCSTRING,
441	)
442	class TFOPTPreTrainedModel(TFPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/owlv2/modeling_owlv2.py:612
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
611	
612	OWLV2_START_DOCSTRING = r"""
613	
614	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
615	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
616	    etc.)
617	
618	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
619	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
620	    and behavior.
621	
622	    Parameters:
623	        config ([`Owvl2Config`]): Model configuration class with all the parameters of the model.
624	            Initializing with a config file does not load the weights associated with the model, only the
625	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
626	"""
627	
628	OWLV2_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/owlvit/modeling_owlvit.py:599
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
598	
599	OWLVIT_START_DOCSTRING = r"""
600	
601	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
602	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
603	    etc.)
604	
605	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
606	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
607	    and behavior.
608	
609	    Parameters:
610	        config ([`OwlViTConfig`]): Model configuration class with all the parameters of the model.
611	            Initializing with a config file does not load the weights associated with the model, only the
612	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
613	"""
614	
615	OWLVIT_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/paligemma/modeling_paligemma.py:166
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
165	
166	PALIGEMMA_START_DOCSTRING = r"""
167	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
168	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
169	    etc.)
170	
171	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
172	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
173	    and behavior.
174	
175	    Parameters:
176	        config ([`PaliGemmaConfig`] or [`PaliGemmaVisionConfig`]):
177	            Model configuration class with all the parameters of the model. Initializing with a config file does not
178	            load the weights associated with the model, only the configuration. Check out the
179	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
180	"""
181	
182	
183	@add_start_docstrings(
184	    "The bare LLaMA Model outputting raw hidden-states without any specific head on top.",
185	    PALIGEMMA_START_DOCSTRING,
186	)
187	class PaliGemmaPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/paligemma/modeling_paligemma.py:221
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
220	
221	PALIGEMMA_INPUTS_DOCSTRING = r"""
222	    Args:
223	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
224	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
225	            it.
226	
227	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
228	            [`PreTrainedTokenizer.__call__`] for details.
229	
230	            [What are input IDs?](../glossary#input-ids)
231	        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):
232	            The tensors corresponding to the input images. Pixel values can be obtained using
233	            [`AutoImageProcessor`]. See [`SiglipImageProcessor.__call__`] for details ([]`PaliGemmaProcessor`] uses
234	            [`SiglipImageProcessor`] for processing images).
235	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
236	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
237	
238	            - 1 for tokens that are **not masked**,
239	            - 0 for tokens that are **masked**.
240	
241	            [What are attention masks?](../glossary#attention-mask)
242	
243	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
244	            [`PreTrainedTokenizer.__call__`] for details.
245	
246	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
247	            `past_key_values`).
248	
249	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
250	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
251	            information on the default strategy.
252	
253	            - 1 indicates the head is **not masked**,
254	            - 0 indicates the head is **masked**.
255	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
256	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
257	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
258	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
259	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
260	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
261	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
262	
263	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
264	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
265	
266	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
267	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
268	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
269	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
270	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
271	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
272	            model's internal embedding lookup matrix.
273	        use_cache (`bool`, *optional*):
274	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
275	            `past_key_values`).
276	        output_attentions (`bool`, *optional*):
277	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
278	            tensors for more detail.
279	        output_hidden_states (`bool`, *optional*):
280	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
281	            more detail.
282	        return_dict (`bool`, *optional*):
283	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
284	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
285	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
286	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
287	            the complete sequence length.
288	"""
289	
290	
291	@add_start_docstrings(
292	    """The PALIGEMMA model which consists of a vision backbone and a language model.""",
293	    PALIGEMMA_START_DOCSTRING,
294	)
295	class PaliGemmaForConditionalGeneration(PaliGemmaPreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py:42
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
41	
42	PATCHTSMIXER_START_DOCSTRING = r"""
43	
44	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
45	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
46	    etc.)
47	
48	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
49	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
50	    and behavior.
51	
52	    Parameters:
53	        config ([`PatchTSMixerConfig`]):
54	            Model configuration class with all the parameters of the model. Initializing with a config file does not
55	            load the weights associated with the model, only the configuration. Check out the
56	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
57	        mask_input (`bool`, *optional*, defaults to `False`):
58	            If True, Masking will be enabled. False otherwise.
59	"""
60	
61	PATCHTSMIXER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/patchtst/modeling_patchtst.py:777
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
776	
777	PATCHTST_START_DOCSTRING = r"""
778	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
779	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
780	    etc.)
781	
782	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
783	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
784	    and behavior.
785	
786	    Parameters:
787	        config ([`PatchTSTConfig`]):
788	            Model configuration class with all the parameters of the model. Initializing with a config file does not
789	            load the weights associated with the model, only the configuration. Check out the
790	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
791	"""
792	
793	
794	@dataclass
795	class PatchTSTModelOutput(ModelOutput):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/pegasus/modeling_flax_pegasus.py:57
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
56	
57	PEGASUS_START_DOCSTRING = r"""
58	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
59	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
60	    etc.)
61	
62	    This model is also a Flax Linen
63	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
64	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
65	
66	    Finally, this model supports inherent JAX features such as:
67	
68	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
69	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
70	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
71	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
72	
73	    Parameters:
74	        config ([`PegasusConfig`]): Model configuration class with all the parameters of the model.
75	            Initializing with a config file does not load the weights associated with the model, only the
76	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
77	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
78	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
79	            `jax.numpy.bfloat16` (on TPUs).
80	
81	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
82	            specified all the computation will be performed with the given `dtype`.
83	
84	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
85	            parameters.**
86	
87	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
88	            [`~FlaxPreTrainedModel.to_bf16`].
89	"""
90	
91	PEGASUS_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/pegasus/modeling_flax_pegasus.py:91
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
90	
91	PEGASUS_INPUTS_DOCSTRING = r"""
92	    Args:
93	        input_ids (`jnp.ndarray` of shape `(batch_size, sequence_length)`):
94	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
95	            it.
96	
97	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
98	            [`PreTrainedTokenizer.__call__`] for details.
99	
100	            [What are input IDs?](../glossary#input-ids)
101	        attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
102	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
103	
104	            - 1 for tokens that are **not masked**,
105	            - 0 for tokens that are **masked**.
106	
107	            [What are attention masks?](../glossary#attention-mask)
108	        decoder_input_ids (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
109	            Indices of decoder input sequence tokens in the vocabulary.
110	
111	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
112	            [`PreTrainedTokenizer.__call__`] for details.
113	
114	            [What are decoder input IDs?](../glossary#decoder-input-ids)
115	        decoder_attention_mask (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
116	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
117	            be used by default.
118	
119	            If you want to change padding behavior, you should modify to your needs. See diagram 1 in [the
120	            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
121	        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
122	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
123	            config.max_position_embeddings - 1]`.
124	        decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
125	            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
126	            range `[0, config.max_position_embeddings - 1]`.
127	        output_attentions (`bool`, *optional*):
128	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
129	            tensors for more detail.
130	        output_hidden_states (`bool`, *optional*):
131	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
132	            more detail.
133	        return_dict (`bool`, *optional*):
134	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
135	"""
136	
137	
138	PEGASUS_ENCODE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/pegasus/modeling_flax_pegasus.py:168
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
167	
168	PEGASUS_DECODE_INPUTS_DOCSTRING = r"""
169	    Args:
170	        decoder_input_ids (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`):
171	            Indices of decoder input sequence tokens in the vocabulary.
172	
173	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
174	            [`PreTrainedTokenizer.__call__`] for details.
175	
176	            [What are decoder input IDs?](../glossary#decoder-input-ids)
177	        encoder_outputs (`tuple(tuple(jnp.ndarray)`):
178	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
179	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
180	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
181	        encoder_attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
182	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
183	
184	            - 1 for tokens that are **not masked**,
185	            - 0 for tokens that are **masked**.
186	
187	            [What are attention masks?](../glossary#attention-mask)
188	        decoder_attention_mask (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
189	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
190	            be used by default.
191	
192	            If you want to change padding behavior, you should modify to your needs. See diagram 1 in [the
193	            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
194	        decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
195	            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
196	            range `[0, config.max_position_embeddings - 1]`.
197	        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):
198	            Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
199	            auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.
200	        output_attentions (`bool`, *optional*):
201	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
202	            tensors for more detail.
203	        output_hidden_states (`bool`, *optional*):
204	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
205	            more detail.
206	        return_dict (`bool`, *optional*):
207	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
208	"""
209	
210	
211	# Copied from transformers.models.bart.modeling_flax_bart.shift_tokens_right
212	def shift_tokens_right(input_ids: jnp.ndarray, pad_token_id: int, decoder_start_token_id: int) -> jnp.ndarray:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/pegasus/modeling_pegasus.py:474
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
473	
474	PEGASUS_START_DOCSTRING = r"""
475	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
476	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
477	    etc.)
478	
479	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
480	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
481	    and behavior.
482	
483	    Parameters:
484	        config ([`PegasusConfig`]):
485	            Model configuration class with all the parameters of the model. Initializing with a config file does not
486	            load the weights associated with the model, only the configuration. Check out the
487	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
488	"""
489	
490	PEGASUS_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/pegasus/modeling_tf_pegasus.py:570
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
569	
570	PEGASUS_START_DOCSTRING = r"""
571	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
572	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
573	    etc.)
574	
575	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
576	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
577	    behavior.
578	
579	    <Tip>
580	
581	    TensorFlow models and layers in `transformers` accept two formats as input:
582	
583	    - having all inputs as keyword arguments (like PyTorch models), or
584	    - having all inputs as a list, tuple or dict in the first positional argument.
585	
586	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
587	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
588	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
589	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
590	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
591	    positional argument:
592	
593	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
594	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
595	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
596	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
597	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
598	
599	    Note that when creating models and layers with
600	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
601	    about any of this, as you can just pass inputs like you would to any other Python function!
602	
603	    </Tip>
604	
605	    Args:
606	        config ([`PegasusConfig`]): Model configuration class with all the parameters of the model.
607	            Initializing with a config file does not load the weights associated with the model, only the
608	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
609	"""
610	
611	PEGASUS_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/pegasus/tokenization_pegasus.py:283
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
282	                content_spiece_model = self.sp_model.serialized_model_proto()
283	                fi.write(content_spiece_model)
284	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/pegasus_x/modeling_pegasus_x.py:764
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
763	
764	PEGASUS_X_START_DOCSTRING = r"""
765	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
766	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
767	    etc.)
768	
769	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
770	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
771	    and behavior.
772	
773	    Parameters:
774	        config ([`PegasusXConfig`]):
775	            Model configuration class with all the parameters of the model. Initializing with a config file does not
776	            load the weights associated with the model, only the configuration. Check out the
777	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
778	"""
779	
780	PEGASUS_X_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/perceiver/modeling_perceiver.py:638
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
637	
638	PERCEIVER_START_DOCSTRING = r"""
639	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
640	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
641	    behavior.
642	
643	    Parameters:
644	        config ([`PerceiverConfig`]): Model configuration class with all the parameters of the model.
645	            Initializing with a config file does not load the weights associated with the model, only the
646	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
647	"""
648	
649	PERCEIVER_MODEL_START_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/perceiver/modeling_perceiver.py:649
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
648	
649	PERCEIVER_MODEL_START_DOCSTRING = r"""
650	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
651	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
652	    behavior.
653	
654	    Parameters:
655	        config ([`PerceiverConfig`]): Model configuration class with all the parameters of the model.
656	            Initializing with a config file does not load the weights associated with the model, only the
657	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
658	        decoder (*DecoderType*, *optional*):
659	            Optional decoder to use to decode the latent representation of the encoder. Examples include
660	            *transformers.models.perceiver.modeling_perceiver.PerceiverBasicDecoder*,
661	            *transformers.models.perceiver.modeling_perceiver.PerceiverClassificationDecoder*,
662	            *transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalDecoder*.
663	        input_preprocessor (*PreprocessorType*, *optional*):
664	            Optional input preprocessor to use. Examples include
665	            *transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor*,
666	            *transformers.models.perceiver.modeling_perceiver.PerceiverAudioPreprocessor*,
667	            *transformers.models.perceiver.modeling_perceiver.PerceiverTextPreprocessor*,
668	            *transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPreprocessor*.
669	        output_postprocessor (*PostprocessorType*, *optional*):
670	            Optional output postprocessor to use. Examples include
671	            *transformers.models.perceiver.modeling_perceiver.PerceiverImagePostprocessor*,
672	            *transformers.models.perceiver.modeling_perceiver.PerceiverAudioPostprocessor*,
673	            *transformers.models.perceiver.modeling_perceiver.PerceiverClassificationPostprocessor*,
674	            *transformers.models.perceiver.modeling_perceiver.PerceiverProjectionPostprocessor*,
675	            *transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPostprocessor*.
676	
677	        Note that you can define your own decoders, preprocessors and/or postprocessors to fit your use-case.
678	"""
679	
680	PERCEIVER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/persimmon/modeling_persimmon.py:374
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
373	
374	PERSIMMON_START_DOCSTRING = r"""
375	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
376	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
377	    etc.)
378	
379	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
380	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
381	    and behavior.
382	
383	    Parameters:
384	        config ([`PersimmonConfig`]):
385	            Model configuration class with all the parameters of the model. Initializing with a config file does not
386	            load the weights associated with the model, only the configuration. Check out the
387	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
388	"""
389	
390	
391	@add_start_docstrings(
392	    "The bare Persimmon Model outputting raw hidden-states without any specific head on top.",
393	    PERSIMMON_START_DOCSTRING,
394	)
395	class PersimmonPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/persimmon/modeling_persimmon.py:417
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
416	
417	PERSIMMON_INPUTS_DOCSTRING = r"""
418	    Args:
419	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
420	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
421	            it.
422	
423	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
424	            [`PreTrainedTokenizer.__call__`] for details.
425	
426	            [What are input IDs?](../glossary#input-ids)
427	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
428	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
429	
430	            - 1 for tokens that are **not masked**,
431	            - 0 for tokens that are **masked**.
432	
433	            [What are attention masks?](../glossary#attention-mask)
434	
435	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
436	            [`PreTrainedTokenizer.__call__`] for details.
437	
438	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
439	            `past_key_values`).
440	
441	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
442	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
443	            information on the default strategy.
444	
445	            - 1 indicates the head is **not masked**,
446	            - 0 indicates the head is **masked**.
447	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
448	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
449	            config.n_positions - 1]`.
450	
451	            [What are position IDs?](../glossary#position-ids)
452	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
453	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
454	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
455	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
456	
457	            Two formats are allowed:
458	            - a [`~cache_utils.Cache`] instance, see our
459	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
460	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
461	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
462	            cache format.
463	
464	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
465	            legacy cache format will be returned.
466	
467	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
468	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
469	            of shape `(batch_size, sequence_length)`.
470	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
471	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
472	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
473	            model's internal embedding lookup matrix.
474	        use_cache (`bool`, *optional*):
475	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
476	            `past_key_values`).
477	        output_attentions (`bool`, *optional*):
478	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
479	            tensors for more detail.
480	        output_hidden_states (`bool`, *optional*):
481	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
482	            more detail.
483	        return_dict (`bool`, *optional*):
484	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
485	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
486	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
487	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
488	            the complete sequence length.
489	"""
490	
491	
492	@add_start_docstrings(
493	    "The bare Persimmon Model outputting raw hidden-states without any specific head on top.",
494	    PERSIMMON_START_DOCSTRING,
495	)
496	class PersimmonModel(PersimmonPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/persimmon/modeling_persimmon.py:566
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
565	                logger.warning_once(
566	                    "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and "
567	                    "will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class "
568	                    "(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/phi/modeling_phi.py:316
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
315	
316	PHI_START_DOCSTRING = r"""
317	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
318	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
319	    etc.)
320	
321	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
322	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
323	    and behavior.
324	
325	    Parameters:
326	        config ([`PhiConfig`]):
327	            Model configuration class with all the parameters of the model. Initializing with a config file does not
328	            load the weights associated with the model, only the configuration. Check out the
329	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
330	"""
331	
332	
333	@add_start_docstrings(
334	    "The bare Phi Model outputting raw hidden-states without any specific head on top.",
335	    PHI_START_DOCSTRING,
336	)
337	class PhiPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/phi/modeling_phi.py:363
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
362	
363	PHI_INPUTS_DOCSTRING = r"""
364	    Args:
365	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
366	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
367	            it.
368	
369	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
370	            [`PreTrainedTokenizer.__call__`] for details.
371	
372	            [What are input IDs?](../glossary#input-ids)
373	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
374	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
375	
376	            - 1 for tokens that are **not masked**,
377	            - 0 for tokens that are **masked**.
378	
379	            [What are attention masks?](../glossary#attention-mask)
380	
381	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
382	            [`PreTrainedTokenizer.__call__`] for details.
383	
384	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
385	            `past_key_values`).
386	
387	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
388	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
389	            information on the default strategy.
390	
391	            - 1 indicates the head is **not masked**,
392	            - 0 indicates the head is **masked**.
393	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
394	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
395	            config.n_positions - 1]`.
396	
397	            [What are position IDs?](../glossary#position-ids)
398	        past_key_values (`Cache`, *optional*):
399	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
400	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
401	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
402	
403	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
404	
405	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
406	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
407	            of shape `(batch_size, sequence_length)`.
408	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
409	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
410	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
411	            model's internal embedding lookup matrix.
412	        use_cache (`bool`, *optional*):
413	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
414	            `past_key_values`).
415	        output_attentions (`bool`, *optional*):
416	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
417	            tensors for more detail.
418	        output_hidden_states (`bool`, *optional*):
419	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
420	            more detail.
421	        return_dict (`bool`, *optional*):
422	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
423	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
424	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
425	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
426	            the complete sequence length.
427	"""
428	
429	
430	@add_start_docstrings(
431	    "The bare Phi Model outputting raw hidden-states without any specific head on top.",
432	    PHI_START_DOCSTRING,
433	)
434	class PhiModel(PhiPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/phi3/modeling_phi3.py:328
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
327	
328	PHI3_START_DOCSTRING = r"""
329	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
330	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
331	    etc.)
332	
333	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
334	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
335	    and behavior.
336	
337	    Parameters:
338	        config ([`Phi3Config`]):
339	            Model configuration class with all the parameters of the model. Initializing with a config file does not
340	            load the weights associated with the model, only the configuration. Check out the
341	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
342	"""
343	
344	
345	@add_start_docstrings(
346	    "The bare Phi3 Model outputting raw hidden-states without any specific head on top.",
347	    PHI3_START_DOCSTRING,
348	)
349	class Phi3PreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/phi3/modeling_phi3.py:410
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
409	
410	PHI3_INPUTS_DOCSTRING = r"""
411	    Args:
412	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
413	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
414	            it.
415	
416	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
417	            [`PreTrainedTokenizer.__call__`] for details.
418	
419	            [What are input IDs?](../glossary#input-ids)
420	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
421	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
422	
423	            - 1 for tokens that are **not masked**,
424	            - 0 for tokens that are **masked**.
425	
426	            [What are attention masks?](../glossary#attention-mask)
427	
428	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
429	            [`PreTrainedTokenizer.__call__`] for details.
430	
431	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
432	            `past_key_values`).
433	
434	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
435	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
436	            information on the default strategy.
437	
438	            - 1 indicates the head is **not masked**,
439	            - 0 indicates the head is **masked**.
440	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
441	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
442	            config.n_positions - 1]`.
443	
444	            [What are position IDs?](../glossary#position-ids)
445	        past_key_values (`Cache`, *optional*):
446	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
447	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
448	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
449	
450	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
451	
452	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
453	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
454	            of shape `(batch_size, sequence_length)`.
455	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
456	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
457	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
458	            model's internal embedding lookup matrix.
459	        use_cache (`bool`, *optional*):
460	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
461	            `past_key_values`).
462	        output_attentions (`bool`, *optional*):
463	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
464	            tensors for more detail.
465	        output_hidden_states (`bool`, *optional*):
466	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
467	            more detail.
468	        return_dict (`bool`, *optional*):
469	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
470	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
471	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
472	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
473	            the complete sequence length.
474	"""
475	
476	
477	@add_start_docstrings(
478	    "The bare Phi3 Model outputting raw hidden-states without any specific head on top.",
479	    PHI3_START_DOCSTRING,
480	)
481	class Phi3Model(Phi3PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py:1608
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1607	
1608	PHI4_MULTIMODAL_START_DOCSTRING = r"""
1609	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1610	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1611	    etc.)
1612	
1613	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1614	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1615	    and behavior.
1616	
1617	    Parameters:
1618	        config ([`Phi4MultimodalConfig`]):
1619	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1620	            load the weights associated with the model, only the configuration. Check out the
1621	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1622	"""
1623	
1624	
1625	@add_start_docstrings(
1626	    "The bare Phi4Multimodal Model outputting raw hidden-states without any specific head on top.",
1627	    PHI4_MULTIMODAL_START_DOCSTRING,
1628	)
1629	class Phi4MultimodalPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py:1690
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1689	
1690	PHI4_MULTIMODAL_MODEL_INPUTS_DOCSTRING = r"""
1691	    Args:
1692	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
1693	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
1694	            it.
1695	
1696	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1697	            [`PreTrainedTokenizer.__call__`] for details.
1698	
1699	            [What are input IDs?](../glossary#input-ids)
1700	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
1701	            Mask to avoid performing attention on padding indices in `input_values`. Mask values selected in `[0, 1]`:
1702	            - 1 for tokens that are **not masked**,
1703	            - 0 for tokens that are **masked**.
1704	            [What are attention masks?](../glossary#attention-mask)
1705	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
1706	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
1707	            config.n_positions - 1]`.
1708	
1709	            [What are position IDs?](../glossary#position-ids)
1710	        past_key_values (`Cache`)`, *optional*):
1711	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
1712	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
1713	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
1714	            See our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
1715	
1716	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
1717	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
1718	            of shape `(batch_size, sequence_length)`.
1719	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
1720	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
1721	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
1722	            model's internal embedding lookup matrix.
1723	        image_pixel_values (`torch.FloatTensor`, *optional*):
1724	            If the input contains images, these correspond to the pixel values after transformations (as returned by
1725	            the Processor)
1726	        image_sizes (`torch.LongTensor`, *optional*):
1727	            If the input contains images, these correspond to size of each image.
1728	        image_attention_mask (`torch.LongTensor`, *optional*):
1729	            Attention mask for the images.
1730	        audio_input_features (`torch.FloatTensor`, *optional*):
1731	            If the input contains audio samples, these correspond to the values after transformation (as returned by
1732	            the Processor).
1733	        audio_embed_sizes (`torch.Tensor`, *optional*):
1734	            Size of the audio inputs.
1735	        audio_attention_mask (`torch.Tensor, *optional*):
1736	            Attention mask for the audio inputs.
1737	        use_cache (`bool`, *optional*):
1738	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
1739	            `past_key_values`).
1740	        output_attentions (`bool`, *optional*):
1741	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1742	            tensors for more detail.
1743	        output_hidden_states (`bool`, *optional*):
1744	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1745	            more detail.
1746	        return_dict (`bool`, *optional*):
1747	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1748	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
1749	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
1750	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
1751	            the complete sequence length.
1752	"""
1753	
1754	
1755	@add_start_docstrings(
1756	    "The bare Phi4Multimodal Model outputting raw hidden-states without any specific head on top.",
1757	    PHI4_MULTIMODAL_START_DOCSTRING,
1758	)
1759	class Phi4MultimodalModel(Phi4MultimodalPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py:1457
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1456	
1457	PHI4_MULTIMODAL_MODEL_INPUTS_DOCSTRING = r"""
1458	    Args:
1459	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
1460	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
1461	            it.
1462	
1463	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1464	            [`PreTrainedTokenizer.__call__`] for details.
1465	
1466	            [What are input IDs?](../glossary#input-ids)
1467	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
1468	            Mask to avoid performing attention on padding indices in `input_values`. Mask values selected in `[0, 1]`:
1469	            - 1 for tokens that are **not masked**,
1470	            - 0 for tokens that are **masked**.
1471	            [What are attention masks?](../glossary#attention-mask)
1472	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
1473	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
1474	            config.n_positions - 1]`.
1475	
1476	            [What are position IDs?](../glossary#position-ids)
1477	        past_key_values (`Cache`)`, *optional*):
1478	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
1479	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
1480	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
1481	            See our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
1482	
1483	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
1484	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
1485	            of shape `(batch_size, sequence_length)`.
1486	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
1487	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
1488	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
1489	            model's internal embedding lookup matrix.
1490	        image_pixel_values (`torch.FloatTensor`, *optional*):
1491	            If the input contains images, these correspond to the pixel values after transformations (as returned by
1492	            the Processor)
1493	        image_sizes (`torch.LongTensor`, *optional*):
1494	            If the input contains images, these correspond to size of each image.
1495	        image_attention_mask (`torch.LongTensor`, *optional*):
1496	            Attention mask for the images.
1497	        audio_input_features (`torch.FloatTensor`, *optional*):
1498	            If the input contains audio samples, these correspond to the values after transformation (as returned by
1499	            the Processor).
1500	        audio_embed_sizes (`torch.Tensor`, *optional*):
1501	            Size of the audio inputs.
1502	        audio_attention_mask (`torch.Tensor, *optional*):
1503	            Attention mask for the audio inputs.
1504	        use_cache (`bool`, *optional*):
1505	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
1506	            `past_key_values`).
1507	        output_attentions (`bool`, *optional*):
1508	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1509	            tensors for more detail.
1510	        output_hidden_states (`bool`, *optional*):
1511	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1512	            more detail.
1513	        return_dict (`bool`, *optional*):
1514	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1515	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
1516	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
1517	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
1518	            the complete sequence length.
1519	"""
1520	
1521	
1522	class Phi4MultimodalModel(Phi3Model, nn.Module):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/phimoe/modeling_phimoe.py:887
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
886	
887	PHIMOE_START_DOCSTRING = r"""
888	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
889	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
890	    etc.)
891	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
892	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
893	    and behavior.
894	    Parameters:
895	        config ([`PhimoeConfig`]):
896	            Model configuration class with all the parameters of the model. Initializing with a config file does not
897	            load the weights associated with the model, only the configuration. Check out the
898	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
899	"""
900	
901	
902	@add_start_docstrings(
903	    "The bare Phimoe Model outputting raw hidden-states without any specific head on top.",
904	    PHIMOE_START_DOCSTRING,
905	)
906	class PhimoePreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/phimoe/modeling_phimoe.py:930
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
929	
930	PHIMOE_INPUTS_DOCSTRING = r"""
931	    Args:
932	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
933	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
934	            it.
935	
936	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
937	            [`PreTrainedTokenizer.__call__`] for details.
938	
939	            [What are input IDs?](../glossary#input-ids)
940	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
941	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
942	
943	            - 1 for tokens that are **not masked**,
944	            - 0 for tokens that are **masked**.
945	
946	            [What are attention masks?](../glossary#attention-mask)
947	
948	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
949	            [`PreTrainedTokenizer.__call__`] for details.
950	
951	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
952	            `past_key_values`).
953	
954	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
955	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
956	            information on the default strategy.
957	
958	            - 1 indicates the head is **not masked**,
959	            - 0 indicates the head is **masked**.
960	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
961	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
962	            config.n_positions - 1]`.
963	
964	            [What are position IDs?](../glossary#position-ids)
965	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
966	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
967	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
968	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
969	
970	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
971	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
972	
973	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
974	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
975	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
976	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
977	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
978	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
979	            model's internal embedding lookup matrix.
980	        use_cache (`bool`, *optional*):
981	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
982	            `past_key_values`).
983	        output_attentions (`bool`, *optional*):
984	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
985	            tensors for more detail.
986	        output_hidden_states (`bool`, *optional*):
987	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
988	            more detail.
989	        output_router_logits (`bool`, *optional*):
990	            Whether or not to return the logits of all the routers. They are useful for computing the router loss, and
991	            should not be returned during inference.
992	        return_dict (`bool`, *optional*):
993	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
994	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
995	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
996	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
997	            the complete sequence length.
998	"""
999	
1000	
1001	@add_start_docstrings(
1002	    "The bare Phimoe Model outputting raw hidden-states without any specific head on top.",
1003	    PHIMOE_START_DOCSTRING,
1004	)
1005	class PhimoeModel(PhimoePreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/phimoe/modeling_phimoe.py:1080
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1079	                logger.warning_once(
1080	                    "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and "
1081	                    "will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class "
1082	                    "(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)"

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/phobert/tokenization_phobert.py:129
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
128	        with open(merges_file, encoding="utf-8") as merges_handle:
129	            merges = merges_handle.read().split("\n")[:-1]
130	        merges = [tuple(merge.split()[:-1]) for merge in merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/phobert/tokenization_phobert.py:314
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
313	                content_spiece_model = self.sp_model.serialized_model_proto()
314	                fi.write(content_spiece_model)
315	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/pix2struct/modeling_pix2struct.py:493
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
492	
493	PIX2STRUCT_VISION_START_DOCSTRING = r"""
494	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
495	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
496	    behavior.
497	
498	    Parameters:
499	        config ([`Pix2StructConfig`]): Model configuration class with all the parameters of the model.
500	            Initializing with a config file does not load the weights associated with the model, only the
501	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
502	"""
503	
504	PIX2STRUCT_VISION_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/pix2struct/modeling_pix2struct.py:504
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
503	
504	PIX2STRUCT_VISION_INPUTS_DOCSTRING = r"""
505	    Args:
506	        flattened_patches (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_channels x patch_height x patch_width)`):
507	            Flattened and padded pixel values. These values can be obtained using [`AutoImageProcessor`]. See
508	            [`Pix2StructVisionImageProcessor.__call__`] for details. Check the [original
509	            paper](https://arxiv.org/abs/2210.03347) (figure 5) for more details.
510	
511	        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):
512	            Mask to avoid performing attention on padding pixel values. Mask values selected in `[0, 1]`:
513	
514	        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):
515	            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:
516	
517	            - 1 indicates the head is **not masked**,
518	            - 0 indicates the head is **masked**.
519	
520	        output_attentions (`bool`, *optional*):
521	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
522	            tensors for more detail.
523	        output_hidden_states (`bool`, *optional*):
524	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
525	            more detail.
526	        return_dict (`bool`, *optional*):
527	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
528	"""
529	
530	
531	@add_start_docstrings(
532	    "The bare Pix2StructVision Model transformer outputting raw hidden-states without any specific head on top.",
533	    PIX2STRUCT_VISION_START_DOCSTRING,
534	)
535	class Pix2StructVisionModel(Pix2StructPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/pix2struct/modeling_pix2struct.py:1069
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1068	
1069	PIX2STRUCT_START_DOCSTRING = r"""
1070	
1071	    The Pix2Struct model was proposed in [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language
1072	    Understanding](https://arxiv.org/abs/2210.03347) by Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu,
1073	    Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova. It's an encoder decoder
1074	    transformer pre-trained in a image-to-text setting.
1075	
1076	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1077	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1078	    etc.)
1079	
1080	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1081	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1082	    and behavior.
1083	
1084	    Parameters:
1085	        config (Union[`Pix2StructConfig`, `Pix2StructTextConfig`]):
1086	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1087	            load the weights associated with the model, only the configuration. Check out the
1088	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1089	"""
1090	
1091	PIX2STRUCT_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/pixtral/modeling_pixtral.py:361
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
360	
361	PIXTRAL_START_DOCSTRING = r"""
362	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
363	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
364	    etc.)
365	
366	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
367	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
368	    and behavior.
369	
370	    Parameters:
371	        config ([`PixtralVisionConfig`]):
372	            Model configuration class with all the parameters of the vision encoder. Initializing with a config file does not
373	            load the weights associated with the model, only the configuration. Check out the
374	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
375	"""
376	
377	
378	class PixtralPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/plbart/modeling_plbart.py:516
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
515	
516	PLBART_START_DOCSTRING = r"""
517	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
518	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
519	    etc.)
520	
521	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
522	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
523	    and behavior.
524	
525	    Parameters:
526	        config ([`PLBartConfig`]):
527	            Model configuration class with all the parameters of the model. Initializing with a config file does not
528	            load the weights associated with the model, only the configuration. Check out the
529	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
530	"""
531	
532	PLBART_GENERATION_EXAMPLE = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/plbart/tokenization_plbart.py:381
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
380	                content_spiece_model = self.sp_model.serialized_model_proto()
381	                fi.write(content_spiece_model)
382	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/poolformer/modeling_poolformer.py:284
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
283	
284	POOLFORMER_START_DOCSTRING = r"""
285	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
286	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
287	    behavior.
288	
289	    Parameters:
290	        config ([`PoolFormerConfig`]): Model configuration class with all the parameters of the model.
291	            Initializing with a config file does not load the weights associated with the model, only the
292	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
293	"""
294	
295	POOLFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/pop2piano/modeling_pop2piano.py:1145
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1144	
1145	Pop2Piano_START_DOCSTRING = r"""
1146	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1147	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1148	    etc.)
1149	
1150	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1151	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1152	    and behavior.
1153	
1154	    Parameters:
1155	        config ([`Pop2PianoConfig`]): Model configuration class with all the parameters of the model.
1156	            Initializing with a config file does not load the weights associated with the model, only the
1157	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1158	"""
1159	
1160	
1161	@add_start_docstrings("""Pop2Piano Model with a `language modeling` head on top.""", Pop2Piano_START_DOCSTRING)
1162	class Pop2PianoForConditionalGeneration(Pop2PianoPreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/pop2piano/modeling_pop2piano.py:1432
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1431	            raise ValueError(
1432	                "`composer_to_feature_token` was not found! Please refer to "
1433	                "https://huggingface.co/sweetcocoa/pop2piano/blob/main/generation_config.json"
1434	                "and parse a dict like that."

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/pop2piano/tokenization_pop2piano.py:361
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
360	        with open(out_vocab_file, "w") as file:
361	            file.write(json.dumps(self.encoder))
362	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/prompt_depth_anything/modeling_prompt_depth_anything.py:387
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
386	
387	PROMPT_DEPTH_ANYTHING_START_DOCSTRING = r"""
388	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
389	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
390	    behavior.
391	
392	    Parameters:
393	        config ([`PromptDepthAnythingConfig`]): Model configuration class with all the parameters of the model.
394	            Initializing with a config file does not load the weights associated with the model, only the
395	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
396	"""
397	
398	PROMPT_DEPTH_ANYTHING_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/prompt_depth_anything/modular_prompt_depth_anything.py:167
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
166	
167	PROMPT_DEPTH_ANYTHING_START_DOCSTRING = r"""
168	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
169	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
170	    behavior.
171	
172	    Parameters:
173	        config ([`PromptDepthAnythingConfig`]): Model configuration class with all the parameters of the model.
174	            Initializing with a config file does not load the weights associated with the model, only the
175	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
176	"""
177	
178	PROMPT_DEPTH_ANYTHING_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/prophetnet/modeling_prophetnet.py:48
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
47	
48	PROPHETNET_START_DOCSTRING = r"""
49	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
50	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
51	    etc.)
52	
53	    Original ProphetNet code can be found [here](https://github.com/microsoft/ProphetNet). Checkpoints were converted
54	    from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
55	    file `convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py`.
56	
57	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
58	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
59	    behavior.
60	
61	    Parameters:
62	        config ([`ProphetNetConfig`]): Model configuration class with all the parameters of the model.
63	            Initializing with a config file does not load the weights associated with the model, only the
64	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
65	"""
66	
67	PROPHETNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/prophetnet/tokenization_prophetnet.py:478
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
477	                    index = token_index
478	                writer.write(token + "\n")
479	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/pvt/modeling_pvt.py:491
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
490	
491	PVT_START_DOCSTRING = r"""
492	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
493	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
494	    behavior.
495	
496	    Parameters:
497	        config ([`~PvtConfig`]): Model configuration class with all the parameters of the model.
498	            Initializing with a config file does not load the weights associated with the model, only the
499	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
500	"""
501	
502	PVT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/pvt_v2/modeling_pvt_v2.py:435
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
434	
435	PVT_V2_START_DOCSTRING = r"""
436	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
437	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
438	    behavior.
439	
440	    Parameters:
441	        config ([`~PvtV2Config`]): Model configuration class with all the parameters of the model.
442	            Initializing with a config file does not load the weights associated with the model, only the
443	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
444	"""
445	
446	PVT_V2_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen2/modeling_qwen2.py:322
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
321	
322	QWEN2_START_DOCSTRING = r"""
323	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
324	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
325	    etc.)
326	
327	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
328	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
329	    and behavior.
330	
331	    Parameters:
332	        config ([`Qwen2Config`]):
333	            Model configuration class with all the parameters of the model. Initializing with a config file does not
334	            load the weights associated with the model, only the configuration. Check out the
335	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
336	"""
337	
338	
339	@add_start_docstrings(
340	    "The bare Qwen2 Model outputting raw hidden-states without any specific head on top.",
341	    QWEN2_START_DOCSTRING,
342	)
343	class Qwen2PreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen2/modeling_qwen2.py:369
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
368	
369	QWEN2_INPUTS_DOCSTRING = r"""
370	    Args:
371	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
372	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
373	            it.
374	
375	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
376	            [`PreTrainedTokenizer.__call__`] for details.
377	
378	            [What are input IDs?](../glossary#input-ids)
379	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
380	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
381	
382	            - 1 for tokens that are **not masked**,
383	            - 0 for tokens that are **masked**.
384	
385	            [What are attention masks?](../glossary#attention-mask)
386	
387	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
388	            [`PreTrainedTokenizer.__call__`] for details.
389	
390	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
391	            `past_key_values`).
392	
393	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
394	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
395	            information on the default strategy.
396	
397	            - 1 indicates the head is **not masked**,
398	            - 0 indicates the head is **masked**.
399	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
400	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
401	            config.n_positions - 1]`.
402	
403	            [What are position IDs?](../glossary#position-ids)
404	        past_key_values (`Cache`, *optional*):
405	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
406	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
407	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
408	
409	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
410	
411	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
412	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
413	            of shape `(batch_size, sequence_length)`.
414	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
415	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
416	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
417	            model's internal embedding lookup matrix.
418	        use_cache (`bool`, *optional*):
419	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
420	            `past_key_values`).
421	        output_attentions (`bool`, *optional*):
422	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
423	            tensors for more detail.
424	        output_hidden_states (`bool`, *optional*):
425	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
426	            more detail.
427	        return_dict (`bool`, *optional*):
428	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
429	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
430	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
431	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
432	            the complete sequence length.
433	"""
434	
435	
436	@add_start_docstrings(
437	    "The bare Qwen2 Model outputting raw hidden-states without any specific head on top.",
438	    QWEN2_START_DOCSTRING,
439	)
440	class Qwen2Model(Qwen2PreTrainedModel):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen2/tokenization_qwen2.py:320
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
319	        with open(vocab_file, "w", encoding="utf-8") as f:
320	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
321	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen2/tokenization_qwen2.py:324
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
323	        with open(merge_file, "w", encoding="utf-8") as writer:
324	            writer.write("#version: 0.2\n")
325	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen2/tokenization_qwen2.py:332
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
331	                    index = token_index
332	                writer.write(" ".join(bpe_tokens) + "\n")
333	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:349
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
348	
349	Qwen2_5_VL_START_DOCSTRING = r"""
350	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
351	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
352	    etc.)
353	
354	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
355	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
356	    and behavior.
357	
358	    Parameters:
359	        config ([`Qwen2_5_VLConfig`]):
360	            Model configuration class with all the parameters of the model. Initializing with a config file does not
361	            load the weights associated with the model, only the configuration. Check out the
362	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
363	"""
364	
365	
366	@add_start_docstrings(
367	    "The bare Qwen2_5_VL Model outputting raw hidden-states without any specific head on top.",
368	    Qwen2_5_VL_START_DOCSTRING,
369	)
370	class Qwen2_5_VLPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1399
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1398	
1399	QWEN2_5_VL_INPUTS_DOCSTRING = r"""
1400	    Args:
1401	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
1402	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
1403	            it.
1404	
1405	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1406	            [`PreTrainedTokenizer.__call__`] for details.
1407	
1408	            [What are input IDs?](../glossary#input-ids)
1409	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
1410	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
1411	
1412	            - 1 for tokens that are **not masked**,
1413	            - 0 for tokens that are **masked**.
1414	
1415	            [What are attention masks?](../glossary#attention-mask)
1416	
1417	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1418	            [`PreTrainedTokenizer.__call__`] for details.
1419	
1420	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
1421	            `past_key_values`).
1422	
1423	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
1424	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
1425	            information on the default strategy.
1426	
1427	            - 1 indicates the head is **not masked**,
1428	            - 0 indicates the head is **masked**.
1429	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
1430	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
1431	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
1432	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
1433	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
1434	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
1435	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
1436	
1437	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
1438	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
1439	
1440	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
1441	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
1442	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
1443	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
1444	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
1445	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
1446	            model's internal embedding lookup matrix.
1447	        use_cache (`bool`, *optional*):
1448	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
1449	            `past_key_values`).
1450	        output_attentions (`bool`, *optional*):
1451	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1452	            tensors for more detail.
1453	        output_hidden_states (`bool`, *optional*):
1454	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1455	            more detail.
1456	        return_dict (`bool`, *optional*):
1457	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1458	        pixel_values (`torch.FloatTensor` of shape `(seq_length, num_channels * image_size * image_size)):
1459	            The tensors corresponding to the input images. Pixel values can be obtained using
1460	            [`AutoImageProcessor`]. See [`Qwen2_5_VLImageProcessor.__call__`] for details. [`Qwen2_5_VLProcessor`] uses
1461	            [`Qwen2_5_VLImageProcessor`] for processing images.
1462	        pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):
1463	            The tensors corresponding to the input videos. Pixel values can be obtained using
1464	            [`AutoImageProcessor`]. See [`Qwen2_5_VLImageProcessor.__call__`] for details. [`Qwen2_5_VLProcessor`] uses
1465	            [`Qwen2_5_VLImageProcessor`] for processing videos.
1466	        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
1467	            The temporal, height and width of feature shape of each image in LLM.
1468	        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
1469	            The temporal, height and width of feature shape of each video in LLM.
1470	        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):
1471	            The rope index difference between sequence length and multimodal rope.
1472	"""
1473	
1474	
1475	class Qwen2_5_VLForConditionalGeneration(Qwen2_5_VLPreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen2_audio/modeling_qwen2_audio.py:441
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
440	
441	QWEN2AUDIO_START_DOCSTRING = r"""
442	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
443	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
444	    etc.)
445	
446	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
447	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
448	    and behavior.
449	
450	    Parameters:
451	        config ([`Qwen2AudioConfig`]):
452	            Model configuration class with all the parameters of the model. Initializing with a config file does not
453	            load the weights associated with the model, only the configuration. Check out the
454	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
455	"""
456	
457	
458	@add_start_docstrings(
459	    "The bare Qwen2Audio Model outputting raw hidden-states without any specific head on top.",
460	    QWEN2AUDIO_START_DOCSTRING,
461	)
462	class Qwen2AudioPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen2_audio/modeling_qwen2_audio.py:486
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
485	
486	QWEN2AUDIOENCODER_START_DOCSTRING = r"""
487	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
488	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
489	    etc.)
490	
491	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
492	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
493	    and behavior.
494	
495	    Parameters:
496	        config ([`Qwen2AudioEncoderConfig`]):
497	            Model configuration class with all the parameters of the model. Initializing with a config file does not
498	            load the weights associated with the model, only the configuration. Check out the
499	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
500	"""
501	
502	
503	@add_start_docstrings(
504	    """The audio model from Qwen2Audio without any head or projection on top.""",
505	    QWEN2AUDIOENCODER_START_DOCSTRING,
506	)
507	# Copied from transformers.models.whisper.modeling_whisper.WhisperEncoder with Whisper->Qwen2Audio
508	class Qwen2AudioEncoder(Qwen2AudioPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen2_audio/modeling_qwen2_audio.py:697
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
696	
697	QWEN2AUDIO_INPUTS_DOCSTRING = r"""
698	    Args:
699	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
700	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
701	            it.
702	
703	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
704	            [`PreTrainedTokenizer.__call__`] for details.
705	
706	            [What are input IDs?](../glossary#input-ids)
707	        input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, feature_sequence_length)`):
708	            Float values mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by
709	            loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via
710	            the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the
711	            [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a
712	            tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]
713	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
714	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
715	
716	            - 1 for tokens that are **not masked**,
717	            - 0 for tokens that are **masked**.
718	
719	            [What are attention masks?](../glossary#attention-mask)
720	
721	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
722	            [`PreTrainedTokenizer.__call__`] for details.
723	
724	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
725	            `past_key_values`).
726	
727	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
728	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
729	            information on the default strategy.
730	
731	            - 1 indicates the head is **not masked**,
732	            - 0 indicates the head is **masked**.
733	        feature_attention_mask (`torch.Tensor` of shape `(batch_size, feature_sequence_length)`):
734	            Mask to avoid performing attention on padding feature indices. Mask values selected in `[0, 1]`:
735	
736	            - 1 for tokens that are **not masked**,
737	            - 0 for tokens that are **masked**.
738	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
739	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
740	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
741	        past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
742	            Pre-computed hidden-states that can be used to speed up auto-regressive (sequential) decoding. There are
743	            two sets of pre-computed hidden-states: key and values states in the self-attention blocks.
744	            The `past_key_values` are returned when `use_cache=True` is passed or when `config.use_cache=True`.
745	            It is a [`~cache_utils.Cache`] instance.
746	
747	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those
748	            that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of
749	            all `input_ids` of shape `(batch_size, sequence_length)`.shape `(batch_size, 1)` instead of all
750	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
751	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
752	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
753	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
754	            model's internal embedding lookup matrix.
755	        use_cache (`bool`, *optional*):
756	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
757	            `past_key_values`).
758	        output_attentions (`bool`, *optional*):
759	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
760	            tensors for more detail.
761	        output_hidden_states (`bool`, *optional*):
762	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
763	            more detail.
764	        return_dict (`bool`, *optional*):
765	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
766	"""
767	
768	
769	@add_start_docstrings(
770	    """The QWEN2AUDIO model which consists of a audio backbone and a language model.""",
771	    QWEN2AUDIO_START_DOCSTRING,
772	)
773	class Qwen2AudioForConditionalGeneration(Qwen2AudioPreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen2_moe/modeling_qwen2_moe.py:740
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
739	
740	QWEN2MOE_START_DOCSTRING = r"""
741	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
742	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
743	    etc.)
744	
745	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
746	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
747	    and behavior.
748	
749	    Parameters:
750	        config ([`Qwen2MoeConfig`]):
751	            Model configuration class with all the parameters of the model. Initializing with a config file does not
752	            load the weights associated with the model, only the configuration. Check out the
753	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
754	"""
755	
756	
757	@add_start_docstrings(
758	    "The bare Qwen2MoE Model outputting raw hidden-states without any specific head on top.",
759	    QWEN2MOE_START_DOCSTRING,
760	)
761	class Qwen2MoePreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen2_moe/modeling_qwen2_moe.py:783
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
782	
783	QWEN2MOE_INPUTS_DOCSTRING = r"""
784	    Args:
785	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
786	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
787	            it.
788	
789	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
790	            [`PreTrainedTokenizer.__call__`] for details.
791	
792	            [What are input IDs?](../glossary#input-ids)
793	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
794	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
795	
796	            - 1 for tokens that are **not masked**,
797	            - 0 for tokens that are **masked**.
798	
799	            [What are attention masks?](../glossary#attention-mask)
800	
801	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
802	            [`PreTrainedTokenizer.__call__`] for details.
803	
804	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
805	            `past_key_values`).
806	
807	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
808	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
809	            information on the default strategy.
810	
811	            - 1 indicates the head is **not masked**,
812	            - 0 indicates the head is **masked**.
813	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
814	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
815	            config.n_positions - 1]`.
816	
817	            [What are position IDs?](../glossary#position-ids)
818	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
819	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
820	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
821	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
822	
823	            Two formats are allowed:
824	            - a [`~cache_utils.Cache`] instance, see our
825	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
826	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
827	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
828	            cache format.
829	
830	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
831	            legacy cache format will be returned.
832	
833	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
834	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
835	            of shape `(batch_size, sequence_length)`.
836	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
837	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
838	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
839	            model's internal embedding lookup matrix.
840	        use_cache (`bool`, *optional*):
841	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
842	            `past_key_values`).
843	        output_attentions (`bool`, *optional*):
844	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
845	            tensors for more detail.
846	        output_hidden_states (`bool`, *optional*):
847	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
848	            more detail.
849	        output_router_logits (`bool`, *optional*):
850	            Whether or not to return the logits of all the routers. They are useful for computing the router loss, and
851	            should not be returned during inference.
852	        return_dict (`bool`, *optional*):
853	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
854	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
855	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
856	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
857	            the complete sequence length.
858	"""
859	
860	
861	@add_start_docstrings(
862	    "The bare Qwen2MoE Model outputting raw hidden-states without any specific head on top.",
863	    QWEN2MOE_START_DOCSTRING,
864	)
865	class Qwen2MoeModel(Qwen2MoePreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen2_moe/modeling_qwen2_moe.py:939
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
938	                logger.warning_once(
939	                    "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and "
940	                    "will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class "
941	                    "(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py:883
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
882	
883	QWEN2VL_START_DOCSTRING = r"""
884	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
885	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
886	    etc.)
887	
888	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
889	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
890	    and behavior.
891	
892	    Parameters:
893	        config ([`Qwen2VLConfig`]):
894	            Model configuration class with all the parameters of the model. Initializing with a config file does not
895	            load the weights associated with the model, only the configuration. Check out the
896	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
897	"""
898	
899	
900	@add_start_docstrings(
901	    "The bare Qwen2VL Model outputting raw hidden-states without any specific head on top.",
902	    QWEN2VL_START_DOCSTRING,
903	)
904	class Qwen2VLPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py:1315
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1314	
1315	QWEN2_VL_INPUTS_DOCSTRING = r"""
1316	    Args:
1317	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
1318	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
1319	            it.
1320	
1321	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1322	            [`PreTrainedTokenizer.__call__`] for details.
1323	
1324	            [What are input IDs?](../glossary#input-ids)
1325	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
1326	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
1327	
1328	            - 1 for tokens that are **not masked**,
1329	            - 0 for tokens that are **masked**.
1330	
1331	            [What are attention masks?](../glossary#attention-mask)
1332	
1333	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1334	            [`PreTrainedTokenizer.__call__`] for details.
1335	
1336	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
1337	            `past_key_values`).
1338	
1339	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
1340	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
1341	            information on the default strategy.
1342	
1343	            - 1 indicates the head is **not masked**,
1344	            - 0 indicates the head is **masked**.
1345	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
1346	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
1347	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
1348	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
1349	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
1350	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
1351	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
1352	
1353	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
1354	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
1355	
1356	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
1357	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
1358	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
1359	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
1360	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
1361	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
1362	            model's internal embedding lookup matrix.
1363	        use_cache (`bool`, *optional*):
1364	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
1365	            `past_key_values`).
1366	        output_attentions (`bool`, *optional*):
1367	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1368	            tensors for more detail.
1369	        output_hidden_states (`bool`, *optional*):
1370	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1371	            more detail.
1372	        return_dict (`bool`, *optional*):
1373	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1374	        pixel_values (`torch.FloatTensor` of shape `(seq_length, num_channels * image_size * image_size)):
1375	            The tensors corresponding to the input images. Pixel values can be obtained using
1376	            [`AutoImageProcessor`]. See [`Qwen2VLImageProcessor.__call__`] for details. [`Qwen2VLProcessor`] uses
1377	            [`Qwen2VLImageProcessor`] for processing images.
1378	        pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):
1379	            The tensors corresponding to the input videos. Pixel values can be obtained using
1380	            [`AutoImageProcessor`]. See [`Qwen2VLImageProcessor.__call__`] for details. [`Qwen2VLProcessor`] uses
1381	            [`Qwen2VLImageProcessor`] for processing videos.
1382	        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
1383	            The temporal, height and width of feature shape of each image in LLM.
1384	        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
1385	            The temporal, height and width of feature shape of each video in LLM.
1386	        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):
1387	            The rope index difference between sequence length and multimodal rope.
1388	"""
1389	
1390	
1391	class Qwen2VLForConditionalGeneration(Qwen2VLPreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen3/modeling_qwen3.py:349
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
348	
349	QWEN3_START_DOCSTRING = r"""
350	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
351	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
352	    etc.)
353	
354	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
355	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
356	    and behavior.
357	
358	    Parameters:
359	        config ([`Qwen3Config`]):
360	            Model configuration class with all the parameters of the model. Initializing with a config file does not
361	            load the weights associated with the model, only the configuration. Check out the
362	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
363	"""
364	
365	
366	@add_start_docstrings(
367	    "The bare Qwen3 Model outputting raw hidden-states without any specific head on top.",
368	    QWEN3_START_DOCSTRING,
369	)
370	class Qwen3PreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen3/modeling_qwen3.py:396
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
395	
396	QWEN3_INPUTS_DOCSTRING = r"""
397	    Args:
398	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
399	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
400	            it.
401	
402	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
403	            [`PreTrainedTokenizer.__call__`] for details.
404	
405	            [What are input IDs?](../glossary#input-ids)
406	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
407	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
408	
409	            - 1 for tokens that are **not masked**,
410	            - 0 for tokens that are **masked**.
411	
412	            [What are attention masks?](../glossary#attention-mask)
413	
414	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
415	            [`PreTrainedTokenizer.__call__`] for details.
416	
417	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
418	            `past_key_values`).
419	
420	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
421	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
422	            information on the default strategy.
423	
424	            - 1 indicates the head is **not masked**,
425	            - 0 indicates the head is **masked**.
426	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
427	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
428	            config.n_positions - 1]`.
429	
430	            [What are position IDs?](../glossary#position-ids)
431	        past_key_values (`Cache`, *optional*):
432	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
433	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
434	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
435	
436	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
437	
438	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
439	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
440	            of shape `(batch_size, sequence_length)`.
441	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
442	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
443	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
444	            model's internal embedding lookup matrix.
445	        use_cache (`bool`, *optional*):
446	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
447	            `past_key_values`).
448	        output_attentions (`bool`, *optional*):
449	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
450	            tensors for more detail.
451	        output_hidden_states (`bool`, *optional*):
452	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
453	            more detail.
454	        return_dict (`bool`, *optional*):
455	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
456	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
457	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
458	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
459	            the complete sequence length.
460	"""
461	
462	
463	@add_start_docstrings(
464	    "The bare Qwen3 Model outputting raw hidden-states without any specific head on top.",
465	    QWEN3_START_DOCSTRING,
466	)
467	class Qwen3Model(Qwen3PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen3_moe/modeling_qwen3_moe.py:444
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
443	
444	QWEN3_MOE_START_DOCSTRING = r"""
445	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
446	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
447	    etc.)
448	
449	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
450	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
451	    and behavior.
452	
453	    Parameters:
454	        config ([`Qwen3MoeConfig`]):
455	            Model configuration class with all the parameters of the model. Initializing with a config file does not
456	            load the weights associated with the model, only the configuration. Check out the
457	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
458	"""
459	
460	
461	@add_start_docstrings(
462	    "The bare Qwen3Moe Model outputting raw hidden-states without any specific head on top.",
463	    QWEN3_MOE_START_DOCSTRING,
464	)
465	class Qwen3MoePreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/qwen3_moe/modeling_qwen3_moe.py:491
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
490	
491	QWEN3_MOE_INPUTS_DOCSTRING = r"""
492	    Args:
493	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
494	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
495	            it.
496	
497	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
498	            [`PreTrainedTokenizer.__call__`] for details.
499	
500	            [What are input IDs?](../glossary#input-ids)
501	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
502	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
503	
504	            - 1 for tokens that are **not masked**,
505	            - 0 for tokens that are **masked**.
506	
507	            [What are attention masks?](../glossary#attention-mask)
508	
509	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
510	            [`PreTrainedTokenizer.__call__`] for details.
511	
512	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
513	            `past_key_values`).
514	
515	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
516	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
517	            information on the default strategy.
518	
519	            - 1 indicates the head is **not masked**,
520	            - 0 indicates the head is **masked**.
521	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
522	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
523	            config.n_positions - 1]`.
524	
525	            [What are position IDs?](../glossary#position-ids)
526	        past_key_values (`Cache`, *optional*):
527	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
528	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
529	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
530	
531	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
532	
533	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
534	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
535	            of shape `(batch_size, sequence_length)`.
536	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
537	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
538	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
539	            model's internal embedding lookup matrix.
540	        use_cache (`bool`, *optional*):
541	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
542	            `past_key_values`).
543	        output_attentions (`bool`, *optional*):
544	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
545	            tensors for more detail.
546	        output_hidden_states (`bool`, *optional*):
547	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
548	            more detail.
549	        return_dict (`bool`, *optional*):
550	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
551	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
552	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
553	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
554	            the complete sequence length.
555	"""
556	
557	
558	@add_start_docstrings(
559	    "The bare Qwen3Moe Model outputting raw hidden-states without any specific head on top.",
560	    QWEN3_MOE_START_DOCSTRING,
561	)
562	class Qwen3MoeModel(Qwen3MoePreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/rag/modeling_rag.py:380
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
379	
380	RAG_START_DOCSTRING = r"""
381	
382	    RAG is a seq2seq model which encapsulates two core components: a question encoder and a generator. During a forward
383	    pass, we encode the input with the question encoder and pass it to the retriever to extract relevant context
384	    documents. The documents are then prepended to the input. Such contextualized inputs is passed to the generator.
385	
386	    The question encoder can be any *autoencoding* model, preferably [`DPRQuestionEncoder`], and the generator can be
387	    any *seq2seq* model, preferably [`BartForConditionalGeneration`].
388	
389	    The model can be initialized with a [`RagRetriever`] for end-to-end generation or used in combination with the
390	    outputs of a retriever in multiple steps---see examples for more details. The model is compatible any
391	    *autoencoding* model as the `question_encoder` and any *seq2seq* model with language model head as the `generator`.
392	    It has been tested with [`DPRQuestionEncoder`] as the `question_encoder` and [`BartForConditionalGeneration`] or
393	    [`T5ForConditionalGeneration`] as the `generator`.
394	
395	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
396	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
397	    etc.)
398	
399	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
400	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
401	    and behavior.
402	
403	
404	    Args:
405	        config ([`RagConfig`]):
406	            Model configuration class with all the parameters of the model. Initializing with a config file does not
407	            load the weights associated with the model, only the configuration. Check out the
408	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
409	        question_encoder ([`PreTrainedModel`]):
410	            An encoder model compatible with the faiss index encapsulated by the `retriever`.
411	        generator ([`PreTrainedModel`]):
412	            A seq2seq model used as the generator in the RAG architecture.
413	        retriever ([`RagRetriever`]):
414	            A retriever class encapsulating a faiss index queried to obtain context documents for current inputs.
415	"""
416	
417	
418	RAG_FORWARD_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/rag/modeling_tf_rag.py:389
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
388	
389	RAG_START_DOCSTRING = r"""
390	
391	    RAG is a sequence-to-sequence model which encapsulates two core components: a question encoder and a generator.
392	    During a forward pass, we encode the input with the question encoder and pass it to the retriever to extract
393	    relevant context documents. The documents are then prepended to the input. Such contextualized inputs is passed to
394	    the generator.
395	
396	    The question encoder can be any *autoencoding* model, preferably [`TFDPRQuestionEncoder`], and the generator can be
397	    any *seq2seq* model, preferably [`TFBartForConditionalGeneration`].
398	
399	    The model can be initialized with a [`RagRetriever`] for end-to-end generation or used in combination with the
400	    outputs of a retriever in multiple steps---see examples for more details. The model is compatible any
401	    *autoencoding* model as the `question_encoder` and any *seq2seq* model with language model head as the `generator`.
402	    It has been tested with [`TFDPRQuestionEncoder`] as the `question_encoder` and [`TFBartForConditionalGeneration`]
403	    as the `generator`.
404	
405	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
406	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
407	    etc.)
408	
409	    This model is also a Tensorflow [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
410	    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
411	    general usage and behavior.
412	
413	    The model is in a developing state as it is now fully supports in eager-mode only, and may not be exported in
414	    SavedModel format.
415	
416	    Args:
417	        config ([`RagConfig`]):
418	            Model configuration class with all the parameters of the model. Initializing with a config file does not
419	            load the weights associated with the model, only the configuration. Check out the
420	            [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
421	        question_encoder ([`TFPreTrainedModel`]):
422	            An encoder model compatible with the faiss index encapsulated by the `retriever`.
423	        generator ([`TFPreTrainedModel`]):
424	            A seq2seq model used as the generator in the RAG architecture.
425	        retriever ([`RagRetriever`]):
426	            A retriever class encapsulating a faiss index queried to obtain context documents for current inputs.
427	"""
428	
429	
430	RAG_FORWARD_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/rag/retrieval_rag.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	
41	LEGACY_INDEX_PATH = "https://storage.googleapis.com/huggingface-nlp/datasets/wiki_dpr/"
42	
43	
44	class Index:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py:514
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
513	
514	RECURRENTGEMMA_START_DOCSTRING = r"""
515	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
516	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
517	    etc.)
518	
519	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
520	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
521	    and behavior.
522	
523	    Parameters:
524	        config ([`RecurrentGemmaConfig`]):
525	            Model configuration class with all the parameters of the model. Initializing with a config file does not
526	            load the weights associated with the model, only the configuration. Check out the
527	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
528	"""
529	
530	
531	@add_start_docstrings(
532	    "The bare RecurrentGemma Model outputting raw hidden-states without any specific head on top.",
533	    RECURRENTGEMMA_START_DOCSTRING,
534	)
535	class RecurrentGemmaPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/reformer/modeling_reformer.py:1894
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1893	
1894	REFORMER_START_DOCSTRING = r"""
1895	    Reformer was proposed in [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by Nikita Kitaev,
1896	    ukasz Kaiser, Anselm Levskaya.
1897	
1898	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1899	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1900	    etc.)
1901	
1902	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1903	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1904	    and behavior.
1905	
1906	    Parameters:
1907	        config ([`ReformerConfig`]): Model configuration class with all the parameters of the model.
1908	            Initializing with a config file does not load the weights associated with the model, only the
1909	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1910	"""
1911	
1912	REFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/reformer/tokenization_reformer.py:169
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
168	                content_spiece_model = self.sp_model.serialized_model_proto()
169	                fi.write(content_spiece_model)
170	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/regnet/modeling_flax_regnet.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	
45	REGNET_START_DOCSTRING = r"""
46	
47	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
48	    library implements for all its model (such as downloading, saving and converting weights from PyTorch models)
49	
50	    This model is also a
51	    [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. Use it as
52	    a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
53	    behavior.
54	
55	    Finally, this model supports inherent JAX features such as:
56	
57	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
58	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
59	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
60	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
61	
62	    Parameters:
63	        config ([`RegNetConfig`]): Model configuration class with all the parameters of the model.
64	            Initializing with a config file does not load the weights associated with the model, only the
65	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
66	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
67	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
68	            `jax.numpy.bfloat16` (on TPUs).
69	
70	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
71	            specified all the computation will be performed with the given `dtype`.
72	
73	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
74	            parameters.**
75	
76	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
77	            [`~FlaxPreTrainedModel.to_bf16`].
78	"""
79	
80	REGNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/regnet/modeling_flax_regnet.py:694
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
693	
694	FLAX_VISION_MODEL_DOCSTRING = """
695	    Returns:
696	
697	    Examples:
698	
699	    ```python
700	    >>> from transformers import AutoImageProcessor, FlaxRegNetModel
701	    >>> from PIL import Image
702	    >>> import requests
703	
704	    >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
705	    >>> image = Image.open(requests.get(url, stream=True).raw)
706	
707	    >>> image_processor = AutoImageProcessor.from_pretrained("facebook/regnet-y-040")
708	    >>> model = FlaxRegNetModel.from_pretrained("facebook/regnet-y-040")
709	
710	    >>> inputs = image_processor(images=image, return_tensors="np")
711	    >>> outputs = model(**inputs)
712	    >>> last_hidden_states = outputs.last_hidden_state
713	    ```
714	"""
715	
716	overwrite_call_docstring(FlaxRegNetModel, FLAX_VISION_MODEL_DOCSTRING)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/regnet/modeling_flax_regnet.py:787
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
786	
787	FLAX_VISION_CLASSIF_DOCSTRING = """
788	    Returns:
789	
790	    Example:
791	
792	    ```python
793	    >>> from transformers import AutoImageProcessor, FlaxRegNetForImageClassification
794	    >>> from PIL import Image
795	    >>> import jax
796	    >>> import requests
797	
798	    >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
799	    >>> image = Image.open(requests.get(url, stream=True).raw)
800	
801	    >>> image_processor = AutoImageProcessor.from_pretrained("facebook/regnet-y-040")
802	    >>> model = FlaxRegNetForImageClassification.from_pretrained("facebook/regnet-y-040")
803	
804	    >>> inputs = image_processor(images=image, return_tensors="np")
805	    >>> outputs = model(**inputs)
806	    >>> logits = outputs.logits
807	
808	    >>> # model predicts one of the 1000 ImageNet classes
809	    >>> predicted_class_idx = jax.numpy.argmax(logits, axis=-1)
810	    >>> print("Predicted class:", model.config.id2label[predicted_class_idx.item()])
811	    ```
812	"""
813	
814	overwrite_call_docstring(FlaxRegNetForImageClassification, FLAX_VISION_CLASSIF_DOCSTRING)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/regnet/modeling_regnet.py:300
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
299	
300	REGNET_START_DOCSTRING = r"""
301	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
302	    as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
303	    behavior.
304	
305	    Parameters:
306	        config ([`RegNetConfig`]): Model configuration class with all the parameters of the model.
307	            Initializing with a config file does not load the weights associated with the model, only the
308	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
309	"""
310	
311	REGNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/regnet/modeling_tf_regnet.py:459
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
458	
459	REGNET_START_DOCSTRING = r"""
460	    This model is a Tensorflow
461	    [keras.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) sub-class. Use it as a
462	    regular Tensorflow Module and refer to the Tensorflow documentation for all matter related to general usage and
463	    behavior.
464	
465	    Parameters:
466	        config ([`RegNetConfig`]): Model configuration class with all the parameters of the model.
467	            Initializing with a config file does not load the weights associated with the model, only the
468	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
469	"""
470	
471	REGNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/rembert/modeling_rembert.py:65
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
64	        logger.error(
65	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
66	            "https://www.tensorflow.org/install/ for installation instructions."
67	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/rembert/modeling_rembert.py:667
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
666	
667	REMBERT_START_DOCSTRING = r"""
668	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
669	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
670	    behavior.
671	
672	    Parameters:
673	        config ([`RemBertConfig`]): Model configuration class with all the parameters of the model.
674	            Initializing with a config file does not load the weights associated with the model, only the
675	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
676	"""
677	
678	REMBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/rembert/modeling_tf_rembert.py:941
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
940	
941	REMBERT_START_DOCSTRING = r"""
942	
943	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
944	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
945	    etc.)
946	
947	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
948	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
949	    behavior.
950	
951	    <Tip>
952	
953	    TensorFlow models and layers in `transformers` accept two formats as input:
954	
955	    - having all inputs as keyword arguments (like PyTorch models), or
956	    - having all inputs as a list, tuple or dict in the first positional argument.
957	
958	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
959	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
960	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
961	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
962	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
963	    positional argument:
964	
965	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
966	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
967	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
968	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
969	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
970	
971	    Note that when creating models and layers with
972	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
973	    about any of this, as you can just pass inputs like you would to any other Python function!
974	
975	    </Tip>
976	
977	    Args:
978	        config ([`RemBertConfig`]): Model configuration class with all the parameters of the model.
979	            Initializing with a config file does not load the weights associated with the model, only the
980	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
981	"""
982	
983	REMBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/rembert/tokenization_rembert.py:260
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
259	                content_spiece_model = self.sp_model.serialized_model_proto()
260	                fi.write(content_spiece_model)
261	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/resnet/modeling_flax_resnet.py:40
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
39	
40	RESNET_START_DOCSTRING = r"""
41	
42	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
43	    library implements for all its model (such as downloading, saving and converting weights from PyTorch models)
44	
45	    This model is also a
46	    [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. Use it as
47	    a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
48	    behavior.
49	
50	    Finally, this model supports inherent JAX features such as:
51	
52	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
53	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
54	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
55	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
56	
57	    Parameters:
58	        config ([`ResNetConfig`]): Model configuration class with all the parameters of the model.
59	            Initializing with a config file does not load the weights associated with the model, only the
60	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
61	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
62	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
63	            `jax.numpy.bfloat16` (on TPUs).
64	
65	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
66	            specified all the computation will be performed with the given `dtype`.
67	
68	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
69	            parameters.**
70	
71	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
72	            [`~FlaxPreTrainedModel.to_bf16`].
73	"""
74	
75	
76	RESNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/resnet/modeling_flax_resnet.py:584
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
583	
584	FLAX_VISION_MODEL_DOCSTRING = """
585	    Returns:
586	
587	    Examples:
588	
589	    ```python
590	    >>> from transformers import AutoImageProcessor, FlaxResNetModel
591	    >>> from PIL import Image
592	    >>> import requests
593	
594	    >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
595	    >>> image = Image.open(requests.get(url, stream=True).raw)
596	    >>> image_processor = AutoImageProcessor.from_pretrained("microsoft/resnet-50")
597	    >>> model = FlaxResNetModel.from_pretrained("microsoft/resnet-50")
598	    >>> inputs = image_processor(images=image, return_tensors="np")
599	    >>> outputs = model(**inputs)
600	    >>> last_hidden_states = outputs.last_hidden_state
601	    ```
602	"""
603	
604	overwrite_call_docstring(FlaxResNetModel, FLAX_VISION_MODEL_DOCSTRING)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/resnet/modeling_flax_resnet.py:671
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
670	
671	FLAX_VISION_CLASSIF_DOCSTRING = """
672	    Returns:
673	
674	    Example:
675	
676	    ```python
677	    >>> from transformers import AutoImageProcessor, FlaxResNetForImageClassification
678	    >>> from PIL import Image
679	    >>> import jax
680	    >>> import requests
681	
682	    >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
683	    >>> image = Image.open(requests.get(url, stream=True).raw)
684	
685	    >>> image_processor = AutoImageProcessor.from_pretrained("microsoft/resnet-50")
686	    >>> model = FlaxResNetForImageClassification.from_pretrained("microsoft/resnet-50")
687	
688	    >>> inputs = image_processor(images=image, return_tensors="np")
689	    >>> outputs = model(**inputs)
690	    >>> logits = outputs.logits
691	
692	    >>> # model predicts one of the 1000 ImageNet classes
693	    >>> predicted_class_idx = jax.numpy.argmax(logits, axis=-1)
694	    >>> print("Predicted class:", model.config.id2label[predicted_class_idx.item()])
695	    ```
696	"""
697	
698	overwrite_call_docstring(FlaxResNetForImageClassification, FLAX_VISION_CLASSIF_DOCSTRING)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/resnet/modeling_resnet.py:290
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
289	
290	RESNET_START_DOCSTRING = r"""
291	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
292	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
293	    behavior.
294	
295	    Parameters:
296	        config ([`ResNetConfig`]): Model configuration class with all the parameters of the model.
297	            Initializing with a config file does not load the weights associated with the model, only the
298	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
299	"""
300	
301	RESNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/resnet/modeling_tf_resnet.py:377
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
376	
377	RESNET_START_DOCSTRING = r"""
378	    This model is a TensorFlow
379	    [keras.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) sub-class. Use it as a
380	    regular TensorFlow Module and refer to the TensorFlow documentation for all matter related to general usage and
381	    behavior.
382	
383	    Parameters:
384	        config ([`ResNetConfig`]): Model configuration class with all the parameters of the model.
385	            Initializing with a config file does not load the weights associated with the model, only the
386	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
387	"""
388	
389	
390	RESNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roberta/modeling_flax_roberta.py:76
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
75	
76	ROBERTA_START_DOCSTRING = r"""
77	
78	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
79	    library implements for all its model (such as downloading, saving and converting weights from PyTorch models)
80	
81	    This model is also a
82	    [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. Use it as
83	    a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
84	    behavior.
85	
86	    Finally, this model supports inherent JAX features such as:
87	
88	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
89	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
90	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
91	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
92	
93	    Parameters:
94	        config ([`RobertaConfig`]): Model configuration class with all the parameters of the
95	            model. Initializing with a config file does not load the weights associated with the model, only the
96	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
97	"""
98	
99	ROBERTA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roberta/modeling_roberta.py:721
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
720	
721	ROBERTA_START_DOCSTRING = r"""
722	
723	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
724	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
725	    etc.)
726	
727	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
728	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
729	    and behavior.
730	
731	    Parameters:
732	        config ([`RobertaConfig`]): Model configuration class with all the parameters of the
733	            model. Initializing with a config file does not load the weights associated with the model, only the
734	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
735	"""
736	
737	ROBERTA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roberta/modeling_tf_roberta.py:884
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
883	
884	ROBERTA_START_DOCSTRING = r"""
885	
886	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
887	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
888	    etc.)
889	
890	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
891	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
892	    behavior.
893	
894	    <Tip>
895	
896	    TensorFlow models and layers in `transformers` accept two formats as input:
897	
898	    - having all inputs as keyword arguments (like PyTorch models), or
899	    - having all inputs as a list, tuple or dict in the first positional argument.
900	
901	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
902	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
903	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
904	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
905	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
906	    positional argument:
907	
908	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
909	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
910	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
911	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
912	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
913	
914	    Note that when creating models and layers with
915	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
916	    about any of this, as you can just pass inputs like you would to any other Python function!
917	
918	    </Tip>
919	
920	    Parameters:
921	        config ([`RobertaConfig`]): Model configuration class with all the parameters of the
922	            model. Initializing with a config file does not load the weights associated with the model, only the
923	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
924	"""
925	
926	ROBERTA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roberta/tokenization_roberta.py:194
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
193	        with open(merges_file, encoding="utf-8") as merges_handle:
194	            bpe_merges = merges_handle.read().split("\n")[1:-1]
195	        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roberta/tokenization_roberta.py:303
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
302	        with open(vocab_file, "w", encoding="utf-8") as f:
303	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
304	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roberta/tokenization_roberta.py:307
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
306	        with open(merge_file, "w", encoding="utf-8") as writer:
307	            writer.write("#version: 0.2\n")
308	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roberta/tokenization_roberta.py:315
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
314	                    index = token_index
315	                writer.write(" ".join(bpe_tokens) + "\n")
316	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roberta_prelayernorm/modeling_flax_roberta_prelayernorm.py:79
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
78	
79	ROBERTA_PRELAYERNORM_START_DOCSTRING = r"""
80	
81	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
82	    library implements for all its model (such as downloading, saving and converting weights from PyTorch models)
83	
84	    This model is also a
85	    [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. Use it as
86	    a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
87	    behavior.
88	
89	    Finally, this model supports inherent JAX features such as:
90	
91	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
92	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
93	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
94	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
95	
96	    Parameters:
97	        config ([`RobertaPreLayerNormConfig`]): Model configuration class with all the parameters of the
98	            model. Initializing with a config file does not load the weights associated with the model, only the
99	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
100	"""
101	
102	ROBERTA_PRELAYERNORM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py:604
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
603	
604	ROBERTA_PRELAYERNORM_START_DOCSTRING = r"""
605	
606	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
607	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
608	    etc.)
609	
610	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
611	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
612	    and behavior.
613	
614	    Parameters:
615	        config ([`RobertaPreLayerNormConfig`]): Model configuration class with all the parameters of the
616	            model. Initializing with a config file does not load the weights associated with the model, only the
617	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
618	"""
619	
620	ROBERTA_PRELAYERNORM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roberta_prelayernorm/modeling_tf_roberta_prelayernorm.py:885
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
884	
885	ROBERTA_PRELAYERNORM_START_DOCSTRING = r"""
886	
887	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
888	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
889	    etc.)
890	
891	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
892	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
893	    behavior.
894	
895	    <Tip>
896	
897	    TensorFlow models and layers in `transformers` accept two formats as input:
898	
899	    - having all inputs as keyword arguments (like PyTorch models), or
900	    - having all inputs as a list, tuple or dict in the first positional argument.
901	
902	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
903	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
904	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
905	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
906	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
907	    positional argument:
908	
909	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
910	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
911	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
912	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
913	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
914	
915	    Note that when creating models and layers with
916	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
917	    about any of this, as you can just pass inputs like you would to any other Python function!
918	
919	    </Tip>
920	
921	    Parameters:
922	        config ([`RobertaPreLayerNormConfig`]): Model configuration class with all the parameters of the
923	            model. Initializing with a config file does not load the weights associated with the model, only the
924	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
925	"""
926	
927	ROBERTA_PRELAYERNORM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roc_bert/modeling_roc_bert.py:88
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
87	        logger.error(
88	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
89	            "https://www.tensorflow.org/install/ for installation instructions."
90	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roc_bert/modeling_roc_bert.py:801
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
800	
801	ROC_BERT_START_DOCSTRING = r"""
802	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
803	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
804	    behavior.
805	
806	    Parameters:
807	        config ([`RoCBertConfig`]): Model configuration class with all the parameters of the model.
808	            Initializing with a config file does not load the weights associated with the model, only the
809	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
810	"""
811	
812	ROC_BERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roc_bert/tokenization_roc_bert.py:261
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
260	            raise NotImplementedError(
261	                "return_offset_mapping is not available when using Python tokenizers. "
262	                "To use this feature, change your tokenizer to one deriving from "
263	                "transformers.PreTrainedTokenizerFast. "

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roc_bert/tokenization_roc_bert.py:886
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
885	                    index = token_index
886	                writer.write(token + "\n")
887	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roformer/modeling_flax_roformer.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	
47	ROFORMER_START_DOCSTRING = r"""
48	
49	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
50	    library implements for all its model (such as downloading, saving and converting weights from PyTorch models)
51	
52	    This model is also a
53	    [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. Use it as
54	    a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
55	    behavior.
56	
57	    Finally, this model supports inherent JAX features such as:
58	
59	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
60	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
61	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
62	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
63	
64	    Parameters:
65	        config ([`RoFormerConfig`]): Model configuration class with all the parameters of the
66	            model. Initializing with a config file does not load the weights associated with the model, only the
67	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
68	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
69	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
70	            `jax.numpy.bfloat16` (on TPUs).
71	
72	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
73	            specified all the computation will be performed with the given `dtype`.
74	
75	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
76	            parameters.**
77	
78	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
79	            [`~FlaxPreTrainedModel.to_bf16`].
80	"""
81	
82	ROFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roformer/modeling_roformer.py:97
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
96	        logger.error(
97	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
98	            "https://www.tensorflow.org/install/ for installation instructions."
99	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roformer/modeling_roformer.py:706
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
705	
706	ROFORMER_START_DOCSTRING = r"""
707	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
708	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
709	    behavior.
710	
711	    Parameters:
712	        config ([`RoFormerConfig`]): Model configuration class with all the parameters of the model.
713	            Initializing with a config file does not load the weights associated with the model, only the
714	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
715	"""
716	
717	ROFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roformer/modeling_tf_roformer.py:820
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
819	
820	ROFORMER_START_DOCSTRING = r"""
821	
822	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
823	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
824	    etc.)
825	
826	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
827	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
828	    behavior.
829	
830	    <Tip>
831	
832	    TensorFlow models and layers in `transformers` accept two formats as input:
833	
834	    - having all inputs as keyword arguments (like PyTorch models), or
835	    - having all inputs as a list, tuple or dict in the first positional argument.
836	
837	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
838	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
839	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
840	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
841	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
842	    positional argument:
843	
844	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
845	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
846	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
847	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
848	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
849	
850	    Note that when creating models and layers with
851	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
852	    about any of this, as you can just pass inputs like you would to any other Python function!
853	
854	    </Tip>
855	
856	    Args:
857	        config ([`RoFormerConfig`]): Model configuration class with all the parameters of the model.
858	            Initializing with a config file does not load the weights associated with the model, only the
859	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
860	"""
861	
862	ROFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roformer/tokenization_roformer.py:361
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
360	            raise ImportError(
361	                "You need to install rjieba to use RoFormerTokenizer. "
362	                "See https://pypi.org/project/rjieba/ for installation."
363	            )

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roformer/tokenization_roformer.py:535
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
534	                    index = token_index
535	                writer.write(token + "\n")
536	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/roformer/tokenization_utils.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	            raise ImportError(
35	                "You need to install rjieba to use RoFormerTokenizer. "
36	                "See https://pypi.org/project/rjieba/ for installation."
37	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/rt_detr/modeling_rt_detr.py:1098
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1097	
1098	RTDETR_START_DOCSTRING = r"""
1099	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1100	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1101	    etc.)
1102	
1103	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1104	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1105	    and behavior.
1106	
1107	    Parameters:
1108	        config ([`RTDetrConfig`]):
1109	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1110	            load the weights associated with the model, only the configuration. Check out the
1111	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1112	"""
1113	
1114	RTDETR_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/rt_detr/modeling_rt_detr_resnet.py:339
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
338	
339	RTDETR_RESNET_START_DOCSTRING = r"""
340	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
341	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
342	    behavior.
343	
344	    Parameters:
345	        config ([`RTDetrResNetConfig`]): Model configuration class with all the parameters of the model.
346	            Initializing with a config file does not load the weights associated with the model, only the
347	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
348	"""
349	
350	RTDETR_RESNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py:1251
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1250	
1251	RTDetrV2_START_DOCSTRING = r"""
1252	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1253	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1254	    etc.)
1255	
1256	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1257	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1258	    and behavior.
1259	
1260	    Parameters:
1261	        config ([`RTDetrV2Config`]):
1262	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1263	            load the weights associated with the model, only the configuration. Check out the
1264	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1265	"""
1266	
1267	RTDetrV2_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/rwkv/modeling_rwkv.py:520
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
519	
520	RWKV_START_DOCSTRING = r"""
521	
522	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
523	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
524	    etc.)
525	
526	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
527	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
528	    and behavior.
529	
530	    Parameters:
531	        config ([`RwkvConfig`]): Model configuration class with all the parameters of the model.
532	            Initializing with a config file does not load the weights associated with the model, only the
533	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
534	"""
535	
536	RWKV_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/sam/modeling_sam.py:1205
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1204	
1205	SAM_START_DOCSTRING = r"""
1206	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1207	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1208	    etc.)
1209	
1210	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1211	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1212	    and behavior.
1213	
1214	    Parameters:
1215	        config ([`SamConfig`]): Model configuration class with all the parameters of the model.
1216	            Initializing with a config file does not load the weights associated with the model, only the
1217	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1218	"""
1219	
1220	
1221	SAM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/sam/modeling_sam.py:1221
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1220	
1221	SAM_INPUTS_DOCSTRING = r"""
1222	    Args:
1223	        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):
1224	            Pixel values. Pixel values can be obtained using [`SamProcessor`]. See [`SamProcessor.__call__`] for
1225	            details.
1226	        input_points (`torch.FloatTensor` of shape `(batch_size, num_points, 2)`):
1227	            Input 2D spatial points, this is used by the prompt encoder to encode the prompt. Generally yields to much
1228	            better results. The points can be obtained by passing a list of list of list to the processor that will
1229	            create corresponding `torch` tensors of dimension 4. The first dimension is the image batch size, the
1230	            second dimension is the point batch size (i.e. how many segmentation masks do we want the model to predict
1231	            per input point), the third dimension is the number of points per segmentation mask (it is possible to pass
1232	            multiple points for a single mask), and the last dimension is the x (vertical) and y (horizontal)
1233	            coordinates of the point. If a different number of points is passed either for each image, or for each
1234	            mask, the processor will create "PAD" points that will correspond to the (0, 0) coordinate, and the
1235	            computation of the embedding will be skipped for these points using the labels.
1236	        input_labels (`torch.LongTensor` of shape `(batch_size, point_batch_size, num_points)`):
1237	            Input labels for the points, this is used by the prompt encoder to encode the prompt. According to the
1238	            official implementation, there are 3 types of labels
1239	
1240	            - `1`: the point is a point that contains the object of interest
1241	            - `0`: the point is a point that does not contain the object of interest
1242	            - `-1`: the point corresponds to the background
1243	
1244	            We added the label:
1245	
1246	            - `-10`: the point is a padding point, thus should be ignored by the prompt encoder
1247	
1248	            The padding labels should be automatically done by the processor.
1249	        input_boxes (`torch.FloatTensor` of shape `(batch_size, num_boxes, 4)`):
1250	            Input boxes for the points, this is used by the prompt encoder to encode the prompt. Generally yields to
1251	            much better generated masks. The boxes can be obtained by passing a list of list of list to the processor,
1252	            that will generate a `torch` tensor, with each dimension corresponding respectively to the image batch
1253	            size, the number of boxes per image and the coordinates of the top left and botton right point of the box.
1254	            In the order (`x1`, `y1`, `x2`, `y2`):
1255	
1256	            - `x1`: the x coordinate of the top left point of the input box
1257	            - `y1`: the y coordinate of the top left point of the input box
1258	            - `x2`: the x coordinate of the bottom right point of the input box
1259	            - `y2`: the y coordinate of the bottom right point of the input box
1260	
1261	        input_masks (`torch.FloatTensor` of shape `(batch_size, image_size, image_size)`):
1262	            SAM model also accepts segmentation masks as input. The mask will be embedded by the prompt encoder to
1263	            generate a corresponding embedding, that will be fed later on to the mask decoder. These masks needs to be
1264	            manually fed by the user, and they need to be of shape (`batch_size`, `image_size`, `image_size`).
1265	
1266	        image_embeddings (`torch.FloatTensor` of shape `(batch_size, output_channels, window_size, window_size)`):
1267	            Image embeddings, this is used by the mask decder to generate masks and iou scores. For more memory
1268	            efficient computation, users can first retrieve the image embeddings using the `get_image_embeddings`
1269	            method, and then feed them to the `forward` method instead of feeding the `pixel_values`.
1270	        multimask_output (`bool`, *optional*):
1271	            In the original implementation and paper, the model always outputs 3 masks per image (or per point / per
1272	            bounding box if relevant). However, it is possible to just output a single mask, that corresponds to the
1273	            "best" mask, by specifying `multimask_output=False`.
1274	        attention_similarity (`torch.FloatTensor`, *optional*):
1275	            Attention similarity tensor, to be provided to the mask decoder for target-guided attention in case the
1276	            model is used for personalization as introduced in [PerSAM](https://arxiv.org/abs/2305.03048).
1277	        target_embedding (`torch.FloatTensor`, *optional*):
1278	            Embedding of the target concept, to be provided to the mask decoder for target-semantic prompting in case
1279	            the model is used for personalization as introduced in [PerSAM](https://arxiv.org/abs/2305.03048).
1280	        output_attentions (`bool`, *optional*):
1281	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1282	            tensors for more detail.
1283	        output_hidden_states (`bool`, *optional*):
1284	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1285	            more detail.
1286	        return_dict (`bool`, *optional*):
1287	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1288	"""
1289	
1290	
1291	SAM_VISION_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/sam/modeling_tf_sam.py:1329
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1328	
1329	SAM_START_DOCSTRING = r"""
1330	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
1331	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1332	    etc.)
1333	
1334	    This model is also a TensorFlow [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
1335	    subclass. Use it as a regular TensorFlow Model and refer to the TensorFlow documentation for all matter related to
1336	    general usage and behavior.
1337	
1338	    Parameters:
1339	        config ([`SamConfig`]): Model configuration class with all the parameters of the model.
1340	            Initializing with a config file does not load the weights associated with the model, only the
1341	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
1342	"""
1343	
1344	
1345	SAM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py:82
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
81	
82	SEAMLESS_M4T_START_DOCSTRING = r"""
83	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
84	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
85	    behavior.
86	
87	    Parameters:
88	        config ([`~SeamlessM4TConfig`]): Model configuration class with all the parameters of the model.
89	            Initializing with a config file does not load the weights associated with the model, only the
90	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
91	"""
92	
93	SEAMLESS_M4T_INPUTS_DOCSTRING_FIRST_PART = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py:125
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
124	
125	SEAMLESS_M4T_INPUTS_DOCSTRING_LAST_PART = r"""
126	        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):
127	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
128	
129	            - 1 for tokens that are **not masked**,
130	            - 0 for tokens that are **masked**.
131	
132	            [What are attention masks?](../glossary#attention-mask)
133	        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
134	            Indices of decoder input sequence tokens in the vocabulary.
135	
136	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
137	            [`PreTrainedTokenizer.__call__`] for details.
138	
139	            [What are decoder input IDs?](../glossary#decoder-input-ids)
140	
141	            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`
142	            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).
143	
144	            For translation and summarization training, `decoder_input_ids` should be provided. If no
145	            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
146	            for denoising pre-training following the paper.
147	        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
148	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
149	            be used by default.
150	
151	            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]
152	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
153	            information on the default strategy.
154	        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):
155	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
156	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
157	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
158	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
159	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
160	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
161	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
162	
163	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
164	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
165	
166	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
167	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
168	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
169	        inputs_embeds (`torch.FloatTensor` of shape`(batch_size, sequence_length, hidden_size)`, *optional*):
170	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
171	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
172	            model's internal embedding lookup matrix.
173	        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):
174	            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded
175	            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be
176	            input (see `past_key_values`). This is useful if you want more control over how to convert
177	            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.
178	
179	            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value
180	            of `inputs_embeds`.
181	        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
182	            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,
183	            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the
184	            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`
185	        use_cache (`bool`, *optional*):
186	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
187	            `past_key_values`).
188	        output_attentions (`bool`, *optional*):
189	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
190	            tensors for more detail.
191	        output_hidden_states (`bool`, *optional*):
192	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
193	            more detail.
194	        return_dict (`bool`, *optional*):
195	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
196	"""
197	
198	M4T_MODEL_INPUTS_DOCSTRING = SEAMLESS_M4T_INPUTS_DOCSTRING_FIRST_PART + SEAMLESS_M4T_INPUTS_DOCSTRING_LAST_PART

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py:2295
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2294	
2295	HIFIGAN_START_DOCSTRING = r"""
2296	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
2297	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
2298	    etc.)
2299	
2300	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
2301	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
2302	    and behavior.
2303	
2304	    Parameters:
2305	        config ([`SeamlessM4TConfig`]):
2306	            Model configuration class with all the parameters of the model. Initializing with a config file does not
2307	            load the weights associated with the model, only the configuration. Check out the
2308	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
2309	"""
2310	
2311	
2312	# Copied from transformers.models.speecht5.modeling_speecht5.HifiGanResidualBlock
2313	class HifiGanResidualBlock(nn.Module):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py:2485
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2484	@add_start_docstrings(
2485	    """Code HiFi-GAN vocoder as described in this [repository](https://github.com/facebookresearch/speech-resynthesis).""",
2486	    HIFIGAN_START_DOCSTRING,
2487	)
2488	class SeamlessM4TCodeHifiGan(PreTrainedModel):

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py:426
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
425	        with open(self.vocab_file, "rb") as f:
426	            sp_model = f.read()
427	            model_pb2 = import_protobuf(f"The new behaviour of {self.__class__.__name__} (with `self.legacy = False`)")

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py:508
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
507	                content_spiece_model = self.sp_model.serialized_model_proto()
508	                fi.write(content_spiece_model)
509	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py:166
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
165	
166	SEAMLESS_M4T_V2_START_DOCSTRING = r"""
167	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
168	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
169	    behavior.
170	
171	    Parameters:
172	        config ([`~SeamlessM4Tv2Config`]): Model configuration class with all the parameters of the model.
173	            Initializing with a config file does not load the weights associated with the model, only the
174	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
175	"""
176	
177	SEAMLESS_M4T_V2_MULTIMODAL_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py:209
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
208	
209	SEAMLESS_M4T_V2_END_INPUTS_DOCSTRING = r"""
210	        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):
211	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
212	
213	            - 1 for tokens that are **not masked**,
214	            - 0 for tokens that are **masked**.
215	
216	            [What are attention masks?](../glossary#attention-mask)
217	        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
218	            Indices of decoder input sequence tokens in the vocabulary.
219	
220	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
221	            [`PreTrainedTokenizer.__call__`] for details.
222	
223	            [What are decoder input IDs?](../glossary#decoder-input-ids)
224	
225	            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`
226	            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).
227	
228	            For translation and summarization training, `decoder_input_ids` should be provided. If no
229	            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
230	            for denoising pre-training following the paper.
231	        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
232	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
233	            be used by default.
234	
235	            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]
236	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
237	            information on the default strategy.
238	        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):
239	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
240	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
241	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
242	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
243	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
244	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
245	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
246	
247	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
248	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
249	
250	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
251	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
252	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
253	        inputs_embeds (`torch.FloatTensor` of shape`(batch_size, sequence_length, hidden_size)`, *optional*):
254	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
255	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
256	            model's internal embedding lookup matrix.
257	        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):
258	            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded
259	            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be
260	            input (see `past_key_values`). This is useful if you want more control over how to convert
261	            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.
262	
263	            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value
264	            of `inputs_embeds`.
265	        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
266	            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,
267	            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the
268	            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`
269	        use_cache (`bool`, *optional*):
270	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
271	            `past_key_values`).
272	        output_attentions (`bool`, *optional*):
273	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
274	            tensors for more detail.
275	        output_hidden_states (`bool`, *optional*):
276	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
277	            more detail.
278	        return_dict (`bool`, *optional*):
279	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
280	"""
281	
282	M4T_MODEL_INPUTS_DOCSTRING = SEAMLESS_M4T_V2_MULTIMODAL_INPUTS_DOCSTRING + SEAMLESS_M4T_V2_END_INPUTS_DOCSTRING

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py:2564
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2563	
2564	HIFIGAN_START_DOCSTRING = r"""
2565	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
2566	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
2567	    etc.)
2568	
2569	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
2570	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
2571	    and behavior.
2572	
2573	    Parameters:
2574	        config ([`SeamlessM4Tv2Config`]):
2575	            Model configuration class with all the parameters of the model. Initializing with a config file does not
2576	            load the weights associated with the model, only the configuration. Check out the
2577	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
2578	"""
2579	
2580	
2581	# Copied from transformers.models.speecht5.modeling_speecht5.HifiGanResidualBlock
2582	class HifiGanResidualBlock(nn.Module):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py:2755
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2754	@add_start_docstrings(
2755	    """Code HiFi-GAN vocoder as described in this [repository](https://github.com/facebookresearch/speech-resynthesis).""",
2756	    HIFIGAN_START_DOCSTRING,
2757	)
2758	class SeamlessM4Tv2CodeHifiGan(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/segformer/modeling_segformer.py:471
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
470	
471	SEGFORMER_START_DOCSTRING = r"""
472	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
473	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
474	    behavior.
475	
476	    Parameters:
477	        config ([`SegformerConfig`]): Model configuration class with all the parameters of the model.
478	            Initializing with a config file does not load the weights associated with the model, only the
479	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
480	"""
481	
482	SEGFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/segformer/modeling_tf_segformer.py:651
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
650	
651	SEGFORMER_START_DOCSTRING = r"""
652	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
653	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
654	    etc.)
655	
656	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
657	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
658	    behavior.
659	
660	    Parameters:
661	        config ([`SegformerConfig`]): Model configuration class with all the parameters of the model.
662	            Initializing with a config file does not load the weights associated with the model, only the
663	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
664	"""
665	
666	SEGFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/seggpt/modeling_seggpt.py:669
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
668	
669	SEGGPT_START_DOCSTRING = r"""
670	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
671	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
672	    behavior.
673	
674	    Parameters:
675	        config ([`SegGptConfig`]): Model configuration class with all the parameters of the model.
676	            Initializing with a config file does not load the weights associated with the model, only the
677	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
678	"""
679	
680	SEGGPT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/sew/modeling_sew.py:1043
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1042	
1043	SEW_START_DOCSTRING = r"""
1044	    SEW was proposed in [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech
1045	    Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger,
1046	    Yoav Artzi.
1047	
1048	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1049	    library implements for all its model (such as downloading or saving etc.).
1050	
1051	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
1052	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
1053	    behavior.
1054	
1055	    Parameters:
1056	        config ([`SEWConfig`]): Model configuration class with all the parameters of the model.
1057	            Initializing with a config file does not load the weights associated with the model, only the
1058	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1059	"""
1060	
1061	
1062	SEW_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/sew_d/modeling_sew_d.py:1292
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1291	
1292	SEWD_START_DOCSTRING = r"""
1293	    SEW-D was proposed in [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech
1294	    Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger,
1295	    Yoav Artzi.
1296	
1297	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1298	    library implements for all its model (such as downloading or saving etc.).
1299	
1300	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
1301	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
1302	    behavior.
1303	
1304	    Parameters:
1305	        config ([`SEWDConfig`]): Model configuration class with all the parameters of the model.
1306	            Initializing with a config file does not load the weights associated with the model, only the
1307	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1308	"""
1309	
1310	
1311	SEWD_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/shieldgemma2/modeling_shieldgemma2.py:39
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
38	
39	SHIELDGEMMA2_INPUTS_DOCSTRING = r"""
40	    Args:
41	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
42	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
43	            it.
44	
45	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
46	            [`PreTrainedTokenizer.__call__`] for details.
47	
48	            [What are input IDs?](../glossary#input-ids)
49	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
50	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
51	
52	            - 1 for tokens that are **not masked**,
53	            - 0 for tokens that are **masked**.
54	
55	            [What are attention masks?](../glossary#attention-mask)
56	
57	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
58	            [`PreTrainedTokenizer.__call__`] for details.
59	
60	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
61	            `past_key_values`).
62	
63	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
64	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
65	            information on the default strategy.
66	
67	            - 1 indicates the head is **not masked**,
68	            - 0 indicates the head is **masked**.
69	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
70	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
71	            config.n_positions - 1]`.
72	
73	            [What are position IDs?](../glossary#position-ids)
74	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
75	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
76	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
77	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
78	
79	            Two formats are allowed:
80	            - a [`~cache_utils.Cache`] instance, see our
81	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
82	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
83	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
84	            cache format.
85	
86	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
87	            legacy cache format will be returned.
88	
89	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
90	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
91	            of shape `(batch_size, sequence_length)`.
92	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
93	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
94	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
95	            model's internal embedding lookup matrix.
96	        use_cache (`bool`, *optional*):
97	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
98	            `past_key_values`).
99	        output_attentions (`bool`, *optional*):
100	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
101	            tensors for more detail.
102	        output_hidden_states (`bool`, *optional*):
103	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
104	            more detail.
105	        return_dict (`bool`, *optional*):
106	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
107	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
108	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
109	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
110	            the complete sequence length.
111	"""
112	
113	
114	@dataclass
115	class ShieldGemma2ImageClassifierOutputWithNoAttention(ImageClassifierOutputWithNoAttention):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/siglip/modeling_siglip.py:583
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
582	
583	SIGLIP_START_DOCSTRING = r"""
584	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
585	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
586	    etc.)
587	
588	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
589	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
590	    and behavior.
591	
592	    Parameters:
593	        config ([`SiglipConfig`]): Model configuration class with all the parameters of the model.
594	            Initializing with a config file does not load the weights associated with the model, only the
595	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
596	"""
597	
598	SIGLIP_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/siglip/tokenization_siglip.py:140
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
139	        with open(self.vocab_file, "rb") as f:
140	            sp_model = f.read()
141	            model_pb2 = import_protobuf()

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/siglip/tokenization_siglip.py:373
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
372	                content_spiece_model = self.sp_model.serialized_model_proto()
373	                fi.write(content_spiece_model)
374	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/siglip2/modeling_siglip2.py:810
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
809	
810	SIGLIP2_START_DOCSTRING = r"""
811	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
812	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
813	    etc.)
814	
815	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
816	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
817	    and behavior.
818	
819	    Parameters:
820	        config ([`Siglip2Config`]): Model configuration class with all the parameters of the model.
821	            Initializing with a config file does not load the weights associated with the model, only the
822	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
823	"""
824	
825	SIGLIP2_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/smolvlm/modeling_smolvlm.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	
50	SMOLVLM_START_DOCSTRING = r"""
51	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
52	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
53	    etc.)
54	
55	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
56	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
57	    and behavior.
58	
59	    Parameters:
60	        config ([`SmolVLMConfig`] or [`SmolVLMVisionConfig`]):
61	            Model configuration class with all the parameters of the model. Initializing with a config file does not
62	            load the weights associated with the model, only the configuration. Check out the
63	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
64	"""
65	
66	
67	@add_start_docstrings(
68	    "The bare SmolVLM Model outputting raw hidden-states without any specific head on top.",
69	    SMOLVLM_START_DOCSTRING,
70	)
71	class SmolVLMPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/smolvlm/modeling_smolvlm.py:407
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
406	
407	SMOLVLM_VISION_START_DOCSTRING = r"""
408	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
409	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
410	    etc.)
411	
412	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
413	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
414	    and behavior.
415	
416	    Parameters:
417	        config ([`SmolVLMVisionConfig`]):
418	            Model configuration class with all the parameters of the model. Initializing with a config file does not
419	            load the weights associated with the model, only the configuration. Check out the
420	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
421	"""
422	
423	
424	@add_start_docstrings(
425	    "The SmolVLM Vision Transformer Model outputting raw image embedding.",
426	    SMOLVLM_VISION_START_DOCSTRING,
427	)
428	class SmolVLMVisionTransformer(SmolVLMPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/smolvlm/modeling_smolvlm.py:581
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
580	
581	SMOLVLM_INPUTS_DOCSTRING = r"""
582	    Args:
583	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
584	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
585	            it.
586	
587	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
588	            [`PreTrainedTokenizer.__call__`] for details.
589	
590	            [What are input IDs?](../glossary#input-ids)
591	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
592	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
593	
594	            - 1 for tokens that are **not masked**,
595	            - 0 for tokens that are **masked**.
596	
597	            [What are attention masks?](../glossary#attention-mask)
598	
599	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
600	            [`PreTrainedTokenizer.__call__`] for details.
601	
602	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
603	            `past_key_values`).
604	
605	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
606	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
607	            information on the default strategy.
608	
609	            - 1 indicates the head is **not masked**,
610	            - 0 indicates the head is **masked**.
611	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
612	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
613	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
614	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
615	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
616	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
617	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
618	
619	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
620	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
621	
622	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
623	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
624	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
625	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
626	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
627	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
628	            model's internal embedding lookup matrix.
629	        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):
630	            The tensors corresponding to the input images. Pixel values can be obtained using
631	            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`LlavaProcessor`] uses
632	            [`CLIPImageProcessor`] for processing images).
633	        pixel_attention_mask (`torch.Tensor` of shape `(batch_size, image_size, image_size)`, *optional*):
634	            Mask to avoid performing attention on padding pixel indices.
635	        image_hidden_states (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):
636	            The hidden states of the image encoder after modality projection.
637	        use_cache (`bool`, *optional*):
638	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
639	            `past_key_values`).
640	        output_attentions (`bool`, *optional*):
641	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
642	            tensors for more detail.
643	        output_hidden_states (`bool`, *optional*):
644	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
645	            more detail.
646	        return_dict (`bool`, *optional*):
647	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
648	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
649	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
650	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
651	            the complete sequence length.
652	"""
653	
654	
655	@add_start_docstrings(
656	    """SmolVLM model consisting of a SIGLIP vision encoder and Llama3 language decoder""",
657	    SMOLVLM_START_DOCSTRING,
658	)
659	class SmolVLMModel(SmolVLMPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py:40
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
39	
40	SPEECH_ENCODER_DECODER_START_DOCSTRING = r"""
41	    This class can be used to initialize a speech-sequence-to-text-sequence model with any pretrained speech
42	    autoencoding model as the encoder and any pretrained text autoregressive model as the decoder. The encoder is
43	    loaded via [`~AutoModel.from_pretrained`] function and the decoder is loaded via
44	    [`~AutoModelForCausalLM.from_pretrained`] function. Cross-attention layers are automatically added to the decoder
45	    and should be fine-tuned on a downstream generative task, like summarization.
46	
47	    The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
48	    tasks was shown in [Leveraging Pre-trained Checkpoints for Sequence Generation
49	    Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
50	    Zhou, Wei Li, Peter J. Liu.
51	
52	    Additionally, in [Large-Scale Self- and Semi-Supervised Learning for Speech
53	    Translation](https://arxiv.org/abs/2104.06678) it is shown how leveraging large pretrained speech models for speech
54	    translation yields a significant performance improvement.
55	
56	    After such an Speech-Encoder Decoder model has been trained/fine-tuned, it can be saved/loaded just like any other
57	    models (see the examples for more information).
58	
59	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
60	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
61	    etc.)
62	
63	    This model is also a Flax Linen
64	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
65	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
66	
67	    Parameters:
68	        config ([`SpeechEncoderDecoderConfig`]): Model configuration class with all the parameters of the model.
69	            Initializing with a config file does not load the weights associated with the model, only the
70	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
71	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
72	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
73	            `jax.numpy.bfloat16` (on TPUs).
74	
75	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
76	            specified all the computation will be performed with the given `dtype`.
77	
78	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
79	            parameters.**
80	
81	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
82	            [`~FlaxPreTrainedModel.to_bf16`].
83	"""
84	
85	SPEECH_ENCODER_DECODER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	
37	SPEECH_ENCODER_DECODER_START_DOCSTRING = r"""
38	    This class can be used to initialize a speech-sequence-to-text-sequence model with any pretrained speech
39	    autoencoding model as the encoder and any pretrained text autoregressive model as the decoder. The encoder is
40	    loaded via [`~AutoModel.from_pretrained`] function and the decoder is loaded via
41	    [`~AutoModelForCausalLM.from_pretrained`] function. Cross-attention layers are automatically added to the decoder
42	    and should be fine-tuned on a downstream generative task, like summarization.
43	
44	    The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
45	    tasks was shown in [Leveraging Pre-trained Checkpoints for Sequence Generation
46	    Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
47	    Zhou, Wei Li, Peter J. Liu.
48	
49	    Additionally, in [Large-Scale Self- and Semi-Supervised Learning for Speech
50	    Translation](https://arxiv.org/abs/2104.06678) it is shown how leveraging large pretrained speech models for speech
51	    translation yields a significant performance improvement.
52	
53	    After such an Speech-Encoder Decoder model has been trained/fine-tuned, it can be saved/loaded just like any other
54	    models (see the examples for more information).
55	
56	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
57	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
58	    etc.)
59	
60	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
61	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
62	    and behavior.
63	
64	    Parameters:
65	        config ([`SpeechEncoderDecoderConfig`]): Model configuration class with all the parameters of the model.
66	            Initializing with a config file does not load the weights associated with the model, only the
67	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
68	"""
69	
70	SPEECH_ENCODER_DECODER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/speech_to_text/modeling_speech_to_text.py:567
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
566	
567	SPEECH_TO_TEXT_START_DOCSTRING = r"""
568	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
569	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
570	    etc.)
571	
572	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
573	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
574	    and behavior.
575	
576	    Parameters:
577	        config ([`Speech2TextConfig`]):
578	            Model configuration class with all the parameters of the model. Initializing with a config file does not
579	            load the weights associated with the model, only the configuration. Check out the
580	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
581	"""
582	
583	SPEECH_TO_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/speech_to_text/modeling_speech_to_text.py:583
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
582	
583	SPEECH_TO_TEXT_INPUTS_DOCSTRING = r"""
584	    Args:
585	        input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, feature_size)`):
586	            Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be obtained
587	            by loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`, *e.g.*
588	            via the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the
589	            [`AutoFeatureExtractor`] should be used for extracting the fbank features, padding and conversion into a
590	            tensor of type `torch.FloatTensor`. See [`~Speech2TextFeatureExtractor.__call__`]
591	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
592	            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,
593	            1]`:
594	
595	            - 1 for tokens that are **not masked**,
596	            - 0 for tokens that are **masked**.
597	
598	            [What are attention masks?](../glossary#attention-mask)
599	        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
600	            Indices of decoder input sequence tokens in the vocabulary.
601	
602	            Indices can be obtained using [`SpeechToTextTokenizer`]. See [`PreTrainedTokenizer.encode`] and
603	            [`PreTrainedTokenizer.__call__`] for details.
604	
605	            [What are decoder input IDs?](../glossary#decoder-input-ids)
606	
607	            SpeechToText uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If
608	            `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
609	            `past_key_values`).
610	        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
611	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
612	            be used by default.
613	
614	            If you want to change padding behavior, you should read
615	            [`modeling_speech_to_text._prepare_decoder_attention_mask`] and modify to your needs. See diagram 1 in [the
616	            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
617	        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
618	            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:
619	
620	            - 1 indicates the head is **not masked**,
621	            - 0 indicates the head is **masked**.
622	
623	        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
624	            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:
625	
626	            - 1 indicates the head is **not masked**,
627	            - 0 indicates the head is **masked**.
628	
629	        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
630	            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:
631	
632	            - 1 indicates the head is **not masked**,
633	            - 0 indicates the head is **masked**.
634	
635	        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):
636	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
637	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
638	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
639	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
640	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
641	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
642	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
643	
644	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
645	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
646	
647	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
648	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
649	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
650	        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):
651	            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded
652	            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be
653	            input (see `past_key_values`). This is useful if you want more control over how to convert
654	            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.
655	        use_cache (`bool`, *optional*):
656	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
657	            `past_key_values`).
658	        output_attentions (`bool`, *optional*):
659	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
660	            tensors for more detail.
661	        output_hidden_states (`bool`, *optional*):
662	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
663	            more detail.
664	        return_dict (`bool`, *optional*):
665	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
666	"""
667	
668	
669	class Speech2TextEncoder(Speech2TextPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/speech_to_text/modeling_tf_speech_to_text.py:647
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
646	
647	SPEECH_TO_TEXT_START_DOCSTRING = r"""
648	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
649	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
650	    etc.)
651	
652	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
653	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
654	    behavior.
655	
656	    <Tip>
657	
658	    TensorFlow models and layers in `transformers` accept two formats as input:
659	
660	    - having all inputs as keyword arguments (like PyTorch models), or
661	    - having all inputs as a list, tuple or dict in the first positional argument.
662	
663	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
664	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
665	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
666	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
667	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
668	    positional argument:
669	
670	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
671	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
672	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
673	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
674	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
675	
676	    Note that when creating models and layers with
677	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
678	    about any of this, as you can just pass inputs like you would to any other Python function!
679	
680	    </Tip>
681	
682	    Parameters:
683	        config ([`Speech2TextConfig`]):
684	            Model configuration class with all the parameters of the model. Initializing with a config file does not
685	            load the weights associated with the model, only the configuration. Check out the
686	            [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
687	"""
688	
689	
690	SPEECH_TO_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/speech_to_text/tokenization_speech_to_text.py:272
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
271	                content_spiece_model = self.sp_model.serialized_model_proto()
272	                fi.write(content_spiece_model)
273	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/speecht5/modeling_speecht5.py:1966
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1965	
1966	SPEECHT5_BASE_START_DOCSTRING = r"""
1967	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1968	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1969	    etc.)
1970	
1971	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1972	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1973	    and behavior.
1974	
1975	    Parameters:
1976	        config ([`SpeechT5Config`]):
1977	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1978	            load the weights associated with the model, only the configuration. Check out the
1979	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1980	        encoder ([`SpeechT5EncoderWithSpeechPrenet`] or [`SpeechT5EncoderWithTextPrenet`] or `None`):
1981	            The Transformer encoder module that applies the appropiate speech or text encoder prenet. If `None`,
1982	            [`SpeechT5EncoderWithoutPrenet`] will be used and the `input_values` are assumed to be hidden states.
1983	        decoder ([`SpeechT5DecoderWithSpeechPrenet`] or [`SpeechT5DecoderWithTextPrenet`] or `None`):
1984	            The Transformer decoder module that applies the appropiate speech or text decoder prenet. If `None`,
1985	            [`SpeechT5DecoderWithoutPrenet`] will be used and the `decoder_input_values` are assumed to be hidden
1986	            states.
1987	"""
1988	
1989	
1990	SPEECHT5_START_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/speecht5/modeling_speecht5.py:1990
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1989	
1990	SPEECHT5_START_DOCSTRING = r"""
1991	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1992	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1993	    etc.)
1994	
1995	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1996	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1997	    and behavior.
1998	
1999	    Parameters:
2000	        config ([`SpeechT5Config`]):
2001	            Model configuration class with all the parameters of the model. Initializing with a config file does not
2002	            load the weights associated with the model, only the configuration. Check out the
2003	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
2004	"""
2005	
2006	
2007	SPEECHT5_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/speecht5/modeling_speecht5.py:2007
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2006	
2007	SPEECHT5_INPUTS_DOCSTRING = r"""
2008	    Args:
2009	        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
2010	            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,
2011	            1]`:
2012	
2013	            - 1 for tokens that are **not masked**,
2014	            - 0 for tokens that are **masked**.
2015	
2016	            [What are attention masks?](../glossary#attention-mask)
2017	
2018	            <Tip warning={true}>
2019	
2020	            `attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask ==
2021	            True`. For all models whose processor has `config.return_attention_mask == False`, `attention_mask` should
2022	            **not** be passed to avoid degraded performance when doing batched inference. For such models
2023	            `input_values` should simply be padded with 0 and passed without `attention_mask`. Be aware that these
2024	            models also yield slightly different results depending on whether `input_values` is padded or not.
2025	
2026	            </Tip>
2027	
2028	        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
2029	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_values`. Causal mask will
2030	            also be used by default.
2031	
2032	            If you want to change padding behavior, you should read [`SpeechT5Decoder._prepare_decoder_attention_mask`]
2033	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
2034	            information on the default strategy.
2035	
2036	        head_mask (`torch.FloatTensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
2037	            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:
2038	
2039	            - 1 indicates the head is **not masked**,
2040	            - 0 indicates the head is **masked**.
2041	
2042	        decoder_head_mask (`torch.FloatTensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
2043	            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:
2044	
2045	            - 1 indicates the head is **not masked**,
2046	            - 0 indicates the head is **masked**.
2047	
2048	        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
2049	            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:
2050	
2051	            - 1 indicates the head is **not masked**,
2052	            - 0 indicates the head is **masked**.
2053	
2054	        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):
2055	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
2056	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
2057	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
2058	
2059	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
2060	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
2061	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
2062	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
2063	
2064	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
2065	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
2066	
2067	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_values` (those
2068	            that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
2069	            `decoder_input_values` of shape `(batch_size, sequence_length)`. decoder_inputs_embeds (`torch.FloatTensor`
2070	            of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*): Optionally, instead of passing
2071	            `decoder_input_values` you can choose to directly pass an embedded representation. If `past_key_values` is
2072	            used, optionally only the last `decoder_inputs_embeds` have to be input (see `past_key_values`). This is
2073	            useful if you want more control over how to convert `decoder_input_values` indices into associated vectors
2074	            than the model's internal embedding lookup matrix.
2075	
2076	        use_cache (`bool`, *optional*):
2077	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
2078	            `past_key_values`).
2079	
2080	        output_attentions (`bool`, *optional*):
2081	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
2082	            tensors for more detail.
2083	
2084	        output_hidden_states (`bool`, *optional*):
2085	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
2086	            more detail.
2087	
2088	        return_dict (`bool`, *optional*):
2089	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
2090	"""
2091	
2092	
2093	@add_start_docstrings(
2094	    "The bare SpeechT5 Encoder-Decoder Model outputting raw hidden-states without any specific pre- or post-nets.",
2095	    SPEECHT5_BASE_START_DOCSTRING,
2096	)
2097	class SpeechT5Model(SpeechT5PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/speecht5/modeling_speecht5.py:2478
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2477	        raise ValueError(
2478	            """`speaker_embeddings` must be specified. For example, you can use a speaker embeddings by following
2479	                    the code snippet provided in this link:
2480	                    https://huggingface.co/datasets/Matthijs/cmu-arctic-xvectors

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/speecht5/modeling_speecht5.py:3194
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
3193	
3194	HIFIGAN_START_DOCSTRING = r"""
3195	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
3196	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
3197	    etc.)
3198	
3199	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
3200	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
3201	    and behavior.
3202	
3203	    Parameters:
3204	        config ([`SpeechT5HifiGanConfig`]):
3205	            Model configuration class with all the parameters of the model. Initializing with a config file does not
3206	            load the weights associated with the model, only the configuration. Check out the
3207	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
3208	"""
3209	
3210	
3211	class HifiGanResidualBlock(nn.Module):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/speecht5/tokenization_speecht5.py:216
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
215	                content_spiece_model = self.sp_model.serialized_model_proto()
216	                fi.write(content_spiece_model)
217	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/splinter/modeling_splinter.py:540
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
539	
540	SPLINTER_START_DOCSTRING = r"""
541	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
542	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
543	    behavior.
544	
545	    Parameters:
546	        config ([`SplinterConfig`]): Model configuration class with all the parameters of the model.
547	            Initializing with a config file does not load the weights associated with the model, only the
548	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
549	"""
550	
551	SPLINTER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/splinter/tokenization_splinter.py:296
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
295	                    index = token_index
296	                writer.write(token + "\n")
297	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/squeezebert/modeling_squeezebert.py:446
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
445	
446	SQUEEZEBERT_START_DOCSTRING = r"""
447	
448	    The SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural
449	    networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W.
450	    Keutzer
451	
452	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
453	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
454	    etc.)
455	
456	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
457	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
458	    and behavior.
459	
460	    For best results finetuning SqueezeBERT on text classification tasks, it is recommended to use the
461	    *squeezebert/squeezebert-mnli-headless* checkpoint as a starting point.
462	
463	    Parameters:
464	        config ([`SqueezeBertConfig`]): Model configuration class with all the parameters of the model.
465	            Initializing with a config file does not load the weights associated with the model, only the
466	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
467	
468	    Hierarchy:
469	
470	    ```
471	    Internal class hierarchy:
472	    SqueezeBertModel
473	        SqueezeBertEncoder
474	            SqueezeBertModule
475	            SqueezeBertSelfAttention
476	                ConvActivation
477	                ConvDropoutLayerNorm
478	    ```
479	
480	    Data layouts:
481	
482	    ```
483	    Input data is in [batch, sequence_length, hidden_size] format.
484	
485	    Data inside the encoder is in [batch, hidden_size, sequence_length] format. But, if `output_hidden_states == True`, the data from inside the encoder is returned in [batch, sequence_length, hidden_size] format.
486	
487	    The final output of the encoder is in [batch, sequence_length, hidden_size] format.
488	    ```
489	"""
490	
491	SQUEEZEBERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/squeezebert/tokenization_squeezebert.py:287
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
286	                    index = token_index
287	                writer.write(token + "\n")
288	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/stablelm/modeling_stablelm.py:626
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
625	
626	STABLELM_START_DOCSTRING = r"""
627	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
628	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
629	    etc.)
630	
631	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
632	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
633	    and behavior.
634	
635	    Parameters:
636	        config ([`StableLmConfig`]):
637	            Model configuration class with all the parameters of the model. Initializing with a config file does not
638	            load the weights associated with the model, only the configuration. Check out the
639	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
640	"""
641	
642	
643	@add_start_docstrings(
644	    "The bare StableLm Model outputting raw hidden-states without any specific head on top.",
645	    STABLELM_START_DOCSTRING,
646	)
647	class StableLmPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/stablelm/modeling_stablelm.py:671
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
670	
671	STABLELM_INPUTS_DOCSTRING = r"""
672	    Args:
673	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
674	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
675	            it.
676	
677	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
678	            [`PreTrainedTokenizer.__call__`] for details.
679	
680	            [What are input IDs?](../glossary#input-ids)
681	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
682	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
683	
684	            - 1 for tokens that are **not masked**,
685	            - 0 for tokens that are **masked**.
686	
687	            [What are attention masks?](../glossary#attention-mask)
688	
689	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
690	            [`PreTrainedTokenizer.__call__`] for details.
691	
692	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
693	            `past_key_values`).
694	
695	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
696	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
697	            information on the default strategy.
698	
699	            - 1 indicates the head is **not masked**,
700	            - 0 indicates the head is **masked**.
701	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
702	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
703	            config.n_positions - 1]`.
704	
705	            [What are position IDs?](../glossary#position-ids)
706	        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
707	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
708	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
709	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
710	
711	            Two formats are allowed:
712	            - a [`~cache_utils.Cache`] instance, see our
713	            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
714	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
715	            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
716	            cache format.
717	
718	            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
719	            legacy cache format will be returned.
720	
721	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
722	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
723	            of shape `(batch_size, sequence_length)`.
724	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
725	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
726	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
727	            model's internal embedding lookup matrix.
728	        use_cache (`bool`, *optional*):
729	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
730	            `past_key_values`).
731	        output_attentions (`bool`, *optional*):
732	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
733	            tensors for more detail.
734	        output_hidden_states (`bool`, *optional*):
735	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
736	            more detail.
737	        return_dict (`bool`, *optional*):
738	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
739	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
740	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
741	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
742	            the complete sequence length.
743	"""
744	
745	
746	@add_start_docstrings(
747	    "The bare StableLm Model outputting raw hidden-states without any specific head on top.",
748	    STABLELM_START_DOCSTRING,
749	)
750	class StableLmModel(StableLmPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/stablelm/modeling_stablelm.py:820
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
819	                logger.warning_once(
820	                    "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and "
821	                    "will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class "
822	                    "(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/starcoder2/modeling_starcoder2.py:313
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
312	
313	STARCODER2_START_DOCSTRING = r"""
314	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
315	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
316	    etc.)
317	
318	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
319	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
320	    and behavior.
321	
322	    Parameters:
323	        config ([`Starcoder2Config`]):
324	            Model configuration class with all the parameters of the model. Initializing with a config file does not
325	            load the weights associated with the model, only the configuration. Check out the
326	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
327	"""
328	
329	
330	@add_start_docstrings(
331	    "The bare Starcoder2 Model outputting raw hidden-states without any specific head on top.",
332	    STARCODER2_START_DOCSTRING,
333	)
334	class Starcoder2PreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/starcoder2/modeling_starcoder2.py:360
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
359	
360	STARCODER2_INPUTS_DOCSTRING = r"""
361	    Args:
362	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
363	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
364	            it.
365	
366	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
367	            [`PreTrainedTokenizer.__call__`] for details.
368	
369	            [What are input IDs?](../glossary#input-ids)
370	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
371	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
372	
373	            - 1 for tokens that are **not masked**,
374	            - 0 for tokens that are **masked**.
375	
376	            [What are attention masks?](../glossary#attention-mask)
377	
378	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
379	            [`PreTrainedTokenizer.__call__`] for details.
380	
381	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
382	            `past_key_values`).
383	
384	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
385	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
386	            information on the default strategy.
387	
388	            - 1 indicates the head is **not masked**,
389	            - 0 indicates the head is **masked**.
390	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
391	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
392	            config.n_positions - 1]`.
393	
394	            [What are position IDs?](../glossary#position-ids)
395	        past_key_values (`Cache`, *optional*):
396	            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
397	            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
398	            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.
399	
400	            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).
401	
402	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
403	            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
404	            of shape `(batch_size, sequence_length)`.
405	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
406	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
407	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
408	            model's internal embedding lookup matrix.
409	        use_cache (`bool`, *optional*):
410	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
411	            `past_key_values`).
412	        output_attentions (`bool`, *optional*):
413	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
414	            tensors for more detail.
415	        output_hidden_states (`bool`, *optional*):
416	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
417	            more detail.
418	        return_dict (`bool`, *optional*):
419	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
420	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
421	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
422	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
423	            the complete sequence length.
424	"""
425	
426	
427	@add_start_docstrings(
428	    "The bare Starcoder2 Model outputting raw hidden-states without any specific head on top.",
429	    STARCODER2_START_DOCSTRING,
430	)
431	class Starcoder2Model(Starcoder2PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/superglue/modeling_superglue.py:572
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
571	
572	SUPERGLUE_START_DOCSTRING = r"""
573	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
574	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
575	    behavior.
576	
577	    Parameters:
578	        config ([`SuperGlueConfig`]): Model configuration class with all the parameters of the model.
579	            Initializing with a config file does not load the weights associated with the model, only the
580	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
581	    """
582	
583	SUPERGLUE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/superpoint/modeling_superpoint.py:366
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
365	
366	SUPERPOINT_START_DOCSTRING = r"""
367	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
368	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
369	    behavior.
370	
371	    Parameters:
372	        config ([`SuperPointConfig`]): Model configuration class with all the parameters of the model.
373	            Initializing with a config file does not load the weights associated with the model, only the
374	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
375	    """
376	
377	SUPERPOINT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/swiftformer/modeling_swiftformer.py:431
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
430	
431	SWIFTFORMER_START_DOCSTRING = r"""
432	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
433	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
434	    behavior.
435	
436	    Parameters:
437	        config ([`SwiftFormerConfig`]): Model configuration class with all the parameters of the model.
438	            Initializing with a config file does not load the weights associated with the model, only the
439	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
440	"""
441	
442	SWIFTFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/swiftformer/modeling_tf_swiftformer.py:594
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
593	
594	TFSWIFTFORMER_START_DOCSTRING = r"""
595	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
596	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
597	    etc.)
598	
599	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
600	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
601	    behavior.
602	
603	    <Tip>
604	
605	    TF 2.0 models accepts two formats as inputs:
606	    - having all inputs as keyword arguments (like PyTorch models), or
607	    - having all inputs as a list, tuple or dict in the first positional arguments.
608	    This second option is useful when using [`keras.Model.fit`] method which currently requires having all the
609	    tensors in the first argument of the model call function: `model(inputs)`.
610	    If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
611	    first positional argument :
612	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
613	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
614	      `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
615	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
616	      `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
617	
618	    </Tip>
619	
620	    Parameters:
621	        config ([`SwiftFormerConfig`]): Model configuration class with all the parameters of the model.
622	            Initializing with a config file does not load the weights associated with the model, only the
623	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
624	"""
625	
626	TFSWIFTFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/swin/modeling_swin.py:955
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
954	
955	SWIN_START_DOCSTRING = r"""
956	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
957	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
958	    behavior.
959	
960	    Parameters:
961	        config ([`SwinConfig`]): Model configuration class with all the parameters of the model.
962	            Initializing with a config file does not load the weights associated with the model, only the
963	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
964	"""
965	
966	SWIN_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/swin/modeling_swin.py:1101
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1100	@add_start_docstrings(
1101	    """Swin Model with a decoder on top for masked image modeling, as proposed in [SimMIM](https://arxiv.org/abs/2111.09886).
1102	
1103	    <Tip>
1104	
1105	    Note that we provide a script to pre-train this model on custom data in our [examples
1106	    directory](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining).
1107	
1108	    </Tip>
1109	    """,
1110	    SWIN_START_DOCSTRING,
1111	)
1112	class SwinForMaskedImageModeling(SwinPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/swin/modeling_tf_swin.py:1086
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1085	
1086	SWIN_START_DOCSTRING = r"""
1087	    This model is a Tensorflow
1088	    [keras.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) sub-class. Use it as a
1089	    regular Tensorflow Module and refer to the Tensorflow documentation for all matter related to general usage and
1090	    behavior.
1091	
1092	    Parameters:
1093	        config ([`SwinConfig`]): Model configuration class with all the parameters of the model.
1094	            Initializing with a config file does not load the weights associated with the model, only the
1095	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1096	"""
1097	
1098	SWIN_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/swin/modeling_tf_swin.py:1431
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1430	@add_start_docstrings(
1431	    "Swin Model with a decoder on top for masked image modeling, as proposed in"
1432	    " [SimMIM](https://arxiv.org/abs/2111.09886).",
1433	    SWIN_START_DOCSTRING,
1434	)
1435	class TFSwinForMaskedImageModeling(TFSwinPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/swin2sr/modeling_swin2sr.py:773
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
772	
773	SWIN2SR_START_DOCSTRING = r"""
774	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
775	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
776	    behavior.
777	
778	    Parameters:
779	        config ([`Swin2SRConfig`]): Model configuration class with all the parameters of the model.
780	            Initializing with a config file does not load the weights associated with the model, only the
781	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
782	"""
783	
784	SWIN2SR_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/swinv2/modeling_swinv2.py:1011
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1010	
1011	SWINV2_START_DOCSTRING = r"""
1012	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
1013	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
1014	    behavior.
1015	
1016	    Parameters:
1017	        config ([`Swinv2Config`]): Model configuration class with all the parameters of the model.
1018	            Initializing with a config file does not load the weights associated with the model, only the
1019	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1020	"""
1021	
1022	SWINV2_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/swinv2/modeling_swinv2.py:1152
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1151	@add_start_docstrings(
1152	    """Swinv2 Model with a decoder on top for masked image modeling, as proposed in
1153	[SimMIM](https://arxiv.org/abs/2111.09886).
1154	
1155	    <Tip>
1156	
1157	    Note that we provide a script to pre-train this model on custom data in our [examples
1158	    directory](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining).
1159	
1160	    </Tip>
1161	    """,
1162	    SWINV2_START_DOCSTRING,
1163	)
1164	# Copied from transformers.models.swin.modeling_swin.SwinForMaskedImageModeling with swin->swinv2, base-simmim-window6-192->tiny-patch4-window8-256,SWIN->SWINV2,Swin->Swinv2,192->256

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py:1267
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1266	
1267	SWITCH_TRANSFORMERS_START_DOCSTRING = r"""
1268	
1269	    The SWITCH_TRANSFORMERS model was proposed in [Switch Transformers: Scaling to Trillion Parameter Models with
1270	    Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) by [William
1271	    Fedus](https://arxiv.org/search/cs?searchtype=author&query=Fedus%2C+W), [Barret
1272	    Zoph](https://arxiv.org/search/cs?searchtype=author&query=Zoph%2C+B), and [Noam
1273	    Shazeer](https://arxiv.org/search/cs?searchtype=author&query=Shazeer%2C+N). It's an encoder-decoder T5-like model
1274	    with sparse Feed Forward that stands for Mixture of Experts (MoE) architecture.
1275	
1276	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1277	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1278	    etc.)
1279	
1280	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1281	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1282	    and behavior.
1283	
1284	    Parameters:
1285	        config ([`SwitchTransformersConfig`]): Model configuration class with all the parameters of the model.
1286	            Initializing with a config file does not load the weights associated with the model, only the
1287	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1288	"""
1289	
1290	SWITCH_TRANSFORMERS_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/t5/modeling_flax_t5.py:826
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
825	
826	T5_DECODE_INPUTS_DOCSTRING = r"""
827	    Args:
828	        decoder_input_ids (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`):
829	            Indices of decoder input sequence tokens in the vocabulary.
830	
831	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
832	            [`PreTrainedTokenizer.__call__`] for details.
833	
834	            [What are decoder input IDs?](../glossary#decoder-input-ids)
835	
836	            For training, `decoder_input_ids` should be provided.
837	        encoder_outputs (`tuple(tuple(jnp.ndarray)`):
838	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
839	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
840	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
841	        encoder_attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
842	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
843	
844	            - 1 for tokens that are **not masked**,
845	            - 0 for tokens that are **masked**.
846	
847	            [What are attention masks?](../glossary#attention-mask)
848	        decoder_attention_mask (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
849	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
850	            be used by default.
851	
852	            If you want to change padding behavior, you should modify to your needs. See diagram 1 in [the
853	            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
854	        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):
855	            Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
856	            auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.
857	        output_attentions (`bool`, *optional*):
858	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
859	            tensors for more detail.
860	        output_hidden_states (`bool`, *optional*):
861	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
862	            more detail.
863	        return_dict (`bool`, *optional*):
864	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
865	"""
866	
867	
868	T5_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/t5/modeling_flax_t5.py:1233
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1232	
1233	T5_START_DOCSTRING = r"""
1234	    The T5 model was proposed in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text
1235	    Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
1236	    Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. It's an encoder decoder transformer pre-trained in a
1237	    text-to-text denoising generative setting.
1238	
1239	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
1240	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1241	    etc.)
1242	
1243	    This model is also a Flax Linen
1244	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
1245	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
1246	
1247	    Finally, this model supports inherent JAX features such as:
1248	
1249	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
1250	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
1251	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
1252	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
1253	
1254	    Parameters:
1255	        config ([`T5Config`]): Model configuration class with all the parameters of the model.
1256	            Initializing with a config file does not load the weights associated with the model, only the
1257	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
1258	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
1259	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
1260	            `jax.numpy.bfloat16` (on TPUs).
1261	
1262	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
1263	            specified all the computation will be performed with the given `dtype`.
1264	
1265	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
1266	            parameters.**
1267	
1268	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
1269	            [`~FlaxPreTrainedModel.to_bf16`].
1270	"""
1271	
1272	
1273	@add_start_docstrings(
1274	    "The bare T5 Model transformer outputting raw hidden-stateswithout any specific head on top.",
1275	    T5_START_DOCSTRING,
1276	)
1277	class FlaxT5Module(nn.Module):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/t5/modeling_t5.py:87
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
86	        logger.error(
87	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
88	            "https://www.tensorflow.org/install/ for installation instructions."
89	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/t5/modeling_t5.py:1336
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1335	
1336	T5_START_DOCSTRING = r"""
1337	
1338	    The T5 model was proposed in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text
1339	    Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
1340	    Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. It's an encoder decoder transformer pre-trained in a
1341	    text-to-text denoising generative setting.
1342	
1343	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1344	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1345	    etc.)
1346	
1347	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1348	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1349	    and behavior.
1350	
1351	    Parameters:
1352	        config ([`T5Config`]): Model configuration class with all the parameters of the model.
1353	            Initializing with a config file does not load the weights associated with the model, only the
1354	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1355	"""
1356	
1357	T5_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/t5/modeling_tf_t5.py:990
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
989	
990	T5_START_DOCSTRING = r"""
991	
992	    The T5 model was proposed in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text
993	    Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
994	    Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. It's an encoder decoder transformer pre-trained in a
995	    text-to-text denoising generative setting.
996	
997	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
998	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
999	    etc.)
1000	
1001	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
1002	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
1003	    behavior.
1004	
1005	    <Tip>
1006	
1007	    TensorFlow models and layers in `transformers` accept two formats as input:
1008	
1009	    - having all inputs as keyword arguments (like PyTorch models), or
1010	    - having all inputs as a list, tuple or dict in the first positional argument.
1011	
1012	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
1013	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
1014	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
1015	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
1016	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
1017	    positional argument:
1018	
1019	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
1020	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
1021	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
1022	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
1023	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
1024	
1025	    Note that when creating models and layers with
1026	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
1027	    about any of this, as you can just pass inputs like you would to any other Python function!
1028	
1029	    </Tip>
1030	
1031	    Parameters:
1032	        config ([`T5Config`]): Model configuration class with all the parameters of the model.
1033	            Initializing with a config file does not load the weights associated with the model, only the
1034	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1035	"""
1036	
1037	T5_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/t5/tokenization_t5.py:175
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
174	            logger.warning_once(
175	                f"You are using the default legacy behaviour of the {self.__class__}. This is"
176	                " expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you."

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/t5/tokenization_t5.py:209
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
208	        with open(self.vocab_file, "rb") as f:
209	            sp_model = f.read()
210	            model_pb2 = import_protobuf(f"The new behaviour of {self.__class__.__name__} (with `self.legacy = False`)")

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/t5/tokenization_t5.py:445
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
444	                content_spiece_model = self.sp_model.serialized_model_proto()
445	                fi.write(content_spiece_model)
446	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/table_transformer/modeling_table_transformer.py:761
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
760	
761	TABLE_TRANSFORMER_START_DOCSTRING = r"""
762	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
763	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
764	    etc.)
765	
766	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
767	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
768	    and behavior.
769	
770	    Parameters:
771	        config ([`TableTransformerConfig`]):
772	            Model configuration class with all the parameters of the model. Initializing with a config file does not
773	            load the weights associated with the model, only the configuration. Check out the
774	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
775	"""
776	
777	TABLE_TRANSFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/tapas/modeling_tapas.py:100
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
99	        logger.error(
100	            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
101	            "https://www.tensorflow.org/install/ for installation instructions."
102	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/tapas/modeling_tapas.py:742
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
741	
742	TAPAS_START_DOCSTRING = r"""
743	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
744	    library implements for all its models (such as downloading or saving, resizing the input embeddings, pruning heads
745	    etc.)
746	
747	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
748	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
749	    and behavior.
750	
751	    Parameters:
752	        config ([`TapasConfig`]): Model configuration class with all the parameters of the model.
753	            Initializing with a config file does not load the weights associated with the model, only the
754	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
755	"""
756	
757	TAPAS_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/tapas/modeling_tf_tapas.py:68
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
67	        logger.error(
68	            "TAPAS models are not usable since `tensorflow_probability` can't be loaded. "
69	            "It seems you have `tensorflow_probability` installed with the wrong tensorflow version. "
70	            "Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/tapas/modeling_tf_tapas.py:986
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
985	
986	TAPAS_START_DOCSTRING = r"""
987	
988	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
989	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
990	    etc.)
991	
992	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
993	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
994	    behavior.
995	
996	    <Tip>
997	
998	    TensorFlow models and layers in `transformers` accept two formats as input:
999	
1000	    - having all inputs as keyword arguments (like PyTorch models), or
1001	    - having all inputs as a list, tuple or dict in the first positional argument.
1002	
1003	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
1004	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
1005	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
1006	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
1007	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
1008	    positional argument:
1009	
1010	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
1011	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
1012	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
1013	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
1014	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
1015	
1016	    Note that when creating models and layers with
1017	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
1018	    about any of this, as you can just pass inputs like you would to any other Python function!
1019	
1020	    </Tip>
1021	
1022	    Parameters:
1023	        config ([`TapasConfig`]): Model configuration class with all the parameters of the model.
1024	            Initializing with a config file does not load the weights associated with the model, only the
1025	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1026	"""
1027	
1028	TAPAS_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/tapas/tokenization_tapas.py:388
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
387	                    index = token_index
388	                writer.write(token + "\n")
389	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/textnet/modeling_textnet.py:227
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
226	
227	TEXTNET_START_DOCSTRING = r"""
228	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
229	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
230	    behavior.
231	
232	    Parameters:
233	        config ([`TextNetConfig`]): Model configuration class with all the parameters of the model.
234	            Initializing with a config file does not load the weights associated with the model, only the
235	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
236	"""
237	
238	TEXTNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/time_series_transformer/modeling_time_series_transformer.py:648
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
647	
648	TIME_SERIES_TRANSFORMER_START_DOCSTRING = r"""
649	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
650	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
651	    etc.)
652	
653	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
654	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
655	    and behavior.
656	
657	    Parameters:
658	        config ([`TimeSeriesTransformerConfig`]):
659	            Model configuration class with all the parameters of the model. Initializing with a config file does not
660	            load the weights associated with the model, only the configuration. Check out the
661	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
662	"""
663	
664	TIME_SERIES_TRANSFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/timesformer/modeling_timesformer.py:487
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
486	
487	TIMESFORMER_START_DOCSTRING = r"""
488	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
489	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
490	    behavior.
491	
492	    Parameters:
493	        config ([`TimesformerConfig`]): Model configuration class with all the parameters of the model.
494	            Initializing with a config file does not load the weights associated with the model, only the
495	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
496	"""
497	
498	TIMESFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/trocr/modeling_trocr.py:435
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
434	
435	TROCR_START_DOCSTRING = r"""
436	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
437	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
438	    etc.)
439	
440	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
441	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
442	    and behavior.
443	
444	    Parameters:
445	        config ([`TrOCRConfig`]):
446	            Model configuration class with all the parameters of the model. Initializing with a config file does not
447	            load the weights associated with the model, only the configuration. Check out the
448	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
449	"""
450	
451	
452	class TrOCRDecoder(TrOCRPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/tvp/modeling_tvp.py:593
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
592	
593	TVP_START_DOCSTRING = r"""
594	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
595	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
596	    behavior.
597	
598	    Parameters:
599	        config ([`TvpConfig`]): Model configuration class with all the parameters of the model.
600	            Initializing with a config file does not load the weights associated with the model, only the
601	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
602	"""
603	
604	TVP_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/udop/modeling_udop.py:64
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
63	
64	UDOP_START_DOCSTRING = r"""
65	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
66	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
67	    etc.)
68	
69	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
70	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
71	    and behavior.
72	
73	    Args:
74	        config ([`UdopConfig`]): Model configuration class with all the parameters of the model.
75	            Initializing with a config file does not load the weights associated with the model, only the
76	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
77	"""
78	
79	
80	UDOP_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/udop/tokenization_udop.py:503
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
502	                content_spiece_model = self.sp_model.serialized_model_proto()
503	                fi.write(content_spiece_model)
504	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/udop/tokenization_udop.py:1042
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1041	            raise NotImplementedError(
1042	                "return_offset_mapping is not available when using Python tokenizers. "
1043	                "To use this feature, change your tokenizer to one deriving from "
1044	                "transformers.PreTrainedTokenizerFast. "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/umt5/modeling_umt5.py:979
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
978	
979	UMT5_START_DOCSTRING = r"""
980	
981	    The UMT5 model was proposed in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text
982	    Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
983	    Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. It's an encoder decoder transformer pre-trained in a
984	    text-to-text denoising generative setting.
985	
986	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
987	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
988	    etc.)
989	
990	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
991	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
992	    and behavior.
993	
994	    Parameters:
995	        config ([`UMT5Config`]): Model configuration class with all the parameters of the model.
996	            Initializing with a config file does not load the weights associated with the model, only the
997	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
998	"""
999	
1000	UMT5_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/unispeech/modeling_unispeech.py:1247
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1246	
1247	UNISPEECH_START_DOCSTRING = r"""
1248	    UniSpeech was proposed in [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled
1249	    Data](https://arxiv.org/abs/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
1250	    Michael Zeng, Xuedong Huang.
1251	
1252	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1253	    library implements for all its model (such as downloading or saving etc.).
1254	
1255	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
1256	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
1257	    behavior.
1258	
1259	    Parameters:
1260	        config ([`UniSpeechConfig`]): Model configuration class with all the parameters of the model.
1261	            Initializing with a config file does not load the weights associated with the model, only the
1262	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1263	"""
1264	
1265	UNISPEECH_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/unispeech/modular_unispeech.py:227
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
226	
227	UNISPEECH_START_DOCSTRING = r"""
228	    UniSpeech was proposed in [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled
229	    Data](https://arxiv.org/abs/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
230	    Michael Zeng, Xuedong Huang.
231	
232	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
233	    library implements for all its model (such as downloading or saving etc.).
234	
235	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
236	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
237	    behavior.
238	
239	    Parameters:
240	        config ([`UniSpeechConfig`]): Model configuration class with all the parameters of the model.
241	            Initializing with a config file does not load the weights associated with the model, only the
242	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
243	"""
244	
245	UNISPEECH_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py:1251
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1250	
1251	UNISPEECH_SAT_START_DOCSTRING = r"""
1252	    UniSpeechSat was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech
1253	    Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael
1254	    Auli.
1255	
1256	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1257	    library implements for all its model (such as downloading or saving etc.).
1258	
1259	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
1260	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
1261	    behavior.
1262	
1263	    Parameters:
1264	        config ([`UniSpeechSatConfig`]): Model configuration class with all the parameters of the model.
1265	            Initializing with a config file does not load the weights associated with the model, only the
1266	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1267	"""
1268	
1269	UNISPEECH_SAT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/unispeech_sat/modular_unispeech_sat.py:249
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
248	
249	UNISPEECH_SAT_START_DOCSTRING = r"""
250	    UniSpeechSat was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech
251	    Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael
252	    Auli.
253	
254	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
255	    library implements for all its model (such as downloading or saving etc.).
256	
257	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
258	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
259	    behavior.
260	
261	    Parameters:
262	        config ([`UniSpeechSatConfig`]): Model configuration class with all the parameters of the model.
263	            Initializing with a config file does not load the weights associated with the model, only the
264	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
265	"""
266	
267	UNISPEECH_SAT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/univnet/modeling_univnet.py:429
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
428	
429	UNIVNET_START_DOCSTRING = r"""
430	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
431	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
432	    etc.)
433	
434	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
435	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
436	    and behavior.
437	
438	    Parameters:
439	        config ([`UnivNetConfig`]):
440	            Model configuration class with all the parameters of the model. Initializing with a config file does not
441	            load the weights associated with the model, only the configuration. Check out the
442	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
443	"""
444	
445	
446	UNIVNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/univnet/modeling_univnet.py:446
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
445	
446	UNIVNET_INPUTS_DOCSTRING = r"""
447	    Converts a noise waveform and a conditioning spectrogram to a speech waveform. Passing a batch of log-mel
448	    spectrograms returns a batch of speech waveforms. Passing a single, un-batched log-mel spectrogram returns a
449	    single, un-batched speech waveform.
450	
451	    Args:
452	        input_features (`torch.FloatTensor`):
453	            Tensor containing the log-mel spectrograms. Can be batched and of shape `(batch_size, sequence_length,
454	            config.num_mel_channels)`, or un-batched and of shape `(sequence_length, config.num_mel_channels)`.
455	        noise_sequence (`torch.FloatTensor`, *optional*):
456	            Tensor containing a noise sequence of standard Gaussian noise. Can be batched and of shape `(batch_size,
457	            sequence_length, config.model_in_channels)`, or un-batched and of shape (sequence_length,
458	            config.model_in_channels)`. If not supplied, will be randomly generated.
459	        padding_mask (`torch.BoolTensor`, *optional*):
460	            Mask indicating which parts of each sequence are padded. Mask values are selected in `[0, 1]`:
461	
462	            - 1 for tokens that are **not masked**
463	            - 0 for tokens that are **masked**
464	
465	            The mask can be batched and of shape `(batch_size, sequence_length)` or un-batched and of shape
466	            `(sequence_length,)`.
467	        generator (`torch.Generator`, *optional*):
468	            A [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation
469	            deterministic.
470	        return_dict:
471	            Whether to return a [`~utils.ModelOutput`] subclass instead of a plain tuple.
472	"""
473	
474	
475	@add_start_docstrings(
476	    """UnivNet GAN vocoder.""",
477	    UNIVNET_START_DOCSTRING,
478	)
479	class UnivNetModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/upernet/modeling_upernet.py:313
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
312	
313	UPERNET_START_DOCSTRING = r"""
314	    Parameters:
315	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
316	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
317	    behavior.
318	        config ([`UperNetConfig`]): Model configuration class with all the parameters of the model.
319	            Initializing with a config file does not load the weights associated with the model, only the
320	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
321	"""
322	
323	UPERNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/video_llava/modeling_video_llava.py:112
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
111	
112	VIDEO_LLAVA_START_DOCSTRING = r"""
113	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
114	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
115	    etc.)
116	
117	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
118	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
119	    and behavior.
120	
121	    Parameters:
122	        config ([`VideoLlavaConfig`] or [`VideoLlavaVisionConfig`]):
123	            Model configuration class with all the parameters of the model. Initializing with a config file does not
124	            load the weights associated with the model, only the configuration. Check out the
125	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
126	"""
127	
128	
129	@add_start_docstrings(
130	    VIDEO_LLAVA_START_DOCSTRING,
131	)
132	class VideoLlavaPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/video_llava/modeling_video_llava.py:164
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
163	
164	VIDEO_LLAVA_INPUTS_DOCSTRING = r"""
165	    Args:
166	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
167	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
168	            it.
169	
170	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
171	            [`PreTrainedTokenizer.__call__`] for details.
172	
173	            [What are input IDs?](../glossary#input-ids)
174	        pixel_values_images (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):
175	            The tensors corresponding to the input images. Pixel values can be obtained using
176	            [`AutoImageProcessor`]. See [`VideoLlavaImageProcessor.__call__`] for details ([]`LlavaProcessor`] uses
177	            [`VideoLlavaImageProcessor`] for processing images).
178	        pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, image_size, image_size)):
179	            The tensors corresponding to the input video. Pixel values can be obtained using
180	            [`AutoImageProcessor`]. See [`VideoLlavaImageProcessor.__call__`] for details ([]`LlavaProcessor`] uses
181	            [`VideoLlavaImageProcessor`] for processing videos).
182	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
183	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
184	
185	            - 1 for tokens that are **not masked**,
186	            - 0 for tokens that are **masked**.
187	
188	            [What are attention masks?](../glossary#attention-mask)
189	
190	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
191	            [`PreTrainedTokenizer.__call__`] for details.
192	
193	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
194	            `past_key_values`).
195	
196	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
197	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
198	            information on the default strategy.
199	
200	            - 1 indicates the head is **not masked**,
201	            - 0 indicates the head is **masked**.
202	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
203	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
204	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
205	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
206	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
207	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
208	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
209	
210	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
211	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
212	
213	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
214	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
215	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
216	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
217	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
218	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
219	            model's internal embedding lookup matrix.
220	        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):
221	            The index of the layer to select the vision feature. If multiple indices are provided,
222	            the vision feature of the corresponding indices will be concatenated to form the
223	            vision features.
224	        vision_feature_select_strategy (`str`, *optional*, defaults to `"default"`):
225	            The feature selection strategy used to select the vision feature from the vision backbone.
226	            Can be one of `"default"` or `"full"`
227	        use_cache (`bool`, *optional*):
228	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
229	            `past_key_values`).
230	        output_attentions (`bool`, *optional*):
231	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
232	            tensors for more detail.
233	        output_hidden_states (`bool`, *optional*):
234	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
235	            more detail.
236	        return_dict (`bool`, *optional*):
237	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
238	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
239	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
240	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
241	            the complete sequence length.
242	"""
243	
244	
245	@add_start_docstrings(
246	    """The VideoLlava model which consists of a vision backbone and a language model.""",
247	    VIDEO_LLAVA_START_DOCSTRING,
248	)
249	class VideoLlavaForConditionalGeneration(VideoLlavaPreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/videomae/modeling_videomae.py:517
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
516	
517	VIDEOMAE_START_DOCSTRING = r"""
518	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
519	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
520	    behavior.
521	
522	    Parameters:
523	        config ([`VideoMAEConfig`]): Model configuration class with all the parameters of the model.
524	            Initializing with a config file does not load the weights associated with the model, only the
525	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
526	"""
527	
528	VIDEOMAE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vilt/modeling_vilt.py:585
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
584	
585	VILT_START_DOCSTRING = r"""
586	    This model is a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ subclass. Use
587	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
588	    behavior.
589	
590	    Parameters:
591	        config ([`ViltConfig`]): Model configuration class with all the parameters of the model.
592	            Initializing with a config file does not load the weights associated with the model, only the
593	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
594	"""
595	
596	VILT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vipllava/modeling_vipllava.py:110
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
109	
110	VIPLLAVA_START_DOCSTRING = r"""
111	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
112	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
113	    etc.)
114	
115	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
116	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
117	    and behavior.
118	
119	    Parameters:
120	        config ([`VipLlavaConfig`] or [`VipLlavaVisionConfig`]):
121	            Model configuration class with all the parameters of the model. Initializing with a config file does not
122	            load the weights associated with the model, only the configuration. Check out the
123	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
124	"""
125	
126	
127	@add_start_docstrings(
128	    "The bare VipLlava Model outputting raw hidden-states without any specific head on top.",
129	    VIPLLAVA_START_DOCSTRING,
130	)
131	# Copied from transformers.models.llava.modeling_llava.LlavaPreTrainedModel with Llava->VipLlava,llava->vipllava
132	class VipLlavaPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vipllava/modeling_vipllava.py:167
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
166	
167	VIPLLAVA_INPUTS_DOCSTRING = r"""
168	    Args:
169	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
170	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
171	            it.
172	
173	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
174	            [`PreTrainedTokenizer.__call__`] for details.
175	
176	            [What are input IDs?](../glossary#input-ids)
177	        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):
178	            The tensors corresponding to the input images. Pixel values can be obtained using
179	            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`LlavaProcessor`] uses
180	            [`CLIPImageProcessor`] for processing images).
181	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
182	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
183	
184	            - 1 for tokens that are **not masked**,
185	            - 0 for tokens that are **masked**.
186	
187	            [What are attention masks?](../glossary#attention-mask)
188	
189	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
190	            [`PreTrainedTokenizer.__call__`] for details.
191	
192	            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
193	            `past_key_values`).
194	
195	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
196	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
197	            information on the default strategy.
198	
199	            - 1 indicates the head is **not masked**,
200	            - 0 indicates the head is **masked**.
201	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
202	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
203	            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
204	        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
205	            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
206	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
207	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
208	
209	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
210	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
211	
212	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
213	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
214	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
215	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
216	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
217	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
218	            model's internal embedding lookup matrix.
219	        use_cache (`bool`, *optional*):
220	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
221	            `past_key_values`).
222	        output_attentions (`bool`, *optional*):
223	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
224	            tensors for more detail.
225	        output_hidden_states (`bool`, *optional*):
226	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
227	            more detail.
228	        return_dict (`bool`, *optional*):
229	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
230	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
231	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
232	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
233	            the complete sequence length.
234	"""
235	
236	
237	@add_start_docstrings(
238	    """The VIPLLAVA model which consists of a vision backbone and a language model.""",
239	    VIPLLAVA_START_DOCSTRING,
240	)
241	# Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration with LLAVA->VIPLLAVA,Llava->VipLlava
242	class VipLlavaForConditionalGeneration(VipLlavaPreTrainedModel, GenerationMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py:40
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
39	
40	VISION_ENCODER_DECODER_START_DOCSTRING = r"""
41	    This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
42	    as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
43	    [`~AutoModel.from_pretrained`] function and the decoder is loaded via [`~AutoModelForCausalLM.from_pretrained`]
44	    function. Cross-attention layers are automatically added to the decoder and should be fine-tuned on a downstream
45	    generative task, like image captioning.
46	
47	    The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
48	    tasks was shown in [Leveraging Pre-trained Checkpoints for Sequence Generation
49	    Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
50	    Zhou, Wei Li, Peter J. Liu.
51	
52	    Additionally, in [TrOCR: Transformer-based Optical Character Recognition with Pre-trained
53	    Models](https://arxiv.org/abs/2109.10282) it is shown how leveraging large pretrained vision models for optical
54	    character recognition (OCR) yields a significant performance improvement.
55	
56	    After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
57	    other models (see the examples for more information).
58	
59	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
60	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
61	    etc.)
62	
63	    This model is also a Flax Linen
64	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
65	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
66	
67	    Parameters:
68	        config ([`VisionEncoderDecoderConfig`]): Model configuration class with all the parameters of the model.
69	            Initializing with a config file does not load the weights associated with the model, only the
70	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
71	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
72	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
73	            `jax.numpy.bfloat16` (on TPUs).
74	
75	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
76	            specified all the computation will be performed with the given `dtype`.
77	
78	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
79	            parameters.**
80	
81	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
82	            [`~FlaxPreTrainedModel.to_bf16`].
83	"""
84	
85	VISION_ENCODER_DECODER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py:53
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
52	
53	VISION_ENCODER_DECODER_START_DOCSTRING = r"""
54	    This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
55	    as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
56	    [`~TFAutoModel.from_pretrained`] function and the decoder is loaded via [`~TFAutoModelForCausalLM.from_pretrained`]
57	    function. Cross-attention layers are automatically added to the decoder and should be fine-tuned on a downstream
58	    generative task, like image captioning.
59	
60	    The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
61	    tasks was shown in [Leveraging Pre-trained Checkpoints for Sequence Generation
62	    Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
63	    Zhou, Wei Li, Peter J. Liu.
64	
65	    Additionally, in [TrOCR: Transformer-based Optical Character Recognition with Pre-trained
66	    Models](https://arxiv.org/abs/2109.10282) it is shown how leveraging large pretrained vision models for optical
67	    character recognition (OCR) yields a significant performance improvement.
68	
69	    After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
70	    other models (see the examples for more information).
71	
72	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
73	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
74	    etc.)
75	
76	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
77	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
78	    behavior.
79	
80	    Parameters:
81	        config ([`VisionEncoderDecoderConfig`]): Model configuration class with all the parameters of the model.
82	            Initializing with a config file does not load the weights associated with the model, only the
83	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
84	"""
85	
86	VISION_ENCODER_DECODER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
57	
58	VISION_ENCODER_DECODER_START_DOCSTRING = r"""
59	    This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
60	    as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
61	    [`~AutoModel.from_pretrained`] function and the decoder is loaded via [`~AutoModelForCausalLM.from_pretrained`]
62	    function. Cross-attention layers are automatically added to the decoder and should be fine-tuned on a downstream
63	    generative task, like image captioning.
64	
65	    The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
66	    tasks was shown in [Leveraging Pre-trained Checkpoints for Sequence Generation
67	    Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
68	    Zhou, Wei Li, Peter J. Liu.
69	
70	    Additionally, in [TrOCR: Transformer-based Optical Character Recognition with Pre-trained
71	    Models](https://arxiv.org/abs/2109.10282) it is shown how leveraging large pretrained vision models for optical
72	    character recognition (OCR) yields a significant performance improvement.
73	
74	    After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
75	    other models (see the examples for more information).
76	
77	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
78	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
79	    etc.)
80	
81	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
82	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
83	    and behavior.
84	
85	    Parameters:
86	        config ([`VisionEncoderDecoderConfig`]): Model configuration class with all the parameters of the model.
87	            Initializing with a config file does not load the weights associated with the model, only the
88	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
89	"""
90	
91	VISION_ENCODER_DECODER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	
37	VISION_TEXT_DUAL_ENCODER_START_DOCSTRING = r"""
38	    This class can be used to initialize a vision-text dual encoder model with any pretrained vision autoencoding model
39	    as the vision encoder and any pretrained text model as the text encoder. The vision and text encoders are loaded
40	    via the [`~FlaxAutoModel.from_pretrained`] method. The projection layers are automatically added to the model and
41	    should be fine-tuned on a downstream task, like contrastive image-text modeling.
42	
43	    In [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991) it is shown how
44	    leveraging pre-trained (locked/frozen) image and text model for contrastive learning yields significant improvment
45	    on new zero-shot vision tasks such as image classification or retrieval.
46	
47	    After such a Vision-Text-Dual-Encoder model has been trained/fine-tuned, it can be saved/loaded just like any other
48	    models (see the examples for more information).
49	
50	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
51	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
52	    etc.)
53	
54	     This model is also a
55	     [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. Use it
56	     as a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
57	     behavior.
58	
59	    Finally, this model supports inherent JAX features such as:
60	
61	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
62	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
63	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
64	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
65	
66	    Parameters:
67	        config ([`VisionTextDualEncoderConfig`]): Model configuration class with all the parameters of the model.
68	            Initializing with a config file does not load the weights associated with the model, only the
69	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
70	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
71	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
72	            `jax.numpy.bfloat16` (on TPUs).
73	
74	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
75	            specified all the computation will be performed with the given `dtype`.
76	
77	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
78	            parameters.**
79	
80	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
81	            [`~FlaxPreTrainedModel.to_bf16`].
82	"""
83	
84	
85	VISION_TEXT_DUAL_ENCODER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py:542
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
541	
542	VISION_TEXT_DUAL_ENCODER_MODEL_DOCSTRING = r"""
543	    Returns:
544	
545	    Examples:
546	
547	    ```python
548	    >>> from PIL import Image
549	    >>> import requests
550	    >>> import jax
551	    >>> from transformers import (
552	    ...     FlaxVisionTextDualEncoderModel,
553	    ...     VisionTextDualEncoderProcessor,
554	    ...     AutoImageProcessor,
555	    ...     AutoTokenizer,
556	    ... )
557	
558	    >>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
559	    >>> image_processor = AutoImageProcesor.from_pretrained("google/vit-base-patch16-224")
560	    >>> processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)
561	    >>> model = FlaxVisionTextDualEncoderModel.from_vision_text_pretrained(
562	    ...     "google/vit-base-patch16-224", "google-bert/bert-base-uncased"
563	    ... )
564	
565	    >>> # contrastive training
566	    >>> urls = [
567	    ...     "http://images.cocodataset.org/val2017/000000039769.jpg",
568	    ...     "https://farm3.staticflickr.com/2674/5850229113_4fe05d5265_z.jpg",
569	    ... ]
570	    >>> images = [Image.open(requests.get(url, stream=True).raw) for url in urls]
571	    >>> inputs = processor(
572	    ...     text=["a photo of a cat", "a photo of a dog"], images=images, return_tensors="np", padding=True
573	    ... )
574	    >>> outputs = model(
575	    ...     input_ids=inputs.input_ids,
576	    ...     attention_mask=inputs.attention_mask,
577	    ...     pixel_values=inputs.pixel_values,
578	    ... )
579	    >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
580	
581	    >>> # save and load from pretrained
582	    >>> model.save_pretrained("vit-bert")
583	    >>> model = FlaxVisionTextDualEncoderModel.from_pretrained("vit-bert")
584	
585	    >>> # inference
586	    >>> outputs = model(**inputs)
587	    >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
588	    >>> probs = jax.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities
589	    ```
590	"""
591	
592	overwrite_call_docstring(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vision_text_dual_encoder/modeling_tf_vision_text_dual_encoder.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	
44	VISION_TEXT_DUAL_ENCODER_START_DOCSTRING = r"""
45	    This class can be used to initialize a vision-text dual encoder model with any pretrained vision autoencoding model
46	    as the vision encoder and any pretrained text model as the text encoder. The vision and text encoders are loaded
47	    via the [`~TFAutoModel.from_pretrained`] method. The projection layers are automatically added to the model and
48	    should be fine-tuned on a downstream task, like contrastive image-text modeling.
49	
50	    In [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991) it is shown how
51	    leveraging pre-trained (locked/frozen) image and text model for contrastive learning yields significant improvment
52	    on new zero-shot vision tasks such as image classification or retrieval.
53	
54	    After such a Vision-Text-Dual-Encoder model has been trained/fine-tuned, it can be saved/loaded just like any other
55	    models (see the examples for more information).
56	
57	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
58	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
59	    etc.)
60	
61	    This model is also a Keras [Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it as a
62	    regular Keras Model and refer to the TF documentation for all matter related to general usage and behavior.
63	
64	    Parameters:
65	        config ([`VisionEncoderDecoderConfig`]): Model configuration class with all the parameters of the model.
66	            Initializing with a config file does not load the weights associated with the model, only the
67	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
68	"""
69	
70	
71	VISION_TEXT_DUAL_ENCODER_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py:34
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
33	
34	VISION_TEXT_DUAL_ENCODER_START_DOCSTRING = r"""
35	    This class can be used to initialize a vision-text dual encoder model with any pretrained vision autoencoding model
36	    as the vision encoder and any pretrained text model as the text encoder. The vision and text encoders are loaded
37	    via the [`~AutoModel.from_pretrained`] method. The projection layers are automatically added to the model and
38	    should be fine-tuned on a downstream task, like contrastive image-text modeling.
39	
40	    In [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991) it is shown how
41	    leveraging pre-trained (locked/frozen) image and text model for contrastive learning yields significant improvment
42	    on new zero-shot vision tasks such as image classification or retrieval.
43	
44	    After such a Vision-Text-Dual-Encoder model has been trained/fine-tuned, it can be saved/loaded just like any other
45	    models (see the examples for more information).
46	
47	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
48	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
49	    etc.)
50	
51	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
52	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
53	    and behavior.
54	
55	    Parameters:
56	        config ([`VisionEncoderDecoderConfig`]): Model configuration class with all the parameters of the model.
57	            Initializing with a config file does not load the weights associated with the model, only the
58	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
59	"""
60	
61	
62	VISION_TEXT_DUAL_ENCODER_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/visual_bert/modeling_visual_bert.py:569
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
568	
569	VISUAL_BERT_START_DOCSTRING = r"""
570	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
571	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
572	    etc.)
573	
574	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
575	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
576	    and behavior.
577	
578	    Parameters:
579	        config ([`VisualBertConfig`]): Model configuration class with all the parameters of the model.
580	            Initializing with a config file does not load the weights associated with the model, only the
581	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
582	"""
583	
584	VISUAL_BERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vit/modeling_flax_vit.py:36
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
35	
36	VIT_START_DOCSTRING = r"""
37	
38	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
39	    library implements for all its model (such as downloading, saving and converting weights from PyTorch models)
40	
41	    This model is also a
42	    [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. Use it as
43	    a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
44	    behavior.
45	
46	    Finally, this model supports inherent JAX features such as:
47	
48	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
49	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
50	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
51	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
52	
53	    Parameters:
54	        config ([`ViTConfig`]): Model configuration class with all the parameters of the model.
55	            Initializing with a config file does not load the weights associated with the model, only the
56	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
57	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
58	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
59	            `jax.numpy.bfloat16` (on TPUs).
60	
61	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
62	            specified all the computation will be performed with the given `dtype`.
63	
64	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
65	            parameters.**
66	
67	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
68	            [`~FlaxPreTrainedModel.to_bf16`].
69	"""
70	
71	VIT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vit/modeling_flax_vit.py:561
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
560	
561	FLAX_VISION_MODEL_DOCSTRING = """
562	    Returns:
563	
564	    Examples:
565	
566	    ```python
567	    >>> from transformers import AutoImageProcessor, FlaxViTModel
568	    >>> from PIL import Image
569	    >>> import requests
570	
571	    >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
572	    >>> image = Image.open(requests.get(url, stream=True).raw)
573	
574	    >>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224-in21k")
575	    >>> model = FlaxViTModel.from_pretrained("google/vit-base-patch16-224-in21k")
576	
577	    >>> inputs = image_processor(images=image, return_tensors="np")
578	    >>> outputs = model(**inputs)
579	    >>> last_hidden_states = outputs.last_hidden_state
580	    ```
581	"""
582	
583	overwrite_call_docstring(FlaxViTModel, FLAX_VISION_MODEL_DOCSTRING)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vit/modeling_flax_vit.py:644
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
643	
644	FLAX_VISION_CLASSIF_DOCSTRING = """
645	    Returns:
646	
647	    Example:
648	
649	    ```python
650	    >>> from transformers import AutoImageProcessor, FlaxViTForImageClassification
651	    >>> from PIL import Image
652	    >>> import jax
653	    >>> import requests
654	
655	    >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
656	    >>> image = Image.open(requests.get(url, stream=True).raw)
657	
658	    >>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
659	    >>> model = FlaxViTForImageClassification.from_pretrained("google/vit-base-patch16-224")
660	
661	    >>> inputs = image_processor(images=image, return_tensors="np")
662	    >>> outputs = model(**inputs)
663	    >>> logits = outputs.logits
664	
665	    >>> # model predicts one of the 1000 ImageNet classes
666	    >>> predicted_class_idx = jax.numpy.argmax(logits, axis=-1)
667	    >>> print("Predicted class:", model.config.id2label[predicted_class_idx.item()])
668	    ```
669	"""
670	
671	overwrite_call_docstring(FlaxViTForImageClassification, FLAX_VISION_CLASSIF_DOCSTRING)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vit/modeling_tf_vit.py:663
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
662	
663	VIT_START_DOCSTRING = r"""
664	
665	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
666	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
667	    etc.)
668	
669	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
670	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
671	    behavior.
672	
673	    <Tip>
674	
675	    TensorFlow models and layers in `transformers` accept two formats as input:
676	
677	    - having all inputs as keyword arguments (like PyTorch models), or
678	    - having all inputs as a list, tuple or dict in the first positional argument.
679	
680	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
681	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
682	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
683	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
684	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
685	    positional argument:
686	
687	    - a single Tensor with `pixel_values` only and nothing else: `model(pixel_values)`
688	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
689	    `model([pixel_values, attention_mask])` or `model([pixel_values, attention_mask, token_type_ids])`
690	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
691	    `model({"pixel_values": pixel_values, "token_type_ids": token_type_ids})`
692	
693	    Note that when creating models and layers with
694	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
695	    about any of this, as you can just pass inputs like you would to any other Python function!
696	
697	    </Tip>
698	
699	    Args:
700	        config ([`ViTConfig`]): Model configuration class with all the parameters of the model.
701	            Initializing with a config file does not load the weights associated with the model, only the
702	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
703	"""
704	
705	VIT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vit/modeling_vit.py:504
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
503	
504	VIT_START_DOCSTRING = r"""
505	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
506	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
507	    behavior.
508	
509	    Parameters:
510	        config ([`ViTConfig`]): Model configuration class with all the parameters of the model.
511	            Initializing with a config file does not load the weights associated with the model, only the
512	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
513	"""
514	
515	VIT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vit/modeling_vit.py:655
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
654	@add_start_docstrings(
655	    """ViT Model with a decoder on top for masked image modeling, as proposed in [SimMIM](https://arxiv.org/abs/2111.09886).
656	
657	    <Tip>
658	
659	    Note that we provide a script to pre-train this model on custom data in our [examples
660	    directory](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining).
661	
662	    </Tip>
663	    """,
664	    VIT_START_DOCSTRING,
665	)
666	class ViTForMaskedImageModeling(ViTPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vit_mae/modeling_tf_vit_mae.py:852
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
851	
852	VIT_MAE_START_DOCSTRING = r"""
853	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
854	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
855	    etc.)
856	
857	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
858	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
859	    behavior.
860	
861	    <Tip>
862	
863	    TensorFlow models and layers in `transformers` accept two formats as input:
864	
865	    - having all inputs as keyword arguments (like PyTorch models), or
866	    - having all inputs as a list, tuple or dict in the first positional argument.
867	
868	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
869	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
870	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
871	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
872	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
873	    positional argument:
874	
875	    - a single Tensor with `pixel_values` only and nothing else: `model(pixel_values)`
876	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
877	    `model([pixel_values, attention_mask])` or `model([pixel_values, attention_mask, token_type_ids])`
878	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
879	    `model({"pixel_values": pixel_values, "token_type_ids": token_type_ids})`
880	
881	    Note that when creating models and layers with
882	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
883	    about any of this, as you can just pass inputs like you would to any other Python function!
884	
885	    </Tip>
886	
887	    Args:
888	        config ([`ViTMAEConfig`]): Model configuration class with all the parameters of the model.
889	            Initializing with a config file does not load the weights associated with the model, only the
890	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
891	"""
892	
893	VIT_MAE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vit_mae/modeling_vit_mae.py:669
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
668	
669	VIT_MAE_START_DOCSTRING = r"""
670	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
671	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
672	    behavior.
673	
674	    Parameters:
675	        config ([`ViTMAEConfig`]): Model configuration class with all the parameters of the model.
676	            Initializing with a config file does not load the weights associated with the model, only the
677	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
678	"""
679	
680	VIT_MAE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vit_mae/modeling_vit_mae.py:952
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
951	@add_start_docstrings(
952	    """The ViTMAE Model transformer with the decoder on top for self-supervised pre-training.
953	
954	    <Tip>
955	
956	    Note that we provide a script to pre-train this model on custom data in our [examples
957	    directory](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining).
958	
959	    </Tip>
960	
961	    """,
962	    VIT_MAE_START_DOCSTRING,
963	)
964	class ViTMAEForPreTraining(ViTMAEPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vit_msn/modeling_vit_msn.py:490
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
489	
490	VIT_MSN_START_DOCSTRING = r"""
491	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
492	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
493	    behavior.
494	
495	    Parameters:
496	        config ([`ViTMSNConfig`]): Model configuration class with all the parameters of the model.
497	            Initializing with a config file does not load the weights associated with the model, only the
498	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
499	"""
500	
501	VIT_MSN_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vitdet/modeling_vitdet.py:668
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
667	
668	VITDET_START_DOCSTRING = r"""
669	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
670	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
671	    behavior.
672	
673	    Parameters:
674	        config ([`VitDetConfig`]): Model configuration class with all the parameters of the model.
675	            Initializing with a config file does not load the weights associated with the model, only the
676	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
677	"""
678	
679	VITDET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vitmatte/modeling_vitmatte.py:227
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
226	
227	VITMATTE_START_DOCSTRING = r"""
228	    Parameters:
229	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
230	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
231	    behavior.
232	        config ([`UperNetConfig`]): Model configuration class with all the parameters of the model.
233	            Initializing with a config file does not load the weights associated with the model, only the
234	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
235	"""
236	
237	VITMATTE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vitpose/modeling_vitpose.py:96
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
95	
96	VITPOSE_START_DOCSTRING = r"""
97	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
98	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
99	    behavior.
100	
101	    Parameters:
102	        config ([`VitPoseConfig`]): Model configuration class with all the parameters of the model.
103	            Initializing with a config file does not load the weights associated with the model, only the
104	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
105	"""
106	
107	VITPOSE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py:451
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
450	
451	VITPOSE_BACKBONE_START_DOCSTRING = r"""
452	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
453	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
454	    behavior.
455	
456	    Parameters:
457	        config ([`VitPoseBackboneConfig`]): Model configuration class with all the parameters of the model.
458	            Initializing with a config file does not load the weights associated with the model, only the
459	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
460	"""
461	
462	VITPOSE_BACKBONE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vits/modeling_vits.py:1280
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1279	
1280	VITS_START_DOCSTRING = r"""
1281	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1282	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1283	    etc.)
1284	
1285	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1286	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1287	    and behavior.
1288	
1289	    Parameters:
1290	        config ([`VitsConfig`]):
1291	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1292	            load the weights associated with the model, only the configuration. Check out the
1293	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1294	"""
1295	
1296	
1297	VITS_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vits/tokenization_vits.py:179
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
178	                logger.warning(
179	                    "Text to the tokenizer contains non-Roman characters. To apply the `uroman` pre-processing "
180	                    "step automatically, ensure the `uroman` Romanizer is installed with: `pip install uroman` "
181	                    "Note `uroman` requires python version >= 3.10"

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vits/tokenization_vits.py:241
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
240	        with open(vocab_file, "w", encoding="utf-8") as f:
241	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
242	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/vivit/modeling_vivit.py:493
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
492	
493	VIVIT_START_DOCSTRING = r"""
494	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
495	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
496	    behavior.
497	
498	    Parameters:
499	        config ([`VivitConfig`]): Model configuration class with all the parameters of the model.
500	            Initializing with a config file does not load the weights associated with the model, only the
501	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
502	"""
503	
504	VIVIT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py:215
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
214	
215	WAV2VEC2_START_DOCSTRING = r"""
216	    Wav2Vec2 was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech
217	    Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael
218	    Auli.
219	
220	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
221	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
222	    etc.)
223	
224	    This model is also a Flax Linen
225	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
226	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
227	
228	    Finally, this model supports inherent JAX features such as:
229	
230	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
231	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
232	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
233	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
234	
235	    Parameters:
236	        config ([`Wav2Vec2Config`]): Model configuration class with all the parameters of the model.
237	            Initializing with a config file does not load the weights associated with the model, only the
238	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
239	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
240	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
241	            `jax.numpy.bfloat16` (on TPUs).
242	
243	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
244	            specified all the computation will be performed with the given `dtype`.
245	
246	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
247	            parameters.**
248	
249	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
250	            [`~FlaxPreTrainedModel.to_bf16`].
251	"""
252	
253	
254	WAV2VEC2_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py:254
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
253	
254	WAV2VEC2_INPUTS_DOCSTRING = r"""
255	    Args:
256	        input_values (`jnp.ndarray` of shape `(batch_size, sequence_length)`):
257	            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file
258	            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install
259	            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and
260	            conversion into a tensor of type `jnp.ndarray`. See [`Wav2Vec2Processor.__call__`] for details.
261	        attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
262	            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,
263	            1]`:
264	
265	            - 1 for tokens that are **not masked**,
266	            - 0 for tokens that are **masked**.
267	
268	            [What are attention masks?](../glossary#attention-mask) .. warning:: `attention_mask` should only be passed
269	            if the corresponding processor has `config.return_attention_mask == True`. For all models whose processor
270	            has `config.return_attention_mask == False`, such as
271	            [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h), `attention_mask` should **not** be
272	            passed to avoid degraded performance when doing batched inference. For such models `input_values` should
273	            simply be padded with 0 and passed without `attention_mask`. Be aware that these models also yield slightly
274	            different results depending on whether `input_values` is padded or not.
275	        mask_time_indices (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
276	            Indices to mask extracted features for contrastive loss. When in training mode, model learns to predict
277	            masked extracted features in *config.proj_codevector_dim* space.
278	        output_attentions (`bool`, *optional*):
279	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
280	            tensors for more detail.
281	        output_hidden_states (`bool`, *optional*):
282	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
283	            more detail.
284	        return_dict (`bool`, *optional*):
285	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
286	"""
287	
288	
289	class FlaxWav2Vec2LayerNormConvLayer(nn.Module):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py:1400
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1399	
1400	WAV2VEC2_START_DOCSTRING = r"""
1401	
1402	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
1403	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1404	    etc.)
1405	
1406	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
1407	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
1408	    behavior.
1409	
1410	    <Tip>
1411	
1412	    TensorFlow models and layers in `transformers` accept two formats as input:
1413	
1414	    - having all inputs as keyword arguments (like PyTorch models), or
1415	    - having all inputs as a list, tuple or dict in the first positional argument.
1416	
1417	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
1418	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
1419	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
1420	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
1421	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
1422	    positional argument:
1423	
1424	    - a single Tensor with `input_values` only and nothing else: `model(input_values)`
1425	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
1426	    `model([input_values, attention_mask])` or `model([input_values, attention_mask, token_type_ids])`
1427	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
1428	    `model({"input_values": input_values, "token_type_ids": token_type_ids})`
1429	
1430	    Note that when creating models and layers with
1431	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
1432	    about any of this, as you can just pass inputs like you would to any other Python function!
1433	
1434	    </Tip>
1435	
1436	    Args:
1437	        config ([`Wav2Vec2Config`]): Model configuration class with all the parameters of the model.
1438	            Initializing with a config file does not load the weights associated with the model, only the
1439	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1440	"""
1441	
1442	WAV2VEC2_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py:1569
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1568	                    raise EnvironmentError(
1569	                        f"Can't load the model for '{model_path_or_id}'. If you were trying to load it"
1570	                        " from 'https://huggingface.co/models', make sure you don't have a local directory with the"
1571	                        f" same name. Otherwise, make sure '{model_path_or_id}' is the correct path to a"
1572	                        f" directory containing a file named {filepath}."
1573	                    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py:1606
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1605	                raise EnvironmentError(
1606	                    f"Can't load the model for '{model_path_or_id}'. If you were trying to load it"
1607	                    " from 'https://huggingface.co/models', make sure you don't have a local directory with the"
1608	                    f" same name. Otherwise, make sure '{model_path_or_id}' is the correct path to a"
1609	                    f" directory containing a file named {filepath}."
1610	                )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py:1637
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1636	
1637	WAV2VEC2_START_DOCSTRING = r"""
1638	    Wav2Vec2 was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech
1639	    Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael
1640	    Auli.
1641	
1642	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1643	    library implements for all its model (such as downloading or saving etc.).
1644	
1645	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
1646	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
1647	    behavior.
1648	
1649	    Parameters:
1650	        config ([`Wav2Vec2Config`]): Model configuration class with all the parameters of the model.
1651	            Initializing with a config file does not load the weights associated with the model, only the
1652	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1653	"""
1654	
1655	
1656	WAV2VEC2_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py:1656
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1655	
1656	WAV2VEC2_INPUTS_DOCSTRING = r"""
1657	    Args:
1658	        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):
1659	            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file
1660	            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install
1661	            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and
1662	            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2Processor.__call__`] for details.
1663	        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
1664	            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,
1665	            1]`:
1666	
1667	            - 1 for tokens that are **not masked**,
1668	            - 0 for tokens that are **masked**.
1669	
1670	            [What are attention masks?](../glossary#attention-mask)
1671	
1672	            <Tip warning={true}>
1673	
1674	            `attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask ==
1675	            True`. For all models whose processor has `config.return_attention_mask == False`, such as
1676	            [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h), `attention_mask` should **not** be
1677	            passed to avoid degraded performance when doing batched inference. For such models `input_values` should
1678	            simply be padded with 0 and passed without `attention_mask`. Be aware that these models also yield slightly
1679	            different results depending on whether `input_values` is padded or not.
1680	
1681	            </Tip>
1682	
1683	        output_attentions (`bool`, *optional*):
1684	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1685	            tensors for more detail.
1686	        output_hidden_states (`bool`, *optional*):
1687	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1688	            more detail.
1689	        return_dict (`bool`, *optional*):
1690	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1691	"""
1692	
1693	
1694	@add_start_docstrings(
1695	    "The bare Wav2Vec2 Model transformer outputting raw hidden-states without any specific head on top.",
1696	    WAV2VEC2_START_DOCSTRING,
1697	)
1698	class Wav2Vec2Model(Wav2Vec2PreTrainedModel):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py:642
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
641	        with open(vocab_file, "w", encoding="utf-8") as f:
642	            f.write(json.dumps(self.vocab, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
643	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py:697
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
696	        "vocab_file": {
697	            "facebook/wav2vec2-base-960h": "https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/vocab.json"
698	        },
699	        "tokenizer_config_file": {
700	            "facebook/wav2vec2-base-960h": (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py:701
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
700	            "facebook/wav2vec2-base-960h": (
701	                "https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/tokenizer.json"
702	            ),
703	        },
704	    }

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py:919
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
918	        with open(vocab_file, "w", encoding="utf-8") as f:
919	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
920	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py:942
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
941	
942	WAV2VEC2_BERT_START_DOCSTRING = r"""
943	    Wav2Vec2Bert was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech
944	    Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael
945	    Auli.
946	
947	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
948	    library implements for all its model (such as downloading or saving etc.).
949	
950	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
951	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
952	    behavior.
953	
954	    Parameters:
955	        config ([`Wav2Vec2BertConfig`]): Model configuration class with all the parameters of the model.
956	            Initializing with a config file does not load the weights associated with the model, only the
957	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
958	"""
959	
960	WAV2VEC2_BERT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:1104
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1103	
1104	WAV2VEC2_CONFORMER_START_DOCSTRING = r"""
1105	    Wav2Vec2Conformer was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech
1106	    Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael
1107	    Auli.
1108	
1109	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1110	    library implements for all its model (such as downloading or saving etc.).
1111	
1112	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
1113	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
1114	    behavior.
1115	
1116	    Parameters:
1117	        config ([`Wav2Vec2ConformerConfig`]): Model configuration class with all the parameters of the model.
1118	            Initializing with a config file does not load the weights associated with the model, only the
1119	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1120	"""
1121	
1122	WAV2VEC2_CONFORMER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:1122
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1121	
1122	WAV2VEC2_CONFORMER_INPUTS_DOCSTRING = r"""
1123	    Args:
1124	        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):
1125	            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file
1126	            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install
1127	            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and
1128	            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2Processor.__call__`] for details.
1129	        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
1130	            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,
1131	            1]`:
1132	
1133	            - 1 for tokens that are **not masked**,
1134	            - 0 for tokens that are **masked**.
1135	
1136	            [What are attention masks?](../glossary#attention-mask)
1137	
1138	            <Tip warning={true}>
1139	
1140	            `attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask ==
1141	            True`. For all models whose processor has `config.return_attention_mask == False`, such as
1142	            [wav2vec2-conformer-rel-pos-large](https://huggingface.co/facebook/wav2vec2-conformer-rel-pos-large),
1143	            `attention_mask` should **not** be passed to avoid degraded performance when doing batched inference. For
1144	            such models `input_values` should simply be padded with 0 and passed without `attention_mask`. Be aware
1145	            that these models also yield slightly different results depending on whether `input_values` is padded or
1146	            not.
1147	
1148	            </Tip>
1149	
1150	        output_attentions (`bool`, *optional*):
1151	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1152	            tensors for more detail.
1153	        output_hidden_states (`bool`, *optional*):
1154	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1155	            more detail.
1156	        return_dict (`bool`, *optional*):
1157	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1158	"""
1159	
1160	Wav2Vec2ConformerBaseModelOutput = Wav2Vec2BaseModelOutput

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wav2vec2_conformer/modular_wav2vec2_conformer.py:688
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
687	
688	WAV2VEC2_CONFORMER_INPUTS_DOCSTRING = r"""
689	    Args:
690	        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):
691	            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file
692	            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install
693	            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and
694	            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2Processor.__call__`] for details.
695	        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
696	            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,
697	            1]`:
698	
699	            - 1 for tokens that are **not masked**,
700	            - 0 for tokens that are **masked**.
701	
702	            [What are attention masks?](../glossary#attention-mask)
703	
704	            <Tip warning={true}>
705	
706	            `attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask ==
707	            True`. For all models whose processor has `config.return_attention_mask == False`, such as
708	            [wav2vec2-conformer-rel-pos-large](https://huggingface.co/facebook/wav2vec2-conformer-rel-pos-large),
709	            `attention_mask` should **not** be passed to avoid degraded performance when doing batched inference. For
710	            such models `input_values` should simply be padded with 0 and passed without `attention_mask`. Be aware
711	            that these models also yield slightly different results depending on whether `input_values` is padded or
712	            not.
713	
714	            </Tip>
715	
716	        output_attentions (`bool`, *optional*):
717	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
718	            tensors for more detail.
719	        output_hidden_states (`bool`, *optional*):
720	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
721	            more detail.
722	        return_dict (`bool`, *optional*):
723	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
724	"""
725	
726	Wav2Vec2ConformerBaseModelOutput = Wav2Vec2BaseModelOutput

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py:575
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
574	        with open(vocab_file, "w", encoding="utf-8") as f:
575	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
576	

--------------------------------------------------
>> Issue: [B839:pool] multiprocessing.Pool
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:411
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b839_pool.html
410	            if default_context == "fork":
411	                cm = pool = get_context().Pool(num_processes)
412	            else:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wavlm/modeling_wavlm.py:999
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
998	
999	WAVLM_START_DOCSTRING = r"""
1000	    WavLM was proposed in [WavLM: Unified Speech Representation Learning with Labeled and Unlabeled
1001	    Data](https://arxiv.org/abs/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo
1002	    Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian,
1003	    Jian Wu, Michael Zeng, Xiangzhan Yu, Furu Wei.
1004	
1005	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1006	    library implements for all its model (such as downloading or saving etc.).
1007	
1008	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
1009	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
1010	    behavior.
1011	
1012	    Parameters:
1013	        config ([`WavLMConfig`]): Model configuration class with all the parameters of the model.
1014	            Initializing with a config file does not load the weights associated with the model, only the
1015	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1016	"""
1017	
1018	WAVLM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/wavlm/modular_wavlm.py:600
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
599	
600	WAVLM_START_DOCSTRING = r"""
601	    WavLM was proposed in [WavLM: Unified Speech Representation Learning with Labeled and Unlabeled
602	    Data](https://arxiv.org/abs/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo
603	    Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian,
604	    Jian Wu, Michael Zeng, Xiangzhan Yu, Furu Wei.
605	
606	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
607	    library implements for all its model (such as downloading or saving etc.).
608	
609	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
610	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
611	    behavior.
612	
613	    Parameters:
614	        config ([`WavLMConfig`]): Model configuration class with all the parameters of the model.
615	            Initializing with a config file does not load the weights associated with the model, only the
616	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
617	"""
618	
619	WAVLM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/feature_extraction_whisper.py:115
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
114	            raise ValueError(
115	                f"Got device `{device}` for feature extraction, but feature extraction on CUDA accelerator "
116	                "devices requires torch, which is not installed. Either set `device='cpu'`, or "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/generation_whisper.py:1317
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1316	            raise ValueError(
1317	                "You are trying to return timestamps, but the generation config is not properly set. "
1318	                "Make sure to initialize the generation config with the correct attributes that are needed such as `no_timestamps_token_id`. "
1319	                "For more details on how to generate the approtiate config, refer to https://github.com/huggingface/transformers/issues/21878#issuecomment-1451902363"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/generation_whisper.py:1338
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1337	                raise ValueError(
1338	                    "The generation config is outdated and is thus not compatible with the `is_multilingual` argument "
1339	                    "to `generate`. Please update the generation config as per the instructions "
1340	                    "https://github.com/huggingface/transformers/issues/25084#issuecomment-1664398224"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/generation_whisper.py:1354
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1353	                raise ValueError(
1354	                    "The generation config is outdated and is thus not compatible with the `language` argument "
1355	                    "to `generate`. Either set the language using the `forced_decoder_ids` in the model config, "
1356	                    "or update the generation config as per the instructions https://github.com/huggingface/transformers/issues/25084#issuecomment-1664398224"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/generation_whisper.py:1363
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1362	                raise ValueError(
1363	                    "The generation config is outdated and is thus not compatible with the `task` argument "
1364	                    "to `generate`. Either set the task using the `forced_decoder_ids` in the model config, "
1365	                    "or update the generation config as per the instructions https://github.com/huggingface/transformers/issues/25084#issuecomment-1664398224"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/generation_whisper.py:1408
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1407	                logger.warning_once(
1408	                    "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English."
1409	                    "This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`."
1410	                )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/generation_whisper.py:1601
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1600	                raise ValueError(
1601	                    "Model generation config has no `alignment_heads`, token-level timestamps not available. "
1602	                    "See https://gist.github.com/hollance/42e32852f24243b748ae6bc1f985b13a on how to add this property to the generation config."
1603	                )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/modeling_flax_whisper.py:75
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
74	
75	WHISPER_START_DOCSTRING = r"""
76	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
77	    library implements for all its models (such as downloading or saving, resizing the input embeddings, pruning heads
78	    etc.) This model is also a Flax Linen
79	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
80	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
81	    Finally, this model supports inherent JAX features such as:
82	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
83	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
84	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
85	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
86	
87	    Parameters:
88	        config ([`WhisperConfig`]): Model configuration class with all the parameters of the model.
89	            Initializing with a config file does not load the weights associated with the model, only the
90	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
91	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
92	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
93	            `jax.numpy.bfloat16` (on TPUs). This can be used to enable mixed-precision training or half-precision
94	            inference on GPUs or TPUs. If specified all the computation will be performed with the given `dtype`.
95	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
96	            parameters.** If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`]
97	            and [`~FlaxPreTrainedModel.to_bf16`].
98	"""
99	
100	WHISPER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/modeling_flax_whisper.py:100
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
99	
100	WHISPER_INPUTS_DOCSTRING = r"""
101	    Args:
102	        input_features (`numpy.ndarray` of shape `(batch_size, feature_size, sequence_length)`):
103	            Float values mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by
104	            loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via
105	            the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the
106	            [`WhisperFeatureExtractor`] should be used for extracting the features, padding and conversion into a
107	            tensor of type `numpy.ndarray`. See [`~WhisperFeatureExtractor.__call__`]
108	        attention_mask (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
109	            Whisper does not support masking of the `input_features`, this argument is preserved for compatibility, but
110	            is not used. By default the silence in the input log mel spectrogram are ignored.
111	        decoder_input_ids (`numpy.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
112	            Indices of decoder input sequence tokens in the vocabulary. Indices can be obtained using
113	            [`WhisperTokenizer`]. See [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.
114	            [What are decoder input IDs?](../glossary#decoder-input-ids) Whisper uses the `decoder_start_token_id` as
115	            the starting token for `decoder_input_ids` generation.
116	        decoder_attention_mask (`numpy.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
117	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
118	            be used by default. If you want to change padding behavior, you should modify to your needs. See diagram 1
119	            in [the paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
120	        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
121	            Whisper does not use `position_ids` in the encoder as `input_features` is always the same size and doesn't
122	            use masking, but this argument is preserved for compatibility. By default the silence in the input log mel
123	            spectrogram are ignored.
124	        decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
125	            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
126	            range `[0, config.max_position_embeddings - 1]`.
127	        output_attentions (`bool`, *optional*):
128	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
129	            tensors for more detail.
130	        output_hidden_states (`bool`, *optional*):
131	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
132	            more detail.
133	        return_dict (`bool`, *optional*):
134	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
135	"""
136	
137	WHISPER_ENCODE_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/modeling_flax_whisper.py:158
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
157	
158	WHISPER_DECODE_INPUTS_DOCSTRING = r"""
159	    Args:
160	        decoder_input_ids (`numpy.ndarray` of shape `(batch_size, target_sequence_length)`):
161	            Indices of decoder input sequence tokens in the vocabulary. Indices can be obtained using
162	            [`WhisperTokenizer`]. See [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.
163	            [What are decoder input IDs?](../glossary#decoder-input-ids)
164	        encoder_outputs (`tuple(tuple(numpy.ndarray)`):
165	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
166	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
167	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
168	        encoder_attention_mask (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
169	           Whisper does not support masking of the `input_features`, this argument is preserved for compatibility,
170	            but it is not used. By default the silence in the input log mel spectrogram are ignored.
171	        decoder_attention_mask (`numpy.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):
172	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
173	            be used by default. If you want to change padding behavior, you should modify to your needs. See diagram 1
174	            in [the paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
175	        decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
176	            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
177	            range `[0, config.max_position_embeddings - 1]`.
178	        past_key_values (`Dict[str, numpy.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):
179	            Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
180	            auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.
181	        output_attentions (`bool`, *optional*):
182	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
183	            tensors for more detail.
184	        output_hidden_states (`bool`, *optional*):
185	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
186	            more detail.
187	        return_dict (`bool`, *optional*):
188	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
189	"""
190	
191	
192	class FlaxWhisperAttention(nn.Module):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/modeling_tf_whisper.py:583
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
582	
583	WHISPER_START_DOCSTRING = r"""
584	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
585	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
586	    etc.)
587	
588	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
589	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
590	    behavior.
591	
592	    Parameters:
593	        config ([`WhisperConfig`]):
594	            Model configuration class with all the parameters of the model. Initializing with a config file does not
595	            load the weights associated with the model, only the configuration. Check out the
596	            [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
597	"""
598	
599	WHISPER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/modeling_tf_whisper.py:599
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
598	
599	WHISPER_INPUTS_DOCSTRING = r"""
600	    Args:
601	        input_features (`tf.Tensor` of shape `(batch_size, feature_size, sequence_length)`):
602	            Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be obtained
603	            by loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`, *e.g.*
604	            via the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the
605	            [`AutoFeatureExtractor`] should be used for extracting the fbank features, padding and conversion into a
606	            tensor of type `tf.Tensor`. See [`~WhisperFeatureExtractor.__call__`]
607	        decoder_input_ids (`tf.Tensor` of shape `(batch_size, target_sequence_length)`, *optional*):
608	            Indices of decoder input sequence tokens in the vocabulary.
609	
610	            Indices can be obtained using [`SpeechToTextTokenizer`]. See [`PreTrainedTokenizer.encode`] and
611	            [`PreTrainedTokenizer.__call__`] for details.
612	
613	            [What are decoder input IDs?](../glossary#decoder-input-ids)
614	
615	            SpeechToText uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If
616	            `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
617	            `past_key_values`).
618	        decoder_attention_mask (`tf.Tensor` of shape `(batch_size, target_sequence_length)`, *optional*):
619	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
620	            be used by default.
621	
622	            If you want to change padding behavior, you should read
623	            [`modeling_whisper._prepare_decoder_attention_mask`] and modify to your needs. See diagram 1 in [the
624	            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
625	        head_mask (`tf.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
626	            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:
627	
628	            - 1 indicates the head is **not masked**,
629	            - 0 indicates the head is **masked**.
630	
631	        decoder_head_mask (`tf.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
632	            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:
633	
634	            - 1 indicates the head is **not masked**,
635	            - 0 indicates the head is **masked**.
636	
637	        cross_attn_head_mask (`tf.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
638	            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:
639	
640	            - 1 indicates the head is **not masked**,
641	            - 0 indicates the head is **masked**.
642	
643	        encoder_outputs (`tuple(tuple(tf.Tensor)`, *optional*):
644	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
645	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
646	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
647	        past_key_values (`tuple(tuple(tf.Tensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
648	            Tuple of `tuple(tf.Tensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
649	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
650	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
651	
652	            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
653	            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
654	
655	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
656	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
657	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
658	        decoder_inputs_embeds (`tf.Tensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):
659	            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded
660	            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be
661	            input (see `past_key_values`). This is useful if you want more control over how to convert
662	            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.
663	        use_cache (`bool`, *optional*):
664	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
665	            `past_key_values`).
666	        output_attentions (`bool`, *optional*):
667	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
668	            tensors for more detail.
669	        output_hidden_states (`bool`, *optional*):
670	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
671	            more detail.
672	        return_dict (`bool`, *optional*):
673	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
674	"""
675	
676	
677	@keras_serializable
678	class TFWhisperEncoder(keras.layers.Layer):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/modeling_tf_whisper.py:1573
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1572	                raise ValueError(
1573	                    "You are trying to return timestamps, but the generation config is not properly set. "
1574	                    "Make sure to initialize the generation config with the correct attributes that are needed such as `no_timestamps_token_id`. "
1575	                    "For more details on how to generate the approtiate config, refer to https://github.com/huggingface/transformers/issues/21878#issuecomment-1451902363"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/modeling_tf_whisper.py:1685
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1684	                raise ValueError(
1685	                    "Model generation config has no `alignment_heads`, token-level timestamps not available. "
1686	                    "See https://gist.github.com/hollance/42e32852f24243b748ae6bc1f985b13a on how to add this property to the generation config."
1687	                )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/modeling_whisper.py:382
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
381	            raise ValueError(
382	                "The `static` cache implementation is not compatible with `attn_implementation='flash_attention_2'`. "
383	                "Use `attn_implementation='sdpa'` in the meantime, and open an issue at https://github.com/huggingface/transformers"
384	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/modeling_whisper.py:803
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
802	
803	WHISPER_START_DOCSTRING = r"""
804	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
805	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
806	    etc.)
807	
808	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
809	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
810	    and behavior.
811	
812	    Parameters:
813	        config ([`WhisperConfig`]):
814	            Model configuration class with all the parameters of the model. Initializing with a config file does not
815	            load the weights associated with the model, only the configuration. Check out the
816	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
817	"""
818	
819	WHISPER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/modeling_whisper.py:819
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
818	
819	WHISPER_INPUTS_DOCSTRING = r"""
820	    Args:
821	        input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, sequence_length)`):
822	            Float values mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by
823	            loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via
824	            the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the
825	            [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a
826	            tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]
827	        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
828	            Mask to avoid performing *SpecAugment* data augmentation on padding token indices. Mask values selected in
829	            `[0, 1]`:
830	
831	            - 1 for tokens that are **not masked**,
832	            - 0 for tokens that are **masked**.
833	
834	            [What are attention masks?](../glossary#attention-mask)
835	        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
836	            Indices of decoder input sequence tokens in the vocabulary.
837	
838	            Indices can be obtained using [`WhisperTokenizer`]. See [`PreTrainedTokenizer.encode`] and
839	            [`PreTrainedTokenizer.__call__`] for details.
840	
841	            [What are decoder input IDs?](../glossary#decoder-input-ids)
842	
843	            Whisper uses the `decoder_start_token_id` as the starting token for `decoder_input_ids` generation. If
844	            `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
845	            `past_key_values`).
846	        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
847	            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
848	            be used by default.
849	
850	            If you want to change padding behavior, you should read
851	            [`modeling_whisper._prepare_decoder_attention_mask`] and modify to your needs. See diagram 1 in [the BART
852	            paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.
853	        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
854	            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:
855	
856	            - 1 indicates the head is **not masked**,
857	            - 0 indicates the head is **masked**.
858	
859	        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
860	            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:
861	
862	            - 1 indicates the head is **not masked**,
863	            - 0 indicates the head is **masked**.
864	
865	        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
866	            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:
867	
868	            - 1 indicates the head is **not masked**,
869	            - 0 indicates the head is **masked**.
870	
871	        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):
872	            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
873	            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
874	            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
875	        past_key_values (`EncoderDecoderCache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
876	            Pre-computed hidden-states that can be used to speed up auto-regressive (sequential) decoding. There are
877	            four sets of pre-computed hidden-states: key and values states in the self-attention blocks (2) and
878	            in the cross-attention blocks (2). The `past_key_values` are returned when `use_cache=True` is passed or
879	            when `config.use_cache=True`
880	
881	            Two formats are allowed:
882	            - An [`~cache_utils.EncoderDecoderCache`] instance;
883	            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
884	            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
885	            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
886	
887	            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
888	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
889	            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
890	        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):
891	            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded
892	            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be
893	            input (see `past_key_values`). This is useful if you want more control over how to convert
894	            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.
895	        use_cache (`bool`, *optional*):
896	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
897	            `past_key_values`).
898	        output_attentions (`bool`, *optional*):
899	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
900	            tensors for more detail.
901	        output_hidden_states (`bool`, *optional*):
902	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
903	            more detail.
904	        return_dict (`bool`, *optional*):
905	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
906	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
907	            Indices depicting the position of the input sequence tokens in the sequence. It is used to update the cache
908	            in the correct position and to infer the complete sequence length.
909	"""
910	
911	WHISPER_ENCODER_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/tokenization_whisper.py:298
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
297	        with open(merges_file, encoding="utf-8") as merges_handle:
298	            bpe_merges = merges_handle.read().split("\n")[1:-1]
299	        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/tokenization_whisper.py:816
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
815	        with open(vocab_file, "w", encoding="utf-8") as f:
816	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
817	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/tokenization_whisper.py:820
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
819	        with open(merge_file, "w", encoding="utf-8") as writer:
820	            writer.write("#version: 0.2\n")
821	            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/tokenization_whisper.py:828
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
827	                    index = token_index
828	                writer.write(" ".join(bpe_tokens) + "\n")
829	                index += 1

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/tokenization_whisper.py:833
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
832	            with open(normalizer_file, "w", encoding="utf-8") as f:
833	                f.write(
834	                    json.dumps(self.english_spelling_normalizer, indent=2, sort_keys=True, ensure_ascii=False) + "\n"
835	                )

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/whisper/tokenization_whisper_fast.py:448
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
447	            with open(normalizer_file, "w", encoding="utf-8") as f:
448	                f.write(
449	                    json.dumps(self.english_spelling_normalizer, indent=2, sort_keys=True, ensure_ascii=False) + "\n"
450	                )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/x_clip/modeling_x_clip.py:572
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
571	
572	X_CLIP_START_DOCSTRING = r"""
573	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
574	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
575	    behavior.
576	
577	    Parameters:
578	        config ([`XCLIPConfig`]): Model configuration class with all the parameters of the model.
579	            Initializing with a config file does not load the weights associated with the model, only the
580	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
581	"""
582	
583	X_CLIP_TEXT_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xglm/modeling_flax_xglm.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	
47	XGLM_START_DOCSTRING = r"""
48	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
49	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
50	    etc.)
51	
52	    This model is also a Flax Linen
53	    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a
54	    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.
55	
56	    Finally, this model supports inherent JAX features such as:
57	
58	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
59	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
60	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
61	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
62	
63	    Parameters:
64	        config ([`XGLMConfig`]): Model configuration class with all the parameters of the model.
65	            Initializing with a config file does not load the weights associated with the model, only the
66	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
67	        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):
68	            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and
69	            `jax.numpy.bfloat16` (on TPUs).
70	
71	            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
72	            specified all the computation will be performed with the given `dtype`.
73	
74	            **Note that this only specifies the dtype of the computation and does not influence the dtype of model
75	            parameters.**
76	
77	            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and
78	            [`~FlaxPreTrainedModel.to_bf16`].
79	"""
80	
81	XGLM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xglm/modeling_tf_xglm.py:671
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
670	
671	XGLM_START_DOCSTRING = r"""
672	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
673	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
674	    etc.)
675	
676	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
677	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
678	    behavior.
679	
680	    <Tip>
681	
682	    TensorFlow models and layers in `transformers` accept two formats as input:
683	
684	    - having all inputs as keyword arguments (like PyTorch models), or
685	    - having all inputs as a list, tuple or dict in the first positional argument.
686	
687	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
688	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
689	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
690	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
691	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
692	    positional argument:
693	
694	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
695	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
696	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
697	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
698	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
699	
700	    Note that when creating models and layers with
701	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
702	    about any of this, as you can just pass inputs like you would to any other Python function!
703	
704	    </Tip>
705	
706	    Args:
707	        config ([`XGLMConfig`]): Model configuration class with all the parameters of the model.
708	            Initializing with a config file does not load the weights associated with the model, only the
709	            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
710	"""
711	
712	XGLM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xglm/modeling_xglm.py:39
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
38	
39	XGLM_START_DOCSTRING = r"""
40	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
41	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
42	    etc.)
43	
44	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
45	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
46	    and behavior.
47	
48	    Parameters:
49	        config ([`XGLMConfig`]):
50	            Model configuration class with all the parameters of the model. Initializing with a config file does not
51	            load the weights associated with the model, only the configuration. Check out the
52	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
53	"""
54	
55	XGLM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xglm/tokenization_xglm.py:295
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
294	                content_spiece_model = self.sp_model.serialized_model_proto()
295	                fi.write(content_spiece_model)
296	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlm/modeling_tf_xlm.py:607
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
606	
607	XLM_START_DOCSTRING = r"""
608	
609	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
610	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
611	    etc.)
612	
613	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
614	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
615	    behavior.
616	
617	    <Tip>
618	
619	    TensorFlow models and layers in `transformers` accept two formats as input:
620	
621	    - having all inputs as keyword arguments (like PyTorch models), or
622	    - having all inputs as a list, tuple or dict in the first positional argument.
623	
624	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
625	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
626	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
627	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
628	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
629	    positional argument:
630	
631	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
632	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
633	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
634	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
635	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
636	
637	    Note that when creating models and layers with
638	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
639	    about any of this, as you can just pass inputs like you would to any other Python function!
640	
641	    </Tip>
642	
643	    Parameters:
644	        config ([`XLMConfig`]): Model configuration class with all the parameters of the model.
645	            Initializing with a config file does not load the weights associated with the model, only the
646	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
647	"""
648	
649	XLM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlm/modeling_xlm.py:295
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
294	
295	XLM_START_DOCSTRING = r"""
296	
297	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
298	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
299	    etc.)
300	
301	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
302	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
303	    and behavior.
304	
305	    Parameters:
306	        config ([`XLMConfig`]): Model configuration class with all the parameters of the model.
307	            Initializing with a config file does not load the weights associated with the model, only the
308	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
309	"""
310	
311	XLM_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlm/tokenization_xlm.py:225
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
224	            raise ImportError(
225	                "You need to install sacremoses to use XLMTokenizer. "
226	                "See https://pypi.org/project/sacremoses/ for installation."
227	            )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlm/tokenization_xlm.py:250
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
249	        with open(merges_file, encoding="utf-8") as merges_handle:
250	            merges = merges_handle.read().split("\n")[:-1]
251	        merges = [tuple(merge.split()[:2]) for merge in merges]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlm/tokenization_xlm.py:304
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
303	                logger.error(
304	                    "Make sure you install KyTea (https://github.com/neubig/kytea) and it's python wrapper"
305	                    " (https://github.com/chezou/Mykytea-python) with the following steps"
306	                )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlm/tokenization_xlm.py:428
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
427	                logger.error(
428	                    "Make sure you install PyThaiNLP (https://github.com/PyThaiNLP/pythainlp) with the following steps"
429	                )
430	                logger.error("1. pip install pythainlp")

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlm/tokenization_xlm.py:440
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
439	            except (AttributeError, ImportError):
440	                logger.error("Make sure you install Jieba (https://github.com/fxsjy/jieba) with the following steps")
441	                logger.error("1. pip install jieba")

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlm/tokenization_xlm.py:571
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
570	        with open(vocab_file, "w", encoding="utf-8") as f:
571	            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")
572	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlm/tokenization_xlm.py:582
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
581	                    index = token_index
582	                writer.write(" ".join(bpe_tokens) + "\n")
583	                index += 1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlm/tokenization_xlm.py:599
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
598	            raise ImportError(
599	                "You need to install sacremoses to use XLMTokenizer. "
600	                "See https://pypi.org/project/sacremoses/ for installation."
601	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py:80
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
79	
80	XLM_ROBERTA_START_DOCSTRING = r"""
81	
82	    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the
83	    library implements for all its model (such as downloading, saving and converting weights from PyTorch models)
84	
85	    This model is also a
86	    [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. Use it as
87	    a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
88	    behavior.
89	
90	    Finally, this model supports inherent JAX features such as:
91	
92	    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)
93	    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
94	    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)
95	    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)
96	
97	    Parameters:
98	        config ([`XLMRobertaConfig`]): Model configuration class with all the parameters of the
99	            model. Initializing with a config file does not load the weights associated with the model, only the
100	            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.
101	"""
102	
103	XLM_ROBERTA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py:70
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
69	
70	XLM_ROBERTA_START_DOCSTRING = r"""
71	
72	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
73	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
74	    etc.)
75	
76	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
77	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
78	    behavior.
79	
80	    <Tip>
81	
82	    TensorFlow models and layers in `transformers` accept two formats as input:
83	
84	    - having all inputs as keyword arguments (like PyTorch models), or
85	    - having all inputs as a list, tuple or dict in the first positional argument.
86	
87	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
88	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
89	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
90	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
91	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
92	    positional argument:
93	
94	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
95	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
96	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
97	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
98	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
99	
100	    Note that when creating models and layers with
101	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
102	    about any of this, as you can just pass inputs like you would to any other Python function!
103	
104	    </Tip>
105	
106	    Parameters:
107	        config ([`XLMRobertaConfig`]): Model configuration class with all the parameters of the
108	            model. Initializing with a config file does not load the weights associated with the model, only the
109	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
110	"""
111	
112	XLM_ROBERTA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py:723
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
722	
723	XLM_ROBERTA_START_DOCSTRING = r"""
724	
725	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
726	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
727	    etc.)
728	
729	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
730	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
731	    and behavior.
732	
733	    Parameters:
734	        config ([`XLMRobertaConfig`]): Model configuration class with all the parameters of the
735	            model. Initializing with a config file does not load the weights associated with the model, only the
736	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
737	"""
738	
739	XLM_ROBERTA_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py:294
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
293	                content_spiece_model = self.sp_model.serialized_model_proto()
294	                fi.write(content_spiece_model)
295	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py:715
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
714	
715	XLM_ROBERTA_XL_START_DOCSTRING = r"""
716	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
717	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
718	    etc.) This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
719	    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
720	    general usage and behavior.
721	
722	    Parameters:
723	        config ([`XLMRobertaXLConfig`]): Model configuration class with all the parameters of the
724	            model. Initializing with a config file does not load the weights associated with the model, only the
725	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
726	"""
727	
728	XLM_ROBERTA_XL_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlnet/modeling_tf_xlnet.py:1050
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1049	
1050	XLNET_START_DOCSTRING = r"""
1051	
1052	    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
1053	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1054	    etc.)
1055	
1056	    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
1057	    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
1058	    behavior.
1059	
1060	    <Tip>
1061	
1062	    TensorFlow models and layers in `transformers` accept two formats as input:
1063	
1064	    - having all inputs as keyword arguments (like PyTorch models), or
1065	    - having all inputs as a list, tuple or dict in the first positional argument.
1066	
1067	    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
1068	    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
1069	    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
1070	    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
1071	    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
1072	    positional argument:
1073	
1074	    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
1075	    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
1076	    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
1077	    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
1078	    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
1079	
1080	    Note that when creating models and layers with
1081	    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
1082	    about any of this, as you can just pass inputs like you would to any other Python function!
1083	
1084	    </Tip>
1085	
1086	    Parameters:
1087	        config ([`XLNetConfig`]): Model configuration class with all the parameters of the model.
1088	            Initializing with a config file does not load the weights associated with the model, only the
1089	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1090	"""
1091	
1092	XLNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlnet/modeling_xlnet.py:139
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
138	        logger.error(
139	            "Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see "
140	            "https://www.tensorflow.org/install/ for installation instructions."
141	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlnet/modeling_xlnet.py:834
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
833	
834	XLNET_START_DOCSTRING = r"""
835	
836	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
837	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
838	    etc.)
839	
840	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
841	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
842	    and behavior.
843	
844	    Parameters:
845	        config ([`XLNetConfig`]): Model configuration class with all the parameters of the model.
846	            Initializing with a config file does not load the weights associated with the model, only the
847	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
848	"""
849	
850	XLNET_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xlnet/tokenization_xlnet.py:380
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
379	                content_spiece_model = self.sp_model.serialized_model_proto()
380	                fi.write(content_spiece_model)
381	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/xmod/modeling_xmod.py:697
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
696	
697	XMOD_START_DOCSTRING = r"""
698	
699	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
700	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
701	    etc.)
702	
703	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
704	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
705	    and behavior.
706	
707	    Parameters:
708	        config ([`XmodConfig`]): Model configuration class with all the parameters of the
709	            model. Initializing with a config file does not load the weights associated with the model, only the
710	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
711	"""
712	
713	XMOD_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/yolos/modeling_yolos.py:567
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
566	
567	YOLOS_START_DOCSTRING = r"""
568	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
569	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
570	    behavior.
571	
572	    Parameters:
573	        config ([`YolosConfig`]): Model configuration class with all the parameters of the model.
574	            Initializing with a config file does not load the weights associated with the model, only the
575	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
576	"""
577	
578	YOLOS_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/yoso/modeling_yoso.py:673
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
672	
673	YOSO_START_DOCSTRING = r"""
674	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
675	    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
676	    behavior.
677	
678	    Parameters:
679	        config ([`YosoConfig`]): Model configuration class with all the parameters of the model.
680	            Initializing with a config file does not load the weights associated with the model, only the
681	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
682	"""
683	
684	YOSO_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/zamba/modeling_zamba.py:384
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
383	            logger.warning_once(
384	                "The fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`"
385	                " is None. To install follow https://github.com/state-spaces/mamba/#installation and"
386	                " https://github.com/Dao-AILab/causal-conv1d. If you want to use the naive implementation, set `use_mamba_kernels=False` in the model config"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/zamba/modeling_zamba.py:811
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
810	
811	ZAMBA_START_DOCSTRING = r"""
812	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
813	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
814	    etc.)
815	
816	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
817	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
818	    and behavior.
819	
820	    Parameters:
821	        config ([`ZambaConfig`]):
822	            Model configuration class with all the parameters of the model. Initializing with a config file does not
823	            load the weights associated with the model, only the configuration. Check out the
824	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
825	"""
826	
827	
828	@add_start_docstrings(
829	    "The bare Zamba Model outputting raw hidden-states without any specific head on top.",
830	    ZAMBA_START_DOCSTRING,
831	)
832	class ZambaPreTrainedModel(PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/zamba/modeling_zamba.py:899
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
898	
899	ZAMBA_INPUTS_DOCSTRING = r"""
900	    Args:
901	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
902	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
903	            it.
904	
905	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
906	            [`PreTrainedTokenizer.__call__`] for details.
907	
908	            [What are input IDs?](../glossary#input-ids)
909	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
910	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
911	
912	            - 1 for tokens that are **not masked**,
913	            - 0 for tokens that are **masked**.
914	
915	            [What are attention masks?](../glossary#attention-mask)
916	
917	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
918	            [`PreTrainedTokenizer.__call__`] for details.
919	
920	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
921	            `past_key_values`).
922	
923	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
924	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
925	            information on the default strategy.
926	
927	            - 1 indicates the head is **not masked**,
928	            - 0 indicates the head is **masked**.
929	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
930	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
931	            config.n_positions - 1]`.
932	
933	            [What are position IDs?](../glossary#position-ids)
934	        past_key_values (`ZambaHybridDynamicCache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
935	            A ZambaHybridDynamicCache object containing pre-computed hidden-states (keys and values in the
936	            self-attention blocks and convolution and ssm states in the mamba blocks) that can be used (see
937	            `past_key_values` input) to speed up sequential decoding.
938	            Key and value cache tensors have shape `(batch_size, num_heads, seq_len, head_dim)`.
939	            Convolution and ssm states tensors have shape `(batch_size, d_inner, d_conv)` and
940	            `(batch_size, d_inner, d_state)` respectively.
941	            See the `ZambaHybridDynamicCache` class for more details.
942	
943	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that
944	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
945	            `input_ids` of shape `(batch_size, sequence_length)`.
946	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
947	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
948	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
949	            model's internal embedding lookup matrix.
950	        use_cache (`bool`, *optional*):
951	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
952	            `past_key_values`).
953	        output_attentions (`bool`, *optional*):
954	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
955	            tensors for more detail.
956	        output_hidden_states (`bool`, *optional*):
957	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
958	            more detail.
959	        return_dict (`bool`, *optional*):
960	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
961	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
962	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
963	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
964	            the complete sequence length.
965	"""
966	
967	
968	@add_start_docstrings(
969	    "The bare Zamba Model outputting raw hidden-states without any specific head on top.",
970	    ZAMBA_START_DOCSTRING,
971	)
972	class ZambaModel(ZambaPreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/zamba2/modeling_zamba2.py:596
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
595	            logger.warning_once(
596	                "The fast path is not available because on of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)`"
597	                " is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and"
598	                " https://github.com/Dao-AILab/causal-conv1d"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/zamba2/modeling_zamba2.py:1245
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1244	
1245	ZAMBA2_START_DOCSTRING = r"""
1246	    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
1247	    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
1248	    etc.)
1249	
1250	    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
1251	    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
1252	    and behavior.
1253	
1254	    Parameters:
1255	        config ([`Zamba2Config`]):
1256	            Model configuration class with all the parameters of the model. Initializing with a config file does not
1257	            load the weights associated with the model, only the configuration. Check out the
1258	            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1259	"""
1260	
1261	
1262	ZAMBA2_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/zamba2/modeling_zamba2.py:1262
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1261	
1262	ZAMBA2_INPUTS_DOCSTRING = r"""
1263	    Args:
1264	        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
1265	            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
1266	            it.
1267	
1268	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1269	            [`PreTrainedTokenizer.__call__`] for details.
1270	
1271	            [What are input IDs?](../glossary#input-ids)
1272	        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
1273	            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
1274	
1275	            - 1 for tokens that are **not masked**,
1276	            - 0 for tokens that are **masked**.
1277	
1278	            [What are attention masks?](../glossary#attention-mask)
1279	
1280	            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
1281	            [`PreTrainedTokenizer.__call__`] for details.
1282	
1283	            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
1284	            `past_key_values`).
1285	
1286	            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
1287	            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
1288	            information on the default strategy.
1289	
1290	            - 1 indicates the head is **not masked**,
1291	            - 0 indicates the head is **masked**.
1292	        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
1293	            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
1294	            config.n_positions - 1]`.
1295	
1296	            [What are position IDs?](../glossary#position-ids)
1297	        past_key_values (`Zamba2HybridDynamicCache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
1298	            A Zamba2HybridDynamicCache object containing pre-computed hidden-states (keys and values in the
1299	            self-attention blocks and convolution and ssm states in the mamba blocks) that can be used (see
1300	            `past_key_values` input) to speed up sequential decoding.
1301	            Key and value cache tensors have shape `(batch_size, num_heads, seq_len, head_dim)`.
1302	            Convolution and ssm states tensors have shape `(batch_size, d_inner, d_conv)` and
1303	            `(batch_size, d_inner, d_state)` respectively.
1304	            See the `Zamba2HybridDynamicCache` class for more details.
1305	
1306	            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that
1307	            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
1308	            `input_ids` of shape `(batch_size, sequence_length)`.
1309	        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
1310	            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
1311	            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
1312	            model's internal embedding lookup matrix.
1313	        use_cache (`bool`, *optional*):
1314	            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
1315	            `past_key_values`).
1316	        output_attentions (`bool`, *optional*):
1317	            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
1318	            tensors for more detail.
1319	        output_hidden_states (`bool`, *optional*):
1320	            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
1321	            more detail.
1322	        return_dict (`bool`, *optional*):
1323	            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
1324	        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
1325	            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
1326	            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
1327	            the complete sequence length.
1328	"""
1329	
1330	
1331	@add_start_docstrings(
1332	    "The bare Zamba2 Model outputting raw hidden-states without any specific head on top.",
1333	    ZAMBA2_START_DOCSTRING,
1334	)
1335	class Zamba2Model(Zamba2PreTrainedModel):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/zamba2/modular_zamba2.py:365
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
364	            logger.warning_once(
365	                "The fast path is not available because on of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)`"
366	                " is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and"
367	                " https://github.com/Dao-AILab/causal-conv1d"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/zoedepth/configuration_zoedepth.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	ZOEDEPTH_PRETRAINED_CONFIG_ARCHIVE_MAP = {
25	    "Intel/zoedepth-nyu": "https://huggingface.co/Intel/zoedepth-nyu/resolve/main/config.json",
26	}
27	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/models/zoedepth/modeling_zoedepth.py:1248
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1247	
1248	ZOEDEPTH_START_DOCSTRING = r"""
1249	    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it
1250	    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
1251	    behavior.
1252	
1253	    Parameters:
1254	        config ([`ViTConfig`]): Model configuration class with all the parameters of the model.
1255	            Initializing with a config file does not load the weights associated with the model, only the
1256	            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
1257	"""
1258	
1259	ZOEDEPTH_INPUTS_DOCSTRING = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/onnx/__main__.py:63
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
62	    logger.info(
63	        "The export was done by optimum.exporters.onnx. We recommend using to use this package directly in future, as "
64	        "transformers.onnx is deprecated, and will be removed in v5. You can find more information here: "
65	        "https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/onnx/__main__.py:181
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
180	        warnings.warn(
181	            "The export was done by transformers.onnx which is deprecated and will be removed in v5. We recommend"
182	            " using optimum.exporters.onnx in future. You can find more information here:"
183	            " https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model.",
184	            FutureWarning,
185	        )
186	

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/pipelines/audio_classification.py:173
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
172	                # like http_huggingface_co.png
173	                inputs = requests.get(inputs).content
174	            else:

--------------------------------------------------
>> Issue: [B817:system] platform.system
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/pipelines/audio_utils.py:92
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b817_system.html
91	
92	    system = platform.system()
93	

--------------------------------------------------
>> Issue: [B308:blacklist] platform.system
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/pipelines/audio_utils.py:92
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b308-platform-system
91	
92	    system = platform.system()
93	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/pipelines/automatic_speech_recognition.py:301
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
300	                logger.warning(
301	                    "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily"
302	                    " be entirely accurate and will have caveats. More information:"
303	                    " https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(...,"

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/pipelines/automatic_speech_recognition.py:364
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
363	                # like http_huggingface_co.png
364	                inputs = requests.get(inputs).content
365	            else:

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/pipelines/automatic_speech_recognition.py:581
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
580	                items = outputs[key].numpy()
581	            stride = outputs.get("stride", None)
582	            if stride is not None and self.type in {"ctc", "ctc_with_lm"}:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/pipelines/base.py:243
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
242	        raise RuntimeError(
243	            "At least one of TensorFlow 2.0 or PyTorch should be installed. "
244	            "To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ "
245	            "To install PyTorch, read the instructions at https://pytorch.org/."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/pipelines/base.py:367
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
366	        raise RuntimeError(
367	            "At least one of TensorFlow 2.0 or PyTorch should be installed. "
368	            "To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ "
369	            "To install PyTorch, read the instructions at https://pytorch.org/."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/pipelines/base.py:809
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
808	            `image_processor`."""
809	    docstring += r"""
810	        modelcard (`str` or [`ModelCard`], *optional*):
811	            Model card attributed to the model for this pipeline.
812	        framework (`str`, *optional*):
813	            The framework to use, either `"pt"` for PyTorch or `"tf"` for TensorFlow. The specified framework must be
814	            installed.
815	
816	            If no framework is specified, will default to the one currently installed. If no framework is specified and
817	            both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is
818	            provided.
819	        task (`str`, defaults to `""`):
820	            A task-identifier for the pipeline.
821	        num_workers (`int`, *optional*, defaults to 8):
822	            When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of
823	            workers to be used.
824	        batch_size (`int`, *optional*, defaults to 1):
825	            When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
826	            the batch to use, for inference this is not always beneficial, please read [Batching with
827	            pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .
828	        args_parser ([`~pipelines.ArgumentHandler`], *optional*):
829	            Reference to the object in charge of parsing supplied pipeline parameters.
830	        device (`int`, *optional*, defaults to -1):
831	            Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on
832	            the associated CUDA device id. You can pass native `torch.device` or a `str` too
833	        torch_dtype (`str` or `torch.dtype`, *optional*):
834	            Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model
835	            (`torch.float16`, `torch.bfloat16`, ... or `"auto"`)"""
836	    if supports_binary_output:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/pipelines/image_text_to_text.py:321
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
320	            logger.warning_once(
321	                "The input data was not formatted as a chat with dicts containing 'role' and 'content' keys, even though this model supports chat. "
322	                "Consider using the chat format for better results. For more information, see https://huggingface.co/docs/transformers/en/chat_templating"
323	            )

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/pipelines/video_classification.py:134
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
133	        if video.startswith("http://") or video.startswith("https://"):
134	            video = BytesIO(requests.get(video).content)
135	

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/pipelines/zero_shot_audio_classification.py:109
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
108	                # like http_huggingface_co.png
109	                audio = requests.get(audio).content
110	            else:

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/processing_utils.py:554
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
553	        with open(json_file_path, "w", encoding="utf-8") as writer:
554	            writer.write(self.to_json_string())
555	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/processing_utils.py:640
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
639	                with open(output_raw_chat_template_file, "w", encoding="utf-8") as writer:
640	                    writer.write(self.chat_template)
641	                logger.info(f"chat template saved in {output_raw_chat_template_file}")

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/processing_utils.py:647
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
646	                with open(output_chat_template_file, "w", encoding="utf-8") as writer:
647	                    writer.write(chat_template_json_string)
648	                logger.info(f"chat template saved in {output_chat_template_file}")

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/processing_utils.py:784
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
783	                raise OSError(
784	                    f"Can't load processor for '{pretrained_model_name_or_path}'. If you were trying to load"
785	                    " it from 'https://huggingface.co/models', make sure you don't have a local directory with the"
786	                    f" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a"
787	                    f" directory containing a {PROCESSOR_NAME} file"
788	                )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/processing_utils.py:793
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
792	            with open(resolved_raw_chat_template_file, encoding="utf-8") as reader:
793	                chat_template = reader.read()
794	            kwargs["chat_template"] = chat_template

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/processing_utils.py:797
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
796	            with open(resolved_chat_template_file, encoding="utf-8") as reader:
797	                text = reader.read()
798	            chat_template = json.loads(text)["chat_template"]

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/processing_utils.py:815
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
814	            with open(resolved_processor_file, encoding="utf-8") as reader:
815	                text = reader.read()
816	            processor_dict = json.loads(text)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/processing_utils.py:1313
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1312	                raise ValueError(
1313	                    "No chat template is set for this processor. Please either set the `chat_template` attribute, "
1314	                    "or provide a chat template as an argument. See "
1315	                    "https://huggingface.co/docs/transformers/main/en/chat_templating for more information."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/quantizers/quantizer_bnb_4bit.py:105
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
104	                raise ValueError(
105	                    "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the "
106	                    "quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules "
107	                    "in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/quantizers/quantizer_bnb_8bit.py:102
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
101	                raise ValueError(
102	                    "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the "
103	                    "quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules "
104	                    "in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/quantizers/quantizer_eetq.py:52
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
51	            raise ImportError(
52	                "Using `eetq` 8-bit quantization requires eetq."
53	                "Please install the latest version of eetq from : https://github.com/NetEase-FuXi/EETQ"
54	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/quantizers/quantizer_fbgemm_fp8.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
57	            raise ImportError(
58	                "Using fbgemm fp8 quantization requires fbgemm-gpu library"
59	                "Please install the latest version of fbgemm-gpu library by following : https://pytorch.org/FBGEMM/fbgemm_gpu-development/InstallationInstructions.html#fbgemm-gpu-install-libraries"
60	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/quantizers/quantizer_hqq.py:65
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
64	            raise ImportError(
65	                "A valid HQQ version (>=0.2.1) is not available. Please follow the instructions to install it: `https://github.com/mobiusml/hqq/`."
66	            )
67	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/quantizers/quantizer_quark.py:70
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
69	            raise ImportError(
70	                "Loading a Quark quantized model requires the `quark` library but it was not found in the environment. Please refer to https://quark.docs.amd.com/latest/install.html."
71	            )
72	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/safetensors_conversion.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	    safetensors_convert_space_url = "https://safetensors-convert.hf.space"
27	    sse_url = f"{safetensors_convert_space_url}/call/run"

--------------------------------------------------
>> Issue: [B821:post] requests.post
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/safetensors_conversion.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b821_post.html
46	
47	    result = requests.post(sse_url, stream=True, json=data).json()
48	    event_id = result["event_id"]

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/safetensors_conversion.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
49	
50	    with requests.get(f"{sse_url}/{event_id}", stream=True) as sse_connection:
51	        try:

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/safetensors_conversion.py:63
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
62	    pr_title = "Adding `safetensors` variant of this model"
63	    token = kwargs.get("token")
64	

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/safetensors_conversion.py:83
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
82	    try:
83	        api = HfApi(token=cached_file_kwargs.get("token"), headers={"user-agent": http_user_agent()})
84	        sha = get_conversion_pr_reference(api, pretrained_model_name_or_path, **cached_file_kwargs)

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/safetensors_conversion.py:97
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
96	            revision=sha,
97	            token=cached_file_kwargs.get("token"),
98	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:195
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
194	USER = "__DUMMY_TRANSFORMERS_USER__"
195	ENDPOINT_STAGING = "https://hub-ci.huggingface.co"
196	
197	# Not critical, only usable on the sandboxed CI instance.
198	TOKEN = "hf_94wBhPGp6KrrTH3KDchhKpRxZwd6dmHWLL"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:620
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
619	        is_ipex_available(),
620	        "test requires Intel Extension for PyTorch to be installed and match current PyTorch version, see"
621	        " https://github.com/intel/intel-extension-for-pytorch",
622	    )(test_case)
623	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:1681
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1680	            if self.replay:
1681	                sys.stdout.write(captured)
1682	            self.out = apply_print_resets(captured)

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:1681
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
1680	            if self.replay:
1681	                sys.stdout.write(captured)
1682	            self.out = apply_print_resets(captured)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:1688
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1687	            if self.replay:
1688	                sys.stderr.write(captured)
1689	            self.err = captured

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:1688
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
1687	            if self.replay:
1688	                sys.stderr.write(captured)
1689	            self.err = captured

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:2000
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
1999	            paths.append(self.tests_dir_str)
2000	        paths.append(env.get("PYTHONPATH", ""))
2001	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:2050
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
2049	            if before is True and path.exists():
2050	                shutil.rmtree(tmp_dir, ignore_errors=True)
2051	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:2109
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
2108	        for path in self.teardown_tmp_dirs:
2109	            shutil.rmtree(path, ignore_errors=True)
2110	        self.teardown_tmp_dirs = []

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:2248
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
2247	            durations_min = 0.05  # sec
2248	            f.write("slowest durations\n")
2249	            for i, rep in enumerate(dlist):

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:2248
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
2247	            durations_min = 0.05  # sec
2248	            f.write("slowest durations\n")
2249	            for i, rep in enumerate(dlist):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:2251
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
2250	                if rep.duration < durations_min:
2251	                    f.write(f"{len(dlist) - i} durations < {durations_min} secs were omitted")
2252	                    break

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:2251
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
2250	                if rep.duration < durations_min:
2251	                    f.write(f"{len(dlist) - i} durations < {durations_min} secs were omitted")
2252	                    break

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:2253
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
2252	                    break
2253	                f.write(f"{rep.duration:02.2f}s {rep.when:<8} {rep.nodeid}\n")
2254	

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:2253
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
2252	                    break
2253	                f.write(f"{rep.duration:02.2f}s {rep.when:<8} {rep.nodeid}\n")
2254	

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:2412
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
2411	    """
2412	    worker = os.environ.get("PYTEST_XDIST_WORKER", "gw0")
2413	    worker = re.sub(r"^gw", "", worker, 0, re.M)

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:2675
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
2674	    if timeout is None:
2675	        timeout = int(os.environ.get("PYTEST_TIMEOUT", 600))
2676	

--------------------------------------------------
>> Issue: [B838:process] multiprocessing.Process
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:2686
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b838_process.html
2685	
2686	    process = ctx.Process(target=target_func, args=(input_queue, output_queue, timeout))
2687	    process.start()

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:2691
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
2690	    try:
2691	        results = output_queue.get(timeout=timeout)
2692	        output_queue.task_done()

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:2714
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
2713	        else:
2714	            test = " ".join(os.environ.get("PYTEST_CURRENT_TEST").split(" ")[:-1])
2715	            try:

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/testing_utils.py:2835
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
2834	    # !!!!!!!!!!! HF Specific !!!!!!!!!!!
2835	    skip_cuda_tests: bool = bool(os.environ.get("SKIP_CUDA_DOCTEST", False))
2836	    # !!!!!!!!!!! HF Specific !!!!!!!!!!!

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/tokenization_utils.py:793
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
792	            raise NotImplementedError(
793	                "return_offset_mapping is not available when using Python tokenizers. "
794	                "To use this feature, change your tokenizer to one deriving from "
795	                "transformers.PreTrainedTokenizerFast. "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/tokenization_utils_base.py:1313
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1312	
1313	INIT_TOKENIZER_DOCSTRING = r"""
1314	    Class attributes (overridden by derived classes)
1315	
1316	        - **vocab_files_names** (`Dict[str, str]`) -- A dictionary with, as keys, the `__init__` keyword name of each
1317	          vocabulary file required by the model, and as associated values, the filename for saving the associated file
1318	          (string).
1319	        - **pretrained_vocab_files_map** (`Dict[str, Dict[str, str]]`) -- A dictionary of dictionaries, with the
1320	          high-level keys being the `__init__` keyword name of each vocabulary file required by the model, the
1321	          low-level being the `short-cut-names` of the pretrained models with, as associated values, the `url` to the
1322	          associated pretrained vocabulary file.
1323	        - **model_input_names** (`List[str]`) -- A list of inputs expected in the forward pass of the model.
1324	        - **padding_side** (`str`) -- The default value for the side on which the model should have padding applied.
1325	          Should be `'right'` or `'left'`.
1326	        - **truncation_side** (`str`) -- The default value for the side on which the model should have truncation
1327	          applied. Should be `'right'` or `'left'`.
1328	
1329	    Args:
1330	        model_max_length (`int`, *optional*):
1331	            The maximum length (in number of tokens) for the inputs to the transformer model. When the tokenizer is
1332	            loaded with [`~tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained`], this will be set to the
1333	            value stored for the associated model in `max_model_input_sizes` (see above). If no value is provided, will
1334	            default to VERY_LARGE_INTEGER (`int(1e30)`).
1335	        padding_side (`str`, *optional*):
1336	            The side on which the model should have padding applied. Should be selected between ['right', 'left'].
1337	            Default value is picked from the class attribute of the same name.
1338	        truncation_side (`str`, *optional*):
1339	            The side on which the model should have truncation applied. Should be selected between ['right', 'left'].
1340	            Default value is picked from the class attribute of the same name.
1341	        chat_template (`str`, *optional*):
1342	            A Jinja template string that will be used to format lists of chat messages. See
1343	            https://huggingface.co/docs/transformers/chat_templating for a full description.
1344	        model_input_names (`List[string]`, *optional*):
1345	            The list of inputs accepted by the forward pass of the model (like `"token_type_ids"` or
1346	            `"attention_mask"`). Default value is picked from the class attribute of the same name.
1347	        bos_token (`str` or `tokenizers.AddedToken`, *optional*):
1348	            A special token representing the beginning of a sentence. Will be associated to `self.bos_token` and
1349	            `self.bos_token_id`.
1350	        eos_token (`str` or `tokenizers.AddedToken`, *optional*):
1351	            A special token representing the end of a sentence. Will be associated to `self.eos_token` and
1352	            `self.eos_token_id`.
1353	        unk_token (`str` or `tokenizers.AddedToken`, *optional*):
1354	            A special token representing an out-of-vocabulary token. Will be associated to `self.unk_token` and
1355	            `self.unk_token_id`.
1356	        sep_token (`str` or `tokenizers.AddedToken`, *optional*):
1357	            A special token separating two different sentences in the same input (used by BERT for instance). Will be
1358	            associated to `self.sep_token` and `self.sep_token_id`.
1359	        pad_token (`str` or `tokenizers.AddedToken`, *optional*):
1360	            A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by
1361	            attention mechanisms or loss computation. Will be associated to `self.pad_token` and `self.pad_token_id`.
1362	        cls_token (`str` or `tokenizers.AddedToken`, *optional*):
1363	            A special token representing the class of the input (used by BERT for instance). Will be associated to
1364	            `self.cls_token` and `self.cls_token_id`.
1365	        mask_token (`str` or `tokenizers.AddedToken`, *optional*):
1366	            A special token representing a masked token (used by masked-language modeling pretraining objectives, like
1367	            BERT). Will be associated to `self.mask_token` and `self.mask_token_id`.
1368	        additional_special_tokens (tuple or list of `str` or `tokenizers.AddedToken`, *optional*):
1369	            A tuple or a list of additional special tokens. Add them here to ensure they are skipped when decoding with
1370	            `skip_special_tokens` is set to True. If they are not part of the vocabulary, they will be added at the end
1371	            of the vocabulary.
1372	        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):
1373	            Whether or not the model should cleanup the spaces that were added when splitting the input text during the
1374	            tokenization process.
1375	        split_special_tokens (`bool`, *optional*, defaults to `False`):
1376	            Whether or not the special tokens should be split during the tokenization process. Passing will affect the
1377	            internal state of the tokenizer. The default behavior is to not split special tokens. This means that if
1378	            `<s>` is the `bos_token`, then `tokenizer.tokenize("<s>") = ['<s>`]. Otherwise, if
1379	            `split_special_tokens=True`, then `tokenizer.tokenize("<s>")` will be give `['<','s', '>']`.
1380	"""
1381	
1382	
1383	@add_end_docstrings(INIT_TOKENIZER_DOCSTRING)
1384	class PreTrainedTokenizerBase(SpecialTokensMixin, PushToHubMixin):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/tokenization_utils_base.py:1823
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1822	                raise ValueError(
1823	                    "Cannot use chat template functions because tokenizer.chat_template is not set and no template "
1824	                    "argument was passed! For information about writing templates and setting the "
1825	                    "tokenizer.chat_template attribute, please see the documentation at "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/tokenization_utils_base.py:2047
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2046	            raise EnvironmentError(
2047	                f"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from "
2048	                "'https://huggingface.co/models', make sure you don't have a local directory with the same name. "
2049	                f"Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory "
2050	                f"containing all relevant files for a {cls.__name__} tokenizer."
2051	            )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/tokenization_utils_base.py:2136
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
2135	            with open(chat_template_file) as chat_template_handle:
2136	                init_kwargs["chat_template"] = chat_template_handle.read()  # Clobbers any template in the config
2137	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/tokenization_utils_base.py:2461
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
2460	                with open(chat_template_file, "w", encoding="utf-8") as f:
2461	                    f.write(self.chat_template)
2462	                saved_raw_chat_template = True

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/tokenization_utils_base.py:2509
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
2508	            out_str = json.dumps(tokenizer_config, indent=2, sort_keys=True, ensure_ascii=False) + "\n"
2509	            f.write(out_str)
2510	        logger.info(f"tokenizer config file saved in {tokenizer_config_file}")

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/tokenization_utils_base.py:2518
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
2517	            out_str = json.dumps(write_dict, indent=2, sort_keys=True, ensure_ascii=False) + "\n"
2518	            f.write(out_str)
2519	        logger.info(f"Special tokens file saved in {special_tokens_map_file}")

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/tokenization_utils_base.py:2571
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
2570	                out_str = json.dumps(added_vocab, indent=2, sort_keys=True, ensure_ascii=False) + "\n"
2571	                f.write(out_str)
2572	                logger.info(f"added tokens file saved in {added_tokens_file}")

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/tokenization_utils_fast.py:716
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
715	                    out_str = json.dumps(added_vocab, indent=2, sort_keys=True, ensure_ascii=False) + "\n"
716	                    f.write(out_str)
717	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:494
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
493	            raise ValueError(
494	                f"The model you have picked ({model.__class__.__name__}) cannot be used as is for training: it only "
495	                "computes hidden states and does not accept any labels. You should choose a model with a head "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:561
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
560	            raise ValueError(
561	                "You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of"
562	                " the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft"
563	                " for more details"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:567
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
566	            raise ValueError(
567	                f"The model you are trying to fine-tune is quantized with {model.hf_quantizer.quantization_config.quant_method}"
568	                " but that quantization method do not support training. Please open an issue on GitHub: https://github.com/huggingface/transformers"
569	                f" to request the support for training support for {model.hf_quantizer.quantization_config.quant_method}"
570	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:758
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
757	                    raise ImportError(
758	                        "Using FP16 with APEX but APEX is not installed, please refer to"
759	                        " https://www.github.com/nvidia/apex."
760	                    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:1554
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1553	            except ImportError:
1554	                raise ValueError("Please install https://github.com/pytorch/torchdistx")
1555	        elif args.optim == OptimizerNames.SGD:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:1571
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1570	                raise ImportError(
1571	                    "You need to install `galore_torch` in order to use GaLore optimizers"
1572	                    " install it with `pip install git+https://github.com/jiaweizzhao/GaLore`"
1573	                )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:1603
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1602	                raise ImportError(
1603	                    "You need to install `apollo_torch` in order to use APOLLO optimizers"
1604	                    " install it with `pip install git+https://github.com/zhuhanqing/APOLLO`"
1605	                )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:1669
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1668	                raise ImportError(
1669	                    "You need to have `torchao>=0.4.0` in order to use torch 4-bit optimizers."
1670	                    "Install it with `pip install torchao` or follow the instructions here: https://github.com/pytorch/ao"
1671	                )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:1935
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1934	            raise ImportError(
1935	                "Using IPEX but IPEX is not installed or IPEX's version does not match current PyTorch, please refer"
1936	                " to https://github.com/intel/intel-extension-for-pytorch."
1937	            )

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:2723
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
2722	                    logger.info(f"Deleting older checkpoint [{checkpoint}] due to args.save_total_limit")
2723	                    shutil.rmtree(checkpoint, ignore_errors=True)
2724	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:2895
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2894	                    logger.warning(
2895	                        "The intermediate checkpoints of PEFT may not be saved correctly, "
2896	                        f"consider using a custom callback to save {ADAPTER_WEIGHTS_NAME} in corresponding saving folders. "
2897	                        "Check some examples here: https://github.com/huggingface/peft/issues/96"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:2996
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2995	                            logger.warning(
2996	                                "The intermediate checkpoints of PEFT may not be saved correctly, "
2997	                                f"consider using a custom callback to save {ADAPTER_WEIGHTS_NAME} in corresponding saving folders. "
2998	                                "Check some examples here: https://github.com/huggingface/peft/issues/96"

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:4084
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
4083	            logger.info(f"Deleting older checkpoint [{checkpoint}] due to args.save_total_limit")
4084	            shutil.rmtree(checkpoint, ignore_errors=True)
4085	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:4697
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
4696	        with open(model_card_filepath, "w") as f:
4697	            f.write(model_card)
4698	

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:4697
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
4696	        with open(model_card_filepath, "w") as f:
4697	            f.write(model_card)
4698	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:4719
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
4718	                with open(index_path) as f:
4719	                    index = json.loads(f.read())
4720	                shard_files = list(set(index["weight_map"].values()))

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:5068
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
5067	            with open(os.path.join(self.repo.local_dir, ".gitignore")) as f:
5068	                current_content = f.read()
5069	        else:

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:5085
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
5084	                logger.debug(f"Writing .gitignore file. Content: {content}")
5085	                f.write(content)
5086	

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer.py:5085
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
5084	                logger.debug(f"Writing .gitignore file. Content: {content}")
5085	                f.write(content)
5086	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/trainer_utils.py:611
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
610	        self.peak_monitoring = True
611	        peak_monitor_thread = threading.Thread(target=self.peak_monitor_func)
612	        peak_monitor_thread.daemon = True

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/training_args.py:94
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
93	            logger.warning(
94	                "Please use the NeuronTrainer from optimum[neuron] instead of the Transformers library to perform "
95	                "training on AWS Trainium instances. More information here: "
96	                "https://github.com/huggingface/optimum-neuron"

--------------------------------------------------
>> Issue: [B803:gethostname] socket.gethostname
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/training_args.py:120
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b803_gethostname.html
119	    current_time = datetime.now().strftime("%b%d_%H-%M-%S")
120	    return os.path.join("runs", current_time + "_" + socket.gethostname())
121	

--------------------------------------------------
>> Issue: [B302:blacklist] socket.gethostname
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/training_args.py:120
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b302-socket-gethostname
119	    current_time = datetime.now().strftime("%b%d_%H-%M-%S")
120	    return os.path.join("runs", current_time + "_" + socket.gethostname())
121	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/training_args.py:893
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
892	        metadata={
893	            "help": "Number of steps to wait before calling `torch.<device>.empty_cache()`."
894	            "This can help avoid CUDA out-of-memory errors by lowering peak VRAM usage at a cost of about [10% slower performance](https://github.com/huggingface/transformers/issues/31372)."
895	            "If left unset or set to None, cache will not be emptied."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/training_args.py:1056
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1055	            "help": (
1056	                "Use Intel extension for PyTorch when it is available, installation:"
1057	                " 'https://github.com/intel/intel-extension-for-pytorch'"
1058	            )
1059	        },

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/training_args.py:1078
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1077	            "help": (
1078	                "For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. "
1079	                "See details at https://nvidia.github.io/apex/amp.html"
1080	            )
1081	        },

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/training_args.py:1463
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1462	            "help": (
1463	                'The scope to use when doing hyperparameter search with Ray. By default, `"last"` will be used. Ray'
1464	                " will then use the last checkpoint of all trials, compare those, and select the best one. However,"
1465	                " other options are also available. See the Ray documentation"
1466	                " (https://docs.ray.io/en/latest/tune/api_docs/analysis.html"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/training_args.py:1509
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1508	        metadata={
1509	            "help": "Activates neftune noise embeddings into the model. NEFTune has been proven to drastically improve model performances for instruction fine-tuning. Check out the original paper here: https://arxiv.org/abs/2310.05914 and the original code here: https://github.com/neelsjain/NEFTune. Only supported for `PreTrainedModel` and `PeftModel` classes."
1510	        },
1511	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/training_args.py:1547
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1546	        metadata={
1547	            "help": "Whether or not to average tokens across devices. If enabled, will use all_reduce to "
1548	            "synchronize num_tokens_in_batch for precise loss calculation. Reference: "
1549	            "https://github.com/huggingface/transformers/issues/34242"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/training_args.py:1677
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1676	            logger.info(
1677	                f"Found safetensors installation, but --save_safetensors={self.save_safetensors}. "
1678	                f"Safetensors should be a preferred weights saving format due to security and performance reasons. "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/training_args.py:1849
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1848	                logger.warning(
1849	                    "When using the Trainer, CodeCarbonCallback requires the `codecarbon` package, which is not compatible with AMD ROCm (https://github.com/mlco2/codecarbon/pull/490). Automatically disabling the codecarbon callback. Reference: https://huggingface.co/docs/transformers/v4.39.3/en/main_classes/trainer#transformers.TrainingArguments.report_to."
1850	                )
1851	                self.report_to.remove("codecarbon")

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/training_args.py:1885
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1884	            logger.warning(
1885	                "When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please"
1886	                " use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather"
1887	                " operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/__init__.py:288
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
287	            error_message = (
288	                "This example requires a source install from HuggingFace Transformers (see "
289	                "`https://huggingface.co/docs/transformers/installation#install-from-source`),"
290	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/__init__.py:296
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
295	            error_message
296	            + "Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other "
297	            "versions of HuggingFace Transformers."
298	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/attention_visualizer.py:171
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
170	        if self.config.model_type in PROCESSOR_MAPPING_NAMES:
171	            img = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg?download=true"
172	            img = Image.open(requests.get(img, stream=True).raw)

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/attention_visualizer.py:172
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
171	            img = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg?download=true"
172	            img = Image.open(requests.get(img, stream=True).raw)
173	            processor = AutoProcessor.from_pretrained(self.repo_id, image_seq_length=5)

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/attention_visualizer.py:225
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
224	            sliding_window=getattr(self.config, "sliding_window", None),
225	            token_type_ids=kwargs.get("token_type_ids", None),
226	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:111
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
110	
111	S3_BUCKET_PREFIX = "https://s3.amazonaws.com/models.huggingface.co/bert"
112	CLOUDFRONT_DISTRIB_PREFIX = "https://cdn.huggingface.co"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:112
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
111	S3_BUCKET_PREFIX = "https://s3.amazonaws.com/models.huggingface.co/bert"
112	CLOUDFRONT_DISTRIB_PREFIX = "https://cdn.huggingface.co"
113	
114	_staging_mode = os.environ.get("HUGGINGFACE_CO_STAGING", "NO").upper() in ENV_VARS_TRUE_VALUES

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:114
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
113	
114	_staging_mode = os.environ.get("HUGGINGFACE_CO_STAGING", "NO").upper() in ENV_VARS_TRUE_VALUES
115	_default_endpoint = "https://hub-ci.huggingface.co" if _staging_mode else "https://huggingface.co"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:115
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
114	_staging_mode = os.environ.get("HUGGINGFACE_CO_STAGING", "NO").upper() in ENV_VARS_TRUE_VALUES
115	_default_endpoint = "https://hub-ci.huggingface.co" if _staging_mode else "https://huggingface.co"
116	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:115
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
114	_staging_mode = os.environ.get("HUGGINGFACE_CO_STAGING", "NO").upper() in ENV_VARS_TRUE_VALUES
115	_default_endpoint = "https://hub-ci.huggingface.co" if _staging_mode else "https://huggingface.co"
116	

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:118
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
117	HUGGINGFACE_CO_RESOLVE_ENDPOINT = _default_endpoint
118	if os.environ.get("HUGGINGFACE_CO_RESOLVE_ENDPOINT", None) is not None:
119	    warnings.warn(

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:124
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
123	    )
124	    HUGGINGFACE_CO_RESOLVE_ENDPOINT = os.environ.get("HUGGINGFACE_CO_RESOLVE_ENDPOINT", None)
125	HUGGINGFACE_CO_RESOLVE_ENDPOINT = os.environ.get("HF_ENDPOINT", HUGGINGFACE_CO_RESOLVE_ENDPOINT)

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:125
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
124	    HUGGINGFACE_CO_RESOLVE_ENDPOINT = os.environ.get("HUGGINGFACE_CO_RESOLVE_ENDPOINT", None)
125	HUGGINGFACE_CO_RESOLVE_ENDPOINT = os.environ.get("HF_ENDPOINT", HUGGINGFACE_CO_RESOLVE_ENDPOINT)
126	HUGGINGFACE_CO_PREFIX = HUGGINGFACE_CO_RESOLVE_ENDPOINT + "/{model_id}/resolve/{revision}/{filename}"

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:147
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
146	    try:
147	        instance_data = requests.get(os.environ["ECS_CONTAINER_METADATA_URI"]).json()
148	        dlc_container_used = instance_data["Image"]

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:185
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
184	    # CI will set this value to True
185	    if os.environ.get("TRANSFORMERS_IS_CI", "").upper() in ENV_VARS_TRUE_VALUES:
186	        ua += "; is_ci/true"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:382
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
381	                    raise OSError(
382	                        f"{path_or_repo_id} does not appear to have a file named {filename}. Checkout "
383	                        f"'https://huggingface.co/{path_or_repo_id}/tree/{revision_}' for available files."
384	                    )

--------------------------------------------------
>> Issue: [B835:load] http.cookiejar.FileCookieJar.load
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:424
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b835_load.html
423	            # This is slightly better for only 1 file
424	            hf_hub_download(
425	                path_or_repo_id,
426	                filenames[0],
427	                subfolder=None if len(subfolder) == 0 else subfolder,
428	                repo_type=repo_type,
429	                revision=revision,
430	                cache_dir=cache_dir,
431	                user_agent=user_agent,
432	                force_download=force_download,
433	                proxies=proxies,
434	                resume_download=resume_download,
435	                token=token,
436	                local_files_only=local_files_only,
437	            )

--------------------------------------------------
>> Issue: [B835:load] http.cookiejar.FileCookieJar.load
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:439
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b835_load.html
438	        else:
439	            snapshot_download(
440	                path_or_repo_id,
441	                allow_patterns=full_filenames,
442	                repo_type=repo_type,
443	                revision=revision,
444	                cache_dir=cache_dir,
445	                user_agent=user_agent,
446	                force_download=force_download,
447	                proxies=proxies,
448	                resume_download=resume_download,
449	                token=token,
450	                local_files_only=local_files_only,
451	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:457
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
456	            raise OSError(
457	                f"{path_or_repo_id} is not a local folder and is not a valid model identifier "
458	                "listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:464
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
463	            raise OSError(
464	                f"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists "
465	                "for this model name. Check the model page at "
466	                f"'https://huggingface.co/{path_or_repo_id}' for available revisions."
467	            ) from e

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:482
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
481	            raise OSError(
482	                "You are trying to access a gated repo.\nMake sure to have access to it at "
483	                f"https://huggingface.co/{path_or_repo_id}.\n{str(e)}"
484	            ) from e

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:492
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
491	                raise OSError(
492	                    f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load the files, and couldn't find them in the"
493	                    f" cached files.\nCheckout your internet connection or see how to run the library in offline mode at"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:518
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
517	        raise EnvironmentError(
518	            f"{path_or_repo_id} does not appear to have {msg}. Checkout 'https://huggingface.co/{path_or_repo_id}/tree/{revision_}'"
519	            "for available files."

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:625
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
624	    with os.fdopen(tmp_fd, "wb") as f:
625	        http_get(url, f, proxies=proxies)
626	    return tmp_file

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:709
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
708	        raise OSError(
709	            f"{path_or_repo} is a gated repository. Make sure to request access at "
710	            f"https://huggingface.co/{path_or_repo} and pass a token having permission to this repo either by "
711	            "logging in with `huggingface-cli login` or by passing `token=<your_token>`."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:715
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
714	        logger.error(e)
715	        raise OSError(f"{path_or_repo} is not a local folder or a valid repository name on 'https://hf.co'.") from e
716	    except RevisionNotFoundError as e:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:719
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
718	        raise OSError(
719	            f"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists for this "
720	            f"model name. Check the model page at 'https://huggingface.co/{path_or_repo}' for available revisions."
721	        ) from e

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:1101
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
1100	    with open(index_filename) as f:
1101	        index = json.loads(f.read())
1102	

--------------------------------------------------
>> Issue: [B835:load] http.cookiejar.FileCookieJar.load
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/hub.py:1155
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b835_load.html
1154	        # Check if the model card is present on the remote repo
1155	        model_card = ModelCard.load(repo_id, token=token, ignore_metadata_errors=ignore_metadata_errors)
1156	    except EntryNotFoundError:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:1465
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1464	# docstyle-ignore
1465	SENTENCEPIECE_IMPORT_ERROR = """
1466	{0} requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the
1467	installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
1468	that match your environment. Please note that you may need to restart your runtime after installation.
1469	"""
1470	
1471	
1472	# docstyle-ignore
1473	PROTOBUF_IMPORT_ERROR = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:1473
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1472	# docstyle-ignore
1473	PROTOBUF_IMPORT_ERROR = """
1474	{0} requires the protobuf library but it was not found in your environment. Checkout the instructions on the
1475	installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
1476	that match your environment. Please note that you may need to restart your runtime after installation.
1477	"""
1478	
1479	
1480	# docstyle-ignore
1481	FAISS_IMPORT_ERROR = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:1481
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1480	# docstyle-ignore
1481	FAISS_IMPORT_ERROR = """
1482	{0} requires the faiss library but it was not found in your environment. Checkout the instructions on the
1483	installation page of its repo: https://github.com/facebookresearch/faiss/blob/master/INSTALL.md and follow the ones
1484	that match your environment. Please note that you may need to restart your runtime after installation.
1485	"""
1486	
1487	
1488	# docstyle-ignore
1489	PYTORCH_IMPORT_ERROR = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:1489
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1488	# docstyle-ignore
1489	PYTORCH_IMPORT_ERROR = """
1490	{0} requires the PyTorch library but it was not found in your environment. Checkout the instructions on the
1491	installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.
1492	Please note that you may need to restart your runtime after installation.
1493	"""
1494	
1495	
1496	# docstyle-ignore
1497	TORCHVISION_IMPORT_ERROR = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:1497
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1496	# docstyle-ignore
1497	TORCHVISION_IMPORT_ERROR = """
1498	{0} requires the Torchvision library but it was not found in your environment. Checkout the instructions on the
1499	installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.
1500	Please note that you may need to restart your runtime after installation.
1501	"""
1502	
1503	# docstyle-ignore
1504	PYTORCH_IMPORT_ERROR_WITH_TF = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:1504
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1503	# docstyle-ignore
1504	PYTORCH_IMPORT_ERROR_WITH_TF = """
1505	{0} requires the PyTorch library but it was not found in your environment.
1506	However, we were able to find a TensorFlow installation. TensorFlow classes begin
1507	with "TF", but are otherwise identically named to our PyTorch classes. This
1508	means that the TF equivalent of the class you tried to import would be "TF{0}".
1509	If you want to use TensorFlow, please use TF classes instead!
1510	
1511	If you really do want to use PyTorch please go to
1512	https://pytorch.org/get-started/locally/ and follow the instructions that
1513	match your environment.
1514	"""
1515	
1516	# docstyle-ignore
1517	TF_IMPORT_ERROR_WITH_PYTORCH = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:1517
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1516	# docstyle-ignore
1517	TF_IMPORT_ERROR_WITH_PYTORCH = """
1518	{0} requires the TensorFlow library but it was not found in your environment.
1519	However, we were able to find a PyTorch installation. PyTorch classes do not begin
1520	with "TF", but are otherwise identically named to our TF classes.
1521	If you want to use PyTorch, please use those classes instead!
1522	
1523	If you really do want to use TensorFlow, please follow the instructions on the
1524	installation page https://www.tensorflow.org/install that match your environment.
1525	"""
1526	
1527	# docstyle-ignore
1528	BS4_IMPORT_ERROR = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:1549
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1548	# docstyle-ignore
1549	TENSORFLOW_IMPORT_ERROR = """
1550	{0} requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the
1551	installation page: https://www.tensorflow.org/install and follow the ones that match your environment.
1552	Please note that you may need to restart your runtime after installation.
1553	"""
1554	
1555	
1556	# docstyle-ignore
1557	DETECTRON2_IMPORT_ERROR = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:1557
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1556	# docstyle-ignore
1557	DETECTRON2_IMPORT_ERROR = """
1558	{0} requires the detectron2 library but it was not found in your environment. Checkout the instructions on the
1559	installation page: https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md and follow the ones
1560	that match your environment. Please note that you may need to restart your runtime after installation.
1561	"""
1562	
1563	
1564	# docstyle-ignore
1565	FLAX_IMPORT_ERROR = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:1565
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1564	# docstyle-ignore
1565	FLAX_IMPORT_ERROR = """
1566	{0} requires the FLAX library but it was not found in your environment. Checkout the instructions on the
1567	installation page: https://github.com/google/flax and follow the ones that match your environment.
1568	Please note that you may need to restart your runtime after installation.
1569	"""
1570	
1571	# docstyle-ignore
1572	FTFY_IMPORT_ERROR = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:1572
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1571	# docstyle-ignore
1572	FTFY_IMPORT_ERROR = """
1573	{0} requires the ftfy library but it was not found in your environment. Checkout the instructions on the
1574	installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones
1575	that match your environment. Please note that you may need to restart your runtime after installation.
1576	"""
1577	
1578	LEVENSHTEIN_IMPORT_ERROR = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:1590
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1589	# docstyle-ignore
1590	PYTORCH_QUANTIZATION_IMPORT_ERROR = """
1591	{0} requires the pytorch-quantization library but it was not found in your environment. You can install it with pip:
1592	`pip install pytorch-quantization --extra-index-url https://pypi.ngc.nvidia.com`
1593	Please note that you may need to restart your runtime after installation.
1594	"""
1595	
1596	# docstyle-ignore
1597	TENSORFLOW_PROBABILITY_IMPORT_ERROR = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:1597
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1596	# docstyle-ignore
1597	TENSORFLOW_PROBABILITY_IMPORT_ERROR = """
1598	{0} requires the tensorflow_probability library but it was not found in your environment. You can install it with pip as
1599	explained here: https://github.com/tensorflow/probability. Please note that you may need to restart your runtime after installation.
1600	"""
1601	
1602	# docstyle-ignore
1603	TENSORFLOW_TEXT_IMPORT_ERROR = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:1603
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1602	# docstyle-ignore
1603	TENSORFLOW_TEXT_IMPORT_ERROR = """
1604	{0} requires the tensorflow_text library but it was not found in your environment. You can install it with pip as
1605	explained here: https://www.tensorflow.org/text/guide/tf_text_intro.
1606	Please note that you may need to restart your runtime after installation.
1607	"""
1608	
1609	# docstyle-ignore
1610	TORCHAUDIO_IMPORT_ERROR = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:1616
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1615	# docstyle-ignore
1616	PANDAS_IMPORT_ERROR = """
1617	{0} requires the pandas library but it was not found in your environment. You can install it with pip as
1618	explained here: https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html.
1619	Please note that you may need to restart your runtime after installation.
1620	"""
1621	
1622	
1623	# docstyle-ignore
1624	PHONEMIZER_IMPORT_ERROR = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:1667
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1666	
1667	NUMEXPR_IMPORT_ERROR = """
1668	{0} requires the numexpr library but it was not found in your environment. You can install it by referring to:
1669	https://numexpr.readthedocs.io/en/latest/index.html.
1670	"""
1671	
1672	
1673	# docstyle-ignore
1674	NLTK_IMPORT_ERROR = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:1674
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1673	# docstyle-ignore
1674	NLTK_IMPORT_ERROR = """
1675	{0} requires the NLTK library but it was not found in your environment. You can install it by referring to:
1676	https://www.nltk.org/install.html. Please note that you may need to restart your runtime after installation.
1677	"""
1678	
1679	
1680	# docstyle-ignore
1681	VISION_IMPORT_ERROR = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:1707
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1706	# docstyle-ignore
1707	CCL_IMPORT_ERROR = """
1708	{0} requires the torch ccl library but it was not found in your environment. You can install it with pip:
1709	`pip install oneccl_bind_pt -f https://developer.intel.com/ipex-whl-stable`
1710	Please note that you may need to restart your runtime after installation.
1711	"""
1712	
1713	# docstyle-ignore
1714	ESSENTIA_IMPORT_ERROR = """

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/import_utils.py:2162
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
2161	        with open(os.path.join(directory, module_name), encoding="utf-8") as f:
2162	            file_content = f.read()
2163	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/quantization_config.py:149
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
148	
149	            writer.write(json_string)
150	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/src/transformers/utils/quantization_config.py:244
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
243	            raise ImportError(
244	                "A valid HQQ version (>=0.2.1) is not available. Please follow the instructions to install it: `https://github.com/mobiusml/hqq/`."
245	            )
246	

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_image_processing_common.py:176
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
175	        dummy_image = Image.open(
176	            requests.get("http://images.cocodataset.org/val2017/000000039769.jpg", stream=True).raw
177	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_image_processing_common.py:176
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
175	        dummy_image = Image.open(
176	            requests.get("http://images.cocodataset.org/val2017/000000039769.jpg", stream=True).raw
177	        )

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_image_processing_common.py:311
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
310	        difference = {
311	            key: dict_slow_0.get(key) if key in dict_slow_0 else dict_slow_1.get(key)
312	            for key in set(dict_slow_0) ^ set(dict_slow_1)

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_image_processing_common.py:311
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
310	        difference = {
311	            key: dict_slow_0.get(key) if key in dict_slow_0 else dict_slow_1.get(key)
312	            for key in set(dict_slow_0) ^ set(dict_slow_1)

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_image_processing_common.py:326
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
325	        difference = {
326	            key: dict_fast_0.get(key) if key in dict_fast_0 else dict_fast_1.get(key)
327	            for key in set(dict_fast_0) ^ set(dict_fast_1)

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_image_processing_common.py:326
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
325	        difference = {
326	            key: dict_fast_0.get(key) if key in dict_fast_0 else dict_fast_1.get(key)
327	            for key in set(dict_fast_0) ^ set(dict_fast_1)

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_image_processing_common.py:361
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
360	        difference = {
361	            key: dict_slow_0.get(key) if key in dict_slow_0 else dict_slow_1.get(key)
362	            for key in set(dict_slow_0) ^ set(dict_slow_1)

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_image_processing_common.py:361
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
360	        difference = {
361	            key: dict_slow_0.get(key) if key in dict_slow_0 else dict_slow_1.get(key)
362	            for key in set(dict_slow_0) ^ set(dict_slow_1)

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_image_processing_common.py:376
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
375	        difference = {
376	            key: dict_fast_0.get(key) if key in dict_fast_0 else dict_fast_1.get(key)
377	            for key in set(dict_fast_0) ^ set(dict_fast_1)

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_image_processing_common.py:376
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
375	        difference = {
376	            key: dict_fast_0.get(key) if key in dict_fast_0 else dict_fast_1.get(key)
377	            for key in set(dict_fast_0) ^ set(dict_fast_1)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_image_processing_common.py:595
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
594	        with open(fixtures_path / "coco_annotations.txt", "r") as f:
595	            detection_target = json.loads(f.read())
596	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_image_processing_common.py:606
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
605	        with open(fixtures_path / "coco_panoptic_annotations.txt", "r") as f:
606	            panoptic_target = json.loads(f.read())
607	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_modeling_flax_common.py:800
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
799	                        with open(shard_file, "rb") as state_f:
800	                            state_file = from_bytes(FlaxBertModel, state_f.read())
801	                            self.assertEqual(len(state_file), 1)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_modeling_flax_common.py:805
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
804	                with open(index_file, "r", encoding="utf-8") as f:
805	                    index = json.loads(f.read())
806	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_processing_common.py:798
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
797	        messages[0][0]["content"].append(
798	            {"type": "image", "url": "https://www.ilankelman.org/stopsigns/australia.jpg"}
799	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_processing_common.py:863
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
862	        batched_messages[0][0]["content"].append(
863	            {"type": "image", "url": "https://www.ilankelman.org/stopsigns/australia.jpg"}
864	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_processing_common.py:866
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
865	        batched_messages[1][0]["content"].append(
866	            {"type": "image", "url": "http://images.cocodataset.org/val2017/000000039769.jpg"}
867	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_processing_common.py:918
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
917	        messages[0][0]["content"].append(
918	            {"type": "image", "url": "https://www.ilankelman.org/stopsigns/australia.jpg"}
919	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_processing_common.py:944
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
943	                "content": [
944	                    {"type": "image", "url": "https://www.ilankelman.org/stopsigns/australia.jpg"},
945	                    {"type": "text", "text": "What is shown in this image?"},

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_processing_common.py:1008
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1007	            "type": "video",
1008	            "url": "https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4",
1009	        }
1010	        num_frames = 3
1011	        out_dict_with_video = processor.apply_chat_template(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_processing_common.py:1062
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1061	            "url": [
1062	                "https://www.ilankelman.org/stopsigns/australia.jpg",
1063	                "https://www.ilankelman.org/stopsigns/australia.jpg",
1064	            ],
1065	        }

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_processing_common.py:1063
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1062	                "https://www.ilankelman.org/stopsigns/australia.jpg",
1063	                "https://www.ilankelman.org/stopsigns/australia.jpg",
1064	            ],
1065	        }
1066	        out_dict_with_video = processor.apply_chat_template(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_processing_common.py:1226
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1225	                        "type": "audio",
1226	                        "url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3",
1227	                    },
1228	                    {"type": "text", "text": "Is it the same sound?"},
1229	                ],
1230	            },

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_processing_common.py:1301
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1300	        messages[1]["content"][0]["audio"] = (
1301	            "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3"
1302	        )
1303	        messages[3]["content"][0]["audio"] = (
1304	            "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_processing_common.py:1304
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1303	        messages[3]["content"][0]["audio"] = (
1304	            "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3"
1305	        )
1306	        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)
1307	        self.assertTrue(self.audio_input_name in out_dict)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_processing_common.py:1334
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1333	                        "type": "audio",
1334	                        "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3",
1335	                    },
1336	                    {"type": "text", "text": "What's that sound?"},
1337	                ],
1338	            },

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_processing_common.py:1348
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1347	                        "type": "audio",
1348	                        "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav",
1349	                    },
1350	                    {"type": "text", "text": "How about this one?"},
1351	                ],
1352	            },

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_tokenization_common.py:257
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
256	        with open(f"{get_tests_dir()}/fixtures/sample_text.txt", encoding="utf-8") as f_data:
257	            cls._data = f_data.read().replace("\n\n", "\n").strip()
258	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_tokenization_common.py:263
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
262	    def tearDownClass(cls):
263	        shutil.rmtree(cls.tmpdirname)
264	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_tokenization_common.py:570
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
569	
570	        shutil.rmtree(tmpdirname_1)
571	        tokenizer_slow_2.save_pretrained(tmpdirname_2)

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_tokenization_common.py:575
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
574	        encoding_tokenizer_slow_3 = tokenizer_slow_3(text)
575	        shutil.rmtree(tmpdirname_2)
576	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_tokenization_common.py:768
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
767	
768	                shutil.rmtree(tmpdirname)
769	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_tokenization_common.py:801
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
800	
801	                shutil.rmtree(tmpdirname)
802	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_tokenization_common.py:836
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
835	
836	                shutil.rmtree(tmpdirname)
837	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_tokenization_common.py:1703
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1702	                    with Path(tmp_dir_name, "chat_template.jinja").open("w") as f:
1703	                        f.write(dummy_template2)
1704	                    new_tokenizer = tokenizer.from_pretrained(tmp_dir_name)

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_tokenization_common.py:1703
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
1702	                    with Path(tmp_dir_name, "chat_template.jinja").open("w") as f:
1703	                        f.write(dummy_template2)
1704	                    new_tokenizer = tokenizer.from_pretrained(tmp_dir_name)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_tokenization_common.py:2090
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2089	    @unittest.skip(
2090	        reason="start to fail after # 29473. See https://github.com/huggingface/transformers/pull/29473#pullrequestreview-1945687810"
2091	    )

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_tokenization_common.py:4138
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
4137	
4138	                shutil.rmtree(tmpdirname2)
4139	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_tokenization_common.py:4157
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
4156	
4157	                shutil.rmtree(tmpdirname2)
4158	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_tokenization_common.py:4176
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
4175	
4176	                shutil.rmtree(tmpdirname2)
4177	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/transformers-4.51.2/transformers-4.51.2/tests/test_tokenization_common.py:4630
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
4629	                finally:
4630	                    shutil.rmtree(tmp_dir)
4631	

--------------------------------------------------

Code scanned:
	Total lines of code: 821305
	Total lines skipped (#nosec): 0

Run metrics:
	Total issues (by severity):
		Undefined: 0.0
		Low: 0.0
		Medium: 973.0
		High: 394.0
	Total issues (by confidence):
		Undefined: 0.0
		Low: 0.0
		Medium: 1355.0
		High: 12.0
Files skipped (0):
