Run started:2025-04-12 10:25:51.511633

Test results:
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/pinecone_text-0.10.0/pinecone_text-0.10.0/pinecone_text/dense/jina_encoder.py:6
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
5	
6	JINA_API_URL: str = "https://api.jina.ai/v1/embeddings"
7	
8	
9	class JinaEncoder(BaseDenseEncoder):

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/pinecone_text-0.10.0/pinecone_text-0.10.0/pinecone_text/dense/jina_encoder.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
29	        if api_key is None:
30	            api_key = os.environ.get("JINA_API_KEY", None)
31	

--------------------------------------------------
>> Issue: [B821:post] requests.post
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/pinecone_text-0.10.0/pinecone_text-0.10.0/pinecone_text/dense/jina_encoder.py:69
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b821_post.html
68	
69	        resp = self._session.post(  # type: ignore
70	            JINA_API_URL, json={"input": texts_input, "model": self._model_name}
71	        ).json()

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/pinecone_text-0.10.0/pinecone_text-0.10.0/pinecone_text/dense/sentence_transformer_encoder.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	            raise ImportError(
37	                """Failed to import torch. Make sure you install dense extra 
38	                dependencies by running: `pip install pinecone-text[dense]`
39	        If this doesn't help, it is probably a CUDA error. If you do want to use GPU, 

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/pinecone_text-0.10.0/pinecone_text-0.10.0/pinecone_text/sparse/bm25_encoder.py:155
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
154	
155	        df = np.array([self.doc_freq.get(idx, 1) for idx in indices])  # type: ignore
156	        idf = np.log((self.n_docs + 1) / (df + 0.5))  # type: ignore

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/pinecone_text-0.10.0/pinecone_text-0.10.0/pinecone_text/sparse/bm25_encoder.py:258
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
257	        bm25 = BM25Encoder()
258	        url = "https://storage.googleapis.com/pinecone-datasets-dev/bm25_params/msmarco_bm25_params_v4_0_0.json"
259	        with tempfile.TemporaryDirectory() as tmp_dir:

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/pinecone_text-0.10.0/pinecone_text-0.10.0/pinecone_text/sparse/bm25_encoder.py:261
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
260	            tmp_path = Path(tmp_dir, "msmarco_bm25_params.json")
261	            response = requests.get(url, stream=True)
262	            response.raise_for_status()

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/pinecone_text-0.10.0/pinecone_text-0.10.0/pinecone_text/sparse/bm25_encoder.py:265
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
264	                for chunk in response.iter_content(chunk_size=8192):
265	                    f.write(chunk)
266	            bm25.load(str(tmp_path))

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/pinecone_text-0.10.0/pinecone_text-0.10.0/pinecone_text/sparse/splade_encoder.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	            raise ImportError(
47	                """Failed to import torch. Make sure you install pytorch extra dependencies by running: `pip install pinecone-text[splade]`
48	        If this doesn't help, it is probably a CUDA error. If you do want to use GPU, please check your CUDA driver.
49	        If you want to use CPU only, run the following command:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/pinecone_text-0.10.0/pinecone_text-0.10.0/setup.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	    'description': 'Text utilities library by Pinecone.io',
37	    'long_description': '<h1 align="center">\n  <img src="https://avatars.githubusercontent.com/u/54333248?s=200&v=4">\n    <br>\n    Pinecone Text Client\n    <br>\n</h1>\n\nThe Pinecone Text Client is a Python package that provides text utilities designed for seamless integration with Pinecone\'s [sparse-dense](https://docs.pinecone.io/docs/hybrid-search) (hybrid) semantic search.\n\n> **_âš ï¸ Warning_**\n>\n> This is a **public preview** ("Beta") version.   \n> For any issues or requests, please reach out to our [support](support@pinecone.io) team.\n## Installation\nTo install the Pinecone Text Client, use the following command:\n```bash\npip install pinecone-text\n```\n\nIf you wish to use `SpladeEncoder`, you will need to install the `splade` extra:\n```bash\npip install pinecone-text[splade]\n```\n\nIf you wish to use `SentenceTransformerEncoder` dense encoder, you will need to install the `dense` extra:\n```bash\npip install pinecone-text[dense]\n```\n\nIf you wish to use `OpenAIEncoder` dense encoder, you will need to install the `openai` extra:\n```bash\npip install pinecone-text[openai]\n```\n\n## Sparse Encoding\n\nTo convert your own text corpus to sparse vectors, you can either use [BM25](https://www.pinecone.io/learn/semantic-search/#bm25) or [SPLADE](https://www.pinecone.io/learn/splade/).\n\n### BM25\nTo encode your documents and queries using BM25 as vector for dot product search, you can use the `BM25Encoder` class.\n\n> **_ðŸ“ NOTE:_**\n> \n> Our current implementation of BM25 supports only static document frequency (meaning that the document frequency values are precomputed and fixed, and do not change dynamically based on new documents added to the collection).\n>\n> When conducting a search, you may come across queries that contain terms not found in the training corpus but are present in the database. To address this scenario, BM25Encoder uses a default document frequency value of 1 when encoding such terms. \n#### Usage\n\nFor an end-to-end example, you can refer to our Quora dataset generation with BM25 [notebook](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/search/semantic-search/sparse/bm25/bm25-vector-generation.ipynb).\n\n```python\nfrom pinecone_text.sparse import BM25Encoder\n\ncorpus = ["The quick brown fox jumps over the lazy dog",\n          "The lazy dog is brown",\n          "The fox is brown"]\n\n# Initialize BM25 and fit the corpus\nbm25 = BM25Encoder()\nbm25.fit(corpus)\n\n# Encode a new document (for upsert to Pinecone index)\ndoc_sparse_vector = bm25.encode_documents("The brown fox is quick")\n# {"indices": [102, 18, 12, ...], "values": [0.22, 0.38, 0.15, ...]}\n\n# Encode a query (for search in Pinecone index)\nquery_sparse_vector = bm25.encode_queries("Which fox is brown?")\n# {"indices": [102, 16, 18, ...], "values": [0.22, 0.11, 0.15, ...]}\n\n# store BM25 params as json\nbm25.dump("bm25_params.json")\n\n# load BM25 params from json\nbm25.load("bm25_params.json")\n```\n\n#### Load Default Parameters\nIf you want to use the default parameters for `BM25Encoder`, you can call the `default` method.\nThe default parameters were fitted on the [MS MARCO](https://microsoft.github.io/msmarco/)  passage ranking dataset.\n```python\nfrom pinecone_text.sparse import BM25Encoder\nbm25 = BM25Encoder.default()\n```\n\n#### BM25 Parameters\nThe `BM25Encoder` class offers configurable parameters to customize the encoding:\n\n* `b`: Controls document length normalization (default: 0.75).\n* `k1`: Controls term frequency saturation (default: 1.2).\n* Tokenization Options: Allows customization of the tokenization process, including options for handling case, punctuation, stopwords, stemming, and language selection.\n\nThese parameters can be specified when initializing the BM25Encoder class. Please read the BM25Encoder documentation for more details.\n\n### SPLADE\n\nCurrently the `SpladeEncoder` class supprts only the [naver/splade-cocondenser-ensembledistil](https://huggingface.co/naver/splade-cocondenser-ensembledistil) model, and follows [SPLADE V2](https://arxiv.org/abs/2109.10086) implementation.\n\n> **_ðŸ“ NOTE:_**\n> \n> Currently pinecone text not supoorts SPLADE with python 3.12 due to compatibility issues with pytorch\n>\n\n#### Usage\n\nFor an end-to-end example, you can refer to our Quora dataset generation with SPLADE [notebook](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/search/semantic-search/sparse/splade/splade-vector-generation.ipynb).\n\nNote: If cuda is available, the model will automatically run on GPU. You can explicitly override the device using the `device` parameter in the constructor.\n\n```python\nfrom pinecone_text.sparse import SpladeEncoder\n\n# Initialize Splade\nsplade = SpladeEncoder()\n\n# encode a batch of documents\ndocuments = ["The quick brown fox jumps over the lazy dog",\n             "The lazy dog is brown",\n             "The fox is brown"]\ndocument_vectors = splade.encode_documents(documents)\n# [{"indices": [102, 18, 12, ...], "values": [0.21, 0.38, 0.15, ...]}, ...]\n\n# encode a query\nquery = "Which fox is brown?"\nquery_vectors = splade.encode_queries(query)\n# {"indices": [102, 18, 12, ...], "values": [0.21, 0.38, 0.15, ...]}\n```\n\n\n## Dense Encoding\n\nFor dense embedding we also provide a thin wrapper for the following models:\n1. All Sentence Transformers models hosted on huggingface [See full list of models](https://huggingface.co/sentence-transformers)\n2. All OpenAI API supported embedding models [See full list of models](https://platform.openai.com/docs/models/embeddings)\n\n### Sentence Transformers models\n\nWhen using `SentenceTransformerEncoder`, the models are downloaded from huggingface and run locally. Also, if cuda is available, the model will automatically run on GPU. You can explicitly override the device using the `device` parameter in the constructor.\n\n> **_ðŸ“ NOTE:_**\n> \n> Currently pinecone text not supoorts sentence transformers with python 3.12 due to compatibility issues with pytorch\n>\n\n#### Usage\n```python\nfrom pinecone_text.dense import SentenceTransformerEncoder\n\nencoder = SentenceTransformerEncoder("sentence-transformers/all-MiniLM-L6-v2")\n\nencoder.encode_documents(["The quick brown fox jumps over the lazy dog"])\n# [[0.21, 0.38, 0.15, ...]]\n\nencoder.encode_queries(["Who jumped over the lazy dog?"])\n# [[0.11, 0.43, 0.67, ...]]\n```\n\n### OpenAI models\n\nWhen using the `OpenAIEncoder`, you need to provide an API key for the OpenAI API, and store it in the `OPENAI_API_KEY` environment variable before you import the encoder.\n\nBy default the encoder will use `text-embedding-3-small` as recommended by OpenAI. You can also specify a different model name using the `model_name` parameter.\n#### Usage\n```python\nfrom pinecone_text.dense import OpenAIEncoder\n\nencoder = OpenAIEncoder() # defaults to the recommended model - "text-embedding-3-small"\n\nencoder.encode_documents(["The quick brown fox jumps over the lazy dog"])\n# [[0.21, 0.38, 0.15, ...]]\n\nencoder.encode_queries(["Who jumped over the lazy dog?"])\n# [[0.11, 0.43, 0.67, ...]]\n```\n\nPinecone text also supports Azure OpenAI API. To use it, you need to import the `AzureOpenAIEncoder` class instead of `OpenAIEncoder`. You also need to pass Azure specific environment variables to the constructor, along with your specific embeddings  deployment as the model name. For more information please follow the `AzureOpenAIEncoder` documentation.\n\n\n### Jina AI models\n\nWhen using the `JinaEncoder`, you need to provide an API key for the Jina Embeddings API, and store it in the `JINA_API_KEY` environment variable before you import the encoder.\n\nBy default the encoder will use `jina-embeddings-v2-base-en`. You can also specify a different model name using the `model_name` parameter.\n\n#### Usage\n\n```python\nfrom pinecone_text.dense import JinaEncoder\n\nencoder = JinaEncoder()\n\nencoder.encode_documents(["The quick brown fox jumps over the lazy dog"])\n# [[-0.62586284, -0.54578537, 0.5570845, ...]]\n\nencoder.encode_queries(["Who jumped over the lazy dog?"])\n# [[-0.43374294, -0.42069837, 0.773763, ...]]\n```\n\n## Combining Sparse and Dense Encodings for Hybrid Search\nTo combine sparse and dense encodings for hybrid search, you can use the `hybrid_convex_scale` method on your query.\n\nThis method receives both a dense vector and a sparse vector, along with a convex scaling parameter `alpha`. It returns a tuple consisting of the scaled dense and sparse vectors according to the following formula: `(alpha * dense_vector, (1 - alpha) * sparse_vector)`.\n```python\nfrom pinecone_text.hybrid import hybrid_convex_scale\nfrom pinecone_text.sparse import SpladeEncoder\nfrom pinecone_text.dense import SentenceTransformerEncoder\n\n# Initialize Splade\nsplade = SpladeEncoder()\n\n# Initialize Sentence Transformer\nsentence_transformer = SentenceTransformerEncoder("sentence-transformers/all-MiniLM-L6-v2")\n\n# encode a query\nsparse_vector = splade.encode_queries("Which fox is brown?")\ndense_vector = sentence_transformer.encode_queries("Which fox is brown?")\n\n# combine sparse and dense vectors\nhybrid_dense, hybrid_sparse = hybrid_convex_scale(dense_vector, sparse_vector, alpha=0.8)\n# ([-0.21, 0.38, 0.15, ...], {"indices": [102, 16, 18, ...], "values": [0.21, 0.11, 0.18, ...]})\n```\n',
38	    'author': 'Pinecone.io',
39	    'author_email': 'None',
40	    'maintainer': 'None',
41	    'maintainer_email': 'None',
42	    'url': 'None',
43	    'packages': packages,
44	    'package_data': package_data,
45	    'install_requires': install_requires,
46	    'extras_require': extras_require,
47	    'python_requires': '>=3.9,<4.0',
48	}
49	
50	
51	setup(**setup_kwargs)

--------------------------------------------------

Code scanned:
	Total lines of code: 869
	Total lines skipped (#nosec): 0

Run metrics:
	Total issues (by severity):
		Undefined: 0.0
		Low: 0.0
		Medium: 5.0
		High: 5.0
	Total issues (by confidence):
		Undefined: 0.0
		Low: 0.0
		Medium: 10.0
		High: 0.0
Files skipped (0):
