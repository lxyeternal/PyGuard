Run started:2025-04-12 13:17:44.471753

Test results:
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/accumulators.py:322
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
321	        SocketServer.TCPServer.shutdown(self)
322	        self.server_close()
323	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/context.py:418
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
417	        if isinstance(
418	            threading.current_thread(), threading._MainThread  # type: ignore[attr-defined]
419	        ):

--------------------------------------------------
>> Issue: [B828:signal] signal.signal
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/context.py:420
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b828_signal.html
419	        ):
420	            signal.signal(signal.SIGINT, signal_handler)
421	

--------------------------------------------------
>> Issue: [B324:blacklist] signal_signal
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/context.py:420
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b324-signal-signal
419	        ):
420	            signal.signal(signal.SIGINT, signal_handler)
421	

--------------------------------------------------
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/context.py:973
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
972	            serializer.dump_stream(data, chunked_out)
973	            chunked_out.close()
974	            # this call will block until the server has read all the data and processed it (or

--------------------------------------------------
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/context.py:986
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
985	                finally:
986	                    tempFile.close()
987	                return reader_func(tempFile.name)

--------------------------------------------------
>> Issue: [B303:blacklist] pwd.getpwuid
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/context.py:2719
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b303-pwd-getpwuid
2718	
2719	                username = pwd.getpwuid(os.getuid()).pw_name
2720	                filename = self._jvm.PythonRDD.runJobToPythonFile(  # type: ignore[attr-defined]

--------------------------------------------------
>> Issue: [B811:getuid] os.getuid
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/context.py:2719
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b811_getuid.html
2718	
2719	                username = pwd.getpwuid(os.getuid()).pw_name
2720	                filename = self._jvm.PythonRDD.runJobToPythonFile(  # type: ignore[attr-defined]

--------------------------------------------------
>> Issue: [B305:blacklist] os.getuid
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/context.py:2719
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
2718	
2719	                username = pwd.getpwuid(os.getuid()).pw_name
2720	                filename = self._jvm.PythonRDD.runJobToPythonFile(  # type: ignore[attr-defined]

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/rdd.py:562
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
561	
562	        return checkpointFile.get() if checkpointFile.isDefined() else None
563	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/rdd.py:1581
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1580	                    s = str(obj).rstrip("\n") + "\n"
1581	                    out.write(s.encode("utf-8"))
1582	                out.close()

--------------------------------------------------
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/rdd.py:1582
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
1581	                    out.write(s.encode("utf-8"))
1582	                out.close()
1583	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/rdd.py:1584
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
1583	
1584	            Thread(target=pipe_objs, args=[pipe.stdin]).start()
1585	

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/rdd.py:1704
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
1703	        """
1704	        if self.ctx._conf.get(self.ctx._jvm.PythonSecurityUtils.USE_FILE_BASED_COLLECT()):
1705	            # This file-based collect path is used for Python DataFrame ACLs.

--------------------------------------------------
>> Issue: [B303:blacklist] pwd.getpwuid
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/rdd.py:1710
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b303-pwd-getpwuid
1709	
1710	                username = pwd.getpwuid(os.getuid()).pw_name
1711	                filename = self.ctx._jvm.PythonRDD.collectToPythonFile(self._jrdd.rdd(), username)

--------------------------------------------------
>> Issue: [B811:getuid] os.getuid
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/rdd.py:1710
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b811_getuid.html
1709	
1710	                username = pwd.getpwuid(os.getuid()).pw_name
1711	                filename = self.ctx._jvm.PythonRDD.collectToPythonFile(self._jrdd.rdd(), username)

--------------------------------------------------
>> Issue: [B305:blacklist] os.getuid
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/rdd.py:1710
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
1709	
1710	                username = pwd.getpwuid(os.getuid()).pw_name
1711	                filename = self.ctx._jvm.PythonRDD.collectToPythonFile(self._jrdd.rdd(), username)

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/rdd.py:1763
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
1762	        )
1763	        if self.ctx._conf.get(self.ctx._jvm.PythonSecurityUtils.USE_FILE_BASED_COLLECT()):
1764	            # This file-based collect path is used for Python DataFrame ACLs.

--------------------------------------------------
>> Issue: [B303:blacklist] pwd.getpwuid
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/rdd.py:1769
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b303-pwd-getpwuid
1768	
1769	                username = pwd.getpwuid(os.getuid()).pw_name
1770	                filename = self.ctx._jvm.PythonRDD.collectToPythonFileWithJobGroup(

--------------------------------------------------
>> Issue: [B811:getuid] os.getuid
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/rdd.py:1769
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b811_getuid.html
1768	
1769	                username = pwd.getpwuid(os.getuid()).pw_name
1770	                filename = self.ctx._jvm.PythonRDD.collectToPythonFileWithJobGroup(

--------------------------------------------------
>> Issue: [B305:blacklist] os.getuid
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/rdd.py:1769
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
1768	
1769	                username = pwd.getpwuid(os.getuid()).pw_name
1770	                filename = self.ctx._jvm.PythonRDD.collectToPythonFileWithJobGroup(

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/rdd.py:4022
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
4021	    def _memory_limit(self) -> int:
4022	        return _parse_memory(self.ctx._conf.get("spark.python.worker.memory", "512m"))
4023	

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/core/rdd.py:5392
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
5391	            self.ctx.profiler_collector
5392	            and self.ctx._conf.get("spark.python.profile", "false") == "true"
5393	        ):

--------------------------------------------------
>> Issue: [B828:signal] signal.signal
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b828_signal.html
57	    """
58	    signal.signal(SIGHUP, SIG_DFL)
59	    signal.signal(SIGCHLD, SIG_DFL)

--------------------------------------------------
>> Issue: [B324:blacklist] signal_signal
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:58
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b324-signal-signal
57	    """
58	    signal.signal(SIGHUP, SIG_DFL)
59	    signal.signal(SIGCHLD, SIG_DFL)

--------------------------------------------------
>> Issue: [B828:signal] signal.signal
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:59
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b828_signal.html
58	    signal.signal(SIGHUP, SIG_DFL)
59	    signal.signal(SIGCHLD, SIG_DFL)
60	    signal.signal(SIGTERM, SIG_DFL)

--------------------------------------------------
>> Issue: [B324:blacklist] signal_signal
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:59
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b324-signal-signal
58	    signal.signal(SIGHUP, SIG_DFL)
59	    signal.signal(SIGCHLD, SIG_DFL)
60	    signal.signal(SIGTERM, SIG_DFL)

--------------------------------------------------
>> Issue: [B828:signal] signal.signal
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:60
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b828_signal.html
59	    signal.signal(SIGCHLD, SIG_DFL)
60	    signal.signal(SIGTERM, SIG_DFL)
61	    # restore the handler for SIGINT,

--------------------------------------------------
>> Issue: [B324:blacklist] signal_signal
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:60
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b324-signal-signal
59	    signal.signal(SIGCHLD, SIG_DFL)
60	    signal.signal(SIGTERM, SIG_DFL)
61	    # restore the handler for SIGINT,

--------------------------------------------------
>> Issue: [B828:signal] signal.signal
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:63
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b828_signal.html
62	    # it's useful for debugging (show the stacktrace before exit)
63	    signal.signal(SIGINT, signal.default_int_handler)
64	

--------------------------------------------------
>> Issue: [B324:blacklist] signal_signal
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:63
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b324-signal-signal
62	    # it's useful for debugging (show the stacktrace before exit)
63	    signal.signal(SIGINT, signal.default_int_handler)
64	

--------------------------------------------------
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:80
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
79	            outfile.flush()
80	            sock.close()
81	            return 1

--------------------------------------------------
>> Issue: [B304:blacklist] socket.socket
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:105
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
104	        # Create a listening socket on the AF_INET loopback interface
105	        listen_sock = socket.socket(AF_INET6, SOCK_STREAM)
106	        # Passing port 0 to bind() causes it to select a random available port

--------------------------------------------------
>> Issue: [B304:blacklist] socket.socket
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:113
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
112	        # Create a listening socket on the AF_INET loopback interface
113	        listen_sock = socket.socket(AF_INET, SOCK_STREAM)
114	        # Passing port 0 to bind() causes it to select a random available port

--------------------------------------------------
>> Issue: [B823:ip_found] ip_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:115
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b823_ip_found.html
114	        # Passing port 0 to bind() causes it to select a random available port
115	        listen_sock.bind(("127.0.0.1", 0))
116	        listen_sock.listen(max(1024, SOMAXCONN))

--------------------------------------------------
>> Issue: [B828:signal] signal.signal
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:128
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b828_signal.html
127	    def shutdown(code):
128	        signal.signal(SIGTERM, SIG_DFL)
129	        # Send SIGHUP to notify workers of shutdown

--------------------------------------------------
>> Issue: [B324:blacklist] signal_signal
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:128
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b324-signal-signal
127	    def shutdown(code):
128	        signal.signal(SIGTERM, SIG_DFL)
129	        # Send SIGHUP to notify workers of shutdown

--------------------------------------------------
>> Issue: [B828:signal] signal.signal
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:136
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b828_signal.html
135	
136	    signal.signal(SIGTERM, handle_sigterm)  # Gracefully exit on SIGTERM
137	    signal.signal(SIGHUP, SIG_IGN)  # Don't die on SIGHUP

--------------------------------------------------
>> Issue: [B324:blacklist] signal_signal
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:136
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b324-signal-signal
135	
136	    signal.signal(SIGTERM, handle_sigterm)  # Gracefully exit on SIGTERM
137	    signal.signal(SIGHUP, SIG_IGN)  # Don't die on SIGHUP

--------------------------------------------------
>> Issue: [B828:signal] signal.signal
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:137
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b828_signal.html
136	    signal.signal(SIGTERM, handle_sigterm)  # Gracefully exit on SIGTERM
137	    signal.signal(SIGHUP, SIG_IGN)  # Don't die on SIGHUP
138	    signal.signal(SIGCHLD, SIG_IGN)

--------------------------------------------------
>> Issue: [B324:blacklist] signal_signal
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:137
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b324-signal-signal
136	    signal.signal(SIGTERM, handle_sigterm)  # Gracefully exit on SIGTERM
137	    signal.signal(SIGHUP, SIG_IGN)  # Don't die on SIGHUP
138	    signal.signal(SIGCHLD, SIG_IGN)

--------------------------------------------------
>> Issue: [B828:signal] signal.signal
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:138
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b828_signal.html
137	    signal.signal(SIGHUP, SIG_IGN)  # Don't die on SIGHUP
138	    signal.signal(SIGCHLD, SIG_IGN)
139	

--------------------------------------------------
>> Issue: [B324:blacklist] signal_signal
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:138
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b324-signal-signal
137	    signal.signal(SIGHUP, SIG_IGN)  # Don't die on SIGHUP
138	    signal.signal(SIGCHLD, SIG_IGN)
139	

--------------------------------------------------
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:190
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
189	                        outfile.flush()
190	                        outfile.close()
191	                        sock.close()

--------------------------------------------------
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:191
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
190	                        outfile.close()
191	                        sock.close()
192	                        continue

--------------------------------------------------
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:196
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
195	                    # in child process
196	                    listen_sock.close()
197	

--------------------------------------------------
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:210
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
209	                    os.dup2(devnull.fileno(), 0)
210	                    devnull.close()
211	

--------------------------------------------------
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:219
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
218	                        outfile.flush()
219	                        outfile.close()
220	                        authenticated = False

--------------------------------------------------
>> Issue: [B809:recv] socket.socket.recv
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:228
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b809_recv.html
227	                                try:
228	                                    while sock.recv(1024):
229	                                        pass

--------------------------------------------------
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/daemon.py:240
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
239	                else:
240	                    sock.close()
241	

--------------------------------------------------
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/databricks/sql/chunk.py:166
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
165	            if hasattr(pa_reader, "close"):
166	                pa_reader.close()
167	        sockfile.close()

--------------------------------------------------
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/databricks/sql/chunk.py:167
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
166	                pa_reader.close()
167	        sockfile.close()

--------------------------------------------------
>> Issue: [B841:cdll] ctypes.CDLL
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/databricks/utils/memory.py:32
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b841_cdll.html
31	def malloc_trim():
32	    ctypes.CDLL("libc.so.6").malloc_trim(0)
33	

--------------------------------------------------
>> Issue: [B809:recv] socket.socket.recv
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/databricks/workerwrap.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b809_recv.html
46	    # a connection attempt.
47	    if not conn.recv(4, socket.MSG_PEEK):
48	        # We do not log here, as liveness checks would only clutter the logs.

--------------------------------------------------
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/databricks/workerwrap.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
48	        # We do not log here, as liveness checks would only clutter the logs.
49	        conn.close()
50	        return

--------------------------------------------------
>> Issue: [B808:settimeout] settimeout
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/databricks/workerwrap.py:60
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b808_settimeout.html
59	
60	    conn.settimeout(None)  # disable timeout, as worker_main does not expect this to be set
61	

--------------------------------------------------
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/databricks/workerwrap.py:108
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
107	
108	    conn.close()
109	    print("run_worker_main finished")

--------------------------------------------------
>> Issue: [B304:blacklist] socket.socket
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/databricks/workerwrap.py:120
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
119	    """
120	    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
121	    s.bind(("0.0.0.0", port))

--------------------------------------------------
>> Issue: [B823:ip_found] ip_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/databricks/workerwrap.py:121
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b823_ip_found.html
120	    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
121	    s.bind(("0.0.0.0", port))
122	    s.listen()

--------------------------------------------------
>> Issue: [B808:settimeout] settimeout
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/databricks/workerwrap.py:125
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b808_settimeout.html
124	        conn, addr = s.accept()
125	        conn.settimeout(30)  # expect some initial bytes to be sent quickly
126	        Thread(target=run_worker_main, args=(conn, addr, worker_main)).start()

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/databricks/workerwrap.py:126
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
125	        conn.settimeout(30)  # expect some initial bytes to be sent quickly
126	        Thread(target=run_worker_main, args=(conn, addr, worker_main)).start()
127	

--------------------------------------------------
>> Issue: [B811:getuid] os.getuid
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/databricks/workerwrap.py:154
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b811_getuid.html
153	       os.environ: {dict(os.environ)}
154	       uid: {os.getuid()}
155	       groups: {os.getgroups()}"""

--------------------------------------------------
>> Issue: [B305:blacklist] os.getuid
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/databricks/workerwrap.py:154
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
153	       os.environ: {dict(os.environ)}
154	       uid: {os.getuid()}
155	       groups: {os.getgroups()}"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/errors_doc_gen.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	    """
23	    header = """..  Licensed to the Apache Software Foundation (ASF) under one
24	    or more contributor license agreements.  See the NOTICE file
25	    distributed with this work for additional information
26	    regarding copyright ownership.  The ASF licenses this file
27	    to you under the Apache License, Version 2.0 (the
28	    "License"); you may not use this file except in compliance
29	    with the License.  You may obtain a copy of the License at
30	
31	..    http://www.apache.org/licenses/LICENSE-2.0
32	
33	..  Unless required by applicable law or agreed to in writing,
34	    software distributed under the License is distributed on an
35	    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
36	    KIND, either express or implied.  See the License for the
37	    specific language governing permissions and limitations
38	    under the License.
39	
40	========================
41	Error classes in PySpark
42	========================
43	
44	This is a list of common, named error classes returned by PySpark which are defined at `error-conditions.json <https://github.com/apache/spark/blob/master/python/pyspark/errors/error-conditions.json>`_.
45	
46	When writing PySpark errors, developers must use an error class from the list. If an appropriate error class is not available, add a new one into the list. For more information, please refer to `Contributing Error and Exception <contributing.rst#contributing-error-and-exception>`_.
47	"""  # noqa
48	    with open(output_rst_file_path, "w") as f:

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/examples/src/main/python/ml/dataframe_example.py:75
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
74	    newDF.printSchema()
75	    shutil.rmtree(tempdir)
76	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/examples/src/main/python/mllib/naive_bayes_example.py:56
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
55	    output_dir = 'target/tmp/myNaiveBayesModel'
56	    shutil.rmtree(output_dir, ignore_errors=True)
57	    model.save(sc, output_dir)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/examples/src/main/python/streaming/recoverable_network_wordcount.py:95
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
94	        with open(outputPath, 'a') as f:
95	            f.write(counts + "\n")
96	

--------------------------------------------------
>> Issue: [B834:open] tarfile.open
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/install.py:146
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b834_open.html
145	            print("Downloading %s from:\n- %s" % (pretty_pkg_name, url))
146	            download_to_file(urllib.request.urlopen(url), package_local_path)
147	

--------------------------------------------------
>> Issue: [B818:urlopen] urllib.request.urlopen
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/install.py:146
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b818_urlopen.html
145	            print("Downloading %s from:\n- %s" % (pretty_pkg_name, url))
146	            download_to_file(urllib.request.urlopen(url), package_local_path)
147	

--------------------------------------------------
>> Issue: [B834:open] tarfile.open
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/install.py:149
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b834_open.html
148	            print("Installing to %s" % dest)
149	            tar = tarfile.open(package_local_path, "r:gz")
150	            for member in tar.getmembers():

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/install.py:160
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
159	            traceback.print_exc()
160	            rmtree(dest, ignore_errors=True)
161	        finally:

--------------------------------------------------
>> Issue: [B834:open] tarfile.open
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/install.py:173
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b834_open.html
172	        try:
173	            response = urllib.request.urlopen(
174	                "https://www.apache.org/dyn/closer.lua?preferred=true"
175	            )

--------------------------------------------------
>> Issue: [B818:urlopen] urllib.request.urlopen
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/install.py:173
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b818_urlopen.html
172	        try:
173	            response = urllib.request.urlopen(
174	                "https://www.apache.org/dyn/closer.lua?preferred=true"
175	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/install.py:174
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
173	            response = urllib.request.urlopen(
174	                "https://www.apache.org/dyn/closer.lua?preferred=true"
175	            )
176	            mirror_urls.append(response.read().decode("utf-8"))

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/install.py:176
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
175	            )
176	            mirror_urls.append(response.read().decode("utf-8"))
177	        except Exception:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/install.py:182
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
181	    default_sites = [
182	        "https://dlcdn.apache.org/",
183	        "https://archive.apache.org/dist",
184	        "https://dist.apache.org/repos/dist/release",
185	    ]
186	    return list(set(mirror_urls)) + [x for x in default_sites if x not in mirror_urls]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/install.py:183
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
182	        "https://dlcdn.apache.org/",
183	        "https://archive.apache.org/dist",
184	        "https://dist.apache.org/repos/dist/release",
185	    ]
186	    return list(set(mirror_urls)) + [x for x in default_sites if x not in mirror_urls]
187	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/install.py:184
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
183	        "https://archive.apache.org/dist",
184	        "https://dist.apache.org/repos/dist/release",
185	    ]
186	    return list(set(mirror_urls)) + [x for x in default_sites if x not in mirror_urls]
187	
188	

--------------------------------------------------
>> Issue: [B834:open] tarfile.open
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/install.py:193
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b834_open.html
192	
193	    with open(path, mode="wb") as dest:
194	        while True:

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/install.py:195
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
194	        while True:
195	            chunk = response.read(chunk_size)
196	            bytes_so_far += len(chunk)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/install.py:199
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
198	                break
199	            dest.write(chunk)
200	            print(

--------------------------------------------------
>> Issue: [B812:system] os.system
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/java_gateway.py:66
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b812_system.html
65	        # proper classpath and settings from spark-env.sh
66	        on_windows = platform.system() == "Windows"
67	        script = "./bin/spark-submit.cmd" if on_windows else "./bin/spark-submit"

--------------------------------------------------
>> Issue: [B817:system] platform.system
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/java_gateway.py:66
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b817_system.html
65	        # proper classpath and settings from spark-env.sh
66	        on_windows = platform.system() == "Windows"
67	        script = "./bin/spark-submit.cmd" if on_windows else "./bin/spark-submit"

--------------------------------------------------
>> Issue: [B308:blacklist] platform.system
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/java_gateway.py:66
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b308-platform-system
65	        # proper classpath and settings from spark-env.sh
66	        on_windows = platform.system() == "Windows"
67	        script = "./bin/spark-submit.cmd" if on_windows else "./bin/spark-submit"

--------------------------------------------------
>> Issue: [B828:signal] signal.signal
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/java_gateway.py:97
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b828_signal.html
96	                def preexec_func():
97	                    signal.signal(signal.SIGINT, signal.SIG_IGN)
98	

--------------------------------------------------
>> Issue: [B324:blacklist] signal_signal
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/java_gateway.py:97
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b324-signal-signal
96	                def preexec_func():
97	                    signal.signal(signal.SIGINT, signal.SIG_IGN)
98	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/java_gateway.py:119
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
118	        finally:
119	            shutil.rmtree(conn_info_dir)
120	

--------------------------------------------------
>> Issue: [B839:pool] multiprocessing.Pool
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/classification.py:3585
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b839_pool.html
3584	
3585	        pool = ThreadPool(processes=min(self.getParallelism(), numClasses))
3586	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/classification.py:4009
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
4008	            subModelPath = os.path.join(path, f"model_{idx}")
4009	            cast(MLWritable, instance.models[idx]).write().session(self.sparkSession).save(
4010	                subModelPath

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/classification.py:4346
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
4345	        try:
4346	            rmtree(temp_path)
4347	        except OSError:

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/clustering.py:2192
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
2191	        try:
2192	            rmtree(temp_path)
2193	        except OSError:

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/connect/io_utils.py:149
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
148	                if os.path.isdir(path):
149	                    shutil.rmtree(path)
150	                else:

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/connect/io_utils.py:255
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
254	        finally:
255	            shutil.rmtree(tmp_local_dir, ignore_errors=True)
256	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/connect/io_utils.py:276
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
275	                with open(os.path.join(tmp_local_dir, file_name), "wb") as f:
276	                    f.write(file_content)
277	

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/connect/io_utils.py:276
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
275	                with open(os.path.join(tmp_local_dir, file_name), "wb") as f:
276	                    f.write(file_content)
277	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/connect/io_utils.py:280
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
279	        finally:
280	            shutil.rmtree(tmp_local_dir, ignore_errors=True)
281	

--------------------------------------------------
>> Issue: [B839:pool] multiprocessing.Pool
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/connect/tuning.py:428
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b839_pool.html
427	
428	        pool = ThreadPool(processes=min(self.getParallelism(), numModels))
429	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/dl_util.py:126
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
125	            if prefix_code != "":
126	                f.write(prefix_code)
127	            f.write(code_snippet)

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/dl_util.py:126
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
125	            if prefix_code != "":
126	                f.write(prefix_code)
127	            f.write(code_snippet)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/dl_util.py:127
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
126	                f.write(prefix_code)
127	            f.write(code_snippet)
128	            if suffix_code != "":

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/dl_util.py:127
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
126	                f.write(prefix_code)
127	            f.write(code_snippet)
128	            if suffix_code != "":

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/dl_util.py:129
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
128	            if suffix_code != "":
129	                f.write(suffix_code)
130	

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/dl_util.py:129
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
128	            if suffix_code != "":
129	                f.write(suffix_code)
130	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/evaluation.py:1161
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
1160	        try:
1161	            rmtree(temp_path)
1162	        except OSError:

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/feature.py:7768
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
7767	        try:
7768	            rmtree(temp_path)
7769	        except OSError:

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/fpm.py:537
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
536	        try:
537	            rmtree(temp_path)
538	        except OSError:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/param/_shared_params_code_gen.py:20
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
19	
20	header = """#
21	# Licensed to the Apache Software Foundation (ASF) under one or more
22	# contributor license agreements.  See the NOTICE file distributed with
23	# this work for additional information regarding copyright ownership.
24	# The ASF licenses this file to You under the Apache License, Version 2.0
25	# (the "License"); you may not use this file except in compliance with
26	# the License.  You may obtain a copy of the License at
27	#
28	#    http://www.apache.org/licenses/LICENSE-2.0
29	#
30	# Unless required by applicable law or agreed to in writing, software
31	# distributed under the License is distributed on an "AS IS" BASIS,
32	# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
33	# See the License for the specific language governing permissions and
34	# limitations under the License.
35	#"""
36	
37	# Code generator for shared params (shared.py). Run under this folder with:
38	# python _shared_params_code_gen.py > shared.py
39	
40	_type_for_type_converter = {

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/pipeline.py:420
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
419	        for index, stage in enumerate(stages):
420	            cast(MLWritable, stage).write().save(
421	                PipelineSharedReadWrite.getStagePath(stage.uid, index, len(stages), stagesDir)

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/recommendation.py:762
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
761	        try:
762	            rmtree(temp_path)
763	        except OSError:

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/regression.py:3329
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
3328	        try:
3329	            rmtree(temp_path)
3330	        except OSError:

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/distributor.py:495
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
494	                    else:
495	                        sys.stdout.write(decoded)
496	                if log_streaming_client:

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/distributor.py:495
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
494	                    else:
495	                        sys.stdout.write(decoded)
496	                if log_streaming_client:

--------------------------------------------------
>> Issue: [B304:blacklist] socket.socket
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/distributor.py:667
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
666	
667	                        sock = socket.socket()
668	                        sock.bind((address, 0))

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/distributor.py:879
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
878	            with open(schema_file_path, "w") as f:
879	                f.write(schema_json_string)
880	

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/distributor.py:879
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
878	            with open(schema_file_path, "w") as f:
879	                f.write(schema_json_string)
880	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/distributor.py:926
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
925	    def _cleanup_files(save_dir: str) -> None:
926	        shutil.rmtree(save_dir, ignore_errors=True)
927	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/log_communication.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
43	        while self.server.is_active:
44	            packed_number_bytes = self.rfile.read(4)
45	            if not packed_number_bytes:

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/log_communication.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
48	            number_bytes = unpack(">i", packed_number_bytes)[0]
49	            message = self.rfile.read(number_bytes)
50	            yield message

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/log_communication.py:56
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
55	            with _get_log_print_lock():
56	                sys.stderr.write(bline.decode("utf-8") + "\n")
57	                sys.stderr.flush()

--------------------------------------------------
>> Issue: [B304:blacklist] socket.socket
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/log_communication.py:69
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
68	    def _get_free_port(spark_host_address: str = "") -> int:
69	        with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as tcp:
70	            tcp.bind((spark_host_address, 0))

--------------------------------------------------
>> Issue: [B823:ip_found] ip_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/log_communication.py:79
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b823_ip_found.html
78	        def serve_task(port: int) -> None:
79	            with socketserver.ThreadingTCPServer(("0.0.0.0", port), WriteLogToStdout) as server:
80	                self.server = server

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/log_communication.py:85
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
84	        self.port = LogStreamingServer._get_free_port(spark_host_address)
85	        self.serve_thread = threading.Thread(target=serve_task, args=(self.port,))
86	        self.serve_thread.daemon = True

--------------------------------------------------
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/log_communication.py:136
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
135	        if LogStreamingClient._log_callback_client is not None:
136	            LogStreamingClient._log_callback_client.close()
137	

--------------------------------------------------
>> Issue: [B304:blacklist] socket.socket
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/log_communication.py:163
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
162	        try:
163	            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
164	            sock.settimeout(self.timeout)

--------------------------------------------------
>> Issue: [B808:settimeout] settimeout
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/log_communication.py:164
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b808_settimeout.html
163	            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
164	            sock.settimeout(self.timeout)
165	            sock.connect((self.address, self.port))

--------------------------------------------------
>> Issue: [B804:connect] socket.socket.connect
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/log_communication.py:165
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b804_connect.html
164	            sock.settimeout(self.timeout)
165	            sock.connect((self.address, self.port))
166	            sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)

--------------------------------------------------
>> Issue: [B804:connect] socket.socket.connect
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/log_communication.py:178
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b804_connect.html
177	            if self.sock is None:
178	                self._connect()
179	            if not self.failed:

--------------------------------------------------
>> Issue: [B806:sendall] socket.socket.sendall
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/log_communication.py:188
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b806_sendall.html
187	                    packed_number_bytes = pack(">i", len(binary_message))
188	                    self.sock.sendall(packed_number_bytes + binary_message)
189	                except Exception:  # pylint: disable=broad-except

--------------------------------------------------
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/log_communication.py:197
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
196	        if self.sock:
197	            self.sock.close()
198	            self.sock = None

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/torch_run_process_wrapper.py:60
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
59	    )
60	    t = threading.Thread(target=check_parent_alive, args=(task,), daemon=True)
61	

--------------------------------------------------
>> Issue: [B828:signal] signal.signal
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/torch_run_process_wrapper.py:66
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b828_signal.html
65	
66	    signal.signal(signal.SIGTERM, sigterm_handler)
67	

--------------------------------------------------
>> Issue: [B324:blacklist] signal_signal
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/torch/torch_run_process_wrapper.py:66
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b324-signal-signal
65	
66	    signal.signal(signal.SIGTERM, sigterm_handler)
67	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/tuning.py:613
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
612	        bestModelPath = os.path.join(path, "bestModel")
613	        cast(MLWritable, instance.bestModel).write().session(self.sparkSession).save(bestModelPath)
614	        if persistSubModels:

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/tuning.py:622
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
621	                    modelPath = os.path.join(splitPath, f"{paramIndex}")
622	                    cast(MLWritable, instance.subModels[splitIndex][paramIndex]).write().session(
623	                        self.sparkSession

--------------------------------------------------
>> Issue: [B839:pool] multiprocessing.Pool
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/tuning.py:840
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b839_pool.html
839	
840	        pool = ThreadPool(processes=min(self.getParallelism(), numModels))
841	        subModels = None

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/tuning.py:1284
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1283	        bestModelPath = os.path.join(path, "bestModel")
1284	        cast(MLWritable, instance.bestModel).write().session(self.sparkSession).save(bestModelPath)
1285	        if persistSubModels:

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/tuning.py:1291
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
1290	                modelPath = os.path.join(subModelsPath, f"{paramIndex}")
1291	                cast(MLWritable, instance.subModels[paramIndex]).write().session(
1292	                    self.sparkSession

--------------------------------------------------
>> Issue: [B839:pool] multiprocessing.Pool
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/tuning.py:1486
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b839_pool.html
1485	        tasks = _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam)
1486	        pool = ThreadPool(processes=min(self.getParallelism(), numModels))
1487	        metrics = [None] * numModels

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/util.py:252
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
251	        if self.shouldOverwrite:
252	            self._handleOverwrite(path)
253	        self.saveImpl(path)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/util.py:304
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
303	        _java_obj = instance._to_java()  # type: ignore[attr-defined]
304	        self._jwrite = _java_obj.write()
305	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/util.py:314
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
313	        """Overwrites if the output path already exists."""
314	        self._jwrite.overwrite()
315	        return self

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/util.py:359
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
358	        """Save this ML instance to the given path, a shortcut of 'write().save(path)'."""
359	        self.write().save(path)
360	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/util.py:409
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
408	        self._clazz = clazz
409	        self._jread = self._load_java_obj(clazz).read()
410	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/ml/util.py:466
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
465	        """Reads an ML instance from the input path, a shortcut of `read().load(path)`."""
466	        return cls.read().load(path)
467	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/mllib/fpm.py:225
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
224	        try:
225	            rmtree(temp_path)
226	        except OSError:

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/pandas/frame.py:13731
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
13730	
13731	    shutil.rmtree(path, ignore_errors=True)
13732	    spark.sql("DROP DATABASE IF EXISTS %s CASCADE" % db_name)

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/pandas/generic.py:3550
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
3549	
3550	    shutil.rmtree(path, ignore_errors=True)
3551	    spark.stop()

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/pandas/namespace.py:3877
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
3876	
3877	    shutil.rmtree(path, ignore_errors=True)
3878	    spark.sql("DROP DATABASE IF EXISTS %s CASCADE" % db_name)

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/pandas/spark/accessors.py:1270
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
1269	
1270	    shutil.rmtree(path, ignore_errors=True)
1271	    spark.sql("DROP DATABASE IF EXISTS %s CASCADE" % db_name)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/pandas/supported_api_gen.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	
43	RST_HEADER = """
44	=====================
45	Supported pandas API
46	=====================
47	
48	.. currentmodule:: pyspark.pandas
49	
50	The following table shows the pandas APIs that implemented or non-implemented from pandas API on
51	Spark. Some pandas API do not implement full parameters, so the third column shows missing
52	parameters for each API.
53	
54	* 'Y' in the second column means it's implemented including its whole parameter.
55	* 'N' means it's not implemented yet.
56	* 'P' means it's partially implemented with the missing of some parameters.
57	
58	All API in the list below computes the data with distributed execution except the ones that require
59	the local execution by design. For example, `DataFrame.to_numpy() <https://spark.apache.org/docs/
60	latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.to_numpy.html>`__
61	requires to collect the data to the driver side.
62	
63	If there is non-implemented pandas API or parameter you want, you can create an `Apache Spark
64	JIRA <https://issues.apache.org/jira/projects/SPARK/summary>`__ to request or to contribute by
65	your own.
66	
67	The API list is updated based on the `latest pandas official API reference
68	<https://pandas.pydata.org/docs/reference/index.html#>`__.
69	
70	"""
71	
72	
73	@unique
74	class Implemented(Enum):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/pandas/supported_api_gen.py:318
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
317	            + "`%s.%s " % (pd_module_path, module_dot_func)
318	            + "<https://pandas.pydata.org/docs/reference/api/"
319	            + "%s.%s.html>`__ and " % (pd_module_path, module_dot_func)
320	            + "`%s.%s " % (ps_module_path, module_dot_func)
321	            + "<https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/pandas/supported_api_gen.py:321
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
320	            + "`%s.%s " % (ps_module_path, module_dot_func)
321	            + "<https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/"
322	            + "%s.%s.html>`__ for detail." % (ps_module_path, module_dot_func)
323	        )
324	        missing_str += additional_str
325	    return missing_str
326	
327	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/pandas/supported_api_gen.py:418
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
417	    with open(output_rst_file_path, "w") as w_fd:
418	        w_fd.write(RST_HEADER)
419	        for module_info, supported_status in all_supported_status.items():

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/pandas/supported_api_gen.py:423
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
422	                _write_table(module, module_path, supported_status, w_fd)
423	                w_fd.write("\n")
424	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/profiler.py:471
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
470	
471	            stream.write("Filename: " + filename + "\n\n")
472	            stream.write(header + "\n")

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/profiler.py:472
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
471	            stream.write("Filename: " + filename + "\n\n")
472	            stream.write(header + "\n")
473	            stream.write("=" * len(header) + "\n")

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/profiler.py:473
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
472	            stream.write(header + "\n")
473	            stream.write("=" * len(header) + "\n")
474	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/profiler.py:502
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
501	                tmp = template.format(lineno, total_mem, inc, occurrences, all_lines[lineno - 1])
502	                stream.write(tmp)
503	            stream.write("\n\n")

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/profiler.py:503
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
502	                stream.write(tmp)
503	            stream.write("\n\n")
504	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/python/pyspark/shell.py:153
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
152	    with open(_pythonstartup) as f:
153	        code = compile(f.read(), _pythonstartup, "exec")
154	        exec(code)

--------------------------------------------------
>> Issue: [B800:exec_used] exec
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/python/pyspark/shell.py:154
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b800_exec_used.html
153	        code = compile(f.read(), _pythonstartup, "exec")
154	        exec(code)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/serializers.py:173
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
172	        write_int(len(serialized), stream)
173	        stream.write(serialized)
174	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/serializers.py:188
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
187	        else:
188	            obj = stream.read(length)
189	            if len(obj) < length:

--------------------------------------------------
>> Issue: [B800:exec_used] exec
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/serializers.py:234
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b800_exec_used.html
233	                # TODO(SC-177220) Use AST to parse the Python code literal
234	                exec(code)
235	            except Exception as e:

--------------------------------------------------
>> Issue: [B800:exec_used] exec
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/serializers.py:271
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b800_exec_used.html
270	        # Evaluate the code
271	        exec(code, module.__dict__)
272	        sys.modules[name] = module

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/serializers.py:407
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
406	            write_int(len(bytes), stream)
407	            stream.write(bytes)
408	

--------------------------------------------------
>> Issue: [B816:decompress] zlib.decompress
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/serializers.py:686
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b816_decompress.html
685	    def loads(self, obj):
686	        return self.serializer.loads(zlib.decompress(obj))
687	

--------------------------------------------------
>> Issue: [B306:blacklist] zlib.decompress
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/serializers.py:686
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b306-zlib-decompress
685	    def loads(self, obj):
686	        return self.serializer.loads(zlib.decompress(obj))
687	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/serializers.py:707
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
706	            return None
707	        s = stream.read(length)
708	        return s.decode("utf-8") if self.use_unicode else s

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/serializers.py:724
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
723	def read_long(stream):
724	    length = stream.read(8)
725	    if not length:

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/serializers.py:731
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
730	def write_long(value, stream):
731	    stream.write(struct.pack("!q", value))
732	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/serializers.py:739
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
738	def read_int(stream):
739	    length = stream.read(4)
740	    if not length:

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/serializers.py:746
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
745	def write_int(value, stream):
746	    stream.write(struct.pack("!i", value))
747	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/serializers.py:750
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
749	def read_bool(stream):
750	    length = stream.read(1)
751	    if not length:

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/serializers.py:758
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
757	    write_int(len(obj), stream)
758	    stream.write(obj)
759	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/serializers.py:764
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
763	    length = read_int(stream)
764	    return stream.read(length)
765	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/serializers.py:806
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
805	                write_int(self.buffer_size, self.wrapped)
806	                self.wrapped.write(self.buffer)
807	                byte_remaining -= space_left

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/serializers.py:814
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
813	            write_int(self.current_pos, self.wrapped)
814	            self.wrapped.write(self.buffer[: self.current_pos])
815	            self.current_pos = 0

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/shell.py:153
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
152	    with open(_pythonstartup) as f:
153	        code = compile(f.read(), _pythonstartup, "exec")
154	        exec(code)

--------------------------------------------------
>> Issue: [B800:exec_used] exec
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/shell.py:154
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b800_exec_used.html
153	        code = compile(f.read(), _pythonstartup, "exec")
154	        exec(code)

--------------------------------------------------
>> Issue: [B812:system] os.system
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/shuffle.py:59
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b812_system.html
58	        """Return the used memory in MiB"""
59	        if platform.system() == "Linux":
60	            for line in open("/proc/self/status"):

--------------------------------------------------
>> Issue: [B817:system] platform.system
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/shuffle.py:59
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b817_system.html
58	        """Return the used memory in MiB"""
59	        if platform.system() == "Linux":
60	            for line in open("/proc/self/status"):

--------------------------------------------------
>> Issue: [B308:blacklist] platform.system
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/shuffle.py:59
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b308-platform-system
58	        """Return the used memory in MiB"""
59	        if platform.system() == "Linux":
60	            for line in open("/proc/self/status"):

--------------------------------------------------
>> Issue: [B812:system] os.system
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/shuffle.py:66
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b812_system.html
65	            warnings.warn("Please install psutil to have better " "support with spilling")
66	            if platform.system() == "Darwin":
67	                import resource

--------------------------------------------------
>> Issue: [B817:system] platform.system
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/shuffle.py:66
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b817_system.html
65	            warnings.warn("Please install psutil to have better " "support with spilling")
66	            if platform.system() == "Darwin":
67	                import resource

--------------------------------------------------
>> Issue: [B308:blacklist] platform.system
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/shuffle.py:66
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b308-platform-system
65	            warnings.warn("Please install psutil to have better " "support with spilling")
66	            if platform.system() == "Darwin":
67	                import resource

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/shuffle.py:440
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
439	        for d in self.localdirs:
440	            shutil.rmtree(d, True)
441	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/shuffle.py:571
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
570	                f.seek(0)
571	                serialized = f.read()
572	        else:

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/shuffle.py:580
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
579	            self._open_file()
580	            self._file.write(serialized)
581	        else:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/column.py:305
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
304	
305	    _eqNullSafe_doc = """
306	    Equality test that is safe for null values.
307	
308	    .. versionadded:: 2.3.0
309	
310	    .. versionchanged:: 3.4.0
311	        Supports Spark Connect.
312	
313	    Parameters
314	    ----------
315	    other
316	        a value or :class:`Column`
317	
318	    Examples
319	    --------
320	    >>> from pyspark.sql import Row
321	    >>> df1 = spark.createDataFrame([
322	    ...     Row(id=1, value='foo'),
323	    ...     Row(id=2, value=None)
324	    ... ])
325	    >>> df1.select(
326	    ...     df1['value'] == 'foo',
327	    ...     df1['value'].eqNullSafe('foo'),
328	    ...     df1['value'].eqNullSafe(None)
329	    ... ).show()
330	    +-------------+---------------+----------------+
331	    |(value = foo)|(value <=> foo)|(value <=> NULL)|
332	    +-------------+---------------+----------------+
333	    |         true|           true|           false|
334	    |         NULL|          false|            true|
335	    +-------------+---------------+----------------+
336	    >>> df2 = spark.createDataFrame([
337	    ...     Row(value = 'bar'),
338	    ...     Row(value = None)
339	    ... ])
340	    >>> df1.join(df2, df1["value"] == df2["value"]).count()
341	    0
342	    >>> df1.join(df2, df1["value"].eqNullSafe(df2["value"])).count()
343	    1
344	    >>> df2 = spark.createDataFrame([
345	    ...     Row(id=1, value=float('NaN')),
346	    ...     Row(id=2, value=42.0),
347	    ...     Row(id=3, value=None)
348	    ... ])
349	    >>> df2.select(
350	    ...     df2['value'].eqNullSafe(None),
351	    ...     df2['value'].eqNullSafe(float('NaN')),
352	    ...     df2['value'].eqNullSafe(42.0)
353	    ... ).show()
354	    +----------------+---------------+----------------+
355	    |(value <=> NULL)|(value <=> NaN)|(value <=> 42.0)|
356	    +----------------+---------------+----------------+
357	    |           false|           true|           false|
358	    |           false|          false|            true|
359	    |            true|          false|           false|
360	    +----------------+---------------+----------------+
361	
362	    Notes
363	    -----
364	    Unlike Pandas, PySpark doesn't consider NaN values to be NULL. See the
365	    `NaN Semantics <https://spark.apache.org/docs/latest/sql-ref-datatypes.html#nan-semantics>`_
366	    for details.
367	    """
368	    eqNullSafe = _bin_op("eqNullSafe", _eqNullSafe_doc)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/connect/client/artifact.py:365
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
364	            with artifact.storage.stream() as stream:
365	                binary = stream.read()
366	            crc32 = zlib.crc32(binary)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/connect/client/artifact.py:392
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
391	        with artifact.storage.stream() as stream:
392	            for chunk in iter(lambda: stream.read(ArtifactManager.CHUNK_SIZE), b""):
393	                if initial_batch:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/connect/client/core.py:2339
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
2338	                        raise Exception(
2339	                            "Python versions in the Spark Connect client and server are different. "
2340	                            "To execute user-defined functions, client and server should have the "
2341	                            "same minor Python version. Please update the Python version in the "

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/connect/client/reattach.py:71
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
70	                max_workers = os.cpu_count() or 8
71	                cls._release_thread_pool_instance = ThreadPoolExecutor(max_workers=max_workers)
72	            return cls._release_thread_pool_instance

--------------------------------------------------
>> Issue: [B303:blacklist] pwd.getpwuid
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/dataframe.py:1486
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b303-pwd-getpwuid
1485	
1486	                username = pwd.getpwuid(os.getuid()).pw_name
1487	                filename = self._jdf.collectToPythonFile(username)

--------------------------------------------------
>> Issue: [B811:getuid] os.getuid
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/dataframe.py:1486
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b811_getuid.html
1485	
1486	                username = pwd.getpwuid(os.getuid()).pw_name
1487	                filename = self._jdf.collectToPythonFile(username)

--------------------------------------------------
>> Issue: [B305:blacklist] os.getuid
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/dataframe.py:1486
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
1485	
1486	                username = pwd.getpwuid(os.getuid()).pw_name
1487	                filename = self._jdf.collectToPythonFile(username)

--------------------------------------------------
>> Issue: [B303:blacklist] pwd.getpwuid
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/dataframe.py:1680
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b303-pwd-getpwuid
1679	
1680	                username = pwd.getpwuid(os.getuid()).pw_name
1681	                filename = self._jdf.tailToPythonFile(num, username)

--------------------------------------------------
>> Issue: [B811:getuid] os.getuid
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/dataframe.py:1680
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b811_getuid.html
1679	
1680	                username = pwd.getpwuid(os.getuid()).pw_name
1681	                filename = self._jdf.tailToPythonFile(num, username)

--------------------------------------------------
>> Issue: [B305:blacklist] os.getuid
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/dataframe.py:1680
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
1679	
1680	                username = pwd.getpwuid(os.getuid()).pw_name
1681	                filename = self._jdf.tailToPythonFile(num, username)

--------------------------------------------------
>> Issue: [B303:blacklist] pwd.getpwuid
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/pandas/conversion.py:359
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b303-pwd-getpwuid
358	
359	                username = pwd.getpwuid(os.getuid()).pw_name
360	                filename = self._jdf.collectAsArrowToPythonFile(username)

--------------------------------------------------
>> Issue: [B811:getuid] os.getuid
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/pandas/conversion.py:359
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b811_getuid.html
358	
359	                username = pwd.getpwuid(os.getuid()).pw_name
360	                filename = self._jdf.collectAsArrowToPythonFile(username)

--------------------------------------------------
>> Issue: [B305:blacklist] os.getuid
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/pandas/conversion.py:359
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
358	
359	                username = pwd.getpwuid(os.getuid()).pw_name
360	                filename = self._jdf.collectAsArrowToPythonFile(username)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/session.py:548
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
547	                raise RuntimeError(
548	                    "Only remote Spark sessions using Databricks Connect are supported. "
549	                    "Use DatabricksSession.builder to create a remote Spark session instead.\n"
550	                    "Refer to https://docs.databricks.com/dev-tools/databricks-connect.html "

--------------------------------------------------
>> Issue: [B304:blacklist] socket.socket
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/streaming/stateful_processor_api_client.py:58
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
57	        if is_isolated_tws_enabled:
58	            self._client_socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
59	            state_server_dir = "/databricks/sparkconnect/state-server"

--------------------------------------------------
>> Issue: [B804:connect] socket.socket.connect
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/streaming/stateful_processor_api_client.py:63
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b804_connect.html
62	            try:
63	                self._client_socket.connect(server_address)
64	            except Exception as e:

--------------------------------------------------
>> Issue: [B304:blacklist] socket.socket
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/streaming/stateful_processor_api_client.py:69
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
68	        else:
69	            self._client_socket = socket.socket()
70	            self._client_socket.connect(("localhost", state_server_id))

--------------------------------------------------
>> Issue: [B804:connect] socket.socket.connect
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/streaming/stateful_processor_api_client.py:70
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b804_connect.html
69	            self._client_socket = socket.socket()
70	            self._client_socket.connect(("localhost", state_server_id))
71	        # END-EDGE

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/streaming/stateful_processor_api_client.py:398
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
397	        write_int(len(message), self.sockfile)
398	        self.sockfile.write(message)
399	        self.sockfile.flush()

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/streaming/stateful_processor_api_client.py:405
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
404	        length = read_int(self.sockfile)
405	        bytes = self.sockfile.read(length)
406	        message = stateMessage.StateResponse()

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/streaming/stateful_processor_api_client.py:414
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
413	        length = read_int(self.sockfile)
414	        bytes = self.sockfile.read(length)
415	        message = stateMessage.StateResponseWithLongTypeVal()

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/streaming/stateful_processor_api_client.py:423
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
422	        length = read_int(self.sockfile)
423	        bytes = self.sockfile.read(length)
424	        message = stateMessage.StateResponseWithStringTypeVal()

--------------------------------------------------
>> Issue: [B802:b64encode] base64.b64encode
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/types.py:1751
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b802_b64encode.html
1750	                "pyClass": "%s.%s" % (self.module(), type(self).__name__),
1751	                "serializedClass": base64.b64encode(b).decode("utf8"),
1752	                "sqlType": self.sqlType().jsonValue(),

--------------------------------------------------
>> Issue: [B301:blacklist] base64.b64encode
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/types.py:1751
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b301-base64-b64encode
1750	                "pyClass": "%s.%s" % (self.module(), type(self).__name__),
1751	                "serializedClass": base64.b64encode(b).decode("utf8"),
1752	                "sqlType": self.sqlType().jsonValue(),

--------------------------------------------------
>> Issue: [B801:b64decode] base64.b64decode
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/types.py:1764
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b801_b64decode.html
1763	        if not hasattr(m, pyClass):
1764	            s = base64.b64decode(json["serializedClass"].encode("utf-8"))
1765	            UDT = CloudPickleSerializer().loads(s)

--------------------------------------------------
>> Issue: [B300:blacklist] base64.b64decode
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/types.py:1764
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b300-base64-b64decode
1763	        if not hasattr(m, pyClass):
1764	            s = base64.b64decode(json["serializedClass"].encode("utf-8"))
1765	            UDT = CloudPickleSerializer().loads(s)

--------------------------------------------------
>> Issue: [B802:b64encode] base64.b64encode
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/variant_utils.py:414
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b802_b64encode.html
413	                # decoding simply converts byte array to string
414	                return '"' + base64.b64encode(value).decode("utf-8") + '"'
415	            if type(value) == datetime.date or type(value) == datetime.datetime:

--------------------------------------------------
>> Issue: [B301:blacklist] base64.b64encode
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/variant_utils.py:414
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b301-base64-b64encode
413	                # decoding simply converts byte array to string
414	                return '"' + base64.b64encode(value).decode("utf-8") + '"'
415	            if type(value) == datetime.date or type(value) == datetime.datetime:

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/worker/plan_data_source_read.py:304
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
303	
304	            output_iter = reader.read(partition)  # type: ignore[arg-type]
305	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/worker/write_into_data_source.py:206
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
205	            if isinstance(writer, DataSourceArrowWriter):
206	                res = writer.write(iterator)
207	            else:

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/sql/worker/write_into_data_source.py:208
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
207	            else:
208	                res = writer.write(batch_to_rows())
209	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/testing/connectutils.py:275
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
274	
275	        spark._client._builder.set("x-databricks-api-url", "http://other.host")
276	        spark._client._artifact_manager._metadata.append(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/testing/connectutils.py:277
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
276	        spark._client._artifact_manager._metadata.append(
277	            ("x-databricks-api-url", "http://other.host")
278	        )

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/testing/connectutils.py:286
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
285	    def tearDownClass(cls):
286	        shutil.rmtree(cls.tempdir.name, ignore_errors=True)
287	        # BEGIN-EDGE

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/testing/pandasutils.py:597
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
596	        finally:
597	            shutil.rmtree(tmp)
598	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/testing/sqlutils.py:321
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
320	        cls.spark.stop()
321	        shutil.rmtree(cls.tempdir.name, ignore_errors=True)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/testing/utils.py:286
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
285	        script = "$(test $(tput colors)) && $(test $(tput colors) -ge 8) && echo true || echo false"
286	        return os.popen(script).read()
287	    except Exception:

--------------------------------------------------
>> Issue: [B813:popen] os.popen
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/testing/utils.py:286
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b813_popen.html
285	        script = "$(test $(tput colors)) && $(test $(tput colors) -ge 8) && echo true || echo false"
286	        return os.popen(script).read()
287	    except Exception:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/util.py:282
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
281	    print(
282	        """
283	________________________________________________________________________________________________
284	
285	  Spark %(lib_name)s libraries not found in class path. Try one of the following.
286	
287	  1. Include the %(lib_name)s library and its dependencies with in the
288	     spark-submit command as
289	
290	     $ bin/spark-submit --packages org.apache.spark:spark-%(pkg_name)s:%(spark_version)s ...
291	
292	  2. Download the JAR of the artifact from Maven Central http://search.maven.org/,
293	     Group Id = org.apache.spark, Artifact Id = spark-%(jar_name)s, Version = %(spark_version)s.
294	     Then, include the jar in the spark-submit command as
295	
296	     $ bin/spark-submit --jars <spark-%(jar_name)s.jar> ...
297	
298	________________________________________________________________________________________________
299	
300	"""
301	        % {
302	            "lib_name": lib_name,
303	            "pkg_name": pkg_name,
304	            "jar_name": jar_name,
305	            "spark_version": spark_version,
306	        }

--------------------------------------------------
>> Issue: [B812:system] os.system
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/util.py:317
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b812_system.html
316	        and session.conf.get("spark.databricks.service.client.enabled") == "true"
317	        and platform.system() == "Windows"
318	        and session.conf.get("spark.databricks.service.client.windows.path.normalizationEnabled")

--------------------------------------------------
>> Issue: [B817:system] platform.system
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/util.py:317
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b817_system.html
316	        and session.conf.get("spark.databricks.service.client.enabled") == "true"
317	        and platform.system() == "Windows"
318	        and session.conf.get("spark.databricks.service.client.windows.path.normalizationEnabled")

--------------------------------------------------
>> Issue: [B308:blacklist] platform.system
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/util.py:317
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b308-platform-system
316	        and session.conf.get("spark.databricks.service.client.enabled") == "true"
317	        and platform.system() == "Windows"
318	        and session.conf.get("spark.databricks.service.client.windows.path.normalizationEnabled")

--------------------------------------------------
>> Issue: [B808:settimeout] settimeout
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/util.py:642
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b808_settimeout.html
641	    # operation, it will very possibly fail. See SPARK-18281.
642	    sock.settimeout(None)
643	    return sockfile

--------------------------------------------------
>> Issue: [B823:ip_found] ip_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/util.py:734
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b823_ip_found.html
733	    # Support for both IPv4 and IPv6.
734	    addr = "127.0.0.1"
735	    if os.environ.get("SPARK_PREFER_IPV6", "false").lower() == "true":

--------------------------------------------------
>> Issue: [B304:blacklist] socket.socket
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/util.py:740
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
739	        try:
740	            sock = socket.socket(af, socktype, proto)
741	            sock.settimeout(int(os.environ.get("SPARK_AUTH_SOCKET_TIMEOUT", 15)))

--------------------------------------------------
>> Issue: [B808:settimeout] settimeout
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/util.py:741
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b808_settimeout.html
740	            sock = socket.socket(af, socktype, proto)
741	            sock.settimeout(int(os.environ.get("SPARK_AUTH_SOCKET_TIMEOUT", 15)))
742	            sock.connect(sa)

--------------------------------------------------
>> Issue: [B804:connect] socket.socket.connect
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/util.py:742
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b804_connect.html
741	            sock.settimeout(int(os.environ.get("SPARK_AUTH_SOCKET_TIMEOUT", 15)))
742	            sock.connect(sa)
743	            sockfile = sock.makefile("rwb", int(os.environ.get("SPARK_BUFFER_SIZE", 65536)))

--------------------------------------------------
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/util.py:750
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
749	            if sock is not None:
750	                sock.close()
751	                sock = None

--------------------------------------------------
>> Issue: [B807:close] socket.socket.close
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/util.py:769
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b807_close.html
768	    if reply != "ok":
769	        conn.close()
770	        raise PySparkRuntimeError(

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/worker.py:203
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
202	        def evaluate(*args: pd.Series) -> pd.Series:
203	            with ThreadPoolExecutor(max_workers=c) as pool:
204	                return pd.Series(list(pool.map(lambda row: result_func(func(*row)), zip(*args))))

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/worker_util.py:334
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
333	    if needs_broadcast_decryption_server:
334	        broadcast_sock_file.write(b"1")
335	        broadcast_sock_file.close()

--------------------------------------------------
>> Issue: [B303:blacklist] pwd.getpwuid
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/wrapped_python.py:90
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b303-pwd-getpwuid
89	    # as a non-root user.
90	    if username != pwd.getpwuid(os.getuid()).pw_name:
91	        # Must make sure that Spark user group exists before creating user

--------------------------------------------------
>> Issue: [B811:getuid] os.getuid
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/wrapped_python.py:90
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b811_getuid.html
89	    # as a non-root user.
90	    if username != pwd.getpwuid(os.getuid()).pw_name:
91	        # Must make sure that Spark user group exists before creating user

--------------------------------------------------
>> Issue: [B305:blacklist] os.getuid
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/wrapped_python.py:90
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
89	    # as a non-root user.
90	    if username != pwd.getpwuid(os.getuid()).pw_name:
91	        # Must make sure that Spark user group exists before creating user

--------------------------------------------------
>> Issue: [B823:ip_found] ip_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/databricks_connect-16.3.1-py2.py3-none-any/pyspark/wrapped_python.py:114
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b823_ip_found.html
113	                    "-d",
114	                    "127.0.0.1",
115	                    "-p",
116	                    "tcp",
117	                    "--destination-port",
118	                    str(gateway_port),
119	                    "-j",
120	                    "ACCEPT",
121	                    # [ES-609068] Wait for 60 seconds for the case of lock contention.
122	                    "-w",
123	                    "60",
124	                ]
125	            )
126	            print(
127	                "do_all_setup_for_username: iptables command took %s seconds"
128	                % (time.time() - iptables_start_time)
129	            )
130	        set_process_identity_start_time = time.time()
131	        set_process_identity(uid, user_gid, spark_users_gid)
132	        print(
133	            "do_all_setup_for_username: set_process_identity took %s seconds"

--------------------------------------------------

Code scanned:
	Total lines of code: 204866
	Total lines skipped (#nosec): 0

Run metrics:
	Total issues (by severity):
		Undefined: 0.0
		Low: 0.0
		Medium: 67.0
		High: 178.0
	Total issues (by confidence):
		Undefined: 0.0
		Low: 0.0
		Medium: 198.0
		High: 47.0
Files skipped (0):
