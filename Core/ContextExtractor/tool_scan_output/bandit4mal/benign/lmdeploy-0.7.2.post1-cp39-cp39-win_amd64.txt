Run started:2025-04-12 14:22:51.056458

Test results:
>> Issue: [B823:ip_found] ip_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/api.py:96
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b823_ip_found.html
95	          chat_template_config: Optional[ChatTemplateConfig] = None,
96	          server_name: str = '0.0.0.0',
97	          server_port: int = 23333,
98	          log_level: str = 'ERROR',
99	          api_keys: Optional[Union[List[str], str]] = None,
100	          ssl: bool = False,
101	          **kwargs):
102	    """This will run the api_server in a subprocess.
103	
104	    Args:
105	        model_path (str): the path of a model.
106	            It could be one of the following options:
107	                - i) A local directory path of a turbomind model which is

--------------------------------------------------
>> Issue: [B838:process] multiprocessing.Process
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/api.py:154
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b838_process.html
153	
154	    task = Process(target=serve,
155	                   args=(model_path, ),
156	                   kwargs=dict(model_name=model_name,
157	                               backend=backend,
158	                               backend_config=backend_config,
159	                               chat_template_config=chat_template_config,
160	                               server_name=server_name,
161	                               server_port=server_port,
162	                               log_level=log_level,
163	                               api_keys=api_keys,
164	                               ssl=ssl,
165	                               **kwargs),
166	                   daemon=True)
167	    task.start()

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/api.py:181
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
180	
181	def client(api_server_url: str = 'http://0.0.0.0:23333', api_key: Optional[str] = None, **kwargs):
182	    """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/cli/serve.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	                            type=str,
30	                            help='The path of the deployed model or the tritonserver url or '
31	                            'restful api url. for example: - ./workspace - 0.0.0.0:23333'

--------------------------------------------------
>> Issue: [B823:ip_found] ip_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/cli/serve.py:33
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b823_ip_found.html
32	                            ' - http://0.0.0.0:23333')
33	        parser.add_argument('--server-name', type=str, default='0.0.0.0', help='The ip address of gradio server')
34	        parser.add_argument('--server-port', type=int, default=6006, help='The port of gradio server')

--------------------------------------------------
>> Issue: [B823:ip_found] ip_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/cli/serve.py:101
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b823_ip_found.html
100	                            ', "baichuan-inc/baichuan2-7b-chat" and so on')
101	        parser.add_argument('--server-name', type=str, default='0.0.0.0', help='Host ip for serving')
102	        parser.add_argument('--server-port', type=int, default=23333, help='Server port')

--------------------------------------------------
>> Issue: [B823:ip_found] ip_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/cli/serve.py:212
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b823_ip_found.html
211	        parser.set_defaults(run=SubCliServe.proxy)
212	        parser.add_argument('--server-name', type=str, default='0.0.0.0', help='Host ip for proxy serving')
213	        parser.add_argument('--server-port', type=int, default=8000, help='Server port of the proxy')

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/cli/utils.py:357
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
356	            help=\
357	            'A JSON file or string that specifies the chat template configuration. '  # noqa
358	            'Please refer to https://lmdeploy.readthedocs.io/en/latest/advance/chat_template.html for the specification'  # noqa
359	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/lite/quantization/modules/linear.py:143
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
142	        if awq_inference_engine is None:
143	            raise RuntimeError('Run the following command to install '
144	                               'the kernel for 4bit inference\n\n'

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/pytorch/backends/dlinfer/ascend/op_backend.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
24	        with open(record_file, 'r') as file:
25	            data = file.read()
26	        scale_offset_pairs = re.findall(r'scale:\s*([\d\.\-]+)\s*offset:\s*(-?\d+)', data)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/pytorch/check_env/triton.py:52
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
51	                msg = (
52	                    'This Error might caused by mismatching between NVIDIA Driver and nvcc compiler. \n'  # noqa: E501
53	                    'Try solution https://github.com/triton-lang/triton/issues/1955#issuecomment-1929908209'  # noqa: E501
54	                    ' or reinstall the driver.')

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/pytorch/configurations/utils.py:18
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
17	    except ImportError:
18	        logger.warning('For higher performance, please install flash_mla https://github.com/deepseek-ai/FlashMLA')
19	    return use_flash_mla

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/pytorch/engine/engine_checker.py:80
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
79	                mod_name='Engine',
80	                message='thread safe mode is no longer supported.\n'
81	                'Read https://github.com/InternLM/lmdeploy/blob/main/docs/en/advance/pytorch_multithread.md for more details.',  # noqa: E501

--------------------------------------------------
>> Issue: [B304:blacklist] socket.socket
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/pytorch/engine/executor/dist_utils.py:13
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b304-b305-ciphers-and-modes
12	    while True:
13	        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
14	            if s.connect_ex(('localhost', port)) != 0:

--------------------------------------------------
>> Issue: [B823:ip_found] ip_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/pytorch/engine/executor/mp_executor.py:214
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b823_ip_found.html
213	        port = find_available_port()
214	        os.environ.setdefault('MASTER_ADDR', '127.0.0.1')
215	        os.environ.setdefault('MASTER_PORT', str(port))

--------------------------------------------------
>> Issue: [B828:signal] signal.signal
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/pytorch/engine/executor/mp_executor.py:285
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b828_signal.html
284	
285	        signal.signal(signal.SIGUSR1, signal_handler)
286	

--------------------------------------------------
>> Issue: [B324:blacklist] signal_signal
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/pytorch/engine/executor/mp_executor.py:285
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b324-signal-signal
284	
285	        signal.signal(signal.SIGUSR1, signal_handler)
286	

--------------------------------------------------
>> Issue: [B838:process] multiprocessing.Process
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/pytorch/engine/executor/mp_executor.py:458
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b838_process.html
457	        assert self._proc is None
458	        proc = self.mp_ctx.Process(target=self._main_loop,
459	                                   kwargs=kwargs,
460	                                   name=f'ExecutorProc-{self.proc_id}',
461	                                   daemon=True)
462	        proc.start()

--------------------------------------------------
>> Issue: [B828:signal] signal.signal
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/pytorch/engine/executor/mp_executor.py:505
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b828_signal.html
504	
505	        signal.signal(signal.SIGTERM, handle_sigterm)
506	

--------------------------------------------------
>> Issue: [B324:blacklist] signal_signal
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/pytorch/engine/executor/mp_executor.py:505
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b324-signal-signal
504	
505	        signal.signal(signal.SIGTERM, handle_sigterm)
506	

--------------------------------------------------
>> Issue: [B800:exec_used] exec
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/pytorch/kernels/cuda/triton_utils.py:251
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b800_exec_used.html
250	        )
251	        exec(src, scope)
252	        return scope[f'_{fn.__name__}_launcher']

--------------------------------------------------
>> Issue: [B800:exec_used] exec
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/pytorch/kernels/cuda/triton_utils.py:419
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b800_exec_used.html
418	        )
419	        exec(src, scope)
420	        return scope[f'_{fn.__name__}_launcher']

--------------------------------------------------
>> Issue: [B800:exec_used] exec
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/pytorch/kernels/dispatcher.py:116
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b800_exec_used.html
115	            scope.update(globals)
116	        exec(src, scope)
117	        return scope[f'{self.func_name}']

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/async_engine.py:166
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
165	        fut = concurrent.futures.Future()
166	        self.thread = Thread(target=partial(self._thread_entry, fut), daemon=daemon)
167	        self.thread.start()

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/async_engine.py:298
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
297	        self.request_logger = RequestLogger(max_log_len)
298	        self.internal_thread = _EventLoopThread(daemon=True)
299	        self.limiter: asyncio.Semaphore = None

--------------------------------------------------
>> Issue: [B823:ip_found] ip_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/gradio/app.py:10
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b823_ip_found.html
9	def run(model_path_or_server: str,
10	        server_name: str = '0.0.0.0',
11	        server_port: int = 6006,
12	        batch_size: int = 32,
13	        backend: Literal['turbomind', 'pytorch'] = 'turbomind',
14	        backend_config: Optional[Union[PytorchEngineConfig, TurbomindEngineConfig]] = None,
15	        chat_template_config: Optional[ChatTemplateConfig] = None,
16	        model_name: str = None,
17	        share: bool = False,
18	        max_log_len: int = None,
19	        **kwargs):
20	    """chat with AI assistant through web ui.
21	

--------------------------------------------------
>> Issue: [B823:ip_found] ip_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/gradio/turbomind_coupled.py:116
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b823_ip_found.html
115	              chat_template_config: Optional[ChatTemplateConfig] = None,
116	              server_name: str = '0.0.0.0',
117	              server_port: int = 6006,
118	              share: bool = False,
119	              max_log_len: int = None,
120	              **kwargs):
121	    """chat with AI assistant through web ui.
122	
123	    Args:
124	        model_path (str): the path of a model.
125	            It could be one of the following options:
126	                - i) A local directory path of a turbomind model which is

--------------------------------------------------
>> Issue: [B823:ip_found] ip_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/gradio/vl.py:73
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b823_ip_found.html
72	              chat_template_config: Optional[ChatTemplateConfig] = None,
73	              server_name: str = '0.0.0.0',
74	              server_port: int = 6006,
75	              tp: int = 1,
76	              **kwargs):
77	
78	    from lmdeploy.serve.vl_async_engine import VLAsyncEngine
79	    engine = VLAsyncEngine(model_path=model_path,
80	                           model_name=model_name,
81	                           backend=backend,
82	                           backend_config=backend_config,

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/openai/api_client.py:12
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
11	    """Get model list from api server."""
12	    response = requests.get(api_url, headers=headers)
13	    logger = get_logger('lmdeploy')

--------------------------------------------------
>> Issue: [B821:post] requests.post
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/openai/api_client.py:82
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b821_post.html
81	        """
82	        response = requests.post(self.encode_v1_url,
83	                                 headers=self.headers,
84	                                 json=dict(input=input, do_preprocess=do_preprocess, add_bos=add_bos),
85	                                 stream=False)
86	        if hasattr(response, 'text'):

--------------------------------------------------
>> Issue: [B821:post] requests.post
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/openai/api_client.py:155
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b821_post.html
154	        pload = {k: v for k, v in locals().copy().items() if k[:2] != '__' and k not in ['self']}
155	        response = requests.post(self.chat_completions_v1_url, headers=self.headers, json=pload, stream=stream)
156	        for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b'\n'):

--------------------------------------------------
>> Issue: [B821:post] requests.post
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/openai/api_client.py:228
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b821_post.html
227	        pload = {k: v for k, v in locals().copy().items() if k[:2] != '__' and k not in ['self']}
228	        response = requests.post(self.chat_intractive_v1_url, headers=self.headers, json=pload, stream=stream)
229	        for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b'\n'):

--------------------------------------------------
>> Issue: [B821:post] requests.post
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/openai/api_client.py:290
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b821_post.html
289	        pload = {k: v for k, v in locals().copy().items() if k[:2] != '__' and k not in ['self']}
290	        response = requests.post(self.completions_v1_url, headers=self.headers, json=pload, stream=stream)
291	        for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b'\n'):

--------------------------------------------------
>> Issue: [B821:post] requests.post
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/openai/api_client.py:406
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b821_post.html
405	    }
406	    response = requests.post(api_url, headers=headers, json=pload, stream=stream)
407	    for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b'\n'):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/openai/api_client.py:416
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
415	
416	def main(api_server_url: str = 'http://0.0.0.0:23333', session_id: int = 0, api_key: Optional[str] = None):
417	    """Main function to chat in terminal."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/openai/api_client.py:419
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
418	    if not api_server_url.startswith('http://'):
419	        print(f'[WARNING] api_server_url of the api_server should '
420	              f'start with "http://", but got "{api_server_url}"')
421	        api_server_url = 'http://' + api_server_url.strip()

--------------------------------------------------
>> Issue: [B825:request] http.client.HTTPConnection.request
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/openai/api_server.py:329
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b825_request.html
328	        request.session_id = VariableInterface.session_id
329	    error_check_ret = await check_request(request)
330	    if error_check_ret is not None:

--------------------------------------------------
>> Issue: [B825:request] http.client.HTTPConnection.request
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/openai/api_server.py:603
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b825_request.html
602	        request.session_id = VariableInterface.session_id
603	    error_check_ret = await check_request(request)
604	    if error_check_ret is not None:

--------------------------------------------------
>> Issue: [B825:request] http.client.HTTPConnection.request
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/openai/api_server.py:844
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b825_request.html
843	            return create_error_response(HTTPStatus.BAD_REQUEST, 'please set a session_id to cancel a request')
844	    error_check_ret = await check_request(request)
845	    if error_check_ret is not None:

--------------------------------------------------
>> Issue: [B821:post] requests.post
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/openai/api_server.py:955
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b821_post.html
954	        headers = {'accept': 'application/json', 'Content-Type': 'application/json'}
955	        response = requests.post(url, headers=headers, json=data)
956	

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/openai/api_server.py:982
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
981	            tokenizer = VariableInterface.async_engine.tokenizer
982	            VariableInterface.reasoning_parser = ReasoningParserManager.get(reasoning_parser)(tokenizer)
983	        else:

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/openai/api_server.py:991
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
990	            tokenizer = VariableInterface.async_engine.tokenizer
991	            VariableInterface.tool_parser = ToolParserManager.get(tool_parser)(tokenizer)
992	        else:

--------------------------------------------------
>> Issue: [B823:ip_found] ip_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/openai/api_server.py:1003
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b823_ip_found.html
1002	          chat_template_config: Optional[ChatTemplateConfig] = None,
1003	          server_name: str = '0.0.0.0',
1004	          server_port: int = 23333,
1005	          allow_origins: List[str] = ['*'],
1006	          allow_credentials: bool = True,
1007	          allow_methods: List[str] = ['*'],
1008	          allow_headers: List[str] = ['*'],
1009	          log_level: str = 'ERROR',
1010	          api_keys: Optional[Union[List[str], str]] = None,
1011	          ssl: bool = False,
1012	          proxy_url: Optional[str] = None,
1013	          max_log_len: int = None,
1014	          disable_fastapi_docs: bool = False,
1015	          max_concurrent_requests: Optional[int] = None,
1016	          reasoning_parser: Optional[str] = None,
1017	          tool_call_parser: Optional[str] = None,
1018	          **kwargs):
1019	    """An example to perform model inference through the command line
1020	    interface.
1021	
1022	    Args:
1023	        model_path (str): the path of a model.
1024	            It could be one of the following options:

--------------------------------------------------
>> Issue: [B835:load] http.cookiejar.FileCookieJar.load
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/proxy/proxy.py:86
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b835_load.html
85	            with open(self.config_path, 'r') as config_file:
86	                self.nodes = yaml.safe_load(config_file)['nodes']
87	                for url, status in self.nodes.items():

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/proxy/proxy.py:88
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
87	                for url, status in self.nodes.items():
88	                    latency = deque(status.get('latency', []), maxlen=LATENCY_DEQUE_LEN)
89	                    status['latency'] = latency

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/proxy/proxy.py:92
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
91	                    self.nodes[url] = status
92	        self.heart_beat_thread = threading.Thread(target=heart_beat_controller, args=(self, ), daemon=True)
93	        self.heart_beat_thread.start()

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/proxy/proxy.py:118
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
117	        if status is None:
118	            status = self.nodes.get(node_url, Status())
119	        if status.models != []:  # force register directly

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/proxy/proxy.py:145
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
144	            try:
145	                response = requests.get(url, headers=headers)
146	                if response.status_code != 200:

--------------------------------------------------
>> Issue: [B821:post] requests.post
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/proxy/proxy.py:276
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b821_post.html
275	            async with aiohttp.ClientSession() as session:
276	                async with session.post(node_url + endpoint, json=request, timeout=self.aiotimeout) as response:
277	                    async for line in response.content:

--------------------------------------------------
>> Issue: [B821:post] requests.post
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/proxy/proxy.py:295
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b821_post.html
294	            async with aiohttp.ClientSession() as session:
295	                async with session.post(node_url + endpoint, json=request, timeout=self.aiotimeout) as response:
296	                    return await response.text()

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/proxy/proxy.py:343
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
342	
343	@app.get('/v1/models', dependencies=[Depends(check_api_key)])
344	def available_models():

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/proxy/proxy.py:352
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
351	
352	@app.get('/nodes/status', dependencies=[Depends(check_api_key)])
353	def node_status():

--------------------------------------------------
>> Issue: [B821:post] requests.post
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/proxy/proxy.py:361
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b821_post.html
360	
361	@app.post('/nodes/add', dependencies=[Depends(check_api_key)])
362	def add_node(node: Node, raw_request: Request = None):

--------------------------------------------------
>> Issue: [B821:post] requests.post
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/proxy/proxy.py:382
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b821_post.html
381	
382	@app.post('/nodes/remove', dependencies=[Depends(check_api_key)])
383	def remove_node(node_url: str):

--------------------------------------------------
>> Issue: [B821:post] requests.post
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/proxy/proxy.py:394
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b821_post.html
393	
394	@app.post('/v1/chat/completions', dependencies=[Depends(check_api_key)])
395	async def chat_completions_v1(request: ChatCompletionRequest, raw_request: Request = None):

--------------------------------------------------
>> Issue: [B821:post] requests.post
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/proxy/proxy.py:470
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b821_post.html
469	
470	@app.post('/v1/completions', dependencies=[Depends(check_api_key)])
471	async def completions_v1(request: CompletionRequest, raw_request: Request = None):

--------------------------------------------------
>> Issue: [B823:ip_found] ip_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/proxy/proxy.py:527
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b823_ip_found.html
526	
527	def proxy(server_name: str = '0.0.0.0',
528	          server_port: int = 8000,
529	          strategy: Literal['random', 'min_expected_latency', 'min_observed_latency'] = 'min_expected_latency',
530	          api_keys: Optional[Union[List[str], str]] = None,
531	          ssl: bool = False,
532	          log_level: str = 'INFO',
533	          disable_cache_status: bool = False,
534	          **kwargs):
535	    """To launch the proxy server.

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/serve/vl_async_engine.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	            raise RuntimeError(
35	                'please specify chat template as guided in https://lmdeploy.readthedocs.io/en/latest/inference/vl_pipeline.html#set-chat-template'  # noqa: E501
36	            )
37	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/turbomind/deploy/converter.py:48
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
47	        print(f'remove workspace in directory {_path}')
48	        shutil.rmtree(_path)
49	    print(f'create workspace in directory {_path}')

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/turbomind/deploy/target_model/base.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
24	    print(*args, **kwargs, file=s, end='')
25	    tqdm.tqdm.write(s.getvalue())
26	

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/turbomind/turbomind.py:112
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
111	
112	        with ThreadPoolExecutor(max_workers=self.gpu_count) as e:
113	            ranks = [self.node_id * self.gpu_count + device_id for device_id in range(self.gpu_count)]

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/turbomind/turbomind.py:135
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
134	
135	        with ThreadPoolExecutor(max_workers=self.gpu_count) as executor:
136	            futures = []

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/turbomind/turbomind.py:151
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
150	        que = Queue()
151	        with ThreadPoolExecutor(max_workers=self.gpu_count) as executor:
152	            futures = []

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/engine.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
40	        self.max_batch_size = vision_config.max_batch_size
41	        self.executor = ThreadPoolExecutor(max_workers=1)
42	        torch.cuda.empty_cache()

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/model/deepseek.py:20
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
19	    except ImportError:
20	        raise ImportError('To use DeepSeekVLModel, please install deepseek_vl by '
21	                          '`pip install git+https://github.com/deepseek-ai/DeepSeek-VL.git'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/model/deepseek.py:147
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
146	            if n_placeholder == 0:
147	                logger.warning(f"""for deepseek-vl model, the user should insert the {IMAGE_TOKEN}
148	                    to user prompt manually, please read https://lmdeploy.readthedocs.io/en/latest/inference/vl_pipeline.html

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/model/deepseek_vl2.py:20
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
19	    except ImportError:
20	        raise ImportError('To use DeepSeek-VL2, please install deepseek_vl2 by '
21	                          '`pip install git+https://github.com/deepseek-ai/DeepSeek-VL2.git'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/model/deepseek_vl2.py:134
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
133	        if n_placeholder == 0:
134	            logger.warning(f"""for deepseek-vl2 model, the user should insert the {IMAGE_TOKEN}
135	                to user prompt manually, please read https://lmdeploy.readthedocs.io/en/latest/inference/vl_pipeline.html

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/model/internvl_llava.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	        raise ImportError(
24	            'To use LlavaVLModel, please install llava by '
25	            '`pip install git+https://github.com/OpenGVLab/InternVL#subdirectory=internvl_chat_llava --no-deps`')
26	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/model/llava.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	    except ImportError:
26	        raise ImportError('To use LlavaVLModel, please install llava by '
27	                          '`pip install git+https://github.com/haotian-liu/LLaVA.git --no-deps`'  # noqa: E501

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/model/mini_gemeni.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	    except ImportError:
23	        raise ImportError('To use MiniGeminiVisionModel, please install minigemini by '
24	                          '`pip install git+https://github.com/dvlab-research/MGM.git'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/model/mllama.py:12
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
11	    except ImportError:
12	        raise ImportError('please install latest transformers by '
13	                          'pip install git+https://github.com/huggingface/transformers.git')

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/model/qwen2.py:19
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
18	    except ImportError:
19	        raise ImportError('please install latest transformers by '
20	                          'pip install git+https://github.com/huggingface/transformers.git')

--------------------------------------------------
>> Issue: [B800:exec_used] exec
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/model/utils.py:120
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b800_exec_used.html
119	            try:
120	                exec('import {}'.format('.'.join(split_path[:i])))
121	                break

--------------------------------------------------
>> Issue: [B800:exec_used] exec
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/model/utils.py:145
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b800_exec_used.html
144	    if isinstance(origin_func_path, str):
145	        exec(f'{origin_func_path} = rewrite_func')
146	    elif method_class:

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/tools/merge_xcomposer2d5_task.py:21
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
20	    if os.path.exists(dst_path):
21	        shutil.rmtree(dst_path)
22	

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/utils.py:18
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
17	    buffered = BytesIO()
18	    FETCH_TIMEOUT = int(os.environ.get('LMDEPLOY_FETCH_TIMEOUT', 10))
19	    headers = {

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/utils.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
27	            if url_or_path.startswith('http'):
28	                response = requests.get(url_or_path, headers=headers, timeout=FETCH_TIMEOUT)
29	                response.raise_for_status()

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/utils.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
29	                response.raise_for_status()
30	                buffered.write(response.content)
31	            elif os.path.exists(url_or_path):

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/utils.py:33
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
32	                with open(url_or_path, 'rb') as image_file:
33	                    buffered.write(image_file.read())
34	        elif isinstance(image, Image.Image):

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/utils.py:33
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
32	                with open(url_or_path, 'rb') as image_file:
33	                    buffered.write(image_file.read())
34	        elif isinstance(image, Image.Image):

--------------------------------------------------
>> Issue: [B802:b64encode] base64.b64encode
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/utils.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b802_b64encode.html
42	        image.save(buffered, format='PNG')
43	    res = base64.b64encode(buffered.getvalue()).decode('utf-8')
44	    return res

--------------------------------------------------
>> Issue: [B301:blacklist] base64.b64encode
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/utils.py:43
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b301-base64-b64encode
42	        image.save(buffered, format='PNG')
43	    res = base64.b64encode(buffered.getvalue()).decode('utf-8')
44	    return res

--------------------------------------------------
>> Issue: [B801:b64decode] base64.b64decode
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/utils.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b801_b64decode.html
48	    """load image from base64 format."""
49	    return Image.open(BytesIO(base64.b64decode(image)))
50	

--------------------------------------------------
>> Issue: [B300:blacklist] base64.b64decode
   Severity: Medium   Confidence: High
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/utils.py:49
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b300-base64-b64decode
48	    """load image from base64 format."""
49	    return Image.open(BytesIO(base64.b64decode(image)))
50	

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/utils.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
53	    """load image from url, local path or openai GPT4V."""
54	    FETCH_TIMEOUT = int(os.environ.get('LMDEPLOY_FETCH_TIMEOUT', 10))
55	    headers = {

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/lmdeploy-0.7.2.post1-cp39-cp39-win_amd64/lmdeploy/vl/utils.py:65
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
64	        elif image_url.startswith('http'):
65	            response = requests.get(image_url, headers=headers, timeout=FETCH_TIMEOUT)
66	            response.raise_for_status()

--------------------------------------------------

Code scanned:
	Total lines of code: 51642
	Total lines skipped (#nosec): 0

Run metrics:
	Total issues (by severity):
		Undefined: 0.0
		Low: 0.0
		Medium: 39.0
		High: 48.0
	Total issues (by confidence):
		Undefined: 0.0
		Low: 0.0
		Medium: 77.0
		High: 10.0
Files skipped (0):
