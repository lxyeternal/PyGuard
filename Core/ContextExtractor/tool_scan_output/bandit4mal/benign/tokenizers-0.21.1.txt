Run started:2025-04-12 13:34:54.209208

Test results:
>> Issue: [B838:process] multiprocessing.Process
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/benches/test_tiktoken.py:110
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b838_process.html
109	            # runtime.
110	            p = Process(target=benchmark_batch, args=(model, documents, num_threads, document_length))
111	            p.start()

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/py_src/tokenizers/implementations/sentencepiece_unigram.py:156
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
155	            raise Exception(
156	                "You don't seem to have the required protobuf file, in order to use this function you need to run `pip install protobuf` and `wget https://raw.githubusercontent.com/google/sentencepiece/master/python/src/sentencepiece/sentencepiece_model_pb2.py` for us to be able to read the intrinsics of your spm_file. `pip install sentencepiece` is not required."
157	            )
158	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/py_src/tokenizers/implementations/sentencepiece_unigram.py:160
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
159	        m = model.ModelProto()
160	        m.ParseFromString(open(filename, "rb").read())
161	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/py_src/tokenizers/tools/visualizer.py:13
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
12	with open(css_filename) as f:
13	    css = f.read()
14	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/scripts/convert.py:48
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
47	        raise Exception(
48	            "You don't seem to have the required protobuf file, in order to use this function you need to run `pip install protobuf` and `wget https://raw.githubusercontent.com/google/sentencepiece/master/python/sentencepiece_model_pb2.py` for us to be able to read the intrinsics of your spm_file. `pip install sentencepiece` is not required."
49	        )
50	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/scripts/convert.py:52
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
51	    m = model.ModelProto()
52	    m.ParseFromString(open(filename, "rb").read())
53	    return m

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/scripts/sentencepiece_extractor.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
36	                merge = f"{piece_l}{piece_r}"
37	                piece_id = vocab.get(merge, None)
38	                if piece_id:

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/scripts/sentencepiece_extractor.py:121
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
120	                logger.info("Writing content from {} to {}".format(args.model, f.name))
121	                response = get(args.model, allow_redirects=True)
122	                f.write(response.content)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/scripts/sentencepiece_extractor.py:122
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
121	                response = get(args.model, allow_redirects=True)
122	                f.write(response.content)
123	

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/scripts/sentencepiece_extractor.py:122
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
121	                response = get(args.model, allow_redirects=True)
122	                f.write(response.content)
123	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/stub.py:145
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
144	        with open(filename, "r") as f:
145	            data = f.read()
146	            assert data == pyi_content, f"The content of {filename} seems outdated, please run `python stub.py`"

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/stub.py:149
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
148	        with open(filename, "w") as f:
149	            f.write(pyi_content)
150	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/stub.py:168
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
167	            with open(filename, "r") as f:
168	                data = f.read()
169	                assert data == py_content, f"The content of {filename} seems outdated, please run `python stub.py`"

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/stub.py:172
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
171	            with open(filename, "w") as f:
172	                f.write(py_content)
173	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/stub.py:175
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
174	    for name, submodule in submodules:
175	        write(submodule, os.path.join(directory, name), f"{name}", check=check)
176	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/stub.py:185
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
184	
185	    write(tokenizers.tokenizers, "py_src/tokenizers/", "tokenizers", check=args.check)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/bindings/test_trainers.py:188
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
187	        with open(filename, "w") as f:
188	            f.write(
189	                """
190	[CLS] The Zen of Python, by Tim Peters [SEP]

--------------------------------------------------
>> Issue: [B819:urlretrieve] urllib.request.urlretrieve
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/documentation/test_pipeline.py:179
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b819_urlretrieve.html
178	        print("Downloading wikitext-103...")
179	        wiki_text, _ = request.urlretrieve(
180	            "https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip"
181	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/documentation/test_pipeline.py:180
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
179	        wiki_text, _ = request.urlretrieve(
180	            "https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip"
181	        )
182	        with ZipFile(wiki_text, "r") as z:

--------------------------------------------------
>> Issue: [B819:urlretrieve] urllib.request.urlretrieve
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/documentation/test_quicktour.py:189
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b819_urlretrieve.html
188	        print("Downloading wikitext-103...")
189	        wiki_text, _ = request.urlretrieve(
190	            "https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip"
191	        )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/documentation/test_quicktour.py:190
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
189	        wiki_text, _ = request.urlretrieve(
190	            "https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip"
191	        )
192	        with ZipFile(wiki_text, "r") as z:

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/documentation/test_tutorial_train_from_iterators.py:46
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
45	                with gzip.open(path, "wt") as f:
46	                    f.write(small.read())
47	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/documentation/test_tutorial_train_from_iterators.py:46
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
45	                with gzip.open(path, "wt") as f:
46	                    f.write(small.read())
47	

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/utils.py:17
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
16	        with open(filepath, "wb") as f:
17	            response = requests.get(url, stream=True)
18	            response.raise_for_status()

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/utils.py:20
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
19	            for chunk in response.iter_content(1024):
20	                f.write(chunk)
21	    return filepath

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/utils.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	    return {
35	        "vocab": download("https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json"),
36	        "merges": download("https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/utils.py:36
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
35	        "vocab": download("https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json"),
36	        "merges": download("https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt"),
37	    }

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/utils.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	    return {
43	        "vocab": download("https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"),
44	    }

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/utils.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	    return {
50	        "vocab": download("https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json"),
51	        "merges": download("https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/utils.py:51
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
50	        "vocab": download("https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json"),
51	        "merges": download("https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt"),
52	    }

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/utils.py:57
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
56	def train_files(data_dir):
57	    big = download("https://norvig.com/big.txt")
58	    small = os.path.join(DATA_PATH, "small.txt")

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/utils.py:62
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
61	            for i, line in enumerate(g):
62	                f.write(line)
63	                if i > 100:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/utils.py:73
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
72	def albert_base(data_dir):
73	    return download("https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v1-tokenizer.json")
74	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/utils.py:79
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
78	    return download(
79	        "https://s3.amazonaws.com/models.huggingface.co/bert/anthony/doc-quicktour/tokenizer.json",
80	        "tokenizer-wiki.json",
81	    )
82	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/utils.py:87
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
86	    return download(
87	        "https://s3.amazonaws.com/models.huggingface.co/bert/anthony/doc-pipeline/tokenizer.json",
88	        "bert-wiki.json",
89	    )
90	

--------------------------------------------------
>> Issue: [B838:process] multiprocessing.Process
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/bindings/python/tests/utils.py:113
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b838_process.html
112	    os.environ["TOKENIZERS_PARALLELISM"] = str(enabled)
113	    p = mp.Process(target=encode, args=(tokenizer,))
114	    p.start()

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/py_src/tokenizers/implementations/sentencepiece_unigram.py:156
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
155	            raise Exception(
156	                "You don't seem to have the required protobuf file, in order to use this function you need to run `pip install protobuf` and `wget https://raw.githubusercontent.com/google/sentencepiece/master/python/src/sentencepiece/sentencepiece_model_pb2.py` for us to be able to read the intrinsics of your spm_file. `pip install sentencepiece` is not required."
157	            )
158	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/py_src/tokenizers/implementations/sentencepiece_unigram.py:160
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
159	        m = model.ModelProto()
160	        m.ParseFromString(open(filename, "rb").read())
161	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home/blue/PyPIAgent/Dataset/study/unzip_benign/tokenizers-0.21.1/tokenizers-0.21.1/py_src/tokenizers/tools/visualizer.py:13
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
12	with open(css_filename) as f:
13	    css = f.read()
14	

--------------------------------------------------

Code scanned:
	Total lines of code: 6365
	Total lines skipped (#nosec): 0

Run metrics:
	Total issues (by severity):
		Undefined: 0.0
		Low: 0.0
		Medium: 14.0
		High: 25.0
	Total issues (by confidence):
		Undefined: 0.0
		Low: 0.0
		Medium: 39.0
		High: 0.0
Files skipped (0):
