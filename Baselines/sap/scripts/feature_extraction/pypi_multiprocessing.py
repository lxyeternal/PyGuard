import os
import time
import pandas as pd
import multiprocessing as mp
from functools import partial, reduce
import nltk
import json
import numpy as np
import statistics
from pathlib import Path
import utilities_functions
from pypi_feature_extractor import PyPI_Feature_Extractor
from pygments.lexers import PythonLexer
from pygments.token import Token
from urlextract import URLExtract

def process_py_file(file_path, dangerous_token, stopwords):
    """并行处理单个Python文件，提取特征"""
    try:
        # initialize the list for the puntuactions and operators token
        operator = []
        punctuation = []
        other = []
        id = []
        strs = []
        p = Path(file_path)
        # package name
        package_name = p.parts[8]  # 这个索引可能需要根据您的文件路径调整
        # name of the file
        js = p.parts[-1]
        
        with open(file_path, "r", encoding="utf8", errors='ignore', newline='\n') as file:
            data = file.read()
            
        # apply the lexer specific for language
        lexer = PythonLexer(stripnl=False, ensurenl=False)
        token_source = lexer.get_tokens(data)
        for token in token_source:
            if token[0] in Token.Operator:
                operator.append(token[1])
            elif token[0] in Token.Punctuation:
                punctuation.append(token[1])
            elif token[0] in Token.Name:
                id.append(token[1])
            elif (token[0] in Token.Literal.String.Single or token[0] in Token.Literal.String.Double or token[0] in Token.Literal.String.Affix 
                  or token[0] in Token.Literal.String.Backtick or token[0] in Token.Literal.String.Char or token[0] in Token.Literal.String.Delimiter 
                  or token[0] in Token.Literal.String.Doc or token[0] in Token.Literal.String.Escape or token[0] in Token.Literal.String.Heredoc 
                  or token[0] in Token.Literal.String.Interpol or token[0] in Token.Literal.String.Other):
                strs.append(token[1])
            else:
                other.append(token[1])
                
        with open(file_path, "r", encoding="utf8", errors='ignore') as fp:
            num_lines = sum(1 for line in fp)
            size = fp.seek(0, os.SEEK_END)
            size += 1
            
        id = [s.replace("'", '') for s in id]
        id = [s.replace('"', '') for s in id]
        id_ = ' '.join(id)
        equalities = operator.count('=') / size
        plus = operator.count('+') / size
        Lbrackets = punctuation.count('[') / size
        count_base64 = 0
        count_IP = 0
        byte = 0
        
        for value in range(0, len(strs)):
            count_base64 += len(utilities_functions.contains_base64(strs[value]))
            count_IP += len(utilities_functions.contains_IPAddress(strs[value]))
            # contains_dangerous_token --> sospicious list
            byte += len(utilities_functions.contains_dangerous_token(strs[value], dangerous_token))
            
        strs = [s.replace("'", '') for s in strs]
        strs = [s.replace('"', '') for s in strs]
        string = ' '.join(strs).split()
        # remove stopwords
        string = list(set(strs) - stopwords)
        string_ = ' '.join(string)
        
        # 返回处理结果
        return {
            'Package Name': package_name,
            '.py': js,
            'sospicious token': byte,
            'lines': num_lines,
            'equal ratio': equalities,
            'plus ratio': plus,
            'bracket ratio': Lbrackets,
            'identifiers': id_,
            'base64': count_base64,
            'IP': count_IP,
            'strings': string_,
            'code': data
        }
    except Exception as e:
        print(f"处理文件 {file_path} 时出错: {str(e)}")
        return None


def process_file_extension(ext, path_to_scan, parts_index=8):
    """并行处理单个文件扩展名，统计出现次数"""
    try:
        Package = []
        extension = []
        
        files_path = utilities_functions.find_files_of_ext(path_to_scan, ext)
        
        for j in range(len(files_path)):
            # extract the path
            p = Path(files_path[j])
            # package name
            if "tar.gz" not in p.parts[-1]:
                try:
                    package_name = p.parts[parts_index]
                    # version name
                    Package.append(package_name)
                    extension.append(ext)
                except IndexError:
                    print(f"文件路径索引错误: {files_path[j]}, parts: {p.parts}")
                    continue
                    
        return {
            'Package': Package,
            'extension': extension
        }
    except Exception as e:
        print(f"处理扩展名 {ext} 时出错: {str(e)}")
        return {'Package': [], 'extension': []}


class MultiprocessingPyPIFeatureExtractor(PyPI_Feature_Extractor):
    """多进程版本的PyPI特征提取器"""
    
    def __init__(self, num_processes=None):
        super().__init__()
        # 如果没有指定进程数，使用CPU核心数
        self.num_processes = num_processes or mp.cpu_count()
        print(f"初始化多进程特征提取器，使用 {self.num_processes} 个进程")
    
    def extract_features_from_py_parallel(self):
        """并行提取Python文件特征"""
        print(f"[进度] 开始并行提取Python文件特征...")
        
        # 获取所有Python文件
        files_path = utilities_functions.find_files_of_ext(self.path_to_scan, ".py")
        print(f"[进度] 找到 {len(files_path)} 个Python文件")
        
        if not files_path:
            print("没有找到Python文件")
            return pd.DataFrame(), pd.DataFrame()
        
        # 创建进程池
        pool = mp.Pool(processes=self.num_processes)
        
        # 准备处理函数
        process_func = partial(process_py_file, dangerous_token=self.dangerous_token, stopwords=self.stopwords)
        
        # 使用进程池并行处理文件
        chunk_size = max(1, len(files_path) // (self.num_processes * 2))
        results = []
        
        # 添加进度显示
        processed = 0
        total = len(files_path)
        print(f"[进度] 开始处理文件: 0/{total} (0.0%)")
        
        # 按块处理文件
        for i in range(0, total, chunk_size):
            end = min(i + chunk_size, total)
            chunk = files_path[i:end]
            chunk_results = pool.map(process_func, chunk)
            results.extend([r for r in chunk_results if r is not None])
            
            processed += len(chunk)
            print(f"[进度] 处理文件: {processed}/{total} ({processed/total*100:.1f}%)")
        
        # 关闭进程池
        pool.close()
        pool.join()
        
        # 转换为DataFrame
        db = pd.DataFrame(results)
        
        if db.empty:
            print("没有成功处理任何Python文件")
            return pd.DataFrame(), pd.DataFrame()
        
        # 分离setup.py和其他文件
        setup_db = db[db['.py'] == 'setup.py']
        db = db[db['.py'] != 'setup.py']
        
        print(f"[进度] 处理完成，有 {len(db)} 个普通Python文件和 {len(setup_db)} 个setup.py文件")
        
        return self.merge_py_of_same_package(db), self.merge_setup_of_same_package(setup_db)
    
    def count_package_files_extension_parallel(self):
        """并行统计文件扩展名"""
        print(f"[进度] 开始并行统计文件扩展名，共 {len(self.classes)} 种扩展名")
        
        # 添加点到扩展名
        extensions = ['.' + cls for cls in self.classes]
        
        # 创建进程池
        pool = mp.Pool(processes=self.num_processes)
        
        # 准备处理函数
        process_func = partial(process_file_extension, path_to_scan=self.path_to_scan)
        
        # 使用进程池并行处理扩展名
        chunk_size = max(1, len(extensions) // (self.num_processes * 2))
        results = []
        
        # 添加进度显示
        processed = 0
        total = len(extensions)
        print(f"[进度] 开始处理扩展名: 0/{total} (0.0%)")
        
        # 按块处理扩展名
        for i in range(0, total, chunk_size):
            end = min(i + chunk_size, total)
            chunk = extensions[i:end]
            chunk_results = pool.map(process_func, chunk)
            
            # 合并结果
            for result in chunk_results:
                if result['Package']:  # 只添加非空结果
                    for j in range(len(result['Package'])):
                        results.append({
                            'Package Name': result['Package'][j],
                            'extension': result['extension'][j]
                        })
            
            processed += len(chunk)
            print(f"[进度] 处理扩展名: {processed}/{total} ({processed/total*100:.1f}%)")
        
        # 关闭进程池
        pool.close()
        pool.join()
        
        # 转换为DataFrame
        db = pd.DataFrame(results)
        
        if db.empty:
            print("没有找到任何扩展名的文件")
            return pd.DataFrame(columns=['Package Name'])
        
        # 统计频率
        db = db.groupby(['Package Name', 'extension']).size().unstack(fill_value=0)
        
        # 对于每个包只保留最新版本
        db = db.groupby('Package Name').last()
        
        # 添加缺失的扩展名列
        f = [c for c in extensions if c not in db.columns]
        db = pd.concat([db, pd.DataFrame(columns=f)])
        
        # 填充NaN，并转换数据类型以避免警告
        for col in f:
            if col in db.columns:
                db[col] = db[col].fillna(0).astype(int)
        
        # 排序列
        db = db[extensions]
        db.reset_index(inplace=True)
        
        print(f"[进度] 文件扩展名统计完成，共 {len(db)} 个包")
        
        return db
    
    def extract_features(self, path: str, dataset: str) -> pd.DataFrame:
        """多进程版本的特征提取主函数"""
        self.path_to_scan = path
        print(f"[进度] 开始多进程特征提取，路径: {path}")
        
        # 使用并行方法提取特征
        py_files_df, setup_files_df = self.extract_features_from_py_parallel()
        
        # 并行统计文件扩展名
        extensions_files_df = self.count_package_files_extension_parallel()
        
        print(f"[进度] 合并数据框...")
        # 确保所有DataFrame都有'Package Name'列
        if py_files_df.empty:
            py_files_df = pd.DataFrame(columns=['Package Name'])
        if setup_files_df.empty:
            setup_files_df = pd.DataFrame(columns=['Package Name'])
        if extensions_files_df.empty:
            extensions_files_df = pd.DataFrame(columns=['Package Name'])
            
        # 合并数据框
        dfs = [df for df in [py_files_df, setup_files_df, extensions_files_df] if not df.empty and 'Package Name' in df.columns]
        
        if not dfs:
            print("没有有效的数据可以处理")
            return pd.DataFrame()
            
        if len(dfs) == 1:
            final_df = dfs[0]
        else:
            try:
                final_df = reduce(lambda left, right: pd.merge(left, right, on=['Package Name'], how='outer'), dfs)
            except Exception as e:
                print(f"合并数据框时出错: {str(e)}")
                print("尝试逐个合并...")
                # 尝试逐个合并
                final_df = dfs[0]
                for i in range(1, len(dfs)):
                    try:
                        final_df = pd.merge(final_df, dfs[i], on=['Package Name'], how='outer')
                    except Exception as e:
                        print(f"合并 DataFrame {i} 时出错: {str(e)}")
        
        if final_df.empty:
            print("合并后的数据框为空，无法继续处理")
            return pd.DataFrame()
            
        # 应用特征提取（这部分保持原样，因为它已经高度优化）
        print(f"[进度] 应用特征提取...")
        try:
            final_df = self.extraction(final_df, utilities_functions.gen_language_4, 4, utilities_functions.gen_language_4, 4)
            
            # 保存结果
            print(f"[进度] 保存数据到 {dataset}")
            final_df.to_csv(dataset, encoding='utf-8', index=False)
            
            print(f"[进度] 完成特征提取，共处理 {len(final_df)} 个包")
        except Exception as e:
            print(f"特征提取或保存过程中出错: {str(e)}")
            import traceback
            traceback.print_exc()
            
        return final_df


def main():
    """示例用法"""
    # import argparse
    
    # parser = argparse.ArgumentParser(description='多进程PyPI特征提取')
    # parser.add_argument('path', help='要处理的文件夹路径')
    # parser.add_argument('output', help='输出CSV文件名')
    # parser.add_argument('--processes', type=int, default=None, help='要使用的进程数，默认为CPU核心数')

    # 数据集根目录列表
    base_datasets = [
        "/home2/blue/Documents/PyPIAgent/Dataset/evaluation",
        "/home2/blue/Documents/PyPIAgent/Dataset/latest",
        "/home2/blue/Documents/PyPIAgent/Dataset/obfuscation"
    ]

    # 数据类型目录名
    data_types = ["unzip_benign", "unzip_malware"]

    print("="*80)
    print(f"开始PyPI多进程特征提取任务")
    print(f"处理 {len(base_datasets)} 个数据集，每个数据集 {len(data_types)} 种类型")
    print(f"使用 {24} 个CPU核心")
    print("="*80)

    # 记录总开始时间
    total_start_time = time.time()
    
    # 计数器
    success_count = 0
    fail_count = 0
    total_datasets = len(base_datasets) * len(data_types)
    dataset_counter = 0

    # 处理每个数据集和数据类型
    for base_dataset in base_datasets:
        for data_type in data_types:
            dataset_counter += 1
            # 构建完整路径
            data_path = os.path.join(base_dataset, data_type)
            if not os.path.exists(data_path):
                print(f"路径不存在: {data_path}")
                fail_count += 1
                continue

            # 提取数据集名称（取最后一个目录名）
            dataset_name = os.path.basename(base_dataset)
            # 提取数据类型名称（benign或malware）
            type_name = data_type.split("_")[-1]
            
            print("\n" + "="*80)
            print(f"[{dataset_counter}/{total_datasets}] 处理数据集: {dataset_name}_{type_name}")
            print("="*80)
            
            # 记录开始时间
            start_time = time.time()
            
            try:
                # 创建多进程特征提取器
                extractor = MultiprocessingPyPIFeatureExtractor(24)
                
                # 输出文件名
                output_file = f"{dataset_name}_{type_name}_pypi_multiprocessing.csv"
                
                # 提取特征
                result_df = extractor.extract_features(data_path, output_file)
                
                if not result_df.empty:
                    success_count += 1
                else:
                    fail_count += 1
                
                # 显示总耗时
                elapsed_time = time.time() - start_time
                print(f"数据集 {dataset_name}_{type_name} 处理完成，耗时: {elapsed_time:.2f} 秒")
                
            except Exception as e:
                print(f"处理数据集时发生错误: {data_path}")
                print(f"错误详情: {str(e)}")
                import traceback
                traceback.print_exc()
                fail_count += 1
                continue

    # 计算总处理时间
    total_elapsed_time = time.time() - total_start_time
    
    print("\n" + "="*80)
    print(f"所有数据集处理完成，总耗时: {total_elapsed_time:.2f} 秒")
    print(f"成功: {success_count}/{total_datasets}, 失败: {fail_count}/{total_datasets}")
    print("="*80)


if __name__ == "__main__":
    # 为了确保Windows上的多进程代码能正常工作
    mp.freeze_support()
    main() 