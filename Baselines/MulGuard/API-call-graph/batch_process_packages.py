#!/usr/bin/env python3
"""
Batch processing script for extracting features from packages in Dataset directories
and saving them to corresponding TestData directories.

This script processes packages from:
- /home2/wenbo/Documents/PyPIAgent/Dataset/evaluation/unzip_benign -> TestData/evaluation/benign
- /home2/wenbo/Documents/PyPIAgent/Dataset/evaluation/unzip_malware -> TestData/evaluation/malware
- /home2/wenbo/Documents/PyPIAgent/Dataset/latest/unzip_benign -> TestData/latest/benign
- /home2/wenbo/Documents/PyPIAgent/Dataset/latest/unzip_malware -> TestData/latest/malware
- /home2/wenbo/Documents/PyPIAgent/Dataset/obfuscation/unzip_benign -> TestData/obfuscation/benign
- /home2/wenbo/Documents/PyPIAgent/Dataset/obfuscation/unzip_malware -> TestData/obfuscation/malware
"""

import os
import sys
from pathlib import Path
from analyze_single_package import process_package
import json
from tqdm import tqdm
from multiprocessing import Pool, cpu_count
import time
import signal

def get_dataset_mapping():
    """Get mapping between Dataset directories and TestData directories"""
    base_dataset = Path("/home2/wenbo/Documents/PyPIAgent/Dataset")
    base_testdata = Path("/home2/wenbo/Documents/PyPIAgent/Tools/MulGuard/TestData")
    
    mappings = []
    
    # Define the three main directories
    main_dirs = ["evaluation", "latest", "obfuscation"]
    
    for main_dir in main_dirs:
        # Benign packages mapping
        source_benign = base_dataset / main_dir / "unzip_benign"
        target_benign = base_testdata / main_dir / "benign"
        
        # Malware packages mapping
        source_malware = base_dataset / main_dir / "unzip_malware"
        target_malware = base_testdata / main_dir / "malware"
        
        mappings.extend([
            (source_benign, target_benign, "benign"),
            (source_malware, target_malware, "malware")
        ])
    
    return mappings

class TimeoutError(Exception):
    """Custom timeout exception"""
    pass

def timeout_handler(signum, frame):
    """Signal handler for timeout"""
    raise TimeoutError("Package analysis timed out")

def process_single_package_wrapper(args):
    """Wrapper function for multiprocessing with timeout"""
    package_dir, output_package_dir, package_name = args
    
    try:
        # Check if already processed (all 4 JSON files exist and are non-empty)
        centrality_files = ["degree_new.json", "closeness_new.json", "harmonic_new.json", "katz_new.json"]
        if output_package_dir.exists():
            all_files_exist = True
            for file_name in centrality_files:
                file_path = output_package_dir / file_name
                if not file_path.exists() or file_path.stat().st_size == 0:
                    all_files_exist = False
                    break
            
            if all_files_exist:
                return f"SKIPPED: {package_name} (already processed - all 4 JSON files exist)"
        
        # Create output directory
        output_package_dir.mkdir(parents=True, exist_ok=True)
        
        # Set up timeout (3 minutes = 180 seconds)
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(180)  # 3 minutes timeout
        
        try:
            # Process the package
            result = process_package(package_dir, output_package_dir)
            
            # Cancel the alarm if processing completed successfully
            signal.alarm(0)
            
            if result:
                return f"SUCCESS: {package_name}"
            else:
                return f"FAILED: {package_name} (no API calls found)"
                
        except TimeoutError:
            signal.alarm(0)  # Cancel the alarm
            return f"TIMEOUT: {package_name} (analysis exceeded 3 minutes)"
            
    except Exception as e:
        signal.alarm(0)  # Cancel the alarm in case of other exceptions
        return f"ERROR: {package_name} - {str(e)}"

def process_single_mapping(source_dir, target_dir, package_type, num_processes=None):
    """Process all packages in a single source directory using multiprocessing"""
    if not source_dir.exists():
        print(f"Source directory does not exist: {source_dir}")
        return 0, 0, 0
    
    # Create target directory if it doesn't exist
    target_dir.mkdir(parents=True, exist_ok=True)
    
    # Get all package directories
    package_dirs = [d for d in source_dir.iterdir() if d.is_dir()]
    
    if not package_dirs:
        print(f"No packages found in: {source_dir}")
        return 0, 0, 0
    
    # Set number of processes
    if num_processes is None:
        num_processes = min(cpu_count(), len(package_dirs))  # Don't use more processes than packages
    
    print(f"\nProcessing {len(package_dirs)} {package_type} packages from: {source_dir}")
    print(f"Output directory: {target_dir}")
    print(f"Using {num_processes} processes")
    
    # Prepare arguments for multiprocessing
    process_args = []
    for package_dir in package_dirs:
        package_name = package_dir.name
        output_package_dir = target_dir / package_name
        process_args.append((package_dir, output_package_dir, package_name))
    
    # Process packages in parallel
    start_time = time.time()
    processed_count = 0
    error_count = 0
    skipped_count = 0
    timeout_count = 0
    
    with Pool(processes=num_processes) as pool:
        # Use imap for progress tracking
        results = list(tqdm(
            pool.imap(process_single_package_wrapper, process_args),
            total=len(process_args),
            desc=f"Processing {package_type} packages"
        ))
    
    # Count results
    for result in results:
        if result.startswith("SUCCESS"):
            processed_count += 1
        elif result.startswith("FAILED"):
            error_count += 1
        elif result.startswith("SKIPPED"):
            skipped_count += 1
        elif result.startswith("TIMEOUT"):
            timeout_count += 1
            print(result)  # Print timeout details
        elif result.startswith("ERROR"):
            error_count += 1
            print(result)  # Print error details
    
    end_time = time.time()
    processing_time = end_time - start_time
    
    print(f"\n{package_type.capitalize()} packages processing completed in {processing_time:.2f} seconds")
    print(f"  Successfully processed: {processed_count}")
    print(f"  Skipped (already processed): {skipped_count}")
    print(f"  Timed out (>3 minutes): {timeout_count}")
    print(f"  Failed/Errors: {error_count}")
    
    return processed_count, error_count, skipped_count, timeout_count

def main():
    """Main function to batch process all packages with 20 processes"""
    num_processes = 20
    
    print("Starting batch processing of packages with multiprocessing...")
    print(f"Available CPU cores: {cpu_count()}, Using: {num_processes} processes")
    print("=" * 60)
    
    # Get all directory mappings
    mappings = get_dataset_mapping()
    
    total_processed = 0
    total_errors = 0
    total_skipped = 0
    total_timeouts = 0
    overall_start_time = time.time()
    
    # Process each mapping
    for source_dir, target_dir, package_type in mappings:
        processed, errors, skipped, timeouts = process_single_mapping(source_dir, target_dir, package_type, num_processes)
        total_processed += processed
        total_errors += errors
        total_skipped += skipped
        total_timeouts += timeouts
    
    overall_end_time = time.time()
    total_time = overall_end_time - overall_start_time
    
    print("\n" + "=" * 60)
    print("Batch processing completed!")
    print(f"Total processing time: {total_time:.2f} seconds")
    print(f"Total packages processed successfully: {total_processed}")
    print(f"Total packages skipped (already processed): {total_skipped}")
    print(f"Total packages timed out (>3 minutes): {total_timeouts}")
    print(f"Total packages with errors: {total_errors}")
    
    if total_errors > 0:
        print(f"\nNote: {total_errors} packages encountered errors during processing.")
        print("This is typically due to packages with no API calls or syntax errors.")
    
    if total_timeouts > 0:
        print(f"\nNote: {total_timeouts} packages exceeded the 3-minute timeout limit.")
        print("These packages likely have very complex code structure or infinite loops.")

def process_specific_directory(dataset_type, package_type):
    """Process a specific directory with 20 processes"""
    num_processes = 20
    base_dataset = Path("/home2/wenbo/Documents/PyPIAgent/Dataset")
    base_testdata = Path("/home2/wenbo/Documents/PyPIAgent/Tools/MulGuard/TestData")
    
    # Map package_type to directory name
    dir_mapping = {
        "benign": "unzip_benign",
        "malware": "unzip_malware"
    }
    
    if package_type not in dir_mapping:
        print(f"Invalid package type: {package_type}. Must be 'benign' or 'malware'")
        return
    
    source_dir = base_dataset / dataset_type / dir_mapping[package_type]
    target_dir = base_testdata / dataset_type / package_type
    
    if not source_dir.exists():
        print(f"Source directory does not exist: {source_dir}")
        return
    
    print(f"Processing specific directory with {num_processes} processes")
    process_single_mapping(source_dir, target_dir, package_type, num_processes)

if __name__ == "__main__":
    if len(sys.argv) == 1:
        # Process all directories with 20 processes
        main()
    elif len(sys.argv) == 3:
        # Process specific directory with 20 processes
        dataset_type = sys.argv[1]  # evaluation, latest, or obfuscation
        package_type = sys.argv[2]  # benign or malware
        
        valid_datasets = ['evaluation', 'latest', 'obfuscation']
        valid_types = ['benign', 'malware']
        
        if dataset_type not in valid_datasets:
            print(f"Invalid dataset type: {dataset_type}. Must be one of: {', '.join(valid_datasets)}")
            sys.exit(1)
        
        if package_type not in valid_types:
            print(f"Invalid package type: {package_type}. Must be one of: {', '.join(valid_types)}")
            sys.exit(1)
        
        process_specific_directory(dataset_type, package_type)
    else:
        print("Usage:")
        print("  python batch_process_packages.py                    # Process all directories (20 processes)")
        print("  python batch_process_packages.py <dataset> <type>   # Process specific directory (20 processes)")
        print("")
        print("Examples:")
        print("  python batch_process_packages.py                    # Process all")
        print("  python batch_process_packages.py evaluation malware # Process evaluation/malware")
        print("  python batch_process_packages.py latest benign      # Process latest/benign")
        print("")
        print("Note: Uses 20 processes by default for optimal performance")
        sys.exit(1)
