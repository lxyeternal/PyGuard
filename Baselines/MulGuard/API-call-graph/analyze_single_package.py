#!/usr/bin/env python3
"""
单个包的完整特征提取脚本
用于对新的包文件构建特征集进行恶意检测

使用方法:
    python analyze_single_package.py <包路径>
    
示例:
    python analyze_single_package.py /path/to/package/10Cent10-999.0.4/
"""

import ast
import re
import os
import json
import networkx as nx
import sys
import argparse
from pathlib import Path

def extract_api_calls(file_path):
    """从Python文件中提取API调用"""
    api_calls = []

    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
            content = file.read()

        try:
            # 首先尝试使用AST解析
            tree = ast.parse(content, filename=file_path)
            for node in ast.walk(tree):
                if isinstance(node, ast.Call):
                    if isinstance(node.func, ast.Name):
                        api_calls.append(node.func.id)
                    elif isinstance(node.func, ast.Attribute):
                        attr_name = f"{node.func.value.id}.{node.func.attr}" if isinstance(node.func.value, ast.Name) else node.func.attr
                        api_calls.append(attr_name)

        except (SyntaxError, ValueError):
            matches = re.finditer(r'\b(\w+)\s*\(', content)
            for match in matches:
                api_calls.append(match.group(1))

        return api_calls

    except Exception as e:
        print(f"Error processing file {file_path}: {e}")
        return []

def build_graph(api_calls):
    """根据API调用构建有向图 - 遵循原始MulGuard逻辑"""
    G = nx.DiGraph()
    for i, caller in enumerate(api_calls):
        G.add_node(caller)
        for callee in api_calls[i+1:]:
            if callee in G:
                G.add_edge(caller, callee)
    return G

def calculate_centrality(G):
    """计算图的中心性指标 - 遵循原始MulGuard逻辑"""
    centralities = {}
    if G.number_of_nodes() > 0:
        try:
            centralities = {
                'degree': {k: v + 1 for k, v in nx.degree_centrality(G).items()},
                'closeness': {k: v + 1 for k, v in nx.closeness_centrality(G).items()},
                'harmonic': {k: v + 1 for k, v in nx.harmonic_centrality(G).items()},
                'katz': {k: v + 1 for k, v in nx.katz_centrality_numpy(G, alpha=0.01).items()}
            }
        except TypeError as e:
            centralities = {
                'degree': {k: v + 1 for k, v in nx.degree_centrality(G).items()},
                'closeness': {k: v + 1 for k, v in nx.closeness_centrality(G).items()},
                'harmonic': {k: v + 1 for k, v in nx.harmonic_centrality(G).items()},
            }
    return centralities

def extract_features_from_centrality(centrality_data, feature_set_path):
    """根据中心性数据和特征集生成特征向量"""
    
    # 加载特征集
    try:
        with open(feature_set_path, 'r', encoding='utf-8') as f:
            feature_set = json.load(f)
        
        # 创建API特征映射
        api_feature_map = {api["api_name"]: 0 for api in feature_set["apis"]}
        
    except FileNotFoundError:
        print(f"特征集文件未找到: {feature_set_path}")
        print("将使用中心性数据中的所有API作为特征")
        api_feature_map = {api: 0 for api in centrality_data.keys()}
    
    # 生成特征向量
    feature_vector = {api: 0 for api in api_feature_map}
    
    for api_name, feature_value in centrality_data.items():
        if api_name in api_feature_map:
            feature_vector[api_name] = feature_value
    
    return feature_vector

def process_file(file_path):
    """处理单个文件 - 遵循原始MulGuard逻辑"""
    api_calls = extract_api_calls(file_path)
    if api_calls:
        G = build_graph(api_calls)
        return calculate_centrality(G)
    return None

def process_package(package_path, output_dir=None):
    """处理单个包并生成特征 - 遵循原始MulGuard逻辑"""
    from collections import defaultdict
    
    package_path = Path(package_path)
    if not package_path.exists():
        print(f"包路径不存在: {package_path}")
        return None
    
    if output_dir is None:
        output_dir = package_path
    else:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"开始分析包: {package_path}")
    
    # 收集所有Python文件
    python_files = []
    for file_path in package_path.rglob("*.py"):
        python_files.append(file_path)
    
    print(f"找到 {len(python_files)} 个Python文件")
    
    if not python_files:
        print("未找到任何Python文件")
        return None
    
    # 聚合所有文件的中心性结果
    aggregated_centralities = defaultdict(lambda: defaultdict(float))
    total_api_calls = 0
    
    # 处理每个文件
    for file_path in python_files:
        file_centralities = process_file(file_path)
        if file_centralities:
            for measure, values in file_centralities.items():
                for api, value in values.items():
                    aggregated_centralities[measure][api] += value
            total_api_calls += 1
    
    if total_api_calls == 0:
        print("未找到任何API调用")
        return None
    
    print(f"提取到来自 {total_api_calls} 个文件的API调用")
    
    # 计算平均值并保存结果
    results = {}
    for measure, api_values in aggregated_centralities.items():
        # 计算平均中心性
        averaged_values = {api: total / total_api_calls for api, total in api_values.items()}
        sorted_values = dict(sorted(averaged_values.items(), key=lambda item: item[1], reverse=True))
        
        # 保存到JSON文件
        output_file = output_dir / f"{measure}_new.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(sorted_values, f, ensure_ascii=False, indent=4)
        
        results[measure] = sorted_values
        print(f"已保存 {measure} 中心性到: {output_file}")
    
    return results

def generate_feature_vectors(package_path, centralities, feature_set_path=None):
    """生成特征向量"""
    
    package_path = Path(package_path)
    
    if feature_set_path is None:
        # 查找closeness_sensitive_api.json文件
        script_dir = Path(__file__).parent
        feature_set_path = script_dir / "closeness_sensitive_api.json"
    
    results = {}
    
    for measure, centrality_data in centralities.items():
        feature_vector = extract_features_from_centrality(centrality_data, feature_set_path)
        
        # 保存特征向量
        output_file = package_path / f"{measure}_feature_vector.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(feature_vector, f, indent=4)
        
        results[measure] = feature_vector
        print(f"已生成 {measure} 特征向量: {output_file}")
    
    return results

def create_prediction_ready_format(feature_vectors, output_path, measure='closeness'):
    """创建可用于模型预测的格式"""
    
    if measure not in feature_vectors:
        print(f"未找到 {measure} 特征向量")
        return
    
    feature_vector = feature_vectors[measure]
    
    # 转换为列表格式（仅数值）
    feature_list = list(feature_vector.values())
    
    # 保存为文本文件（每行一个样本）
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(' '.join(map(str, feature_list)) + '\n')
    
    print(f"已生成模型预测格式文件: {output_path}")
    print(f"特征维度: {len(feature_list)}")
    
    return feature_list

def main():
    parser = argparse.ArgumentParser(description='分析单个Python包并生成特征向量')
    parser.add_argument('package_path', help='包的路径')
    parser.add_argument('--output-dir', '-o', help='输出目录（默认为包路径）')
    parser.add_argument('--feature-set', '-f', help='特征集JSON文件路径')
    parser.add_argument('--measure', '-m', default='closeness', 
                       choices=['degree', 'closeness', 'harmonic', 'katz'],
                       help='用于最终预测的中心性度量（默认: closeness）')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("Python包恶意检测特征提取工具")
    print("=" * 60)
    
    # 步骤1: 处理包并计算中心性
    centralities = process_package(args.package_path, args.output_dir)
    if centralities is None:
        print("包处理失败")
        return
    
    print("\n" + "=" * 60)
    print("生成特征向量...")
    print("=" * 60)
    
    # 步骤2: 生成特征向量
    feature_vectors = generate_feature_vectors(
        args.package_path, 
        centralities, 
        args.feature_set
    )
    
    # 步骤3: 创建模型预测格式
    output_path = Path(args.output_dir or args.package_path)
    prediction_file = output_path / f"{args.measure}_prediction_input.txt"
    
    feature_list = create_prediction_ready_format(
        feature_vectors, 
        prediction_file, 
        args.measure
    )
    
    print("\n" + "=" * 60)
    print("分析完成!")
    print("=" * 60)
    print(f"包路径: {args.package_path}")
    print(f"输出目录: {output_path}")
    print(f"主要度量: {args.measure}")
    print(f"特征维度: {len(feature_list) if feature_list else 0}")
    print(f"预测输入文件: {prediction_file}")

if __name__ == "__main__":
    if len(sys.argv) == 1:
        # 如果没有命令行参数，显示帮助信息
        print("使用方法:")
        print("  python analyze_single_package.py <包路径>")
        print("\n示例:")
        print("  python analyze_single_package.py /path/to/10Cent10-999.0.4/")
        print("\n使用 --help 查看更多选项")
    else:
        main()
