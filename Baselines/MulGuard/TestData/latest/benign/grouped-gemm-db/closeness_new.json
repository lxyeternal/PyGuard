{
    "backend.gmm": 0.5833333333333333,
    "torch.tensor": 0.43525641025641026,
    "out.append": 0.4180232558139535,
    "randn": 0.41055555555555556,
    "to": 0.3974489795918367,
    "torch.rand": 0.38136363636363635,
    "torch.manual_seed": 0.37901785714285713,
    "view": 0.37901785714285713,
    "a.requires_grad_": 0.3767543859649123,
    "b.requires_grad_": 0.3767543859649123,
    "requires_grad_": 0.3767543859649123,
    "ops.gmm": 0.3767543859649123,
    "gmm": 0.3767543859649123,
    "self.assertTrue": 0.3767543859649123,
    "backward": 0.3767543859649123,
    "get": 0.3653846153846154,
    "mask.numel": 0.359469696969697,
    "clone": 0.3547101449275362,
    "out.sum": 0.3547101449275362,
    "expected_out.sum": 0.3547101449275362,
    "batch_sizes.cuda": 0.3450657894736842,
    "allclose": 0.3450657894736842,
    "nvcc_flags.extend": 0.3129370629370629,
    "a.detach": 0.25625,
    "b.detach": 0.25625,
    "Path": 0.25,
    "set": 0.25,
    "setup": 0.25,
    "get_device_capability": 0.25,
    "dirname": 0.25,
    "CUDAExtension": 0.25,
    "ModuleNotFoundError": 0.25,
    "abspath": 0.25,
    "read": 0.25,
    "find_packages": 0.25,
    "extra_deps.values": 0.25,
    "open": 0.25,
    "GroupedGemm.apply": 0.25,
    "ctx.save_for_backward": 0.25,
    "grad.contiguous": 0.25,
    "torch.empty": 0.25,
    "_allocate_output": 0.25,
    "add_flags": 0.25,
    "parameterized.parameters": 0.25,
    "torch.isclose": 0.25,
    "numpy": 0.25,
    "enumerate": 0.25,
    "torch.cat": 0.25,
    "unittest.main": 0.25,
    "print": 0.25,
    "dist.sum": 0.25,
    "format": 0.25,
    "out.cuda": 0.25,
    "batch_sizes.cpu": 0.25,
    "t": 0.25,
    "batch_sizes.sum": 0.25,
    "all": 0.25,
    "mask.sum": 0.25,
    "torch.logical_not": 0.25,
    "cuda": 0.25,
    "torch.ones": 0.25
}