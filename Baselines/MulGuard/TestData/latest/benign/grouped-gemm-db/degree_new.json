{
    "backend.gmm": 0.75,
    "randn": 0.51875,
    "torch.tensor": 0.48750000000000004,
    "to": 0.4625,
    "out.append": 0.4375,
    "torch.rand": 0.4375,
    "get": 0.4230769230769231,
    "a.requires_grad_": 0.41875,
    "b.requires_grad_": 0.41875,
    "requires_grad_": 0.41875,
    "ops.gmm": 0.41875,
    "gmm": 0.41875,
    "self.assertTrue": 0.41875,
    "backward": 0.41875,
    "torch.manual_seed": 0.4125,
    "view": 0.4125,
    "mask.numel": 0.35,
    "grad.contiguous": 0.3333333333333333,
    "dist.sum": 0.33125,
    "nvcc_flags.extend": 0.3269230769230769,
    "batch_sizes.cuda": 0.3125,
    "allclose": 0.3125,
    "clone": 0.3125,
    "out.sum": 0.3125,
    "expected_out.sum": 0.3125,
    "batch_sizes.sum": 0.3,
    "all": 0.2875,
    "format": 0.28125,
    "torch.logical_not": 0.28125,
    "out.cuda": 0.275,
    "batch_sizes.cpu": 0.275,
    "t": 0.275,
    "a.detach": 0.275,
    "b.detach": 0.275,
    "Path": 0.2692307692307692,
    "set": 0.2692307692307692,
    "setup": 0.2692307692307692,
    "get_device_capability": 0.2692307692307692,
    "dirname": 0.2692307692307692,
    "print": 0.26875,
    "mask.sum": 0.26875,
    "cuda": 0.26875,
    "parameterized.parameters": 0.2625,
    "torch.ones": 0.2625,
    "numpy": 0.25625,
    "enumerate": 0.25625,
    "torch.cat": 0.25625,
    "unittest.main": 0.25625,
    "CUDAExtension": 0.25,
    "ModuleNotFoundError": 0.25,
    "abspath": 0.25,
    "read": 0.25,
    "find_packages": 0.25,
    "extra_deps.values": 0.25,
    "open": 0.25,
    "GroupedGemm.apply": 0.25,
    "ctx.save_for_backward": 0.25,
    "torch.empty": 0.25,
    "_allocate_output": 0.25,
    "add_flags": 0.25,
    "torch.isclose": 0.25
}