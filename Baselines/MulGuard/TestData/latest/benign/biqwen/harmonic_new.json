{
    "to": 78.33333333333333,
    "isinstance": 74.0,
    "getattr": 73.5,
    "expand": 72.77777777777777,
    "float": 72.33333333333333,
    "torch.cat": 64.44444444444444,
    "transpose": 59.0,
    "torch.arange": 58.77777777777778,
    "contiguous": 57.333333333333336,
    "logger.warning_once": 57.0,
    "ValueError": 56.77777777777778,
    "hidden_states.to": 56.333333333333336,
    "__init__": 55.0,
    "nn.Linear": 54.5,
    "normal_": 53.11111111111111,
    "get": 52.77777777777777,
    "super": 50.77777777777778,
    "torch.finfo": 50.555555555555564,
    "cache_position.reshape": 50.388888888888886,
    "past_key_values.get_seq_length": 50.22222222222223,
    "zero_": 49.27777777777778,
    "range": 47.388888888888886,
    "self.post_init": 46.11111111111111,
    "config.get_text_config": 45.22222222222222,
    "self.loss_function": 39.666666666666664,
    "torch.matmul": 36.333333333333336,
    "self.model": 35.22222222222222,
    "self.score": 34.888888888888886,
    "view": 25.555555555555557,
    "Qwen3RMSNorm": 23.61111111111111,
    "Qwen2RMSNorm": 22.222222222222225,
    "Qwen3Model": 18.027777777777775,
    "Qwen2Model": 17.86111111111111,
    "add_start_docstrings_to_model_forward": 2.0,
    "is_torch_flex_attn_available": 0.6666666666666666,
    "logging.get_logger": 0.6666666666666666,
    "use_kernel_forward_from_hub": 0.6666666666666666,
    "add_start_docstrings": 0.6666666666666666,
    "cos.unsqueeze": 0.6666666666666666,
    "sin.unsqueeze": 0.6666666666666666,
    "hidden_states.reshape": 0.6666666666666666,
    "repeat_kv": 0.6666666666666666,
    "dropout": 0.6666666666666666,
    "torch.no_grad": 0.6666666666666666,
    "replace_return_docstrings": 0.6666666666666666,
    "add_code_sample_docstrings": 0.6666666666666666,
    "self.down_proj": 0.6666666666666666,
    "apply_rotary_pos_emb": 0.6666666666666666,
    "attention_interface": 0.6666666666666666,
    "self.o_proj": 0.6666666666666666,
    "nn.Parameter": 0.6666666666666666,
    "mean": 0.6666666666666666,
    "self.input_layernorm": 0.6666666666666666,
    "self.self_attn": 0.6666666666666666,
    "self.post_attention_layernorm": 0.6666666666666666,
    "self.mlp": 0.6666666666666666,
    "self.rope_init_fn": 0.6666666666666666,
    "self.register_buffer": 0.6666666666666666,
    "nn.Embedding": 0.6666666666666666,
    "nn.ModuleList": 0.6666666666666666,
    "self._update_causal_mask": 0.6666666666666666,
    "self.rotary_emb": 0.6666666666666666,
    "zip": 0.6666666666666666,
    "self.norm": 0.6666666666666666,
    "BaseModelOutputWithPast": 0.6666666666666666,
    "self._prepare_4d_causal_attention_mask_with_cache_position": 0.6666666666666666,
    "self.lm_head": 0.6666666666666666,
    "CausalLMOutputWithPast": 0.6666666666666666,
    "SequenceClassifierOutputWithPast": 0.6666666666666666,
    "nn.Dropout": 0.6666666666666666,
    "self.dropout": 0.6666666666666666,
    "TokenClassifierOutput": 0.6666666666666666,
    "self.transformer": 0.6666666666666666,
    "self.qa_outputs": 0.6666666666666666,
    "logits.split": 0.6666666666666666,
    "QuestionAnsweringModelOutput": 0.6666666666666666,
    "rotate_half": 0.6666666666666666,
    "key_states.transpose": 0.6666666666666666,
    "softmax": 0.6666666666666666,
    "attn_output.transpose": 0.6666666666666666,
    "past_key_value.update": 0.6666666666666666,
    "torch.ones": 0.6666666666666666,
    "torch.rsqrt": 0.6666666666666666,
    "hasattr": 0.6666666666666666,
    "torch.autocast": 0.6666666666666666,
    "cos.to": 0.6666666666666666,
    "sin.to": 0.6666666666666666,
    "self.embed_tokens": 0.6666666666666666,
    "DynamicCache": 0.6666666666666666,
    "cache_position.unsqueeze": 0.6666666666666666,
    "torch.zeros_like": 0.6666666666666666,
    "decoder_layer": 0.6666666666666666,
    "AttentionMaskConverter._ignore_causal_mask_sdpa": 0.6666666666666666,
    "past_key_values.get_max_cache_shape": 0.6666666666666666,
    "AttentionMaskConverter._unmask_unattended": 0.6666666666666666,
    "torch.full": 0.6666666666666666,
    "slice": 0.6666666666666666,
    "self.act_fn": 0.6666666666666666,
    "self.up_proj": 0.6666666666666666,
    "kwargs.get": 0.6666666666666666,
    "attn_output.reshape": 0.6666666666666666,
    "hidden_states.pow": 0.6666666666666666,
    "tuple": 0.6666666666666666,
    "emb.cos": 0.6666666666666666,
    "emb.sin": 0.6666666666666666,
    "make_flex_block_causal_mask": 0.6666666666666666,
    "attention_mask.dim": 0.6666666666666666,
    "causal_mask.clone": 0.6666666666666666,
    "masked_fill": 0.6666666666666666,
    "argmax": 0.6666666666666666,
    "start_logits.squeeze": 0.6666666666666666,
    "end_logits.squeeze": 0.6666666666666666,
    "self.gate_proj": 0.6666666666666666,
    "fill_": 0.6666666666666666,
    "type": 0.6666666666666666,
    "item": 0.6666666666666666,
    "diagonal_attend_mask.bitwise_or_": 0.6666666666666666,
    "self.q_proj": 0.6666666666666666,
    "self.k_proj": 0.6666666666666666,
    "self.v_proj": 0.6666666666666666,
    "inv_freq_expanded.float": 0.6666666666666666,
    "position_ids_expanded.float": 0.6666666666666666,
    "input_tensor.size": 0.6666666666666666,
    "sum": 0.6666666666666666,
    "Qwen2Attention": 0.3333333333333333,
    "Qwen2MLP": 0.3333333333333333,
    "Qwen2RotaryEmbedding": 0.3333333333333333,
    "Qwen2DecoderLayer": 0.3333333333333333,
    "int": 0.3333333333333333,
    "logging.basicConfig": 0.3333333333333333,
    "logging.getLogger": 0.3333333333333333,
    "os.getenv": 0.3333333333333333,
    "Qwen3Attention": 0.3333333333333333,
    "Qwen3MLP": 0.3333333333333333,
    "Qwen3RotaryEmbedding": 0.3333333333333333,
    "self.q_norm": 0.3333333333333333,
    "self.k_norm": 0.3333333333333333,
    "Qwen3DecoderLayer": 0.3333333333333333
}