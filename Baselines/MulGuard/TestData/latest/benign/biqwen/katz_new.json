{
    "to": 0.7831240850298832,
    "isinstance": 0.7763972085473129,
    "getattr": 0.7755291542495609,
    "expand": 0.7745141142000289,
    "float": 0.7738393649973618,
    "torch.cat": 0.7602223460538764,
    "torch.arange": 0.7510985799218579,
    "transpose": 0.7508992147755543,
    "contiguous": 0.7482181142569507,
    "logger.warning_once": 0.7481246786488857,
    "ValueError": 0.7473699337939053,
    "hidden_states.to": 0.7461852744293168,
    "__init__": 0.7446216312048982,
    "nn.Linear": 0.7437130613901566,
    "normal_": 0.7415102028531845,
    "get": 0.7409986341431941,
    "super": 0.7378419176360606,
    "cache_position.reshape": 0.7371230095017774,
    "torch.finfo": 0.7369829463995168,
    "past_key_values.get_seq_length": 0.7364713776895265,
    "zero_": 0.7355388839782364,
    "range": 0.7319455170462921,
    "self.post_init": 0.7289504526713189,
    "view": 0.727609749820956,
    "config.get_text_config": 0.7274059695208,
    "torch.matmul": 0.7242467823377328,
    "self.model": 0.7237605975725906,
    "self.score": 0.7232490288626003,
    "self.loss_function": 0.7202550793395183,
    "add_start_docstrings_to_model_forward": 0.71937374587778,
    "add_start_docstrings": 0.7183402737363855,
    "repeat_kv": 0.7183402737363855,
    "rotate_half": 0.7183402737363855,
    "is_torch_flex_attn_available": 0.7178235376656884,
    "logging.get_logger": 0.7178235376656884,
    "use_kernel_forward_from_hub": 0.7178235376656884,
    "cos.unsqueeze": 0.7178235376656884,
    "sin.unsqueeze": 0.7178235376656884,
    "hidden_states.reshape": 0.7178235376656884,
    "dropout": 0.7178235376656884,
    "torch.no_grad": 0.7178235376656884,
    "replace_return_docstrings": 0.7178235376656884,
    "add_code_sample_docstrings": 0.7178235376656884,
    "self.down_proj": 0.7178235376656884,
    "apply_rotary_pos_emb": 0.7178235376656884,
    "attention_interface": 0.7178235376656884,
    "self.o_proj": 0.7178235376656884,
    "nn.Parameter": 0.7178235376656884,
    "mean": 0.7178235376656884,
    "self.input_layernorm": 0.7178235376656884,
    "self.self_attn": 0.7178235376656884,
    "self.post_attention_layernorm": 0.7178235376656884,
    "self.mlp": 0.7178235376656884,
    "self.rope_init_fn": 0.7178235376656884,
    "self.register_buffer": 0.7178235376656884,
    "nn.Embedding": 0.7178235376656884,
    "nn.ModuleList": 0.7178235376656884,
    "self._update_causal_mask": 0.7178235376656884,
    "self.rotary_emb": 0.7178235376656884,
    "zip": 0.7178235376656884,
    "self.norm": 0.7178235376656884,
    "BaseModelOutputWithPast": 0.7178235376656884,
    "self._prepare_4d_causal_attention_mask_with_cache_position": 0.7178235376656884,
    "self.lm_head": 0.7178235376656884,
    "CausalLMOutputWithPast": 0.7178235376656884,
    "SequenceClassifierOutputWithPast": 0.7178235376656884,
    "nn.Dropout": 0.7178235376656884,
    "self.dropout": 0.7178235376656884,
    "TokenClassifierOutput": 0.7178235376656884,
    "self.transformer": 0.7178235376656884,
    "self.qa_outputs": 0.7178235376656884,
    "logits.split": 0.7178235376656884,
    "QuestionAnsweringModelOutput": 0.7178235376656884,
    "key_states.transpose": 0.7178235376656884,
    "softmax": 0.7178235376656884,
    "attn_output.transpose": 0.7178235376656884,
    "past_key_value.update": 0.7178235376656884,
    "torch.ones": 0.7178235376656884,
    "torch.rsqrt": 0.7178235376656884,
    "hasattr": 0.7178235376656884,
    "torch.autocast": 0.7178235376656884,
    "cos.to": 0.7178235376656884,
    "sin.to": 0.7178235376656884,
    "self.embed_tokens": 0.7178235376656884,
    "DynamicCache": 0.7178235376656884,
    "cache_position.unsqueeze": 0.7178235376656884,
    "torch.zeros_like": 0.7178235376656884,
    "decoder_layer": 0.7178235376656884,
    "AttentionMaskConverter._ignore_causal_mask_sdpa": 0.7178235376656884,
    "past_key_values.get_max_cache_shape": 0.7178235376656884,
    "AttentionMaskConverter._unmask_unattended": 0.7178235376656884,
    "torch.full": 0.7178235376656884,
    "slice": 0.7178235376656884,
    "self.act_fn": 0.7178235376656884,
    "self.up_proj": 0.7178235376656884,
    "kwargs.get": 0.7178235376656884,
    "attn_output.reshape": 0.7178235376656884,
    "hidden_states.pow": 0.7178235376656884,
    "tuple": 0.7178235376656884,
    "emb.cos": 0.7178235376656884,
    "emb.sin": 0.7178235376656884,
    "make_flex_block_causal_mask": 0.7178235376656884,
    "attention_mask.dim": 0.7178235376656884,
    "causal_mask.clone": 0.7178235376656884,
    "masked_fill": 0.7178235376656884,
    "argmax": 0.7178235376656884,
    "start_logits.squeeze": 0.7178235376656884,
    "end_logits.squeeze": 0.7178235376656884,
    "self.gate_proj": 0.7178235376656884,
    "fill_": 0.7178235376656884,
    "type": 0.7178235376656884,
    "item": 0.7178235376656884,
    "diagonal_attend_mask.bitwise_or_": 0.7178235376656884,
    "self.q_proj": 0.7178235376656884,
    "self.k_proj": 0.7178235376656884,
    "self.v_proj": 0.7178235376656884,
    "inv_freq_expanded.float": 0.7178235376656884,
    "position_ids_expanded.float": 0.7178235376656884,
    "input_tensor.size": 0.7178235376656884,
    "sum": 0.7178235376656884,
    "int": 0.5,
    "logging.basicConfig": 0.5,
    "logging.getLogger": 0.5,
    "os.getenv": 0.5,
    "Qwen3RMSNorm": 0.36524338076302393,
    "Qwen2RMSNorm": 0.3634990088685421,
    "Qwen2Model": 0.3625807864233391,
    "Qwen3Model": 0.3622029485692318,
    "Qwen2Attention": 0.35907433264002536,
    "Qwen2MLP": 0.35907433264002536,
    "Qwen2RotaryEmbedding": 0.35907433264002536,
    "Qwen2DecoderLayer": 0.35907433264002536,
    "Qwen3Attention": 0.35874920502566304,
    "Qwen3MLP": 0.35874920502566304,
    "Qwen3RotaryEmbedding": 0.35874920502566304,
    "self.q_norm": 0.35874920502566304,
    "self.k_norm": 0.35874920502566304,
    "Qwen3DecoderLayer": 0.35874920502566304
}