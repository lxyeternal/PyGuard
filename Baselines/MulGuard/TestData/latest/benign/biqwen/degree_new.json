{
    "to": 1.3677060367454068,
    "isinstance": 1.3254173228346458,
    "getattr": 1.2988766404199474,
    "expand": 1.2565879265091864,
    "float": 1.2434225721784777,
    "torch.cat": 1.08997375328084,
    "transpose": 1.0370603674540682,
    "torch.arange": 1.0343937007874016,
    "logger.warning_once": 1.0184776902887138,
    "contiguous": 1.0026456692913386,
    "ValueError": 0.9946876640419947,
    "__init__": 0.9894383202099738,
    "hidden_states.to": 0.9788136482939632,
    "nn.Linear": 0.9762729658792652,
    "normal_": 0.9496902887139108,
    "get": 0.9443989501312336,
    "super": 0.9126509186351704,
    "torch.finfo": 0.9073595800524933,
    "past_key_values.get_seq_length": 0.9020682414698163,
    "zero_": 0.8888608923884515,
    "cache_position.reshape": 0.8888608923884515,
    "self.post_init": 0.8465721784776902,
    "range": 0.846530183727034,
    "view": 0.8247769028871392,
    "self.model": 0.7989501312335957,
    "self.score": 0.7936587926509185,
    "config.get_text_config": 0.7724934383202099,
    "torch.matmul": 0.7618267716535433,
    "self.loss_function": 0.7513280839895012,
    "SequenceClassifierOutputWithPast": 0.7460367454068241,
    "nn.Dropout": 0.7460367454068241,
    "self.lm_head": 0.7407454068241469,
    "CausalLMOutputWithPast": 0.7407454068241469,
    "self.dropout": 0.7407454068241469,
    "kwargs.get": 0.7380787401574803,
    "attn_output.reshape": 0.7380787401574803,
    "TokenClassifierOutput": 0.7354540682414697,
    "self.act_fn": 0.7354540682414697,
    "self.up_proj": 0.7354540682414697,
    "hidden_states.pow": 0.7354540682414697,
    "tuple": 0.7354540682414697,
    "nn.Embedding": 0.7301627296587926,
    "nn.ModuleList": 0.7301627296587926,
    "self._update_causal_mask": 0.7301627296587926,
    "self.rotary_emb": 0.7301627296587926,
    "zip": 0.7301627296587926,
    "self.norm": 0.7301627296587926,
    "BaseModelOutputWithPast": 0.7301627296587926,
    "self._prepare_4d_causal_attention_mask_with_cache_position": 0.7301627296587926,
    "past_key_values.get_max_cache_shape": 0.7301627296587926,
    "AttentionMaskConverter._unmask_unattended": 0.7301627296587926,
    "torch.full": 0.7301627296587926,
    "slice": 0.7301627296587926,
    "emb.cos": 0.727496062992126,
    "emb.sin": 0.727496062992126,
    "make_flex_block_causal_mask": 0.727496062992126,
    "attention_mask.dim": 0.727496062992126,
    "causal_mask.clone": 0.727496062992126,
    "masked_fill": 0.727496062992126,
    "argmax": 0.727496062992126,
    "self.rope_init_fn": 0.7248713910761154,
    "self.register_buffer": 0.7248713910761154,
    "AttentionMaskConverter._ignore_causal_mask_sdpa": 0.7248713910761154,
    "self.input_layernorm": 0.7195800524934383,
    "self.self_attn": 0.7195800524934383,
    "self.post_attention_layernorm": 0.7195800524934383,
    "self.mlp": 0.7195800524934383,
    "torch.autocast": 0.7195800524934383,
    "cache_position.unsqueeze": 0.7195800524934383,
    "torch.zeros_like": 0.7195800524934383,
    "decoder_layer": 0.7195800524934383,
    "rotate_half": 0.7169553805774278,
    "add_start_docstrings_to_model_forward": 0.7142887139107611,
    "self.transformer": 0.7142887139107611,
    "self.qa_outputs": 0.7142887139107611,
    "logits.split": 0.7142887139107611,
    "hasattr": 0.7142887139107611,
    "self.embed_tokens": 0.7142887139107611,
    "DynamicCache": 0.7142887139107611,
    "apply_rotary_pos_emb": 0.714246719160105,
    "attention_interface": 0.714246719160105,
    "self.o_proj": 0.714246719160105,
    "start_logits.squeeze": 0.714246719160105,
    "end_logits.squeeze": 0.714246719160105,
    "self.gate_proj": 0.714246719160105,
    "mean": 0.7090393700787402,
    "QuestionAnsweringModelOutput": 0.7089973753280839,
    "torch.ones": 0.7089973753280839,
    "torch.rsqrt": 0.7089973753280839,
    "cos.to": 0.7089973753280839,
    "sin.to": 0.7089973753280839,
    "key_states.transpose": 0.7063727034120735,
    "softmax": 0.7063727034120735,
    "attn_output.transpose": 0.7063727034120735,
    "past_key_value.update": 0.7063727034120735,
    "self.down_proj": 0.7063307086614173,
    "nn.Parameter": 0.703748031496063,
    "fill_": 0.7037060367454068,
    "replace_return_docstrings": 0.6984146981627296,
    "add_code_sample_docstrings": 0.6984146981627296,
    "type": 0.6984146981627296,
    "item": 0.6984146981627296,
    "diagonal_attend_mask.bitwise_or_": 0.6984146981627296,
    "torch.no_grad": 0.6931233595800524,
    "self.v_proj": 0.6931233595800524,
    "repeat_kv": 0.6878320209973753,
    "inv_freq_expanded.float": 0.6878320209973753,
    "position_ids_expanded.float": 0.6878320209973753,
    "input_tensor.size": 0.6878320209973753,
    "self.q_proj": 0.685249343832021,
    "self.k_proj": 0.685249343832021,
    "dropout": 0.6825406824146981,
    "add_start_docstrings": 0.677249343832021,
    "hidden_states.reshape": 0.677249343832021,
    "sum": 0.677249343832021,
    "cos.unsqueeze": 0.6719580052493438,
    "sin.unsqueeze": 0.6719580052493438,
    "is_torch_flex_attn_available": 0.6666666666666666,
    "logging.get_logger": 0.6666666666666666,
    "use_kernel_forward_from_hub": 0.6666666666666666,
    "Qwen3RMSNorm": 0.41732283464566927,
    "Qwen2Model": 0.4053333333333333,
    "Qwen3Model": 0.4041994750656168,
    "Qwen2RMSNorm": 0.39999999999999997,
    "self.q_norm": 0.3674540682414698,
    "self.k_norm": 0.3674540682414698,
    "Qwen3DecoderLayer": 0.36482939632545935,
    "Qwen2RotaryEmbedding": 0.3626666666666667,
    "Qwen2DecoderLayer": 0.3626666666666667,
    "Qwen3RotaryEmbedding": 0.36220472440944884,
    "Qwen3Attention": 0.35958005249343833,
    "Qwen3MLP": 0.35958005249343833,
    "Qwen2Attention": 0.35733333333333334,
    "Qwen2MLP": 0.35733333333333334,
    "int": 0.3333333333333333,
    "logging.basicConfig": 0.3333333333333333,
    "logging.getLogger": 0.3333333333333333,
    "os.getenv": 0.3333333333333333
}