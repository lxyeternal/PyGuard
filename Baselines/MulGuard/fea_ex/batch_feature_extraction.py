#!/usr/bin/env python3
"""
Batch Feature Vector Extraction Script

This script processes all packages in the TestData directory structure and generates
feature vectors for each package based on the centrality metrics.

Directory structure:
/home2/wenbo/Documents/PyPIAgent/Tools/MulGuard/TestData/
â”œâ”€â”€ latest/
â”‚   â”œâ”€â”€ benign/
â”‚   â””â”€â”€ malware/
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ benign/
â”‚   â””â”€â”€ malware/
â””â”€â”€ obfuscation/
    â”œâ”€â”€ benign/
    â””â”€â”€ malware/

For each package, it generates 4 feature vector files:
- closeness_feature_vector.json
- degree_feature_vector.json  
- harmonic_feature_vector.json
- katz_feature_vector.json

Skip condition: If all 8 JSON files exist (4 *_new.json + 4 *_feature_vector.json), skip processing.
"""

import os
import json
import time
import signal
from pathlib import Path
from multiprocessing import Pool, cpu_count
from tqdm import tqdm


class TimeoutError(Exception):
    """Custom timeout exception"""
    pass


def timeout_handler(signum, frame):
    """Signal handler for timeout"""
    raise TimeoutError("Feature extraction timed out")


def extract_features_for_package(fea_set_path, package_path, metric_name):
    """Extract features for a single package and metric"""
    try:
        # Load feature set
        with open(fea_set_path, 'r', encoding='utf-8') as f:
            feature_set = json.load(f)
        
        # Create API feature map
        api_feature_map = {api["api_name"]: 0 for api in feature_set["apis"]}
        
        # Load API extraction data
        api_ex_file = package_path / f"{metric_name}_new.json"
        if not api_ex_file.exists():
            return False, f"Missing input file: {api_ex_file}"
            
        with open(api_ex_file, 'r', encoding='utf-8') as f:
            api_ex_data = json.load(f)
        
        # Create feature vector
        feature_vector = {api: 0 for api in api_feature_map}
        
        for api_name, feature_value in api_ex_data.items():
            if api_name in api_feature_map:
                feature_vector[api_name] = feature_value
        
        # Save feature vector
        output_file = package_path / f"{metric_name}_feature_vector.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(feature_vector, f, indent=4)
            
        return True, "Success"
        
    except Exception as e:
        return False, f"Error extracting {metric_name} features: {str(e)}"


def process_single_package_wrapper(args):
    """Wrapper function for multiprocessing with timeout"""
    package_path, api_call_graph_dir, package_name = args
    
    try:
        # Check if all 8 JSON files exist (skip condition)
        required_files = [
            "degree_new.json", "closeness_new.json", "harmonic_new.json", "katz_new.json",
            "degree_feature_vector.json", "closeness_feature_vector.json", 
            "harmonic_feature_vector.json", "katz_feature_vector.json"
        ]
        
        all_files_exist = True
        for file_name in required_files:
            file_path = package_path / file_name
            if not file_path.exists() or file_path.stat().st_size == 0:
                all_files_exist = False
                break
        
        if all_files_exist:
            return f"SKIPPED: {package_name} (all 8 JSON files exist)"
        
        # Set up timeout (3 minutes = 180 seconds)
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(180)
        
        try:
            # Extract features for all four centrality metrics
            metrics = ["closeness", "degree", "harmonic", "katz"]
            success_count = 0
            error_details = []
            
            for metric in metrics:
                fea_set_path = api_call_graph_dir / f"{metric}_sensitive_api.json"
                if not fea_set_path.exists():
                    error_details.append(f"Missing feature set file: {fea_set_path}")
                    continue
                    
                success, message = extract_features_for_package(fea_set_path, package_path, metric)
                if success:
                    success_count += 1
                else:
                    error_details.append(f"{metric}: {message}")
            
            # Cancel the alarm if processing completed successfully
            signal.alarm(0)
            
            if success_count == 4:
                return f"SUCCESS: {package_name} (4/4 feature vectors generated)"
            elif success_count > 0:
                error_msg = "; ".join(error_details)
                return f"PARTIAL: {package_name} ({success_count}/4 feature vectors generated) - Errors: {error_msg}"
            else:
                error_msg = "; ".join(error_details)
                return f"FAILED: {package_name} - Path: {package_path} - Errors: {error_msg}"
                
        except TimeoutError:
            signal.alarm(0)
            return f"TIMEOUT: {package_name} (feature extraction exceeded 3 minutes)"
            
    except Exception as e:
        signal.alarm(0)
        return f"ERROR: {package_name} - {str(e)}"


def get_testdata_mapping():
    """Get mapping between TestData directories"""
    base_testdata = Path("/home2/wenbo/Documents/PyPIAgent/Tools/MulGuard/TestData")
    
    mappings = []
    
    # Define dataset types and package types
    dataset_types = ["latest", "evaluation", "obfuscation"]
    package_types = ["benign", "malware"]
    
    for dataset_type in dataset_types:
        for package_type in package_types:
            source_dir = base_testdata / dataset_type / package_type
            if source_dir.exists():
                mappings.append((source_dir, f"{dataset_type}_{package_type}"))
    
    return mappings


def process_single_mapping(source_dir, mapping_name, num_processes=None):
    """Process all packages in a single source directory using multiprocessing"""
    if not source_dir.exists():
        print(f"Source directory does not exist: {source_dir}")
        return 0, 0, 0, 0
    
    # Get API call graph directory
    api_call_graph_dir = Path("/home2/wenbo/Documents/PyPIAgent/Tools/MulGuard/API-call-graph")
    
    if num_processes is None:
        num_processes = min(cpu_count(), 20)  # Use at most 20 processes
    
    print(f"\nProcessing {mapping_name} packages with {num_processes} processes...")
    print(f"Source directory: {source_dir}")
    
    # Get all package directories
    package_dirs = [d for d in source_dir.iterdir() if d.is_dir()]
    
    if not package_dirs:
        print(f"No packages found in {source_dir}")
        return 0, 0, 0, 0
    
    print(f"Found {len(package_dirs)} packages to process")
    
    # Prepare arguments for multiprocessing
    process_args = []
    for package_dir in package_dirs:
        package_name = package_dir.name
        process_args.append((package_dir, api_call_graph_dir, package_name))
    
    # Process packages in parallel
    start_time = time.time()
    processed_count = 0
    error_count = 0
    skipped_count = 0
    timeout_count = 0
    partial_count = 0
    
    with Pool(processes=num_processes) as pool:
        # Use imap for progress tracking
        results = list(tqdm(
            pool.imap(process_single_package_wrapper, process_args),
            total=len(process_args),
            desc=f"Processing {mapping_name} packages"
        ))
    
    # Count results and collect error details
    error_details = []
    partial_details = []
    timeout_details = []
    
    for result in results:
        if result.startswith("SUCCESS"):
            processed_count += 1
        elif result.startswith("PARTIAL"):
            partial_count += 1
            partial_details.append(result)
        elif result.startswith("FAILED"):
            error_count += 1
            error_details.append(result)
        elif result.startswith("SKIPPED"):
            skipped_count += 1
        elif result.startswith("TIMEOUT"):
            timeout_count += 1
            timeout_details.append(result)
        elif result.startswith("ERROR"):
            error_count += 1
            error_details.append(result)
    
    # Print detailed error information
    if error_details:
        print(f"\n--- ERROR DETAILS for {mapping_name} ---")
        for error in error_details:
            print(f"  {error}")
    
    if partial_details:
        print(f"\n--- PARTIAL PROCESSING DETAILS for {mapping_name} ---")
        for partial in partial_details:
            print(f"  {partial}")
            
    if timeout_details:
        print(f"\n--- TIMEOUT DETAILS for {mapping_name} ---")
        for timeout in timeout_details:
            print(f"  {timeout}")
    
    end_time = time.time()
    processing_time = end_time - start_time
    
    print(f"\n{mapping_name} packages processing completed in {processing_time:.2f} seconds")
    print(f"  Successfully processed: {processed_count}")
    print(f"  Partially processed: {partial_count}")
    print(f"  Skipped (already processed): {skipped_count}")
    print(f"  Timed out (>3 minutes): {timeout_count}")
    print(f"  Failed/Errors: {error_count}")
    
    return processed_count, error_count, skipped_count, timeout_count, partial_count


def main():
    """Main function to batch process all packages with feature extraction"""
    num_processes = 20
    
    print("=" * 80)
    print("Starting batch feature vector extraction for all packages...")
    print(f"Using {num_processes} processes")
    print("=" * 80)
    
    # Get all directory mappings
    mappings = get_testdata_mapping()
    
    total_processed = 0
    total_errors = 0
    total_skipped = 0
    total_timeouts = 0
    total_partial = 0
    overall_start_time = time.time()
    
    # Process each mapping
    for source_dir, mapping_name in mappings:
        processed, errors, skipped, timeouts, partial = process_single_mapping(
            source_dir, mapping_name, num_processes
        )
        total_processed += processed
        total_errors += errors
        total_skipped += skipped
        total_timeouts += timeouts
        total_partial += partial
    
    overall_end_time = time.time()
    total_time = overall_end_time - overall_start_time
    
    print("\n" + "=" * 80)
    print("Batch feature extraction completed!")
    print(f"Total processing time: {total_time:.2f} seconds")
    print(f"Total packages processed successfully: {total_processed}")
    print(f"Total packages partially processed: {total_partial}")
    print(f"Total packages skipped (already processed): {total_skipped}")
    print(f"Total packages timed out (>3 minutes): {total_timeouts}")
    print(f"Total packages with errors: {total_errors}")
    
    total_packages = total_processed + total_partial + total_skipped + total_timeouts + total_errors
    print(f"Total packages found: {total_packages}")
    
    if total_errors > 0:
        print(f"\nâš ï¸  ERROR SUMMARY: {total_errors} packages encountered errors during feature extraction.")
        print("This is typically due to missing input files or processing errors.")
        print("Detailed error information is shown above for each dataset.")
    
    if total_timeouts > 0:
        print(f"\nâ° TIMEOUT SUMMARY: {total_timeouts} packages exceeded the 3-minute timeout limit.")
        print("These packages likely have very large feature sets or processing issues.")
        print("Detailed timeout information is shown above for each dataset.")
    
    if total_partial > 0:
        print(f"\nðŸ”¶ PARTIAL PROCESSING SUMMARY: {total_partial} packages were only partially processed.")
        print("Some centrality metrics may be missing for these packages.")
        print("Detailed partial processing information is shown above for each dataset.")


def process_specific_directory(dataset_type, package_type):
    """Process a specific directory with 20 processes"""
    num_processes = 20
    base_testdata = Path("/home2/wenbo/Documents/PyPIAgent/Tools/MulGuard/TestData")
    
    source_dir = base_testdata / dataset_type / package_type
    mapping_name = f"{dataset_type}_{package_type}"
    
    if not source_dir.exists():
        print(f"Directory does not exist: {source_dir}")
        return
    
    print("=" * 80)
    print(f"Processing {mapping_name} packages...")
    print("=" * 80)
    
    start_time = time.time()
    processed, errors, skipped, timeouts, partial = process_single_mapping(
        source_dir, mapping_name, num_processes
    )
    end_time = time.time()
    
    print(f"\nProcessing completed in {end_time - start_time:.2f} seconds")
    print(f"Results: {processed} success, {partial} partial, {skipped} skipped, {timeouts} timeout, {errors} errors")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) == 3:
        # Process specific directory
        dataset_type = sys.argv[1]  # e.g., "latest"
        package_type = sys.argv[2]  # e.g., "benign"
        process_specific_directory(dataset_type, package_type)
    else:
        # Process all directories
        main()
