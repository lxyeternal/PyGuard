#!/usr/bin/env python3
"""
Performance Analysis Script for MulGuard Models

This script analyzes the prediction results from different models across different datasets
and calculates performance metrics including F1-score, Precision, Recall, FN, FP, TN, TP.

CSV Format:
dataset,package_type,package_name,decision_tree_(dt),multi-layer_perceptron_(mlp),naive_bayes_(nb),random_forest_(rf),sgd_classifier_(svm),xgboost_(xgb)

Prediction values:
- 0: Benign (negative class)
- 1: Malware (positive class)  
- -1: Failed prediction (excluded from analysis)
"""

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score
import json


def load_predictions(dataset_name, base_path):
    """Load predictions for a specific dataset"""
    dataset_path = Path(base_path) / dataset_name
    
    benign_file = dataset_path / "predictions_benign.csv"
    malware_file = dataset_path / "predictions_malware.csv"
    
    data_frames = []
    
    # Load benign predictions
    if benign_file.exists():
        benign_df = pd.read_csv(benign_file)
        data_frames.append(benign_df)
        print(f"Loaded {len(benign_df)} benign samples from {dataset_name}")
    
    # Load malware predictions  
    if malware_file.exists():
        malware_df = pd.read_csv(malware_file)
        data_frames.append(malware_df)
        print(f"Loaded {len(malware_df)} malware samples from {dataset_name}")
    
    if data_frames:
        combined_df = pd.concat(data_frames, ignore_index=True)
        return combined_df
    else:
        print(f"No prediction files found for dataset: {dataset_name}")
        return None


def calculate_metrics(y_true, y_pred, model_name):
    """Calculate performance metrics for a model"""
    # Filter out failed predictions (-1)
    valid_mask = (y_pred != -1)
    y_true_filtered = y_true[valid_mask]
    y_pred_filtered = y_pred[valid_mask]
    
    if len(y_true_filtered) == 0:
        return {
            'model': model_name,
            'accuracy': 0.0,
            'precision': 0.0,
            'recall': 0.0,
            'f1_score': 0.0,
            'fp': 0,
            'fn': 0
        }
    
    # Calculate confusion matrix
    tn, fp, fn, tp = confusion_matrix(y_true_filtered, y_pred_filtered).ravel()
    
    # Calculate metrics
    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0
    precision = precision_score(y_true_filtered, y_pred_filtered, zero_division=0)
    recall = recall_score(y_true_filtered, y_pred_filtered, zero_division=0)
    f1 = f1_score(y_true_filtered, y_pred_filtered, zero_division=0)
    
    return {
        'model': model_name,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'fp': int(fp),
        'fn': int(fn)
    }


def analyze_dataset(dataset_name, base_path):
    """Analyze performance for a specific dataset"""
    print(f"\n{'='*60}")
    print(f"Analyzing dataset: {dataset_name.upper()}")
    print(f"{'='*60}")
    
    # Load predictions
    df = load_predictions(dataset_name, base_path)
    if df is None:
        return None
    
    # Create true labels (0 for benign, 1 for malware)
    y_true = (df['package_type'] == 'malware').astype(int)
    
    # Model columns (excluding metadata columns)
    model_columns = [col for col in df.columns if col not in ['dataset', 'package_type', 'package_name']]
    
    results = []
    
    print(f"\nDataset Summary:")
    print(f"Total samples: {len(df)}")
    print(f"Benign samples: {len(df[df['package_type'] == 'benign'])}")
    print(f"Malware samples: {len(df[df['package_type'] == 'malware'])}")
    
    print(f"\nModel Performance:")
    print("-" * 100)
    print(f"{'Model':<25} {'Accuracy':<12} {'Precision':<12} {'Recall':<10} {'F1-Score':<10} {'FP':<6} {'FN':<6}")
    print("-" * 100)
    
    for model_col in model_columns:
        y_pred = df[model_col].values
        metrics = calculate_metrics(y_true, y_pred, model_col)
        results.append(metrics)
        
        # Print results with percentage format
        print(f"{metrics['model']:<25} {metrics['accuracy']*100:<11.2f}% {metrics['precision']*100:<11.2f}% "
              f"{metrics['recall']*100:<9.2f}% {metrics['f1_score']*100:<9.2f}% "
              f"{metrics['fp']:<6} {metrics['fn']:<6}")
    
    return results


def save_results_to_json(all_results, output_path):
    """Save results to JSON file"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    print(f"\nResults saved to: {output_path}")


def create_summary_table(all_results):
    """Create a summary table across all datasets"""
    print(f"\n{'='*100}")
    print("SUMMARY: PERFORMANCE METRICS ACROSS ALL DATASETS")
    print(f"{'='*100}")
    
    # Extract model names
    if not all_results:
        return
    
    first_dataset = list(all_results.keys())[0]
    model_names = [result['model'] for result in all_results[first_dataset]]
    
    # Create summary table for each metric
    metrics_to_show = ['accuracy', 'precision', 'recall', 'f1_score']
    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    
    for i, metric in enumerate(metrics_to_show):
        print(f"\n{metric_names[i]} Comparison:")
        print(f"{'Model':<25}", end="")
        for dataset in all_results.keys():
            print(f"{dataset.capitalize():<15}", end="")
        print("Average")
        print("-" * (25 + 15 * len(all_results) + 10))
        
        for model_name in model_names:
            print(f"{model_name:<25}", end="")
            metric_scores = []
            
            for dataset in all_results.keys():
                dataset_results = all_results[dataset]
                model_result = next((r for r in dataset_results if r['model'] == model_name), None)
                if model_result:
                    score = model_result[metric] * 100  # Convert to percentage
                    metric_scores.append(score)
                    print(f"{score:<14.2f}%", end="")
                else:
                    print(f"{'N/A':<15}", end="")
            
            # Calculate average score
            if metric_scores:
                avg_score = np.mean(metric_scores)
                print(f"{avg_score:.2f}%")
            else:
                print("N/A")


def main():
    """Main function to analyze all datasets"""
    base_path = "/home2/wenbo/Documents/PyPIAgent/Tools/MulGuard/TestData"
    datasets = ["evaluation", "latest", "obfuscation"]
    
    print("MulGuard Model Performance Analysis")
    print("=" * 80)
    
    all_results = {}
    
    # Analyze each dataset
    for dataset in datasets:
        results = analyze_dataset(dataset, base_path)
        if results:
            all_results[dataset] = results
    
    # Skip summary table as requested
    
    # Save results to JSON
    output_path = Path(base_path) / "performance_analysis_results.json"
    save_results_to_json(all_results, output_path)
    
    print(f"\nâœ… Analysis completed! Results saved to JSON file.")


if __name__ == "__main__":
    main()
