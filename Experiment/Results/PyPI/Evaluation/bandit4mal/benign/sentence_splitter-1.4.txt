Run started:2025-05-25 13:15:08.530020

Test results:
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/sentence_splitter-1.4/sentence_splitter-1.4/setup.py:10
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
9	with open(REQUIREMENTS_FILE) as requirements:
10	    install_requirements = requirements.read().splitlines()
11	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/sentence_splitter-1.4/sentence_splitter-1.4/setup.py:16
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
15	    description='Text to sentence splitter using heuristic algorithm by Philipp Koehn and Josh Schroeder',
16	    long_description="""
17	Text to sentence splitter using heuristic algorithm by Philipp Koehn and Josh Schroeder.
18	
19	This module allows splitting of text paragraphs into sentences. It is based on scripts developed by Philipp
20	Koehn and Josh Schroeder for processing the `Europarl corpus <http://www.statmt.org/europarl/>`_.
21	
22	The module is a port of `Lingua::Sentence Perl module <http://search.cpan.org/perldoc?Lingua::Sentence>`_ with
23	some extra additions (improved non-breaking prefix lists for some languages and added support for Danish,
24	Finnish, Lithuanian, Norwegian (BokmÃ¥l), Romanian, and Turkish).
25	    """,
26	    author='Philip Koehn, Josh Schroeder, Digital Silk Road, Linas Valiukas',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/sentence_splitter-1.4/sentence_splitter-1.4/setup.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	    author_email='lvaliukas@cyber.law.harvard.edu',
28	    url='https://github.com/berkmancenter/mediacloud-sentence-splitter',
29	    keywords="sentence splitter tokenization tokenizer tokenize",

--------------------------------------------------

Code scanned:
	Total lines of code: 231
	Total lines skipped (#nosec): 0

Run metrics:
	Total issues (by severity):
		Undefined: 0.0
		Low: 0.0
		Medium: 2.0
		High: 1.0
	Total issues (by confidence):
		Undefined: 0.0
		Low: 0.0
		Medium: 3.0
		High: 0.0
Files skipped (0):
