Run started:2025-05-25 13:25:42.520706

Test results:
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/setup.py:236
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
235	    author_email='packages@tensorflow.org',
236	    url='https://github.com/tensorflow/datasets',
237	    download_url='https://github.com/tensorflow/datasets/tags',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/setup.py:237
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
236	    url='https://github.com/tensorflow/datasets',
237	    download_url='https://github.com/tensorflow/datasets/tags',
238	    license='Apache 2.0',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/commonvoice.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	
29	_DEMOGRAPHICS_URL = "https://github.com/common-voice/common-voice/blob/main/web/src/stores/demographics.ts"
30	_SPLITS = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/commonvoice.py:109
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
108	  return (
109	      "https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4."
110	      f"s3.amazonaws.com/cv-corpus-6.1-2020-12-11/{language}.tar.gz"
111	  )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/commonvoice.py:217
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
216	        }),
217	        homepage="https://voice.mozilla.org/en/datasets",
218	        license=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/commonvoice.py:219
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
218	        license=(
219	            "https://github.com/common-voice/common-voice/blob/main/LICENSE"
220	        ),
221	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/crema_d.py:51
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
50	
51	_HOMEPAGE = 'https://github.com/CheyneyComputerScience/CREMA-D'
52	
53	_CHECKSUMS_URL = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/crema_d.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	_CHECKSUMS_URL = (
54	    'https://storage.googleapis.com/tfds-data/manual_checksums/crema_d.txt'
55	)
56	SUMMARY_TABLE_URL = 'https://raw.githubusercontent.com/CheyneyComputerScience/CREMA-D/master/processedResults/summaryTable.csv'
57	WAV_DATA_URL = 'https://media.githubusercontent.com/media/CheyneyComputerScience/CREMA-D/master/AudioWAV/'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/crema_d.py:56
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
55	)
56	SUMMARY_TABLE_URL = 'https://raw.githubusercontent.com/CheyneyComputerScience/CREMA-D/master/processedResults/summaryTable.csv'
57	WAV_DATA_URL = 'https://media.githubusercontent.com/media/CheyneyComputerScience/CREMA-D/master/AudioWAV/'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/crema_d.py:57
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
56	SUMMARY_TABLE_URL = 'https://raw.githubusercontent.com/CheyneyComputerScience/CREMA-D/master/processedResults/summaryTable.csv'
57	WAV_DATA_URL = 'https://media.githubusercontent.com/media/CheyneyComputerScience/CREMA-D/master/AudioWAV/'
58	LABELS = ['NEU', 'HAP', 'SAD', 'ANG', 'FEA', 'DIS']

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/dementiabank.py:128
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
127	  MANUAL_DOWNLOAD_INSTRUCTIONS = textwrap.dedent(
128	      """
129	  manual dir should contain 2 folders with mp3 files:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/dementiabank.py:151
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
150	        supervised_keys=('audio', 'label'),
151	        homepage='https://dementia.talkbank.org/',
152	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/fuss.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_CITATION = r"""\
29	@inproceedings{wisdom2020fuss,
30	  title = {What's All the {FUSS} About Free Universal Sound Separation Data?},
31	  author = {Scott Wisdom and Hakan Erdogan and Daniel P. W. Ellis and Romain Serizel and Nicolas Turpault and Eduardo Fonseca and Justin Salamon and Prem Seetharaman and John R. Hershey},
32	  year = {2020},
33	  url = {https://arxiv.org/abs/2011.00803},
34	}
35	
36	@inproceedings{fonseca2020fsd50k,
37	  author = {Eduardo Fonseca and Xavier Favory and Jordi Pons and Frederic Font Corbera and Xavier Serra},
38	  title = {{FSD}50k: an open dataset of human-labeled sound events},
39	  year = {2020},
40	  url = {https://arxiv.org/abs/2010.00475},
41	}
42	"""
43	
44	_DESCRIPTION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/fuss.py:69
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
68	
69	_URL = "https://github.com/google-research/sound-separation/blob/master/datasets/fuss/FUSS_license_doc/README.md"
70	_DL_METADATA = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/fuss.py:72
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
71	    "reverberant": (
72	        "https://zenodo.org/record/3743844/files/FUSS_ssdata_reverb.tar.gz",
73	        "ssdata_reverb",
74	    ),
75	    "unprocessed": (
76	        "https://zenodo.org/record/3743844/files/FUSS_ssdata.tar.gz",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/fuss.py:76
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
75	    "unprocessed": (
76	        "https://zenodo.org/record/3743844/files/FUSS_ssdata.tar.gz",
77	        "ssdata",
78	    ),
79	}

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/fuss.py:186
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
185	        segments = self._parse_segments(os.path.join(split_dir, "%s.txt" % key))
186	        jams = tf.io.gfile.GFile(
187	            os.path.join(split_dir, "%s.jams" % key)
188	        ).read()

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/gtzan/gtzan.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_CITATION = """
24	@misc{tzanetakis_essl_cook_2001,
25	author    = "Tzanetakis, George and Essl, Georg and Cook, Perry",
26	title     = "Automatic Musical Genre Classification Of Audio Signals",
27	url       = "http://ismir2001.ismir.net/pdf/tzanetakis.pdf",
28	publisher = "The International Society for Music Information Retrieval",
29	year      = "2001"
30	}
31	"""
32	
33	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/gtzan/gtzan.py:53
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
52	
53	_DOWNLOAD_URL = "http://opihi.cs.uvic.ca/sound/genres.tar.gz"
54	_HOMEPAGE_URL = "http://marsyas.info/index.html"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/gtzan/gtzan.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	_DOWNLOAD_URL = "http://opihi.cs.uvic.ca/sound/genres.tar.gz"
54	_HOMEPAGE_URL = "http://marsyas.info/index.html"
55	
56	_CLASS_LABELS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/gtzan_music_speech/gtzan_music_speech.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_CITATION = """
25	@ONLINE {Music Speech,
26	    author = "Tzanetakis, George",
27	    title  = "GTZAN Music/Speech Collection",
28	    year   = "1999",
29	    url    = "http://marsyas.info/index.html"
30	}
31	"""
32	
33	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/gtzan_music_speech/gtzan_music_speech.py:40
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
39	
40	_DOWNLOAD_URL = "http://opihi.cs.uvic.ca/sound/music_speech.tar.gz"
41	_HOMEPAGE_URL = "http://marsyas.info/index.html"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/gtzan_music_speech/gtzan_music_speech.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	_DOWNLOAD_URL = "http://opihi.cs.uvic.ca/sound/music_speech.tar.gz"
41	_HOMEPAGE_URL = "http://marsyas.info/index.html"
42	
43	_CLASS_LABELS = ["music", "speech"]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/userlibri_audio_data/userlibri_audio_data.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	
35	_DESCRIPTION = """\
36	UserLibri is a dataset containing paired audio-transcripts and additional text
37	only data for each of 107 users. It is a reformatting of the LibriSpeech dataset
38	found at http://www.openslr.org/12, reorganizing the data into users with an
39	average of 52 LibriSpeech utterances and about 6,700 text example sentences per
40	user. The UserLibriAudio class provides access to the audio-transcript pairs.
41	See UserLibriText for the additional text data.
42	"""
43	
44	_URL = "https://www.kaggle.com/datasets/google/userlibri"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/userlibri_audio_data/userlibri_audio_data.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	
44	_URL = "https://www.kaggle.com/datasets/google/userlibri"
45	_DL_URL = tfds.download.Resource(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/userlibri_audio_data/userlibri_audio_data.py:46
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
45	_DL_URL = tfds.download.Resource(
46	    url="https://www.kaggle.com/datasets/google/userlibri/download",
47	    extract_method=tfds.download.ExtractMethod.ZIP,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/vctk.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	
44	_URL = "https://doi.org/10.7488/ds/2645"
45	_DL_URL = "https://datashare.is.ed.ac.uk/bitstream/handle/10283/3443/VCTK-Corpus-0.92.zip"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/vctk.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	_URL = "https://doi.org/10.7488/ds/2645"
45	_DL_URL = "https://datashare.is.ed.ac.uk/bitstream/handle/10283/3443/VCTK-Corpus-0.92.zip"
46	
47	
48	class Vctk(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/vctk.py:59
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
58	          name="mic1",
59	          description="""
60	              Audio recorded using an omni-directional microphone (DPA 4035).
61	              Contains very low frequency noises.
62	
63	              This is the same audio released in previous versions of VCTK:
64	              https://doi.org/10.7488/ds/1994
65	          """,
66	          version=tfds.core.Version("1.0.1"),

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/vctk.py:247
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
246	    path = os.path.join(extracted_dir, "speaker-info.txt")
247	    speaker_info = tf.io.gfile.GFile(path).read()
248	    speaker_to_gender = {}

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/vctk.py:268
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
267	      if tf.io.gfile.exists(text_path):
268	        text = tf.io.gfile.GFile(text_path).read().strip()
269	      else:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/voxceleb.py:40
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
39	
40	_HOMEPAGE = 'http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html'
41	
42	IDEN_SPLITS_URL = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/voxceleb.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	IDEN_SPLITS_URL = (
43	    'http://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/iden_split.txt'
44	)
45	NUM_CLASSES = 1252
46	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/voxforge.py:32
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
31	
32	_CITATION = """
33	@article{maclean2018voxforge,
34	  title={Voxforge},
35	  author={MacLean, Ken},
36	  journal={Ken MacLean.[Online]. Available: http://www.voxforge.org/home.[Acedido em 2012]},
37	  year={2018}
38	}
39	"""
40	
41	_LAST_DATE = datetime.date(2020, 1, 1)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/voxforge.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	
55	_HOMEPAGE = 'http://www.voxforge.org/'
56	
57	_SAMPLE_RATE = 16000

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/voxforge.py:59
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
58	
59	_URLS_LIST_FILE = 'https://storage.googleapis.com/tfds-data/downloads/voxforge/voxforge_urls.txt'
60	
61	LABELS = ['de', 'en', 'es', 'fr', 'it', 'ru']

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/voxforge.py:106
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
105	  """Read a `tarfile.ExFileObject`."""
106	  sample_rate, samples = tfds.core.lazy_imports.scipy.io.wavfile.read(
107	      io.BytesIO(wav_obj.read())
108	  )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/voxforge.py:107
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
106	  sample_rate, samples = tfds.core.lazy_imports.scipy.io.wavfile.read(
107	      io.BytesIO(wav_obj.read())
108	  )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/xtreme_s/xtreme_s.py:439
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
438	_CITATIONS = {
439	    "fleurs": """\
440	@article{fleurs2022arxiv,
441	  title = {FLEURS: Few-shot Learning Evaluation of Universal Representations of Speech},

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/xtreme_s/xtreme_s.py:450
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
449	_HOMEPAGE_URLS = {
450	    "fleurs": "https://arxiv.org/abs/2205.12446",
451	}
452	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/xtreme_s/xtreme_s.py:455
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
454	    "fleurs": [
455	        "https://storage.googleapis.com/xtreme_translations/FLEURS102/{}.tar.gz"
456	    ],
457	}

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/xtreme_s/xtreme_s.py:478
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
477	            "2.0.0": (
478	                "Initial release on TFDS, FLEURS-only. Named to match version"
479	                " 2.0.0 on huggingface which has the same FLEURS data ("
480	                " https://huggingface.co/datasets/google/xtreme_s)."
481	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/yesno/yesno.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_CITATION = """\
24	@ONLINE {YesNo,
25	    author = "Created for the Kaldi Project",
26	    title  = "YesNo",
27	    url    = "http://www.openslr.org/1/"
28	}
29	"""
30	
31	_DESCRIPTION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/yesno/yesno.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	
43	_HOMEPAGE_URL = "https://www.openslr.org/1/"
44	_DOWNLOAD_URL = "http://www.openslr.org/resources/1/waves_yesno.tar.gz"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/audio/yesno/yesno.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	_HOMEPAGE_URL = "https://www.openslr.org/1/"
44	_DOWNLOAD_URL = "http://www.openslr.org/resources/1/waves_yesno.tar.gz"
45	
46	
47	class YesNo(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/community/register_package_test.py:134
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
133	  # Checksums have been correctly installed
134	  assert 'http://dummy.org/data.txt' in builder_cls.url_infos
135	

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/community/register_path.py:218
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
217	
218	  with concurrent.futures.ThreadPoolExecutor(max_workers=10) as ex:
219	    builder_names_futures = []

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/community/registry.py:162
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
161	
162	    with concurrent.futures.ThreadPoolExecutor(
163	        max_workers=max_workers
164	    ) as executor:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/constants.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	# Github base URL
26	SRC_BASE_URL = 'https://github.com/tensorflow/datasets/tree/master/'
27	
28	# Directory where to store processed datasets.
29	# If modifying this, should also update `scripts/cli/build.py` `--data_dir`
30	DATA_DIR = os.environ.get(

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/dataset_builder.py:1553
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
1552	    decode_fn = functools.partial(features.decode_example, decoders=decoders)
1553	    return reader.read(
1554	        instructions=split,
1555	        split_infos=self.info.splits.values(),
1556	        decode_fn=decode_fn,
1557	        read_config=read_config,
1558	        shuffle_files=shuffle_files,
1559	        disable_shuffling=self.info.disable_shuffling,
1560	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/dataset_builders/croissant_builder_test.py:279
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
278	  assert crs_builder._info().description == "Dummy description."
279	  assert crs_builder._info().homepage == "https://dummy_url"
280	  assert crs_builder._info().redistribution_info.license == "Public"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/dataset_builders/croissant_builder_test.py:289
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
288	      crs_builder.metadata.ctx.conforms_to.value
289	      == "http://mlcommons.org/croissant/1.0"
290	  )
291	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/dataset_builders/huggingface_dataset_builder.py:158
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
157	
158	  example_writer.write(
159	      os.fspath(shard_spec.path),
160	      tqdm_utils.tqdm(
161	          enumerate(get_serialized_examples_iter()),
162	          desc=f'Writing {shard_spec.path} examples...',
163	          unit=' examples',
164	          total=shard_spec.num_examples,
165	          leave=False,
166	          mininterval=1.0,
167	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/dataset_builders/huggingface_dataset_builder.py:229
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
228	    self.name = conversion_utils.to_tfds_name(hf_repo_id)
229	    self.homepage = f'https://huggingface.co/datasets/{hf_repo_id}'
230	    self._hf_hub_token = hf_hub_token

--------------------------------------------------
>> Issue: [B839:pool] multiprocessing.Pool
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/dataset_builders/huggingface_dataset_builder.py:467
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b839_pool.html
466	    else:
467	      with multiprocessing.Pool(processes=self._tfds_num_proc) as pool:
468	        shard_infos = pool.map(write_shard, shard_specs)

--------------------------------------------------
>> Issue: [B323:blacklist] multiprocessing_Pool
   Severity: Medium   Confidence: High
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/dataset_builders/huggingface_dataset_builder.py:467
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b323-multiprocessing-pool
466	    else:
467	      with multiprocessing.Pool(processes=self._tfds_num_proc) as pool:
468	        shard_infos = pool.map(write_shard, shard_specs)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/dataset_builders/huggingface_dataset_builder_test.py:112
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
111	  assert builder.info.redistribution_info.license == 'test-license'
112	  assert builder.info.homepage == 'https://huggingface.co/datasets/foo/bar'
113	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/dataset_info.py:421
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
420	    urls = self.as_proto.location.urls
421	    tfds_homepage = f"https://www.tensorflow.org/datasets/catalog/{self.name}"
422	    return urls and urls[0] or tfds_homepage

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/dataset_info_test.py:188
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
187	      with tf.io.gfile.GFile(dataset_info.license_path(tmp_dir)) as f:
188	        license_ = f.read()
189	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/dataset_info_test.py:206
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
205	        supervised_keys=("input", "output"),
206	        homepage="http://some-location",
207	        citation="some citation",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/dataset_info_test.py:243
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
242	          supervised_keys=("input (new)", "output (new)"),
243	          homepage="http://some-location-new",
244	          citation="some citation (new)",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/dataset_info_test.py:268
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
267	      )
268	      self.assertEqual(restored_info.homepage, "http://some-location-new")
269	      self.assertEqual(restored_info.citation, "some citation (new)")

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/dataset_info_test.py:812
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
811	# pylint: disable=g-inconsistent-quotes
812	_INFO_STR = '''tfds.core.DatasetInfo(
813	    name='mnist',
814	    full_name='mnist/3.0.1',
815	    description="""
816	    The MNIST database of handwritten digits.
817	    """,
818	    homepage='https://storage.googleapis.com/cvdf-datasets/mnist/',
819	    data_dir='%s',
820	    file_format=tfrecord,
821	    download_size=1.95 KiB,
822	    dataset_size=11.06 MiB,
823	    features=FeaturesDict({
824	        'image': Image(shape=(28, 28, 1), dtype=uint8),
825	        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),
826	    }),
827	    supervised_keys=('image', 'label'),
828	    disable_shuffling=False,
829	    nondeterministic_order=False,
830	    splits={
831	        'test': <SplitInfo num_examples=20, num_shards=1>,
832	        'train': <SplitInfo num_examples=20, num_shards=1>,
833	    },
834	    citation="""@article{lecun2010mnist,
835	      title={MNIST handwritten digit database},
836	      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
837	      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},
838	      volume={2},
839	      year={2010}
840	    }
841	    """,
842	    redistribution_info=license: "test license",
843	)'''
844	# pylint: enable=g-inconsistent-quotes
845	
846	
847	class LazyMetadataDictTest(testing.TestCase):

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/decode/base_test.py:59
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
58	    with tf.io.gfile.GFile(image_path, 'rb') as f:
59	      serialized_img = f.read()
60	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/decode/base_test.py:94
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
93	    with tf.io.gfile.GFile(image_path, 'rb') as f:
94	      serialized_img = f.read()
95	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/checksums.py:114
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
113	  logging.warning(
114	      '`tfds.core.add_checksums_dir` is deprecated. Refactor dataset in '
115	      'self-contained folders (`my_dataset/` folder containing '
116	      'my_dataset.py, my_dataset_test.py, dummy_data/, checksums.tsv). '

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/checksums_test.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	  url_infos = {
28	      'http://abc.org/data': checksums.UrlInfo(
29	          checksum='abcd',
30	          size=1234,
31	          filename='a.zip',
32	      ),
33	      'http://edf.org/data': checksums.UrlInfo(
34	          checksum='abcd',
35	          size=1234,
36	          filename='b.zip',
37	      ),
38	  }

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/checksums_test.py:33
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
32	      ),
33	      'http://edf.org/data': checksums.UrlInfo(
34	          checksum='abcd',
35	          size=1234,
36	          filename='b.zip',
37	      ),
38	  }
39	
40	  checksums.save_url_infos(path, url_infos)
41	  loaded_url_infos = checksums.load_url_infos(path)
42	  assert loaded_url_infos == url_infos
43	

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/download_manager.py:287
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
286	    # executors threads.
287	    self._executor = concurrent.futures.ThreadPoolExecutor(1)
288	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/download_manager.py:517
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
516	        msg = (
517	            f'Artifact {url}, downloaded to {path}, has wrong checksum:\n'
518	            f'* Expected: {expected_url_info}\n'
519	            f'* Got: {url_info}\n'
520	            'To debug, see: '

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/download_manager_test.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	    self.name = name
55	    self.url = url or f'http://foo-bar.ch/{self.name}'
56	    self.content = content or f'content of {self.name}'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/download_manager_test.py:330
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
329	    # not from URL.
330	    a = Artifact('a', url='http://a?key=1234')
331	    self.dl_results[a.url] = a.url_info

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/downloader.py:46
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
45	
46	_DRIVE_URL = re.compile(r'^https://drive\.google\.com/')
47	MAX_RETRIES = 10

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/downloader.py:125
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
124	def _get_filename(response: Response) -> str:
125	  content_disposition = response.headers.get('content-disposition', None)
126	  if content_disposition:

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/downloader.py:153
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
152	    )
153	  action = form.get('action', '')
154	  if not action:

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/downloader.py:164
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
163	    if input_tag:
164	      params[name] = input_tag.get('value', '')
165	  query_string = urllib.parse.urlencode(params)

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/downloader.py:195
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
194	    """
195	    self._executor = concurrent.futures.ThreadPoolExecutor(
196	        max_workers=max_simultaneous_downloads
197	        or self._DEFAULT_MAX_SIMULTANEOUS_DOWNLOADS
198	    )

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/downloader.py:297
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
296	      unit_mb = units.MiB
297	      total_size = int(response.headers.get('Content-length', 0)) // unit_mb
298	      self._pbar_dl_size.update_total(total_size)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/downloader.py:304
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
303	          checksum.update(block)
304	          file_.write(block)
305	

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/downloader.py:359
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
358	    )
359	    with session.get(url, stream=True, **kwargs) as response:
360	      if (

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/downloader.py:365
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
364	        download_url = _process_gdrive_confirmation(url, response.text)
365	        with session.get(
366	            download_url, stream=True, **kwargs
367	        ) as download_response:

--------------------------------------------------
>> Issue: [B818:urlopen] urllib.request.urlopen
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/downloader.py:387
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b818_urlopen.html
386	  del kwargs
387	  with urllib.request.urlopen(url) as response:  # pytype: disable=attribute-error
388	    yield (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/downloader_test.py:61
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
60	    self.tmp_dir = epath.Path(self.tmp_dir)
61	    self.url = 'http://example.com/foo.tar.gz'
62	    self.resource = resource_lib.Resource(url=self.url)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/downloader_test.py:101
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
100	  def test_drive_no_cookies(self):
101	    url = 'https://drive.google.com/uc?export=download&id=a1b2bc3'
102	    promise = self.downloader.download(url, self.tmp_dir)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/downloader_test.py:251
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
250	    headers = None
251	  resp = _FakeResponse('http://foo.bar/baz.zip', b'content', headers=headers)
252	  assert downloader._get_filename(resp), filename

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/downloader_test.py:259
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
258	        (
259	            'http://test.com/appspot.com/tsvsWithoutLabels%2FAX.tsv?'  # pylint: disable=implicit-str-concat
260	            'Id=firebase&Expires=2498860800',
261	            'tsvsWithoutLabels_AX.tsv',  # `%2F` -> `_`
262	        ),
263	    ],

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/extractor.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
57	    max_workers = max_workers or multiprocessing.cpu_count()
58	    self._executor = concurrent.futures.ThreadPoolExecutor(
59	        max_workers=max_workers * 4
60	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/extractor.py:111
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
110	        file_utils.makedirs_cached(os.path.dirname(dst_path))
111	        future = self._executor.submit(_copy, handle.read(), dst_path)
112	        futures.append(future)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/extractor.py:124
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
123	        msg += (
124	            '\nOn windows, path lengths greater than 260 characters may result'
125	            ' in an error. See the doc to remove the limitation: '
126	            'https://docs.python.org/3/using/windows.html#removing-the-max-path-limitation'

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/extractor.py:148
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
147	  with tf.io.gfile.GFile(dest_path, 'wb') as dest_file:
148	    dest_file.write(src_data)
149	

--------------------------------------------------
>> Issue: [B834:open] tarfile.open
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/extractor.py:187
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b834_open.html
186	  with _open_or_pass(arch_f) as fobj:
187	    tar = tarfile.open(mode=read_type, fileobj=fobj)
188	    for member in tar:

--------------------------------------------------
>> Issue: [B834:open] tarfile.open
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/extractor.py:238
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b834_open.html
237	        continue
238	      extract_file = z.open(member)
239	      path = _normpath(member.filename)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/extractor_test.py:38
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
37	  with tf.io.gfile.GFile(path, 'rb') as f:
38	    return f.read()
39	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/extractor_test.py:46
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
45	    super(ExtractorTest, cls).setUpClass()
46	    cls.f1_content = _read(os.path.join(cls.test_data, '6pixels.png'))
47	    cls.f2_content = _read(os.path.join(cls.test_data, 'foo.csv'))

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/extractor_test.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
46	    cls.f1_content = _read(os.path.join(cls.test_data, '6pixels.png'))
47	    cls.f2_content = _read(os.path.join(cls.test_data, 'foo.csv'))
48	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/extractor_test.py:70
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
69	      path = os.path.join(self.to_path, name)
70	      self.assertEqual(_read(path), content, 'File %s has bad content.' % path)
71	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/extractor_test.py:111
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
110	    arch1_path = os.path.join(self.test_data, 'archives', 'arch1.tar')
111	    self.assertEqual(_read(self.to_path), _read(arch1_path))
112	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/extractor_test.py:111
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
110	    arch1_path = os.path.join(self.test_data, 'archives', 'arch1.tar')
111	    self.assertEqual(_read(self.to_path), _read(arch1_path))
112	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/extractor_test.py:118
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
117	    foo_csv_path = os.path.join(self.test_data, 'foo.csv')
118	    self.assertEqual(_read(self.to_path), _read(foo_csv_path))
119	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/extractor_test.py:118
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
117	    foo_csv_path = os.path.join(self.test_data, 'foo.csv')
118	    self.assertEqual(_read(self.to_path), _read(foo_csv_path))
119	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/extractor_test.py:124
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
123	    foo_csv_path = os.path.join(self.test_data, 'foo.csv')
124	    self.assertEqual(_read(self.to_path), _read(foo_csv_path))
125	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/extractor_test.py:124
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
123	    foo_csv_path = os.path.join(self.test_data, 'foo.csv')
124	    self.assertEqual(_read(self.to_path), _read(foo_csv_path))
125	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/kaggle.py:88
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
87	          textwrap.dedent(
88	              """\
89	      Error for command: {}
90	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/kaggle_test.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
34	        with tf.io.gfile.GFile(os.path.join(out_path, 'output.txt')) as f:
35	          self.assertEqual('digit-recognizer', f.read())
36	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/kaggle_test.py:46
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
45	        with tf.io.gfile.GFile(os.path.join(out_path, 'output.txt')) as f:
46	          self.assertEqual('user/dataset', f.read())
47	

--------------------------------------------------
>> Issue: [B802:b64encode] base64.b64encode
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/resource.py:187
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b802_b64encode.html
186	    checksum = checksums_lib.sha256(url)
187	  checksum = base64.urlsafe_b64encode(_decode_hex(checksum))
188	  checksum = checksum.decode()[:-1]

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/resource.py:267
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
266	
267	  with utils.atomic_write(info_path, 'w') as info_f:
268	    json.dump(info, info_f, sort_keys=True)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/resource_test.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	      (
43	          'http://data.statmt.org/wmt17/translation-task/dev.tgz',
44	          'data.statmt.org_wmt17_translation-task_devDjZ11PU9sKPPvF2sZTAzTsV7Pi3IYHaPDMOoeEuby2E.tgz',
45	      ),
46	      (
47	          'http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/resource_test.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	      (
47	          'http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz',
48	          'data.stat.org_wmt1_tran-task_trai-para-nc-6LWgxBgzCHdv_LtotNmnXjpCH6OhzkF8D3v10aRrznA.tgz',
49	      ),
50	      (
51	          'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/resource_test.py:51
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
50	      (
51	          'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz',
52	          'fashion-mnist_train-images-idx3-ubytepR2BibiiUp2twRbpoktblvl2KbaPDel0VUV9KrXm91Y.gz',
53	      ),
54	      (
55	          'http://ufldl.stanford.edu/housenumbers/test_32x32.mat',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/resource_test.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	      (
55	          'http://ufldl.stanford.edu/housenumbers/test_32x32.mat',
56	          'ufldl.stanford.edu_housenumbers_test_32x32kIzM03CdHZsHqtImuAxFCXPGHhEH4JT7Owsqi_QawO4.mat',
57	      ),
58	      (
59	          'http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/resource_test.py:59
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
58	      (
59	          'http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz',
60	          'stat.org_lm-benc_1-bill-word-lang-mode-fPxXes4bTZ_y2eAI2mGRqBKUvUJm1CS1Idm0DH98KN8.tar.gz',
61	      ),
62	      (
63	          'http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/resource_test.py:63
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
62	      (
63	          'http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz',
64	          'statmt.org_wmt13_traini-parall-europa-v71cKcs9sx8w9ctm8xHloEI83SJqzD7piDNK3xUXpQIB4.tgz',
65	      ),
66	      (
67	          'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/resource_test.py:67
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
66	      (
67	          'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',
68	          'yann.lecu.com_exdb_mnis_trai-imag-idx3-ubyt5m0Lc_VeEzGZ1PUycLKoWNyYkH_vWEKNi0mu7m4Hmbk.gz',
69	      ),
70	      (
71	          'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/resource_test.py:71
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
70	      (
71	          'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',
72	          'yann.lecu.com_exdb_mnis_trai-labe-idx1-ubyt7cc_IeM51G_ngIY2ORleKjMjLVCXd-TCUHlYvEiRiKI.gz',
73	      ),
74	      (
75	          'https://drive.google.com/uc?export=download&id=0B7EVK8r0v71pd0FJY3Blby1HUTQ',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/resource_test.py:75
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
74	      (
75	          'https://drive.google.com/uc?export=download&id=0B7EVK8r0v71pd0FJY3Blby1HUTQ',
76	          'ucexport_download_id_0B7EVK8r0v71pd0FJY3Blby1HbdQ1eXJPJLYv0yq8hL1lCD5T2aOraaQwvj25ndmE7pg',
77	      ),
78	      (
79	          'https://github.com/brendenlake/omniglot/raw/master/python/images_background_small2.zip',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/resource_test.py:79
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
78	      (
79	          'https://github.com/brendenlake/omniglot/raw/master/python/images_background_small2.zip',
80	          'bren_omni_raw_mast_pyth_imag_back_smalUSA8LkdUW89lgXr31txDoVFbI9BtQhxvtZWYTIdAJAg.zip',
81	      ),
82	      (
83	          'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/resource_test.py:83
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
82	      (
83	          'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json',
84	          'rajpurkar_SQuAD-explorer_train-v1.1uLsZc14btZFRCgHMAy9Mn5abwO6wga4bMozTBvOyQAg.json',
85	      ),
86	      (
87	          'https://storage.googleapis.com/scv_dataset/data/Brawl_64x64_png/valid-00000-of-00001.tfrecords',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/resource_test.py:87
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
86	      (
87	          'https://storage.googleapis.com/scv_dataset/data/Brawl_64x64_png/valid-00000-of-00001.tfrecords',
88	          'scv_Brawl_64x64_png_valid-0_1Ez3yPwN0QDCxBd0xHeLb2DfUERJjkqFd2dyL5Z7-ULg.tfrecords',
89	      ),
90	      (
91	          'https://storage.googleapis.com/scv_dataset/data/CollectMineralShards_128x128_png/train-00005-of-00010.tfrecords',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/resource_test.py:91
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
90	      (
91	          'https://storage.googleapis.com/scv_dataset/data/CollectMineralShards_128x128_png/train-00005-of-00010.tfrecords',
92	          'scv_CollectMi_128x128_png_train-5_10kiunW_2RTDhXuPrxCVkUZKCoWpADYBUWE8DpraC8zAA.tfrecords',
93	      ),
94	      (
95	          'https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/download/resource_test.py:95
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
94	      (
95	          'https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz',
96	          'cs.toronto.edu_kriz_cifar-100-pythonJDFhDchdt5UW8GUAkvf_-H_r_LnFs6sHlOrqTidrpSI.tar.gz',
97	      ),
98	  )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/features/audio_feature.py:170
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
169	  ) -> np.ndarray:
170	    return np.array(fobj.read(), dtype=tf.string.as_numpy_dtype)
171	

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/features/features_dict.py:307
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
306	
307	    with concurrent.futures.ThreadPoolExecutor(
308	        max_workers=WORKER_COUNT
309	    ) as executor:

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/features/image_feature.py:121
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
120	    else:
121	      encoded_image = image_or_path_or_fobj.read()
122	    # If encoding is explicitly set, should verify that bytes match encoding.

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/features/image_feature_test.py:116
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
115	      with tf.io.gfile.GFile(img_file_path, 'rb') as f:
116	        img_byte_content = f.read()
117	      img_file_expected_content = np.array(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/features/text_feature.py:71
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
70	      logging.warning(
71	          "TFDS datasets with text encoding are deprecated and will be removed "
72	          "in a future version. Instead, you should use the plain text version "
73	          "and tokenize the text using `tensorflow_text` (See: "

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/features/video_feature.py:145
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
144	      ffmpeg_args = ['-i', 'pipe:0']
145	      ffmpeg_stdin = path_or_fobj.read()
146	    ffmpeg_args += self._extra_ffmpeg_args

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/features/video_feature.py:176
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
175	        with tf.io.gfile.GFile(video_temp_path, 'wb') as f:
176	          f.write(video_or_path_or_fobj)
177	        encoded_video = self._ffmpeg_decode(video_temp_path)

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/features/video_feature.py:176
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
175	        with tf.io.gfile.GFile(video_temp_path, 'wb') as f:
176	          f.write(video_or_path_or_fobj)
177	        encoded_video = self._ffmpeg_decode(video_temp_path)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/features/video_feature_test.py:61
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
60	      with tf.io.gfile.GFile(frame_path, 'rb') as frame_fp:
61	        frames.append(tf.image.decode_jpeg(frame_fp.read(), channels=3))
62	    video = tf.stack(frames)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/features/video_feature_test.py:105
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
104	      def read(self, *args, **kwargs):
105	        data_read = super(GFileWithSeekOnRead, self).read(*args, **kwargs)
106	        self.seek(0)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/file_adapters.py:225
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
224	      for _, serialized_example in iterator:
225	        writer.write(serialized_example)
226	      writer.flush()

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/file_adapters.py:325
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
324	    for _, serialized_example in iterator:
325	      writer.write(serialized_example)
326	    writer.close()

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/folder_dataset/translate_folder.py:170
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
169	  with epath.Path(file).open() as f:
170	    sentences = f.read().splitlines()
171	  return sentences

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/github_api/github_path.py:39
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
38	  # curl -H "Authorization: token ${GITHUB_TOKEN}" https://api.github.com/rate_limit  # pylint: disable=line-too-long
39	  return os.environ.get('GITHUB_TOKEN')
40	

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/github_api/github_path.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
42	def get_content(url: str) -> bytes:
43	  resp = requests.get(url)
44	  if resp.status_code != 200:

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/github_api/github_path.py:64
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
63	      headers['Authorization'] = f'token {self._token}'
64	    resp = requests.get(url, headers=headers)
65	    if resp.status_code != 200:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/github_api/github_path.py:88
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
87	    """
88	    url = f'https://api.github.com/repos/{repo}/git/trees/{branch}?recursive=1'
89	    return self.query(url)

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/github_api/github_path.py:148
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
147	    parent_folder = _get_parent_folder(path)
148	    files = self.files_per_folder.get(parent_folder)
149	    if not files:

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/github_api/github_path.py:181
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
180	    # an exception.
181	    assert not tree_json.get('truncated', False)  # pytype: disable=attribute-error  # dynamic-method-lookup
182	    return _GithubTree.from_json(tree_json)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/github_api/github_path.py:288
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
287	    return (
288	        'https://raw.githubusercontent.com/'
289	        f'{self.repo}/{self.branch}/{self.subpath}'
290	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/github_api/github_path.py:294
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
293	    """Returns the human friendly url."""
294	    return f'https://github.com/{self.repo}/blob/{self.branch}/{self.subpath}'
295	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/github_api/github_path_test.py:132
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
131	  expected = (
132	      'https://raw.githubusercontent.com/tensorflow/datasets/v3.1.0/README.md'
133	  )
134	  assert p.as_raw_url() == expected
135	

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/read_only_builder.py:399
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
398	  else:
399	    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
400	      # Keep track of each new thread's error context, and add it to the main

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader.py:213
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
212	      raise ValueError(
213	          'Cannot shard the pipeline with given `input_context`.'
214	          '`num_shards={}` but `num_input_pipelines={}`. This means that some '

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader_test.py:69
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
68	  )
69	  example_writer.write(shard_spec.path, itertools.chain(*iterators))
70	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader_test.py:129
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
128	      )
129	      self.reader.read(
130	          instructions='train[0:0]',
131	          split_infos=[train_info],
132	      )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader_test.py:136
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
135	    train_info = self._write_tfrecord('train', 5, 'abcdefghijkl')
136	    ds = self.reader.read(
137	        instructions='train',
138	        split_infos=[train_info],
139	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader_test.py:151
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
150	    train_info = self._write_tfrecord('train', 5, 'abcdefghijkl')
151	    ds = self.reader.read(
152	        instructions='train+train[:2]',
153	        split_infos=[train_info],
154	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader_test.py:172
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
171	    split_info = [train_info, test_info]
172	    ds = self.reader.read(
173	        instructions='train[1:-1]+test[:-50%]',
174	        split_infos=split_info,
175	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader_test.py:188
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
187	    train_info = self._write_tfrecord('train', 5, chars)
188	    ds = self.reader.read(
189	        instructions='train',
190	        split_infos=[train_info],
191	        shuffle_files=True,
192	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader_test.py:208
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
207	    )
208	    ds = self.reader.read(
209	        instructions='train',
210	        split_infos=[split_info],
211	        read_config=read_config,
212	        shuffle_files=True,
213	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader_test.py:241
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
240	    ]
241	    tests = self.reader.read(
242	        instructions=instructions,
243	        split_infos=[train_info],
244	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader_test.py:252
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
251	    ]
252	    trains = self.reader.read(
253	        instructions=instructions,
254	        split_infos=[train_info],
255	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader_test.py:316
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
315	          tfds.as_numpy(
316	              self.reader.read(
317	                  instructions='train',
318	                  split_infos=[split_info],
319	                  read_config=read_config_lib.ReadConfig(
320	                      input_context=tf.distribute.InputContext(
321	                          num_input_pipelines=num_workers,
322	                          input_pipeline_id=index,
323	                      ),
324	                  ),
325	                  # Workers should read a deterministic subset of the examples,
326	                  # even if examples within one worker may be shuffled.
327	                  shuffle_files=True,
328	              )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader_test.py:337
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
336	    # Read all the data (single pipeline)
337	    self.assertCountEqual(read(num_workers=1, index=0), _b(b'abcdefghijkl'))
338	    # Read part of the data (workers should not overlapp)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader_test.py:339
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
338	    # Read part of the data (workers should not overlapp)
339	    self.assertCountEqual(read(num_workers=3, index=0), _b(b'abhij'))  # 0, 3
340	    self.assertCountEqual(read(num_workers=3, index=1), _b(b'cdekl'))  # 1, 4

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader_test.py:340
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
339	    self.assertCountEqual(read(num_workers=3, index=0), _b(b'abhij'))  # 0, 3
340	    self.assertCountEqual(read(num_workers=3, index=1), _b(b'cdekl'))  # 1, 4
341	    self.assertEqual(read(num_workers=3, index=2), _b(b'fg'))  # Shards 2

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader_test.py:341
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
340	    self.assertCountEqual(read(num_workers=3, index=1), _b(b'cdekl'))  # 1, 4
341	    self.assertEqual(read(num_workers=3, index=2), _b(b'fg'))  # Shards 2
342	    # If num_workers == num_shards, then a single shard is read

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader_test.py:343
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
342	    # If num_workers == num_shards, then a single shard is read
343	    self.assertEqual(read(num_workers=5, index=1), _b(b'cde'))  # Shard 1
344	    # If num_workers > num_shards, raise error

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader_test.py:346
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
345	    with self.assertRaisesRegex(ValueError, 'Cannot shard the pipeline'):
346	      read(num_workers=6, index=0)
347	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader_test.py:436
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
435	    train_info = self._write_tfrecord('train', 5, 'abcdefghijkl')
436	    self.reader.read(instructions='train', split_infos=[train_info])
437	    assert_cardinality.assert_called_with(12)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/reader_test.py:444
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
443	    train_info = self._write_tfrecord('train', 5, 'abcdefghijkl')
444	    self.reader.read(
445	        instructions='train',
446	        split_infos=[train_info],
447	        read_config=read_config_lib.ReadConfig(assert_cardinality=False),
448	    )

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/sequential_writer.py:61
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
60	    """Writes a new example."""
61	    self.writer.write(serialized_example)
62	    self.num_examples += 1

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/shuffle.py:164
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
163	    try:
164	      self._fobj.write(_hkey_to_bytes(key))
165	    except tf.errors.ResourceExhaustedError as error:

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/shuffle.py:169
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
168	        _increase_open_files_limit()
169	        self._fobj.write(_hkey_to_bytes(key))
170	      else:

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/shuffle.py:179
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
178	    # temporary files are going to be written and read by the same platform.
179	    self._fobj.write(struct.pack('=Q', data_size))
180	    self._fobj.write(data)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/shuffle.py:180
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
179	    self._fobj.write(struct.pack('=Q', data_size))
180	    self._fobj.write(data)
181	    self._length += 1

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/shuffle.py:200
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
199	      while True:
200	        buff = fobj.read(HKEY_SIZE_BYTES)
201	        if not buff:

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/shuffle.py:204
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
203	        hkey = _read_hkey(buff)
204	        size_bytes = fobj.read(8)
205	        size = struct.unpack('=Q', size_bytes)[0]

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/shuffle.py:206
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
205	        size = struct.unpack('=Q', size_bytes)[0]
206	        data = fobj.read(size)
207	        yield hkey, data

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/split_builder.py:328
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
327	    if not self._beam_runner and not self._beam_options:
328	      msg = utils.dedent("""
329	          **************************** WARNING *********************************

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/splits.py:622
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
621	    with utils.try_reraise(
622	        f'Error parsing split {spec!r}. See format at: '
623	        'https://www.tensorflow.org/datasets/splits\n'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/splits.py:754
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
753	  err_msg = (
754	      f'Unrecognized split format: {spec!r}. See format at '
755	      'https://www.tensorflow.org/datasets/splits'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/utils/croissant_utils.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	
30	_HUGGINGFACE_URL_PREFIX = "https://huggingface.co/datasets/"
31	
32	
33	@dataclasses.dataclass(frozen=True)
34	class SplitReference:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/utils/croissant_utils_test.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	            'Name+1',
26	            'https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k',
27	            'huggingfaceh4__ultrachat_200k',
28	        ),
29	        ('Name+1', 'bad_url', 'name_1'),
30	        ('Name+1', None, 'name_1'),
31	    ],

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/utils/croissant_utils_test.py:42
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
41	      name='dummy_dataset',
42	      url='https://dummy_url',
43	      record_sets=[

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/utils/croissant_utils_test.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	              id='record_set_2',
50	              data_types=['http://mlcommons.org/croissant/Split'],
51	              fields=[

--------------------------------------------------
>> Issue: [B823:ip_found] ip_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/utils/file_utils_test.py:308
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b823_ip_found.html
307	  _add_features(
308	      mock_fs, _DATA_DIR / 'invalid_version2' / '1.2.3.4', content='x'
309	  )

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/utils/gcs_utils.py:133
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
132	
133	    with concurrent.futures.ThreadPoolExecutor(
134	        max_workers=max_simultaneous_downloads
135	    ) as executor:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/utils/image_utils.py:116
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
115	    raise FileNotFoundError(
116	        'It seems that ffmpeg is not installed on the system. Please follow '
117	        'the instrutions at https://ffmpeg.org/. '
118	        f'Original exception: {e}'
119	    ) from e

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/utils/image_utils_test.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
42	    with tf.io.gfile.GFile(path, 'rb') as img_f:
43	      return img_f.read()
44	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/utils/lazy_imports_utils.py:32
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
31	  print(
32	      "Failed to import TensorFlow. Please note that TensorFlow is not "
33	      "installed by default when you install TFDS. This allows you "
34	      "to choose to install either `tf-nightly` or `tensorflow`. "

--------------------------------------------------
>> Issue: [B802:b64encode] base64.b64encode
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/utils/py_utils.py:532
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b802_b64encode.html
531	    bytes_value = buffer.getvalue()
532	  return base64.b64encode(bytes_value).decode('ascii')  # pytype: disable=bad-return-type
533	

--------------------------------------------------
>> Issue: [B301:blacklist] base64.b64encode
   Severity: Medium   Confidence: High
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/utils/py_utils.py:532
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b301-base64-b64encode
531	    bytes_value = buffer.getvalue()
532	  return base64.b64encode(bytes_value).decode('ascii')  # pytype: disable=bad-return-type
533	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/utils/tqdm_utils.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
48	  def write(self, x):
49	    tqdm_lib.tqdm.write(x, end='')
50	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer.py:288
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
287	    ]
288	    self._example_writer.write(path=path, examples=serialized_examples)
289	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer.py:305
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
304	      del dummy_value
305	      num_examples = self.write(examples=example_gen(), path=path)
306	      return shard_index, num_examples

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer.py:398
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
397	        )
398	        record_keys = self._example_writer.write(shard_spec.path, iterator)
399	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer.py:579
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
578	      )
579	      record_keys = self._example_writer.write(
580	          tmp_path, sorted(example_by_key.items())
581	      )

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer.py:827
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
826	
827	    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
828	      shard_sizes = executor.map(_get_length_and_size, shards)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:290
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
289	    for key, record in to_write:
290	      writer.write(key, record)
291	    return writer.finalize()

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:290
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
289	    for key, record in to_write:
290	      writer.write(key, record)
291	    return writer.finalize()

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:296
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
295	    path = os.path.join(self.tmp_dir, 'foo-train.tfrecord')
296	    shards_length, total_size = self._write(to_write=self.RECORDS_TO_WRITE)
297	    self.assertLen(shards_length, self.NUM_SHARDS)

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:296
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
295	    path = os.path.join(self.tmp_dir, 'foo-train.tfrecord')
296	    shards_length, total_size = self._write(to_write=self.RECORDS_TO_WRITE)
297	    self.assertLen(shards_length, self.NUM_SHARDS)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:319
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
318	    path = os.path.join(self.tmp_dir, 'foo-train.tfrecord')
319	    shards_length, total_size = self._write(
320	        to_write=self.RECORDS_TO_WRITE, disable_shuffling=True
321	    )

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:319
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
318	    path = os.path.join(self.tmp_dir, 'foo-train.tfrecord')
319	    shards_length, total_size = self._write(
320	        to_write=self.RECORDS_TO_WRITE, disable_shuffling=True
321	    )

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:344
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
343	    path = os.path.join(self.tmp_dir, 'foo-train.tfrecord')
344	    shards_length, total_size = self._write(
345	        to_write=self.RECORDS_WITH_HOLES, disable_shuffling=True
346	    )

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:344
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
343	    path = os.path.join(self.tmp_dir, 'foo-train.tfrecord')
344	    shards_length, total_size = self._write(
345	        to_write=self.RECORDS_WITH_HOLES, disable_shuffling=True
346	    )

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:368
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
367	    custom_example_writer = CustomExampleWriter()
368	    _, total_size = self._write(
369	        to_write=self.RECORDS_TO_WRITE, example_writer=custom_example_writer
370	    )

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:368
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
367	    custom_example_writer = CustomExampleWriter()
368	    _, total_size = self._write(
369	        to_write=self.RECORDS_TO_WRITE, example_writer=custom_example_writer
370	    )

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:381
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
380	      shard_config = shard_utils.ShardConfig(num_shards=1)
381	      self._write(to_write=to_write, shard_config=shard_config)
382	

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:381
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
380	      shard_config = shard_utils.ShardConfig(num_shards=1)
381	      self._write(to_write=to_write, shard_config=shard_config)
382	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:387
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
386	    shard_config = shard_utils.ShardConfig(num_shards=1)
387	    shards_length, total_size = self._write(
388	        to_write=to_write, shard_config=shard_config, ignore_duplicates=True
389	    )

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:387
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
386	    shard_config = shard_utils.ShardConfig(num_shards=1)
387	    shards_length, total_size = self._write(
388	        to_write=to_write, shard_config=shard_config, ignore_duplicates=True
389	    )

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:399
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
398	      shard_config = shard_utils.ShardConfig(num_shards=1)
399	      self._write(to_write=to_write, shard_config=shard_config)
400	

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:399
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
398	      shard_config = shard_utils.ShardConfig(num_shards=1)
399	      self._write(to_write=to_write, shard_config=shard_config)
400	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:407
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
406	      shard_config = shard_utils.ShardConfig(num_shards=2)
407	      self._write(to_write=to_write, shard_config=shard_config)
408	      self._write(to_write=to_write)

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:407
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
406	      shard_config = shard_utils.ShardConfig(num_shards=2)
407	      self._write(to_write=to_write, shard_config=shard_config)
408	      self._write(to_write=to_write)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:408
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
407	      self._write(to_write=to_write, shard_config=shard_config)
408	      self._write(to_write=to_write)
409	

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:408
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
407	      self._write(to_write=to_write, shard_config=shard_config)
408	      self._write(to_write=to_write)
409	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:488
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
487	    path = os.path.join(self.tmp_dir, 'foo-train.tfrecord')
488	    shards_length, total_size = self._write(to_write=self.RECORDS_TO_WRITE)
489	    self.assertLen(shards_length, self.NUM_SHARDS)

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:488
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
487	    path = os.path.join(self.tmp_dir, 'foo-train.tfrecord')
488	    shards_length, total_size = self._write(to_write=self.RECORDS_TO_WRITE)
489	    self.assertLen(shards_length, self.NUM_SHARDS)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:512
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
511	    ):
512	      self._write(to_write=self.RECORDS_WITH_DUPLICATES)
513	

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:512
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
511	    ):
512	      self._write(to_write=self.RECORDS_WITH_DUPLICATES)
513	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:515
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
514	  def test_write_tfrecord_with_ignored_duplicates(self):
515	    shards_length, total_size = self._write(
516	        to_write=self.RECORDS_WITH_DUPLICATES, ignore_duplicates=True
517	    )

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:515
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
514	  def test_write_tfrecord_with_ignored_duplicates(self):
515	    shards_length, total_size = self._write(
516	        to_write=self.RECORDS_WITH_DUPLICATES, ignore_duplicates=True
517	    )

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:527
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
526	    ):
527	      self._write(to_write=[])
528	

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:527
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
526	    ):
527	      self._write(to_write=[])
528	

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:532
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
531	    path = os.path.join(self.tmp_dir, 'foo-train.tfrecord')
532	    shards_length, total_size = self._write(
533	        to_write=self.RECORDS_TO_WRITE, disable_shuffling=True
534	    )

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:532
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
531	    path = os.path.join(self.tmp_dir, 'foo-train.tfrecord')
532	    shards_length, total_size = self._write(
533	        to_write=self.RECORDS_TO_WRITE, disable_shuffling=True
534	    )

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:567
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
566	
567	    shards_length, total_size = self._write(
568	        to_write=records_with_holes, disable_shuffling=True
569	    )

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:567
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
566	
567	    shards_length, total_size = self._write(
568	        to_write=records_with_holes, disable_shuffling=True
569	    )

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:659
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
658	    ])
659	    writer.write(path=path, examples=iterator)
660	    tfrecord_writer.write.assert_called_once_with(path, mock.ANY)

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/core/writer_test.py:659
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
658	    ])
659	    writer.write(path=path, examples=iterator)
660	    tfrecord_writer.write.assert_called_once_with(path, mock.ANY)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/d4rl/d4rl_antmaze/d4rl_antmaze.py:34
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
33	  _TASK_REFERENCE = (
34	      'See more details about the task and its versions in '
35	      'https://github.com/rail-berkeley/d4rl/wiki/Tasks#antmaze'
36	  )
37	
38	  # pylint: disable=protected-access
39	  # Step metadata fields.
40	  QPOS = dataset_builder._QPOS
41	  QVEL = dataset_builder._QVEL

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/d4rl/d4rl_antmaze/d4rl_antmaze.py:174
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
173	            'file_path': (
174	                'http://rail.eecs.berkeley.edu/datasets/offline_rl/'
175	                + ds_dir
176	                + '/'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/d4rl/dataset_builder.py:106
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
105	_MUJOCO_DESCRIPTION = (
106	    'See more details about the task and its versions in '
107	    'https://github.com/rail-berkeley/d4rl/wiki/Tasks#gym'
108	)
109	_ADROIT_DESCRIPTION = (
110	    'See more details about the task and its versions in '

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/d4rl/dataset_builder.py:110
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
109	_ADROIT_DESCRIPTION = (
110	    'See more details about the task and its versions in '
111	    'https://github.com/rail-berkeley/d4rl/wiki/Tasks#adroit'
112	)
113	
114	# pytype: disable=wrong-keyword-args
115	MUJOCO_BUILDER_CONFIGS = [
116	    BuilderConfig(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/d4rl/dataset_builder.py:554
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
553	            'file_path': (
554	                'http://rail.eecs.berkeley.edu/datasets/offline_rl/'
555	                + ds_dir
556	                + '/'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/d4rl/dataset_utils.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_DESCRIPTION = """
24	D4RL is an open-source benchmark for offline reinforcement learning. It provides
25	standardized environments and datasets for training and benchmarking algorithms.
26	
27	The datasets follow the [RLDS format](https://github.com/google-research/rlds)
28	to represent steps and episodes.
29	"""
30	
31	_CITATION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/d4rl/dataset_utils.py:52
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
51	def url():
52	  return 'https://sites.google.com/view/d4rl-anonymous'
53	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/dataset_collections/longt5/longt5.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	        },
35	        homepage="https://github.com/google-research/longt5",
36	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/dataset_collections/xtreme/xtreme.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	        },
35	        homepage="https://sites.research.google/xtreme",
36	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/abstract_reasoning/abstract_reasoning_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	
25	_URL = "https://github.com/deepmind/abstract-reasoning-matrices"
26	
27	_DESCRIPTION_NEUTRAL = r"""The structures encoding the matrices in both the \

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/abstract_reasoning/abstract_reasoning_dataset_builder.py:99
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
98	
99	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
100	  Data can be downloaded from
101	  https://console.cloud.google.com/storage/browser/ravens-matrices
102	  Please put all the tar.gz files in manual_dir.
103	  """
104	
105	  BUILDER_CONFIGS = [

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/abstract_reasoning/abstract_reasoning_dataset_builder.py:203
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
202	        if len(split_name) > 2 and split_name[2] == split:
203	          yield [name, fobj.read()]
204	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/accentdb/accentdb_dataset_builder.py:38
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
37	
38	_DOWNLOAD_URL = 'https://drive.google.com/uc?export=download&id=1NO1NKQSpyq3DMLEwiqA-BHIqXli8vtIL'
39	
40	
41	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/accentdb/accentdb_dataset_builder.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	        supervised_keys=('audio', 'label'),
54	        homepage='https://accentdb.github.io/',
55	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/aeslc/aeslc_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_URL = "https://github.com/ryanzhumich/AESLC/archive/master.zip"
25	
26	_DOCUMENT = "email_body"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/aeslc/aeslc_dataset_builder.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	        supervised_keys=(_DOCUMENT, _SUMMARY),
41	        homepage="https://github.com/ryanzhumich/AESLC",
42	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/aflw2k3d/aflw2k3d_dataset_builder.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	        homepage=(
44	            "http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm"
45	        ),
46	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/aflw2k3d/aflw2k3d_dataset_builder.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	    extracted_path = dl_manager.download_and_extract(
50	        "http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/Database/AFLW2000-3D.zip"
51	    )
52	    return [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ag_news_subset/ag_news_subset_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_HOMEPAGE_URL = "https://arxiv.org/abs/1509.01626"
25	_DOWNLOAD_URL = "https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbUDNpeUdjb0wxRms"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ag_news_subset/ag_news_subset_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	_HOMEPAGE_URL = "https://arxiv.org/abs/1509.01626"
25	_DOWNLOAD_URL = "https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbUDNpeUdjb0wxRms"
26	
27	_LABEL_NAMES = ["World", "Sports", "Business", "Sci/Tech"]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ai2_arc/ai2_arc_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_HOMEPAGE = "https://allenai.org/data/arc"
25	_URL = "https://ai2-public-datasets.s3-us-west-2.amazonaws.com/arc/ARC-V1-Feb2018.zip"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ai2_arc/ai2_arc_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	_HOMEPAGE = "https://allenai.org/data/arc"
25	_URL = "https://ai2-public-datasets.s3-us-west-2.amazonaws.com/arc/ARC-V1-Feb2018.zip"
26	
27	
28	class Ai2ArcConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ai2_arc_with_ir/ai2_arc_with_ir_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_HOMEPAGE = "https://allenai.org/data/arc"
29	_URL = "http://aristo-data.s3.amazonaws.com/custom-datasets/ARC-IR10V8.zip"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ai2_arc_with_ir/ai2_arc_with_ir_dataset_builder.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	_HOMEPAGE = "https://allenai.org/data/arc"
29	_URL = "http://aristo-data.s3.amazonaws.com/custom-datasets/ARC-IR10V8.zip"
30	
31	
32	class Ai2ArcWithIRConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ai2dcaption/ai2dcaption_dataset_builder.py:64
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
63	
64	JSON_URL_TMPL = 'https://huggingface.co/datasets/abhayzala/AI2D-Caption/resolve/main/ai2d_caption_{split}.json?download=true'
65	
66	IMAGES_URL = 'http://ai2-website.s3.amazonaws.com/data/ai2d-all.zip'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ai2dcaption/ai2dcaption_dataset_builder.py:66
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
65	
66	IMAGES_URL = 'http://ai2-website.s3.amazonaws.com/data/ai2d-all.zip'
67	
68	
69	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ai2dcaption/ai2dcaption_dataset_builder.py:110
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
109	        supervised_keys=None,
110	        homepage='https://huggingface.co/datasets/abhayzala/AI2D-Caption',
111	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/amazon_us_reviews/amazon_us_reviews_dataset_builder.py:87
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
86	_DL_URLS = {
87	    name: f"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_{name}.tsv.gz"
88	    for name in _DATA_OPTIONS

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/amazon_us_reviews/amazon_us_reviews_dataset_builder.py:152
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
151	        supervised_keys=None,
152	        homepage="https://s3.amazonaws.com/amazon-reviews-pds/readme.html",
153	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/anli/anli_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_ANLI_URL = "https://dl.fbaipublicfiles.com/anli/anli_v0.1.zip"
25	
26	EXTRACT_PATH_TOKEN = "anli_v0.1"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/anli/anli_dataset_builder.py:75
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
74	        supervised_keys=None,
75	        homepage="https://github.com/facebookresearch/anli",
76	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/answer_equivalence/answer_equivalence_dataset_builder.py:53
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
52	        supervised_keys=None,
53	        homepage='https://github.com/google-research-datasets/answer-equivalence-dataset',
54	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/answer_equivalence/answer_equivalence_dataset_builder.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
57	    """Returns SplitGenerators."""
58	    homepage = 'https://raw.githubusercontent.com/google-research-datasets/answer-equivalence-dataset/main/v1/'
59	    archive = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/arc/arc_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_BASE_URL = "https://github.com/fchollet/ARC/"
27	
28	
29	class ARCConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/asqa/asqa_dataset_builder.py:94
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
93	        supervised_keys=None,
94	        homepage='https://github.com/google-research/language/tree/master/language/asqa',
95	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/asqa/asqa_dataset_builder.py:100
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
99	    file_path = dl_manager.download(
100	        'https://storage.googleapis.com/gresearch/ASQA/ASQA.json'
101	    )
102	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/asset/asset_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_HOMEPAGE = 'https://github.com/facebookresearch/asset'
25	
26	_LICENSE = 'Creative Common Attribution-NonCommercial 4.0 International'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/asset/asset_dataset_builder.py:31
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
30	        'human_ratings.csv',
31	        'https://raw.githubusercontent.com/facebookresearch/asset/main/human_ratings/human_ratings.csv',
32	    ),
33	    (
34	        'asset.valid.orig',
35	        'https://raw.githubusercontent.com/facebookresearch/asset/main/dataset/asset.valid.orig',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/asset/asset_dataset_builder.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	        'asset.valid.orig',
35	        'https://raw.githubusercontent.com/facebookresearch/asset/main/dataset/asset.valid.orig',
36	    ),
37	    (
38	        'asset.test.orig',
39	        'https://raw.githubusercontent.com/facebookresearch/asset/main/dataset/asset.test.orig',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/asset/asset_dataset_builder.py:39
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
38	        'asset.test.orig',
39	        'https://raw.githubusercontent.com/facebookresearch/asset/main/dataset/asset.test.orig',
40	    ),
41	]
42	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/asset/asset_dataset_builder.py:48
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
47	        f'asset.{spl}.simp.{i}',
48	        f'https://raw.githubusercontent.com/facebookresearch/asset/main/dataset/asset.{spl}.simp.{i}',
49	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/assin2/assin2_dataset_builder.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_HOMEPAGE = 'https://sites.google.com/view/assin2/english'
24	
25	# pylint: disable=line-too-long
26	_DESCRIPTION = f"""\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/assin2/assin2_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	# pylint: disable=line-too-long
26	_DESCRIPTION = f"""\
27	## Contextualization
28	ASSIN 2 is the second edition of the Avaliao de Similaridade Semntica e
29	Inferncia Textual (Evaluating Semantic Similarity and Textual Entailment),
30	and was a workshop collocated with [STIL 2019](http://www.google.com/url?q=http%3A%2F%2Fcomissoes.sbc.org.br%2Fce-pln%2Fstil2019%2F&sa=D&sntz=1&usg=AFQjCNHN8DosAsJ-gd48TfkXFX5YD6xM7g). It follows the [first edition of ASSIN](http://www.google.com/url?q=http%3A%2F%2Fpropor2016.di.fc.ul.pt%2F%3Fpage_id%3D381&sa=D&sntz=1&usg=AFQjCNHV7ySeNzH4k6MWKBLqO9yUkqiUqw),
31	proposing a new shared task with new data.
32	
33	The workshop evaluated systems that assess two types of relations between
34	two sentences: Semantic Textual Similarity and Textual Entailment.
35	
36	Semantic Textual Similarity consists of quantifying the level of semantic
37	equivalence between sentences, while Textual Entailment Recognition consists of
38	classifying whether the first sentence entails the second.
39	
40	## Data
41	The corpus used in ASSIN 2 is composed of rather simple sentences. Following
42	the procedures of SemEval 2014 Task 1, we tried to remove from the corpus named
43	entities and indirect speech, and tried to have all verbs in the present tense.
44	The [annotation instructions](https://drive.google.com/open?id=1aUPhywEHD0r_pxPiTqZwS0fRj-1Xda2w)
45	given to annotators are available (in Portuguese).
46	
47	The training and validation data are composed, respectively, of 6,500 and 500
48	sentence pairs in Brazilian Portuguese, annotated for entailment and
49	semantic similarity. Semantic similarity values range from 1 to 5, and text
50	entailment classes are either entailment or none. The test data are composed of
51	approximately 3,000 sentence pairs with the same annotation. All data were
52	manually annotated.
53	
54	## Evaluation
55	Evaluation
56	The evaluation of submissions to ASSIN 2 was with the same metrics as the first
57	ASSIN, with the F1 of precision and recall as the main metric for text
58	entailment and Pearson correlation for semantic similarity.
59	The [evaluation scripts](https://github.com/erickrf/assin) are the same as in
60	the last edition.
61	
62	PS.: Description is extracted from [official homepage]({_HOMEPAGE}).
63	"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/assin2/assin2_dataset_builder.py:68
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
67	_DOWNLOAD_URLS = {
68	    'train': 'https://drive.google.com/u/0/uc?id=1Q9j1a83CuKzsHCGaNulSkNxBm7Dkn7Ln&export=download',
69	    'validation': 'https://drive.google.com/u/0/uc?id=1kb7xq6Mb3eaqe9cOAo70BaG9ypwkIqEU&export=download',
70	    'test': 'https://drive.google.com/u/0/uc?id=1J3FpQaHxpM-FDfBUyooh-sZF-B-bM_lU&export=download',
71	}
72	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/assin2/assin2_dataset_builder.py:69
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
68	    'train': 'https://drive.google.com/u/0/uc?id=1Q9j1a83CuKzsHCGaNulSkNxBm7Dkn7Ln&export=download',
69	    'validation': 'https://drive.google.com/u/0/uc?id=1kb7xq6Mb3eaqe9cOAo70BaG9ypwkIqEU&export=download',
70	    'test': 'https://drive.google.com/u/0/uc?id=1J3FpQaHxpM-FDfBUyooh-sZF-B-bM_lU&export=download',
71	}
72	
73	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/assin2/assin2_dataset_builder.py:70
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
69	    'validation': 'https://drive.google.com/u/0/uc?id=1kb7xq6Mb3eaqe9cOAo70BaG9ypwkIqEU&export=download',
70	    'test': 'https://drive.google.com/u/0/uc?id=1J3FpQaHxpM-FDfBUyooh-sZF-B-bM_lU&export=download',
71	}
72	
73	
74	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bair_robot_pushing_small/bair_robot_pushing_small_dataset_builder.py:31
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
30	DATA_URL = (
31	    "http://rail.eecs.berkeley.edu/datasets/bair_robot_pushing_dataset_v0.tar"
32	)
33	
34	# There are exactly 30 frames in each video.
35	FRAMES_PER_VIDEO = 30
36	IMG_SHAPE = (64, 64, 3)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bair_robot_pushing_small/bair_robot_pushing_small_dataset_builder.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	  RELEASE_NOTES = {
44	      "2.0.0": "New split API (https://tensorflow.org/datasets/splits)",
45	  }
46	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bair_robot_pushing_small/bair_robot_pushing_small_dataset_builder.py:64
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
63	        features=features,
64	        homepage="https://sites.google.com/view/sna-visual-mpc/",
65	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bccd/bccd_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_HOMEPAGE_URL = "https://github.com/Shenggan/BCCD_Dataset"
27	_DOWNLOAD_URL = "https://github.com/Shenggan/BCCD_Dataset/archive/v1.0.zip"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bccd/bccd_dataset_builder.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	_HOMEPAGE_URL = "https://github.com/Shenggan/BCCD_Dataset"
27	_DOWNLOAD_URL = "https://github.com/Shenggan/BCCD_Dataset/archive/v1.0.zip"
28	_CLASS_LABELS = [

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bccd/dummy_data_generation.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
53	  with tf.io.gfile.GFile(filepath, "w") as f:
54	    f.write(content)
55	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/beans/beans_dataset_builder.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	
22	_TRAIN_URL = "https://storage.googleapis.com/ibeans/train.zip"
23	_VALIDATION_URL = "https://storage.googleapis.com/ibeans/validation.zip"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/beans/beans_dataset_builder.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	_TRAIN_URL = "https://storage.googleapis.com/ibeans/train.zip"
23	_VALIDATION_URL = "https://storage.googleapis.com/ibeans/validation.zip"
24	_TEST_URL = "https://storage.googleapis.com/ibeans/test.zip"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/beans/beans_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	_VALIDATION_URL = "https://storage.googleapis.com/ibeans/validation.zip"
24	_TEST_URL = "https://storage.googleapis.com/ibeans/test.zip"
25	
26	_IMAGE_SIZE = 500

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/beans/beans_dataset_builder.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	        supervised_keys=("image", "label"),
44	        homepage="https://github.com/AI-Lab-Makerere/ibean/",
45	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bee_dataset/bee_dataset_dataset_builder.py:46
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
45	
46	  URL = 'https://raspbee.de/BeeDataset_20201121.zip'
47	
48	  BEE_CFG_300 = BeeDatasetConfig(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bee_dataset/bee_dataset_dataset_builder.py:94
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
93	        supervised_keys=('input', 'output'),
94	        homepage='https://raspbee.de',
95	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/beir/beir_dataset_builder.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	_BASE_DOWNLOAD_URL = (
29	    'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets'
30	)
31	
32	
33	class BeirConfig(tfds.core.BuilderConfig):
34	  """BuilderConfig for BEIR datasets."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/beir/beir_dataset_builder.py:247
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
246	        }),
247	        homepage='https://github.com/beir-cellar/beir',
248	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/big_patent/big_patent_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	# Raw data provided by Eva Sharma (evasharma@ccs.neu.edu).
26	_URL = "https://drive.google.com/uc?export=download&id=1mwH7eSh1kNci31xduR4Da_XcmTE8B8C3"
27	
28	_DOCUMENT = "description"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/big_patent/big_patent_dataset_builder.py:98
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
97	        supervised_keys=(_DOCUMENT, _SUMMARY),
98	        homepage="https://evasharma.github.io/bigpatent/",
99	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bigearthnet/bigearthnet_dataset_builder.py:77
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
76	_ZIP_FILE = (
77	    'https://zenodo.org/records/12687186/files//BigEarthNet-S2-v1.0.tar.gz'
78	)
79	_ZIP_SUBIDR = 'BigEarthNet-v1.0'
80	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bigearthnet/bigearthnet_dataset_builder.py:105
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
104	        release_notes={
105	            '1.0.0': 'New split API (https://tensorflow.org/datasets/splits)',
106	        },
107	        **kwargs,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bigearthnet/bigearthnet_dataset_builder.py:171
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
170	        supervised_keys=supervised_keys,
171	        homepage='http://bigearth.net',
172	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bigearthnet/bigearthnet_dataset_builder.py:208
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
207	    if fname.endswith('_labels_metadata.json'):
208	      example['metadata'] = fobj.read()
209	    elif fname.endswith('.tif'):

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bigearthnet/bigearthnet_dataset_builder.py:214
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
213	      ):
214	        example[band] = fobj.read()
215	        example.setdefault('bands', []).append(band)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/billsum/billsum_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_URL = "https://drive.google.com/uc?export=download&id=1g89WgFHMRbr4QrvA0ngh26PY081Nv3lx"
25	
26	_DOCUMENT = "text"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/billsum/billsum_dataset_builder.py:46
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
45	        supervised_keys=(_DOCUMENT, _SUMMARY),
46	        homepage="https://github.com/FiscalNote/BillSum",
47	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/binarized_mnist/binarized_mnist_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_URL = "http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/"
25	_TRAIN_DATA_FILENAME = "binarized_mnist_train.amat"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/binarized_mnist/binarized_mnist_dataset_builder.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	        ),
43	        homepage="http://www.dmi.usherb.ca/~larocheh/mlpython/_modules/datasets/binarized_mnist.html",
44	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/binary_alpha_digits/binary_alpha_digits_dataset_builder.py:21
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
20	
21	_URL = 'https://cs.nyu.edu/~roweis/data/'
22	
23	_DESCRIPTION = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ble_wind_field/ble_wind_field_dataset_builder.py:67
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
66	        supervised_keys=None,
67	        homepage='https://github.com/google/balloon-learning-environment',
68	    )

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ble_wind_field/ble_wind_field_dataset_builder.py:113
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
112	    # thread that loads chunks asynchronously.
113	    with concurrent.futures.ThreadPoolExecutor() as executor:
114	      load_fn = lambda next_slice: zarr_array[next_slice]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/blimp/blimp_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_PROJECT_URL = 'https://github.com/alexwarstadt/blimp/tree/master/'
27	_DOWNLOAD_URL = 'https://raw.githubusercontent.com/alexwarstadt/blimp/master'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/blimp/blimp_dataset_builder.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	_PROJECT_URL = 'https://github.com/alexwarstadt/blimp/tree/master/'
27	_DOWNLOAD_URL = 'https://raw.githubusercontent.com/alexwarstadt/blimp/master'
28	
29	
30	class BlimpConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/booksum/booksum_dataset_builder.py:63
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
62	  }
63	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
64	  1) Go to https://github.com/salesforce/booksum, and run steps 1-3. Place the
65	     whole `booksum` git project in the manual folder.
66	  2) Download the chapterized books from https://storage.cloud.google.com/sfr-books-dataset-chapters-research/all_chapterized_books.zip
67	     and unzip to the manual folder.
68	
69	  The manual folder should contain the following directories:
70	
71	      - `booksum/`
72	      - `all_chapterized_books/`
73	
74	  Note: Because the BookSum dataset is based on the availability of web-scraped
75	  data and may be incomplete, the `_generate_examples` method will automatically
76	  skip missing entries.
77	  """
78	  BUILDER_CONFIGS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/booksum/booksum_dataset_builder.py:98
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
97	        supervised_keys=(_DOCUMENT, _SUMMARY),
98	        homepage="https://github.com/salesforce/booksum",
99	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/booksum/booksum_dataset_builder.py:143
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
142	    with tf.io.gfile.GFile(alignments_path, "r") as f:
143	      for i, line in enumerate(f.read().strip().splitlines()):
144	        example_data = json.loads(line)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/booksum/booksum_dataset_builder.py:158
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
157	        with tf.io.gfile.GFile(input_path, "r") as f:
158	          input_text = f.read().strip()
159	        with tf.io.gfile.GFile(summary_path, "r") as f:

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/booksum/booksum_dataset_builder.py:160
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
159	        with tf.io.gfile.GFile(summary_path, "r") as f:
160	          summary_text = " ".join(json.loads(f.read())["summary"]).strip()
161	        yield str(i), {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bool_q/bool_q_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_HOMEPAGE_URL = "https://github.com/google-research-datasets/boolean-questions"
27	
28	_TRAIN_DOWNLOAD_URL = "https://storage.googleapis.com/boolq/train.jsonl"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bool_q/bool_q_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_TRAIN_DOWNLOAD_URL = "https://storage.googleapis.com/boolq/train.jsonl"
29	_VALIDATION_DOWNLOAD_URL = "https://storage.googleapis.com/boolq/dev.jsonl"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bool_q/bool_q_dataset_builder.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	_TRAIN_DOWNLOAD_URL = "https://storage.googleapis.com/boolq/train.jsonl"
29	_VALIDATION_DOWNLOAD_URL = "https://storage.googleapis.com/boolq/dev.jsonl"
30	
31	
32	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bot_adversarial_dialogue/bot_adversarial_dialogue_dataset_builder.py:90
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
89	        supervised_keys=None,
90	        homepage="https://github.com/facebookresearch/ParlAI/tree/main/parlai/tasks/bot_adversarial_dialogue",
91	        license="https://github.com/facebookresearch/ParlAI/blob/main/LICENSE",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bot_adversarial_dialogue/bot_adversarial_dialogue_dataset_builder.py:91
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
90	        homepage="https://github.com/facebookresearch/ParlAI/tree/main/parlai/tasks/bot_adversarial_dialogue",
91	        license="https://github.com/facebookresearch/ParlAI/blob/main/LICENSE",
92	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bot_adversarial_dialogue/bot_adversarial_dialogue_dataset_builder.py:98
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
97	    bot_adversarial_dialogue_home = (
98	        "http://parl.ai/downloads/bot_adversarial_dialogue/"
99	    )
100	
101	    if self.builder_config.name == "dialogue_datasets":
102	      path = dl_manager.download_and_extract(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bucc/bucc_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_DATA_URLS = 'https://comparable.limsi.fr/bucc2018/'
27	
28	
29	class BuccConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/bucc/bucc_dataset_builder.py:65
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
64	        supervised_keys=None,
65	        homepage='https://comparable.limsi.fr/bucc2018/',
66	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/caltech101/caltech101_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	_LABELS_FNAME = "image_classification/caltech101_labels.txt"
25	_URL = "https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip?download=1"
26	_TRAIN_POINTS_PER_CLASS = 30

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/caltech101/caltech101_dataset_builder.py:34
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
33	  RELEASE_NOTES = {
34	      "3.0.0": "New split API (https://tensorflow.org/datasets/splits)",
35	      "3.0.1": "Website URL update",
36	      "3.0.2": "Download URL update",
37	  }
38	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/caltech101/caltech101_dataset_builder.py:48
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
47	        supervised_keys=("image", "label"),
48	        homepage="https://doi.org/10.22002/D1.20086",
49	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/celeb_a/celeb_a_dataset_builder.py:34
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
33	IMG_ALIGNED_DATA = (
34	    "https://drive.google.com/uc?export=download&"
35	    "id=0B7EVK8r0v71pZjFTYXZWM3FlRnM"
36	)
37	EVAL_LIST = (
38	    "https://drive.google.com/uc?export=download&"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/celeb_a/celeb_a_dataset_builder.py:38
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
37	EVAL_LIST = (
38	    "https://drive.google.com/uc?export=download&"
39	    "id=0B7EVK8r0v71pY0NSMzRuSXJEVkk"
40	)
41	# Landmark coordinates: left_eye, right_eye etc.
42	LANDMARKS_DATA = (
43	    "https://drive.google.com/uc?export=download&"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/celeb_a/celeb_a_dataset_builder.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	LANDMARKS_DATA = (
43	    "https://drive.google.com/uc?export=download&"
44	    "id=0B7EVK8r0v71pd0FJY3Blby1HUTQ"
45	)
46	
47	# Attributes in the image (Eyeglasses, Mustache etc).
48	ATTR_DATA = (
49	    "https://drive.google.com/uc?export=download&"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/celeb_a/celeb_a_dataset_builder.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
48	ATTR_DATA = (
49	    "https://drive.google.com/uc?export=download&"
50	    "id=0B7EVK8r0v71pblRyaVFSWGxPY0U"
51	)
52	
53	IDENTITY_DATA = (
54	    "https://drive.google.com/uc?export=download&"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/celeb_a/celeb_a_dataset_builder.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	IDENTITY_DATA = (
54	    "https://drive.google.com/uc?export=download&"
55	    "id=1_ee_0u7vcNLOfNLegJRHmolfH5ICW-XS"
56	)
57	
58	LANDMARK_HEADINGS = (
59	    "lefteye_x lefteye_y righteye_x righteye_y "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/celeb_a/celeb_a_dataset_builder.py:81
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
80	  RELEASE_NOTES = {
81	      "2.0.1": "New split API (https://tensorflow.org/datasets/splits)",
82	      "2.1.0": "Identity feature added.",
83	  }
84	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/celeb_a/celeb_a_dataset_builder.py:96
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
95	        }),
96	        homepage="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html",
97	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/celeb_a/celeb_a_dataset_builder.py:159
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
158	    with epath.Path(file_path).open() as f:
159	      data_raw = f.read()
160	    lines = data_raw.split("\n")

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/celeb_a/celeb_a_dataset_builder.py:189
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
188	    with epath.Path(file_path).open() as f:
189	      data_raw = f.read()
190	    lines = data_raw.split("\n")

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/celeb_a_hq/celeb_a_hq_dataset_builder.py:42
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
41	        release_notes={
42	            "2.0.0": "New split API (https://tensorflow.org/datasets/splits)",
43	        },
44	        **kwargs,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/celeb_a_hq/celeb_a_hq_dataset_builder.py:53
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
52	
53	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
54	  manual_dir should contain multiple tar files with images (data2x2.tar,
55	  data4x4.tar .. data1024x1024.tar).
56	  Detailed instructions are here:
57	  https://github.com/tkarras/progressive_growing_of_gans#preparing-datasets-for-training
58	  """
59	
60	  BUILDER_CONFIGS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/celeb_a_hq/celeb_a_hq_dataset_builder.py:89
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
88	        ),
89	        homepage="https://github.com/tkarras/progressive_growing_of_gans",
90	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/cityscapes/cityscapes_dataset_builder.py:113
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
112	
113	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
114	  You have to download files from https://www.cityscapes-dataset.com/login/
115	  (This dataset requires registration).
116	  For basic config (semantic_segmentation) you must download
117	  'leftImg8bit_trainvaltest.zip' and 'gtFine_trainvaltest.zip'.
118	  Other configs do require additional files - please see code for more details.
119	  """
120	
121	  BUILDER_CONFIGS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/cityscapes/cityscapes_dataset_builder.py:187
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
186	        features=tfds.features.FeaturesDict(features),
187	        homepage='https://www.cityscapes-dataset.com',
188	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/clevr/clevr_dataset_builder.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_BASE_URL = "https://cs.stanford.edu/people/jcjohns/clevr/"
28	_DOWNLOAD_URL = "https://dl.fbaipublicfiles.com/clevr/CLEVR_v1.0.zip"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/clevr/clevr_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	_BASE_URL = "https://cs.stanford.edu/people/jcjohns/clevr/"
28	_DOWNLOAD_URL = "https://dl.fbaipublicfiles.com/clevr/CLEVR_v1.0.zip"
29	
30	
31	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/clic/clic_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	CLIC_MOBILE_TRAIN = (
24	    'https://data.vision.ee.ethz.ch/cvl/clic/mobile_train_2020.zip'
25	)
26	CLIC_PROFESSIONAL_TRAIN = (
27	    'https://data.vision.ee.ethz.ch/cvl/clic/professional_train_2020.zip'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/clic/clic_dataset_builder.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	CLIC_PROFESSIONAL_TRAIN = (
27	    'https://data.vision.ee.ethz.ch/cvl/clic/professional_train_2020.zip'
28	)
29	CLIC_MOBILE_VALIDATION = (
30	    'https://data.vision.ee.ethz.ch/cvl/clic/mobile_valid_2020.zip'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/clic/clic_dataset_builder.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	CLIC_MOBILE_VALIDATION = (
30	    'https://data.vision.ee.ethz.ch/cvl/clic/mobile_valid_2020.zip'
31	)
32	CLIC_PROFESSIONAL_VALIDATION = (
33	    'https://data.vision.ee.ethz.ch/cvl/clic/professional_valid_2020.zip'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/clic/clic_dataset_builder.py:33
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
32	CLIC_PROFESSIONAL_VALIDATION = (
33	    'https://data.vision.ee.ethz.ch/cvl/clic/professional_valid_2020.zip'
34	)
35	CLIC_MOBILE_TEST = (
36	    'https://data.vision.ee.ethz.ch/cvl/clic/test/CLIC2020Mobile_test.zip'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/clic/clic_dataset_builder.py:36
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
35	CLIC_MOBILE_TEST = (
36	    'https://data.vision.ee.ethz.ch/cvl/clic/test/CLIC2020Mobile_test.zip'
37	)
38	CLIC_PROFESSIONAL_TEST = (
39	    'https://data.vision.ee.ethz.ch/cvl/clic/test/CLIC2020Professional_test.zip'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/clic/clic_dataset_builder.py:39
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
38	CLIC_PROFESSIONAL_TEST = (
39	    'https://data.vision.ee.ethz.ch/cvl/clic/test/CLIC2020Professional_test.zip'
40	)
41	
42	
43	class Builder(tfds.core.GeneratorBasedBuilder):
44	  """CLIC dataset."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/clic/clic_dataset_builder.py:57
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
56	        ),
57	        homepage='https://www.compression.cc/',
58	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/coil100/coil100_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_URL = "http://www.cs.columbia.edu/CAVE/databases/SLAM_coil-20_coil-100/coil-100/coil-100.zip"
27	
28	_DESCRIPTION = """The dataset contains 7200 color images of 100 objects

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/coil100/coil100_dataset_builder.py:65
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
64	        homepage=(
65	            "http://www.cs.columbia.edu/CAVE/software/softlib/coil-100.php"
66	        ),
67	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/corr2cause/corr2cause_dataset_builder.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_URL_PATH = 'https://huggingface.co/datasets/causalnlp/corr2cause/'
24	
25	
26	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/corr2cause/corr2cause_dataset_builder.py:42
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
41	        supervised_keys=None,
42	        homepage='https://github.com/causalNLP/corr2cause/tree/main',
43	        license='https://github.com/causalNLP/corr2cause/blob/main/LICENSE',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/corr2cause/corr2cause_dataset_builder.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	        homepage='https://github.com/causalNLP/corr2cause/tree/main',
43	        license='https://github.com/causalNLP/corr2cause/blob/main/LICENSE',
44	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/databricks_dolly/databricks_dolly_dataset_builder.py:40
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
39	        }),
40	        homepage='https://github.com/databrickslabs/dolly',
41	        license='CC BY-SA 3.0',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/databricks_dolly/databricks_dolly_dataset_builder.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	        {
47	            'train': 'https://github.com/databrickslabs/dolly/raw/master/data/databricks-dolly-15k.jsonl'
48	        }
49	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/dices/dices_dataset_builder.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	_BASE_URL = (
27	    'https://raw.githubusercontent.com/google-research-datasets/'
28	    'dices-dataset/main/'
29	)
30	
31	
32	@dataclasses.dataclass
33	class DICESConfig(tfds.core.BuilderConfig):
34	  """Builder configuration for the DICES dataset."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/dices/dices_dataset_builder.py:339
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
338	        supervised_keys=None,
339	        homepage='https://github.com/google-research-datasets/dices-dataset',
340	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/div2k/div2k_dataset_builder.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_DL_URL = "https://data.vision.ee.ethz.ch/cvl/DIV2K/"
28	
29	_DL_URLS = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/dolma/dolma_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	_URL = (
25	    'https://huggingface.co/datasets/allenai/dolma/resolve/main/urls/v1_7.txt'
26	)
27	
28	
29	class Builder(tfds.core.GeneratorBasedBuilder):
30	  """DatasetBuilder for Dolma dataset."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/dolma/dolma_dataset_builder.py:48
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
47	        supervised_keys=None,
48	        homepage='https://github.com/allenai/dolma',
49	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/downsampled_imagenet/downsampled_imagenet_dataset_builder.py:20
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
19	
20	_DL_URL = "http://image-net.org/small/"
21	
22	_DATA_OPTIONS = ["32x32", "64x64"]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/downsampled_imagenet/downsampled_imagenet_dataset_builder.py:56
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
55	          release_notes={
56	              "2.0.0": "New split API (https://tensorflow.org/datasets/splits)",
57	          },
58	      )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/downsampled_imagenet/downsampled_imagenet_dataset_builder.py:70
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
69	        supervised_keys=None,
70	        homepage="http://image-net.org/small/download.php",
71	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/dsprites/dsprites_dataset_builder.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	_URL = (
22	    "https://github.com/deepmind/dsprites-dataset/blob/master/"
23	    "dsprites_ndarray_co1sh3sc6or40x32y32_64x64.hdf5?raw=true"
24	)
25	
26	
27	class Builder(tfds.core.GeneratorBasedBuilder):
28	  """dSprites data set."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/dsprites/dsprites_dataset_builder.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	  RELEASE_NOTES = {
35	      "2.0.0": "New split API (https://tensorflow.org/datasets/splits)",
36	  }
37	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/dsprites/dsprites_dataset_builder.py:56
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
55	        features=tfds.features.FeaturesDict(features_dict),
56	        homepage="https://github.com/deepmind/dsprites-dataset",
57	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/duke_ultrasound/duke_ultrasound_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	_DATA_URL = {
28	    'phantom_data': 'https://research.repository.duke.edu/downloads/vt150j912',
29	    'mark_data': 'https://research.repository.duke.edu/downloads/4x51hj56d',
30	}
31	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/duke_ultrasound/duke_ultrasound_dataset_builder.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	    'phantom_data': 'https://research.repository.duke.edu/downloads/vt150j912',
29	    'mark_data': 'https://research.repository.duke.edu/downloads/4x51hj56d',
30	}
31	
32	_DEFAULT_SPLITS = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/duke_ultrasound/duke_ultrasound_dataset_builder.py:33
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
32	_DEFAULT_SPLITS = {
33	    'train': 'https://research.repository.duke.edu/downloads/tt44pn391',
34	    'test': 'https://research.repository.duke.edu/downloads/zg64tm441',
35	    'validation': 'https://research.repository.duke.edu/downloads/dj52w535x',
36	    'MARK': 'https://research.repository.duke.edu/downloads/wd375w77v',
37	    'A': 'https://research.repository.duke.edu/downloads/nc580n18d',
38	    'B': 'https://research.repository.duke.edu/downloads/7h149q56p',
39	}
40	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/duke_ultrasound/duke_ultrasound_dataset_builder.py:34
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
33	    'train': 'https://research.repository.duke.edu/downloads/tt44pn391',
34	    'test': 'https://research.repository.duke.edu/downloads/zg64tm441',
35	    'validation': 'https://research.repository.duke.edu/downloads/dj52w535x',
36	    'MARK': 'https://research.repository.duke.edu/downloads/wd375w77v',
37	    'A': 'https://research.repository.duke.edu/downloads/nc580n18d',
38	    'B': 'https://research.repository.duke.edu/downloads/7h149q56p',
39	}
40	
41	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/duke_ultrasound/duke_ultrasound_dataset_builder.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	    'test': 'https://research.repository.duke.edu/downloads/zg64tm441',
35	    'validation': 'https://research.repository.duke.edu/downloads/dj52w535x',
36	    'MARK': 'https://research.repository.duke.edu/downloads/wd375w77v',
37	    'A': 'https://research.repository.duke.edu/downloads/nc580n18d',
38	    'B': 'https://research.repository.duke.edu/downloads/7h149q56p',
39	}
40	
41	
42	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/duke_ultrasound/duke_ultrasound_dataset_builder.py:36
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
35	    'validation': 'https://research.repository.duke.edu/downloads/dj52w535x',
36	    'MARK': 'https://research.repository.duke.edu/downloads/wd375w77v',
37	    'A': 'https://research.repository.duke.edu/downloads/nc580n18d',
38	    'B': 'https://research.repository.duke.edu/downloads/7h149q56p',
39	}
40	
41	
42	class Builder(tfds.core.GeneratorBasedBuilder):
43	  """DAS beamformed phantom images and paired post-processed images."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/duke_ultrasound/duke_ultrasound_dataset_builder.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	    'MARK': 'https://research.repository.duke.edu/downloads/wd375w77v',
37	    'A': 'https://research.repository.duke.edu/downloads/nc580n18d',
38	    'B': 'https://research.repository.duke.edu/downloads/7h149q56p',
39	}
40	
41	
42	class Builder(tfds.core.GeneratorBasedBuilder):
43	  """DAS beamformed phantom images and paired post-processed images."""
44	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/duke_ultrasound/duke_ultrasound_dataset_builder.py:38
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
37	    'A': 'https://research.repository.duke.edu/downloads/nc580n18d',
38	    'B': 'https://research.repository.duke.edu/downloads/7h149q56p',
39	}
40	
41	
42	class Builder(tfds.core.GeneratorBasedBuilder):
43	  """DAS beamformed phantom images and paired post-processed images."""
44	
45	  VERSION = tfds.core.Version('2.0.0')

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/duke_ultrasound/duke_ultrasound_dataset_builder.py:82
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
81	        supervised_keys=('das/dB', 'dtce'),
82	        homepage='https://github.com/ouwen/mimicknet',
83	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/e2e_cleaned/e2e_cleaned_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_HOMEPAGE_URL = 'https://github.com/tuetschek/e2e-cleaning'
27	
28	_TRAIN_URL = 'https://github.com/tuetschek/e2e-cleaning/raw/master/cleaned-data/train-fixed.no-ol.csv'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/e2e_cleaned/e2e_cleaned_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_TRAIN_URL = 'https://github.com/tuetschek/e2e-cleaning/raw/master/cleaned-data/train-fixed.no-ol.csv'
29	_DEV_URL = 'https://github.com/tuetschek/e2e-cleaning/raw/master/cleaned-data/devel-fixed.no-ol.csv'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/e2e_cleaned/e2e_cleaned_dataset_builder.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	_TRAIN_URL = 'https://github.com/tuetschek/e2e-cleaning/raw/master/cleaned-data/train-fixed.no-ol.csv'
29	_DEV_URL = 'https://github.com/tuetschek/e2e-cleaning/raw/master/cleaned-data/devel-fixed.no-ol.csv'
30	_TEST_URL = 'https://github.com/tuetschek/e2e-cleaning/raw/master/cleaned-data/test-fixed.csv'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/e2e_cleaned/e2e_cleaned_dataset_builder.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	_DEV_URL = 'https://github.com/tuetschek/e2e-cleaning/raw/master/cleaned-data/devel-fixed.no-ol.csv'
30	_TEST_URL = 'https://github.com/tuetschek/e2e-cleaning/raw/master/cleaned-data/test-fixed.csv'
31	
32	
33	def _get_table_from_mr(mr):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/efron_morris75/efron_morris75_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	URL = 'https://raw.githubusercontent.com/pymc-devs/pymc-examples/main/examples/data/efron-morris-75-data.tsv'
25	
26	
27	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/flic/flic_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_HOMEPAGE_URL = "https://bensapp.github.io/flic-dataset.html"
27	
28	_URL_SUBSET = "https://drive.google.com/uc?id=0B4K3PZp8xXDJN0Fpb0piVjQ3Y3M&export=download"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/flic/flic_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_URL_SUBSET = "https://drive.google.com/uc?id=0B4K3PZp8xXDJN0Fpb0piVjQ3Y3M&export=download"
29	_URL_SUPERSET = "https://drive.google.com/uc?id=0B4K3PZp8xXDJd2VwblhhOVBfMDg&export=download"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/flic/flic_dataset_builder.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	_URL_SUBSET = "https://drive.google.com/uc?id=0B4K3PZp8xXDJN0Fpb0piVjQ3Y3M&export=download"
29	_URL_SUPERSET = "https://drive.google.com/uc?id=0B4K3PZp8xXDJd2VwblhhOVBfMDg&export=download"
30	
31	
32	def _normalize_bbox(raw_bbox, img_path):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/groove/groove_dataset_builder.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	
54	_DOWNLOAD_URL = "https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0.zip"
55	_DOWNLOAD_URL_MIDI_ONLY = "https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0-midionly.zip"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/groove/groove_dataset_builder.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	_DOWNLOAD_URL = "https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0.zip"
55	_DOWNLOAD_URL_MIDI_ONLY = "https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0-midionly.zip"
56	
57	
58	class GrooveConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/groove/groove_dataset_builder.py:139
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
138	        features=tfds.features.FeaturesDict(features_dict),
139	        homepage="https://g.co/magenta/groove-dataset",
140	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/groove/groove_dataset_builder.py:174
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
173	      ) as midi_f:
174	        midi = midi_f.read()
175	      audio = None

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/groove/groove_dataset_builder.py:225
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
224	          midi_split = io.BytesIO()
225	          pm_split.write(midi_split)
226	          example["midi"] = midi_split.getvalue()

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012/imagenet2012_dataset_builder.py:67
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
66	      '5.1.0': 'Added test split.',
67	      '5.0.0': 'New split API (https://tensorflow.org/datasets/splits)',
68	      '4.0.0': '(unpublished)',
69	      '3.0.0': """
70	      Fix colorization on ~12 images (CMYK -> RGB).
71	      Fix format for consistency (convert the single png image to Jpeg).
72	      Faster generation reading directly from the archive.
73	      """,
74	      '2.0.1': 'Encoding fix. No changes from user point of view.',
75	      '2.0.0': 'Fix validation labels.',
76	  }
77	
78	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012/imagenet2012_dataset_builder.py:78
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
77	
78	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
79	  manual_dir should contain two files: ILSVRC2012_img_train.tar and
80	  ILSVRC2012_img_val.tar.
81	  You need to register on https://image-net.org/download-images in order
82	  to get the link to download the dataset.
83	  """
84	
85	  def _info(self):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012/imagenet2012_dataset_builder.py:94
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
93	        supervised_keys=('image', 'label'),
94	        homepage='https://image-net.org/',
95	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012/imagenet2012_dataset_builder.py:138
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
137	    if image_fname in CMYK_IMAGES:
138	      image = io.BytesIO(tfds.core.utils.jpeg_cmyk_to_rgb(image.read()))
139	    elif image_fname in PNG_IMAGES:

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012/imagenet2012_dataset_builder.py:140
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
139	    elif image_fname in PNG_IMAGES:
140	      image = io.BytesIO(tfds.core.utils.png_to_jpeg(image.read()))
141	    return image

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012/imagenet2012_dataset_builder.py:162
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
161	      # alternative, as this loads ~150MB in RAM.
162	      fobj_mem = io.BytesIO(fobj.read())
163	      for image_fname, image in tfds.download.iter_archive(

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012/imagenet_common.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
48	    # `splitlines` to remove trailing `\r` in Windows
49	    labels = labels_f.read().strip().splitlines()
50	  with tf.io.gfile.GFile(val_path, 'rb') as tar_f_obj:

--------------------------------------------------
>> Issue: [B834:open] tarfile.open
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012/imagenet_common.py:51
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b834_open.html
50	  with tf.io.gfile.GFile(val_path, 'rb') as tar_f_obj:
51	    tar = tarfile.open(mode='r:', fileobj=tar_f_obj)
52	    images = sorted(tar.getnames())

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012_corrupted/imagenet2012_corrupted_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_FROST_FILEBASE = 'https://raw.githubusercontent.com/hendrycks/robustness/master/ImageNet-C/imagenet_c/imagenet_c/frost'
29	_FROST_FILENAMES = [f'{_FROST_FILEBASE}/frost{i}.png' for i in range(1, 4)] + [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012_corrupted/imagenet2012_corrupted_dataset_builder.py:163
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
162	        supervised_keys=('image', 'label'),
163	        homepage='https://openreview.net/forum?id=HJz6tiCqYm',
164	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012_fewshot/imagenet2012_fewshot_dataset_builder.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	        supervised_keys=('image', 'label'),
55	        homepage='http://image-net.org/',
56	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012_multilabel/imagenet2012_multilabel_dataset_builder.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_MULTI_LABELS_URL = 'https://storage.googleapis.com/brain-car-datasets/imagenet-mistakes/human_accuracy_v3.0.0.json'
28	
29	
30	def _get_multi_labels_and_problematic_images(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012_multilabel/imagenet2012_multilabel_dataset_builder.py:74
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
73	
74	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
75	  manual_dir should contain `ILSVRC2012_img_val.tar` file.
76	  You need to register on http://www.image-net.org/download-images in order
77	  to get the link to download the dataset.
78	  """
79	
80	  def _info(self) -> tfds.core.DatasetInfo:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012_multilabel/imagenet2012_multilabel_dataset_builder.py:100
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
99	        supervised_keys=('image', 'correct_multi_labels'),
100	        homepage='https://github.com/modestyachts/evaluating_machine_accuracy_on_imagenet',
101	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012_real/imagenet2012_real_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_REAL_LABELS_URL = 'https://raw.githubusercontent.com/google-research/reassessed-imagenet/master/real.json'
27	
28	
29	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012_real/imagenet2012_real_dataset_builder.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	
37	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
38	  manual_dir should contain `ILSVRC2012_img_val.tar` file.
39	  You need to register on http://www.image-net.org/download-images in order
40	  to get the link to download the dataset.
41	  """
42	
43	  def _info(self):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012_real/imagenet2012_real_dataset_builder.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	        supervised_keys=('image', 'real_label'),
55	        homepage='https://github.com/google-research/reassessed-imagenet',
56	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012_subset/imagenet2012_subset_dataset_builder.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	SUBSET2FILES = {
29	    '1pct': 'https://raw.githubusercontent.com/google-research/simclr/master/imagenet_subsets/1percent.txt',
30	    '10pct': 'https://raw.githubusercontent.com/google-research/simclr/master/imagenet_subsets/10percent.txt',
31	}
32	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012_subset/imagenet2012_subset_dataset_builder.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	    '1pct': 'https://raw.githubusercontent.com/google-research/simclr/master/imagenet_subsets/1percent.txt',
30	    '10pct': 'https://raw.githubusercontent.com/google-research/simclr/master/imagenet_subsets/10percent.txt',
31	}
32	
33	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012_subset/imagenet2012_subset_dataset_builder.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	        supervised_keys=('image', 'label'),
55	        homepage='http://image-net.org/',
56	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012_subset/imagenet2012_subset_dataset_builder.py:79
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
78	    with epath.Path(subset_file).open() as fp:
79	      subset = set(fp.read().splitlines())  # remove trailing `\r` in Windows
80	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet2012_subset/imagenet2012_subset_dataset_builder.py:114
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
113	      # alternative, as this loads ~150MB in RAM.
114	      fobj_mem = io.BytesIO(fobj.read())
115	      for image_fname, image in tfds.download.iter_archive(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_a/imagenet_a_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_IMAGENET_A_URL = r'https://people.eecs.berkeley.edu/~hendrycks/imagenet-a.tar'
25	
26	
27	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_a/imagenet_a_dataset_builder.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	        # Homepage of the dataset for documentation
43	        homepage='https://github.com/hendrycks/natural-adv-examples',
44	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_lt/imagenet_lt_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	_TRAIN_SUBSET = (
26	    'https://drive.google.com/uc?export=download&'
27	    'id=1Sl1cwy6Dei1I1BMS1YKjI35fkaR1UA_7'
28	)
29	
30	_VAL_SUBSET = (
31	    'https://drive.google.com/uc?export=download&'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_lt/imagenet_lt_dataset_builder.py:31
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
30	_VAL_SUBSET = (
31	    'https://drive.google.com/uc?export=download&'
32	    'id=1AjYczW4khrQrwPIygXUkHF-Qv4QGEsCF'
33	)
34	
35	
36	class Builder(tfds.core.GeneratorBasedBuilder):
37	  """Long-tailed version of the ImageNet2012 dataset."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_lt/imagenet_lt_dataset_builder.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	
44	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
45	  manual_dir should contain two files: ILSVRC2012_img_train.tar and
46	  ILSVRC2012_img_val.tar.
47	  You need to register on http://www.image-net.org/download-images in order
48	  to get the link to download the dataset.
49	  """
50	
51	  def _info(self):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_lt/imagenet_lt_dataset_builder.py:61
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
60	        supervised_keys=('image', 'label'),
61	        homepage='https://github.com/zhmiao/OpenLongTailRecognition-OLTR',
62	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_lt/imagenet_lt_dataset_builder.py:98
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
97	    with tf.io.gfile.GFile(downloaded_dirs['train']) as fp:
98	      train_subset = fp.read().splitlines()
99	    train_subset = self._postprocess_subset_list(train_subset)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_lt/imagenet_lt_dataset_builder.py:102
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
101	    with tf.io.gfile.GFile(downloaded_dirs['validation']) as fp:
102	      val_subset = fp.read().splitlines()
103	    val_subset = self._postprocess_subset_list(val_subset)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_lt/imagenet_lt_dataset_builder.py:135
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
134	      # alternative, as this loads ~150MB in RAM.
135	      fobj_mem = io.BytesIO(fobj.read())
136	      for image_fname, image in tfds.download.iter_archive(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_pi/imagenet_pi_dataset_builder.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	
37	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
38	  manual_dir should contain two files: ILSVRC2012_img_train.tar and
39	  ILSVRC2012_img_val.tar.
40	  You need to register on http://www.image-net.org/download-images in order
41	  to get the link to download the dataset.
42	  """
43	
44	  def _info(self) -> tfds.core.DatasetInfo:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_pi/imagenet_pi_dataset_builder.py:60
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
59	        supervised_keys=('image', 'annotator_labels'),
60	        homepage='https://github.com/google-research-datasets/imagenet_pi/',
61	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_pi/imagenet_pi_dataset_builder.py:189
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
188	      label = fname[:-4]  # fname is something like 'n01632458.tar'
189	      fobj_mem = io.BytesIO(fobj.read())
190	      for image_fname, image in tfds.download.iter_archive(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_r/imagenet_r_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_IMAGENET_R_URL = r'https://people.eecs.berkeley.edu/~hendrycks/imagenet-r.tar'
25	
26	
27	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_r/imagenet_r_dataset_builder.py:52
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
51	        # Homepage of the dataset for documentation
52	        homepage='https://github.com/hendrycks/imagenet-r',
53	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_resized/imagenet_resized_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	_LABELS_FNAME = 'image_classification/imagenet_resized_labels.txt'
28	_URL_PREFIX = 'http://www.image-net.org/data/downsample/'
29	
30	
31	class ImagenetResizedConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_resized/imagenet_resized_dataset_builder.py:74
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
73	        supervised_keys=('image', 'label'),
74	        homepage='https://patrykchrabaszcz.github.io/Imagenet32/',
75	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_sketch/imagenet_sketch_dataset_builder.py:21
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
20	
21	_BASE_URL = 'https://github.com/HaohanWang/ImageNet-Sketch'
22	_IMAGENET_SKETCH_URL = 'https://drive.google.com/u/0/uc?id=1Mj0i5HBthqH1p_yeXzsg22gZduvgoNeA&export=download'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_sketch/imagenet_sketch_dataset_builder.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	_BASE_URL = 'https://github.com/HaohanWang/ImageNet-Sketch'
22	_IMAGENET_SKETCH_URL = 'https://drive.google.com/u/0/uc?id=1Mj0i5HBthqH1p_yeXzsg22gZduvgoNeA&export=download'
23	
24	
25	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_v2/imagenet_v2_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	
25	_ROOT_URL = 'https://huggingface.co/datasets/vaishaal/ImageNetV2/resolve/main'
26	_IMAGENET_V2_URLS = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenet_v2/imagenet_v2_dataset_builder.py:98
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
97	        # Homepage of the dataset for documentation
98	        homepage='https://github.com/modestyachts/ImageNetV2',
99	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenette/imagenette_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	_LABELS_FNAME = "image_classification/imagenette_labels.txt"
28	_URL_PREFIX = "https://s3.amazonaws.com/fast-ai-imageclas/"
29	
30	
31	class ImagenetteConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagenette/imagenette_dataset_builder.py:75
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
74	        supervised_keys=("image", "label"),
75	        homepage="https://github.com/fastai/imagenette",
76	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagewang/imagewang_dataset_builder.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	_LABELS_FNAME = "image_classification/imagewang_labels.txt"
29	_URL_PREFIX = "https://s3.amazonaws.com/fast-ai-imageclas"
30	_SIZES = ["full-size", "320px", "160px"]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imagewang/imagewang_dataset_builder.py:71
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
70	        supervised_keys=("image", "label"),
71	        homepage="https://github.com/fastai/imagenette",
72	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imdb_reviews/imdb_reviews_dataset_builder.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_DOWNLOAD_URL = "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
24	
25	
26	class IMDBReviewsConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imdb_reviews/imdb_reviews_dataset_builder.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	        release_notes={
41	            "1.0.0": "New split API (https://tensorflow.org/datasets/splits)",
42	        },
43	        **kwargs,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imdb_reviews/imdb_reviews_dataset_builder.py:69
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
68	        supervised_keys=("text", "label"),
69	        homepage="http://ai.stanford.edu/~amaas/data/sentiment/",
70	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/imdb_reviews/imdb_reviews_dataset_builder.py:123
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
122	        continue
123	      text = imdb_f.read().strip()
124	      label = res.groupdict()["label"] if labeled else -1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/irc_disentanglement/irc_disentanglement_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	_DOWNLOAD_URL = (
26	    "https://github.com/jkkummerfeld/irc-disentanglement/zipball/fd379e9"
27	)
28	_DOWNLOAD_ARCHIVE_SUBDIR = os.path.join(
29	    "jkkummerfeld-irc-disentanglement-fd379e9", "data"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/irc_disentanglement/irc_disentanglement_dataset_builder.py:158
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
157	        ),
158	        homepage="https://jkk.name/irc-disentanglement",
159	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/kddcup99/kddcup99_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	
25	_TRAIN_URL = 'http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz'
26	_TEST_URL = 'http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/kddcup99/kddcup99_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	_TRAIN_URL = 'http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz'
26	_TEST_URL = 'http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz'
27	
28	
29	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/kddcup99/kddcup99_dataset_builder.py:222
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
221	        supervised_keys=None,  # Set to `None` to disable
222	        homepage='https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html',
223	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/kitti/kitti_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	
25	_HOMEPAGE_URL = "http://www.cvlibs.net/datasets/kitti/"
26	_DATA_URL = "https://s3.eu-central-1.amazonaws.com/avg-kitti"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/kitti/kitti_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	_HOMEPAGE_URL = "http://www.cvlibs.net/datasets/kitti/"
26	_DATA_URL = "https://s3.eu-central-1.amazonaws.com/avg-kitti"
27	_IMAGES_FNAME = "data_object_image_2.zip"

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/kitti/kitti_dataset_builder.py:204
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
203	      img = cv2.imdecode(
204	          np.frombuffer(fobj.read(), dtype=np.uint8), cv2.IMREAD_COLOR
205	      )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/kitti/kitti_dataset_builder.py:304
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
303	      mapping_line_ids = [
304	          int(x.strip()) - 1 for x in fobj.read().decode("utf-8").split(",")
305	      ]

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/kitti/kitti_dataset_builder.py:307
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
306	    elif fpath == os.path.join("mapping", "train_mapping.txt"):
307	      mapping_lines = fobj.read().splitlines()
308	      mapping_lines = [x.decode("utf-8") for x in mapping_lines]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/lambada/lambada_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	_LAMBADA_DATASET_URL = (
24	    'https://zenodo.org/record/2630551/files/lambada-dataset.tar.gz?download=1'
25	)
26	
27	
28	class Builder(tfds.core.GeneratorBasedBuilder):
29	  """DatasetBuilder for lambada dataset."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/lambada/lambada_dataset_builder.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	        supervised_keys=None,
45	        homepage='https://zenodo.org/record/2630551#.X4Xzn5NKjUI',
46	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/lfw/lfw_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	# https://github.com/scikit-learn/scikit-learn/blob/98ed9dc73a86f5f11781a0e21f24c8f47979ec67/sklearn/datasets/_lfw.py#L36
25	_URL = "https://ndownloader.figshare.com/files/5976018"
26	
27	LFW_IMAGE_SHAPE = (250, 250, 3)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/lfw/lfw_dataset_builder.py:42
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
41	        supervised_keys=("label", "image"),
42	        homepage="http://vis-www.cs.umass.edu/lfw",
43	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/librispeech/librispeech_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_URL = "http://www.openslr.org/12"
29	_DL_URL = "http://www.openslr.org/resources/12/"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/librispeech/librispeech_dataset_builder.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	_URL = "http://www.openslr.org/12"
29	_DL_URL = "http://www.openslr.org/resources/12/"
30	_DL_URLS = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/librispeech_lm/librispeech_lm_dataset_builder.py:20
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
19	
20	_URL = 'http://www.openslr.org/11'
21	
22	_DL_URL = 'http://www.openslr.org/resources/11/librispeech-lm-norm.txt.gz'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/librispeech_lm/librispeech_lm_dataset_builder.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	
22	_DL_URL = 'http://www.openslr.org/resources/11/librispeech-lm-norm.txt.gz'
23	
24	
25	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/libritts/libritts_dataset_builder.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	
29	_URL = "http://www.openslr.org/60"
30	_DL_URL = "http://www.openslr.org/resources/60/"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/libritts/libritts_dataset_builder.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	_URL = "http://www.openslr.org/60"
30	_DL_URL = "http://www.openslr.org/resources/60/"
31	_DL_URLS = {

--------------------------------------------------
>> Issue: [B834:open] tarfile.open
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/libritts/libritts_dataset_builder.py:70
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b834_open.html
69	    with tf.io.gfile.GFile(archive_path, "rb") as f:
70	      tarf = tarfile.open(mode="r:gz", fileobj=f)
71	      speakers_tsv = tarf.extractfile("LibriTTS/speakers.tsv")

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/libritts/libritts_dataset_builder.py:132
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
131	      key = six.ensure_text(os.path.splitext(os.path.basename(path))[0])
132	      memfile = io.BytesIO(contents.read())
133	      example = {"speech": memfile}

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ljspeech/ljspeech_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_URL = "https://keithito.com/LJ-Speech-Dataset/"
27	_DL_URL = "https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ljspeech/ljspeech_dataset_builder.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	_URL = "https://keithito.com/LJ-Speech-Dataset/"
27	_DL_URL = "https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2"
28	
29	
30	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/lm1b/lm1b_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	_DOWNLOAD_URL = (
25	    "http://www.statmt.org/lm-benchmark/"
26	    "1-billion-word-language-modeling-benchmark-r13output.tar.gz"
27	)
28	_TOP_LEVEL_DIR = "1-billion-word-language-modeling-benchmark-r13output"
29	_TRAIN_FILE_FORMAT = os.path.join(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/lm1b/lm1b_dataset_builder.py:60
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
59	        supervised_keys=("text", "text"),
60	        homepage="http://www.statmt.org/lm-benchmark/",
61	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/lost_and_found/lost_and_found_dataset_builder.py:136
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
135	        # Homepage of the dataset for documentation
136	        homepage='http://www.6d-vision.com/lostandfounddataset',
137	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/lost_and_found/lost_and_found_dataset_builder.py:142
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
141	    base_url = (
142	        'http://www.dhbw-stuttgart.de/~sgehrig/lostAndFoundDataset/{}.zip'
143	    )
144	
145	    # For each feature, this is the name of the zipfile and
146	    # root-directory in the archive
147	    zip_file_names = {
148	        'image_left': self.builder_config.left_image_string,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/lvis/lvis_dataset_builder.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	    'train_annotation': (
30	        'https://dl.fbaipublicfiles.com/LVIS/lvis_v1_train.json.zip'
31	    ),
32	    'train_images': 'http://images.cocodataset.org/zips/train2017.zip',
33	    'validation_annotation': (
34	        'https://dl.fbaipublicfiles.com/LVIS/lvis_v1_val.json.zip'
35	    ),
36	    'validation_images': 'http://images.cocodataset.org/zips/val2017.zip',
37	    'test_annotation': 'https://dl.fbaipublicfiles.com/LVIS/lvis_v1_image_info_test_dev.json.zip',
38	    'test_images': 'http://images.cocodataset.org/zips/test2017.zip',
39	    # Minival from https://github.com/ashkamath/mdetr/blob/main/.github/lvis.md:
40	    'minival_annotation': (
41	        'https://nyu.box.com/shared/static/2yk9x8az9pnlsy2v8gd95yncwn2q7vj6.zip'
42	    ),
43	}
44	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/lvis/lvis_dataset_builder.py:32
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
31	    ),
32	    'train_images': 'http://images.cocodataset.org/zips/train2017.zip',
33	    'validation_annotation': (
34	        'https://dl.fbaipublicfiles.com/LVIS/lvis_v1_val.json.zip'
35	    ),
36	    'validation_images': 'http://images.cocodataset.org/zips/val2017.zip',
37	    'test_annotation': 'https://dl.fbaipublicfiles.com/LVIS/lvis_v1_image_info_test_dev.json.zip',
38	    'test_images': 'http://images.cocodataset.org/zips/test2017.zip',
39	    # Minival from https://github.com/ashkamath/mdetr/blob/main/.github/lvis.md:
40	    'minival_annotation': (
41	        'https://nyu.box.com/shared/static/2yk9x8az9pnlsy2v8gd95yncwn2q7vj6.zip'
42	    ),
43	}
44	
45	# Annotations with invalid bounding boxes. Will not be used.
46	_INVALID_ANNOTATIONS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/lvis/lvis_dataset_builder.py:34
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
33	    'validation_annotation': (
34	        'https://dl.fbaipublicfiles.com/LVIS/lvis_v1_val.json.zip'
35	    ),
36	    'validation_images': 'http://images.cocodataset.org/zips/val2017.zip',
37	    'test_annotation': 'https://dl.fbaipublicfiles.com/LVIS/lvis_v1_image_info_test_dev.json.zip',
38	    'test_images': 'http://images.cocodataset.org/zips/test2017.zip',
39	    # Minival from https://github.com/ashkamath/mdetr/blob/main/.github/lvis.md:
40	    'minival_annotation': (
41	        'https://nyu.box.com/shared/static/2yk9x8az9pnlsy2v8gd95yncwn2q7vj6.zip'
42	    ),
43	}
44	
45	# Annotations with invalid bounding boxes. Will not be used.
46	_INVALID_ANNOTATIONS = [
47	    # Train split.
48	    662101,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/lvis/lvis_dataset_builder.py:36
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
35	    ),
36	    'validation_images': 'http://images.cocodataset.org/zips/val2017.zip',
37	    'test_annotation': 'https://dl.fbaipublicfiles.com/LVIS/lvis_v1_image_info_test_dev.json.zip',
38	    'test_images': 'http://images.cocodataset.org/zips/test2017.zip',
39	    # Minival from https://github.com/ashkamath/mdetr/blob/main/.github/lvis.md:
40	    'minival_annotation': (
41	        'https://nyu.box.com/shared/static/2yk9x8az9pnlsy2v8gd95yncwn2q7vj6.zip'
42	    ),
43	}
44	
45	# Annotations with invalid bounding boxes. Will not be used.
46	_INVALID_ANNOTATIONS = [
47	    # Train split.
48	    662101,
49	    81217,
50	    462924,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/lvis/lvis_dataset_builder.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	    'validation_images': 'http://images.cocodataset.org/zips/val2017.zip',
37	    'test_annotation': 'https://dl.fbaipublicfiles.com/LVIS/lvis_v1_image_info_test_dev.json.zip',
38	    'test_images': 'http://images.cocodataset.org/zips/test2017.zip',
39	    # Minival from https://github.com/ashkamath/mdetr/blob/main/.github/lvis.md:
40	    'minival_annotation': (
41	        'https://nyu.box.com/shared/static/2yk9x8az9pnlsy2v8gd95yncwn2q7vj6.zip'
42	    ),
43	}
44	
45	# Annotations with invalid bounding boxes. Will not be used.
46	_INVALID_ANNOTATIONS = [
47	    # Train split.
48	    662101,
49	    81217,
50	    462924,
51	    227817,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/lvis/lvis_dataset_builder.py:38
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
37	    'test_annotation': 'https://dl.fbaipublicfiles.com/LVIS/lvis_v1_image_info_test_dev.json.zip',
38	    'test_images': 'http://images.cocodataset.org/zips/test2017.zip',
39	    # Minival from https://github.com/ashkamath/mdetr/blob/main/.github/lvis.md:
40	    'minival_annotation': (
41	        'https://nyu.box.com/shared/static/2yk9x8az9pnlsy2v8gd95yncwn2q7vj6.zip'
42	    ),
43	}
44	
45	# Annotations with invalid bounding boxes. Will not be used.
46	_INVALID_ANNOTATIONS = [
47	    # Train split.
48	    662101,
49	    81217,
50	    462924,
51	    227817,
52	    29381,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/lvis/lvis_dataset_builder.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	    'minival_annotation': (
41	        'https://nyu.box.com/shared/static/2yk9x8az9pnlsy2v8gd95yncwn2q7vj6.zip'
42	    ),
43	}
44	
45	# Annotations with invalid bounding boxes. Will not be used.
46	_INVALID_ANNOTATIONS = [
47	    # Train split.
48	    662101,
49	    81217,
50	    462924,
51	    227817,
52	    29381,
53	    601484,
54	    412185,
55	    504667,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/lvis/lvis_dataset_builder.py:107
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
106	        supervised_keys=None,
107	        homepage='https://www.lvisdataset.org/',
108	        description=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/lvis/lvis_dataset_builder.py:109
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
108	        description=(
109	            'LVIS: A dataset for large vocabulary instance'
110	            ' segmentation.\n\nOfficial splits: train, validation, and test.'
111	            ' The minival split was introduced by MDETR'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/malaria/malaria_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_URL = "https://data.lhncbc.nlm.nih.gov/public/Malaria/cell_images.zip"
25	
26	_DESCRIPTION = """The Malaria dataset contains a total of 27,558 cell images

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/malaria/malaria_dataset_builder.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
48	        supervised_keys=("image", "label"),
49	        homepage="https://lhncbc.nlm.nih.gov/publication/pub9932",
50	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/math_dataset/math_dataset_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	
25	_DATA_URL = "https://storage.googleapis.com/mathematics-dataset/mathematics_dataset-v1.0.tar.gz"
26	
27	_TRAIN_CATEGORY = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/math_dataset/math_dataset_dataset_builder.py:188
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
187	        supervised_keys=(_QUESTION, _ANSWER),
188	        homepage="https://github.com/deepmind/mathematics_dataset",
189	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/math_dataset/math_dataset_dataset_builder.py:197
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
196	        with epath.Path(data_file).open() as f:
197	          ls = f.read().split("\n")
198	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/math_qa/math_qa_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	
25	_URL = 'https://math-qa.github.io/data/MathQA.zip'
26	
27	_FEATURES = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/math_qa/math_qa_dataset_builder.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	        supervised_keys=None,
54	        homepage='https://math-qa.github.io/',
55	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/mctaco/mctaco_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	_SPLIT_DOWNLOAD_URL = {
28	    'validation': 'https://raw.githubusercontent.com/CogComp/MCTACO/master/dataset/dev_3783.tsv',
29	    'test': 'https://raw.githubusercontent.com/CogComp/MCTACO/master/dataset/test_9442.tsv',
30	}
31	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/mctaco/mctaco_dataset_builder.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	    'validation': 'https://raw.githubusercontent.com/CogComp/MCTACO/master/dataset/dev_3783.tsv',
29	    'test': 'https://raw.githubusercontent.com/CogComp/MCTACO/master/dataset/test_9442.tsv',
30	}
31	
32	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/mctaco/mctaco_dataset_builder.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
57	        supervised_keys=None,
58	        homepage='https://github.com/CogComp/MCTACO',
59	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/media_sum/media_sum_dataset_builder.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	
35	  MANUAL_DOWNLOAD_INSTRUCTIONS = """
36	  manual_dir should contain the files:
37	
38	    * news_dialogue.json
39	    * train_val_test_split.json
40	
41	  The files can be downloaded and extracted from the dataset's GitHub page:
42	  https://github.com/zcgzcgzcg1/MediaSum/tree/main/data
43	  """
44	
45	  def _info(self) -> tfds.core.DatasetInfo:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/media_sum/media_sum_dataset_builder.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
57	        supervised_keys=('utt', 'summary'),
58	        homepage='https://github.com/zcgzcgzcg1/MediaSum',
59	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/mlqa/mlqa_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_DOWNLOAD_URL = "https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip"
25	
26	
27	class MlqaConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/mlqa/mlqa_dataset_builder.py:61
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
60	        supervised_keys=None,
61	        homepage="https://github.com/facebookresearch/MLQA",
62	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/natural_instructions/natural_instructions_dataset_builder.py:21
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
20	
21	_DATASET_DIR = "https://raw.githubusercontent.com/allenai/natural-instructions/master/tasks"
22	
23	
24	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/natural_instructions/natural_instructions_dataset_builder.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
48	        supervised_keys=None,
49	        homepage="https://github.com/allenai/natural-instructions",
50	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/natural_questions/natural_questions_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_URL = 'https://ai.google.com/research/NaturalQuestions/dataset'
29	
30	_BASE_DOWNLOAD_URL = 'https://storage.googleapis.com/natural_questions/v1.0'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/natural_questions/natural_questions_dataset_builder.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	
30	_BASE_DOWNLOAD_URL = 'https://storage.googleapis.com/natural_questions/v1.0'
31	_DOWNLOAD_URLS = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/natural_questions_open/natural_questions_open_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_DOWNLOAD_URL_FMT = 'https://raw.githubusercontent.com/google-research-datasets/natural-questions/a7a113c9fdc2d9986d624bd43b8a18d6d5779eaa/nq_open/NQ-open.{split}.jsonl'
27	
28	_URL = 'https://github.com/google-research-datasets/natural-questions/tree/master/nq_open'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/natural_questions_open/natural_questions_open_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_URL = 'https://github.com/google-research-datasets/natural-questions/tree/master/nq_open'
29	
30	
31	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/newsroom/newsroom_dataset_builder.py:46
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
45	  VERSION = tfds.core.Version("1.0.0")
46	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
47	  You should download the dataset from https://summari.es/download/
48	  The webpage requires registration.
49	  After downloading, please put dev.jsonl, test.jsonl and train.jsonl
50	  files in the manual_dir.
51	  """
52	
53	  def _info(self):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/newsroom/newsroom_dataset_builder.py:67
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
66	        supervised_keys=(_DOCUMENT, _SUMMARY),
67	        homepage="https://summari.es",
68	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/nsynth/nsynth_dataset_builder.py:32
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
31	
32	_GANSYNTH_DESCRIPTION = """\
33	NSynth Dataset limited to acoustic instruments in the MIDI pitch interval
34	[24, 84]. Uses alternate splits that have overlap in instruments (but not exact
35	notes) between the train set and valid/test sets. This variant was originally
36	introduced in the ICLR 2019 GANSynth paper (https://arxiv.org/abs/1902.08710).
37	"""
38	
39	_F0_AND_LOUDNESS_ADDENDUM = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/nsynth/nsynth_dataset_builder.py:83
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
82	_BASE_DOWNLOAD_PATH = (
83	    "http://download.magenta.tensorflow.org/datasets/nsynth/nsynth-"
84	)
85	
86	_SPLITS = ["train", "valid", "test"]
87	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/nsynth/nsynth_dataset_builder.py:126
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
125	            "2.3.3": (
126	                "F0 computed with fix in CREPE wave normalization "
127	                "(https://github.com/marl/crepe/issues/49)."
128	            ),
129	            "2.3.2": "Use Audio feature.",
130	            "2.3.1": "F0 computed with normalization fix in CREPE.",
131	            "2.3.0": "New `loudness_db` feature in decibels (unormalized).",
132	        },
133	        **kwargs,
134	    )  # pytype: disable=wrong-arg-types  # gen-stub-imports

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/nsynth/nsynth_dataset_builder.py:184
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
183	        features=tfds.features.FeaturesDict(features),
184	        homepage="https://g.co/magenta/nsynth-dataset",
185	        metadata=tfds.core.BeamMetadataDict(

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/nsynth/nsynth_dataset_builder.py:204
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
203	    with tf.io.gfile.GFile(dl_paths["instrument_labels"]) as f:
204	      instrument_labels = f.read().strip().splitlines()
205	    self.info.features["instrument"]["label"].names = instrument_labels

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/nyu_depth_v2/nyu_depth_v2_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_URL = 'http://datasets.lids.mit.edu/fastdepth/data/nyudepthv2.tar.gz'
25	
26	
27	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/nyu_depth_v2/nyu_depth_v2_dataset_builder.py:39
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
38	        supervised_keys=('image', 'depth'),
39	        homepage='https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html',
40	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ogbg_molpcba/ogbg_molpcba_dataset_builder.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	# URL.
29	_OGB_URL = 'https://ogb.stanford.edu/docs/graphprop'
30	_DOWNLOAD_URL = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ogbg_molpcba/ogbg_molpcba_dataset_builder.py:31
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
30	_DOWNLOAD_URL = (
31	    'https://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/pcba.zip'
32	)
33	
34	# File containing the names of individual tasks.
35	_TASKS_FNAME = 'datasets/ogbg_molpcba/tasks.txt'
36	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/open_images_challenge2019_detection/open_images_challenge2019_detection_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_URL = "https://storage.googleapis.com/openimages/web/challenge2019.html"
29	
30	_GOOGLE_URL_PREFIX = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/open_images_challenge2019_detection/open_images_challenge2019_detection_dataset_builder.py:31
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
30	_GOOGLE_URL_PREFIX = (
31	    "https://storage.googleapis.com/openimages/challenge_2019/challenge-2019-"
32	)
33	_FIGURE_EIGHT_BASE_URL = (
34	    "https://datasets.figure-eight.com/figure_eight_datasets/open-images/"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/open_images_challenge2019_detection/open_images_challenge2019_detection_dataset_builder.py:34
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
33	_FIGURE_EIGHT_BASE_URL = (
34	    "https://datasets.figure-eight.com/figure_eight_datasets/open-images/"
35	)
36	_TRAIN_IMAGES_URLS = [
37	    "{}zip_files_copy/train_{:02d}.zip".format(_FIGURE_EIGHT_BASE_URL, n)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/open_images_v4/open_images_v4_dataset_builder.py:42
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
41	        tfds.download.Resource(  # pylint:disable=g-complex-comprehension
42	            url='http://open-images-dataset.s3.amazonaws.com/tar/train_%s.tar.gz'
43	            % i_,
44	            extract_method=tfds.download.ExtractMethod.GZIP,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/open_images_v4/open_images_v4_dataset_builder.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
48	    'test_images': tfds.download.Resource(
49	        url='http://open-images-dataset.s3.amazonaws.com/tar/test.tar.gz',
50	        extract_method=tfds.download.ExtractMethod.GZIP,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/open_images_v4/open_images_v4_dataset_builder.py:53
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
52	    'validation_images': tfds.download.Resource(
53	        url='http://open-images-dataset.s3.amazonaws.com/tar/validation.tar.gz',
54	        extract_method=tfds.download.ExtractMethod.GZIP,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/open_images_v4/open_images_v4_dataset_builder.py:56
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
55	    ),
56	    'train_human_labels': 'https://storage.googleapis.com/openimages/2018_04/train/train-annotations-human-imagelabels.csv',
57	    'train_machine_labels': 'https://storage.googleapis.com/openimages/2018_04/train/train-annotations-machine-imagelabels.csv',
58	    'test_human_labels': 'https://storage.googleapis.com/openimages/2018_04/test/test-annotations-human-imagelabels.csv',
59	    'test_machine_labels': 'https://storage.googleapis.com/openimages/2018_04/test/test-annotations-machine-imagelabels.csv',
60	    'validation_human_labels': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-human-imagelabels.csv',
61	    'validation_machine_labels': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-machine-imagelabels.csv',
62	    'train-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/train/train-annotations-bbox.csv',
63	    'test-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/test/test-annotations-bbox.csv',
64	    'validation-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-bbox.csv',
65	}
66	# pylint: enable=line-too-long
67	
68	_Object = collections.namedtuple('Object', ['label', 'confidence', 'source'])
69	_Bbox = collections.namedtuple(
70	    'Bbox',
71	    [
72	        'label',
73	        'source',
74	        'bbox',
75	        'is_occluded',
76	        'is_truncated',
77	        'is_group_of',
78	        'is_depiction',
79	        'is_inside',
80	    ],
81	)
82	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/open_images_v4/open_images_v4_dataset_builder.py:57
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
56	    'train_human_labels': 'https://storage.googleapis.com/openimages/2018_04/train/train-annotations-human-imagelabels.csv',
57	    'train_machine_labels': 'https://storage.googleapis.com/openimages/2018_04/train/train-annotations-machine-imagelabels.csv',
58	    'test_human_labels': 'https://storage.googleapis.com/openimages/2018_04/test/test-annotations-human-imagelabels.csv',
59	    'test_machine_labels': 'https://storage.googleapis.com/openimages/2018_04/test/test-annotations-machine-imagelabels.csv',
60	    'validation_human_labels': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-human-imagelabels.csv',
61	    'validation_machine_labels': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-machine-imagelabels.csv',
62	    'train-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/train/train-annotations-bbox.csv',
63	    'test-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/test/test-annotations-bbox.csv',
64	    'validation-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-bbox.csv',
65	}
66	# pylint: enable=line-too-long
67	
68	_Object = collections.namedtuple('Object', ['label', 'confidence', 'source'])
69	_Bbox = collections.namedtuple(
70	    'Bbox',
71	    [
72	        'label',
73	        'source',
74	        'bbox',
75	        'is_occluded',
76	        'is_truncated',
77	        'is_group_of',
78	        'is_depiction',
79	        'is_inside',
80	    ],
81	)
82	
83	IMAGE_LEVEL_SOURCES = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/open_images_v4/open_images_v4_dataset_builder.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
57	    'train_machine_labels': 'https://storage.googleapis.com/openimages/2018_04/train/train-annotations-machine-imagelabels.csv',
58	    'test_human_labels': 'https://storage.googleapis.com/openimages/2018_04/test/test-annotations-human-imagelabels.csv',
59	    'test_machine_labels': 'https://storage.googleapis.com/openimages/2018_04/test/test-annotations-machine-imagelabels.csv',
60	    'validation_human_labels': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-human-imagelabels.csv',
61	    'validation_machine_labels': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-machine-imagelabels.csv',
62	    'train-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/train/train-annotations-bbox.csv',
63	    'test-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/test/test-annotations-bbox.csv',
64	    'validation-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-bbox.csv',
65	}
66	# pylint: enable=line-too-long
67	
68	_Object = collections.namedtuple('Object', ['label', 'confidence', 'source'])
69	_Bbox = collections.namedtuple(
70	    'Bbox',
71	    [
72	        'label',
73	        'source',
74	        'bbox',
75	        'is_occluded',
76	        'is_truncated',
77	        'is_group_of',
78	        'is_depiction',
79	        'is_inside',
80	    ],
81	)
82	
83	IMAGE_LEVEL_SOURCES = [
84	    'verification',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/open_images_v4/open_images_v4_dataset_builder.py:59
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
58	    'test_human_labels': 'https://storage.googleapis.com/openimages/2018_04/test/test-annotations-human-imagelabels.csv',
59	    'test_machine_labels': 'https://storage.googleapis.com/openimages/2018_04/test/test-annotations-machine-imagelabels.csv',
60	    'validation_human_labels': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-human-imagelabels.csv',
61	    'validation_machine_labels': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-machine-imagelabels.csv',
62	    'train-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/train/train-annotations-bbox.csv',
63	    'test-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/test/test-annotations-bbox.csv',
64	    'validation-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-bbox.csv',
65	}
66	# pylint: enable=line-too-long
67	
68	_Object = collections.namedtuple('Object', ['label', 'confidence', 'source'])
69	_Bbox = collections.namedtuple(
70	    'Bbox',
71	    [
72	        'label',
73	        'source',
74	        'bbox',
75	        'is_occluded',
76	        'is_truncated',
77	        'is_group_of',
78	        'is_depiction',
79	        'is_inside',
80	    ],
81	)
82	
83	IMAGE_LEVEL_SOURCES = [
84	    'verification',
85	    'crowdsource-verification',  # human labels

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/open_images_v4/open_images_v4_dataset_builder.py:60
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
59	    'test_machine_labels': 'https://storage.googleapis.com/openimages/2018_04/test/test-annotations-machine-imagelabels.csv',
60	    'validation_human_labels': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-human-imagelabels.csv',
61	    'validation_machine_labels': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-machine-imagelabels.csv',
62	    'train-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/train/train-annotations-bbox.csv',
63	    'test-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/test/test-annotations-bbox.csv',
64	    'validation-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-bbox.csv',
65	}
66	# pylint: enable=line-too-long
67	
68	_Object = collections.namedtuple('Object', ['label', 'confidence', 'source'])
69	_Bbox = collections.namedtuple(
70	    'Bbox',
71	    [
72	        'label',
73	        'source',
74	        'bbox',
75	        'is_occluded',
76	        'is_truncated',
77	        'is_group_of',
78	        'is_depiction',
79	        'is_inside',
80	    ],
81	)
82	
83	IMAGE_LEVEL_SOURCES = [
84	    'verification',
85	    'crowdsource-verification',  # human labels
86	    'machine',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/open_images_v4/open_images_v4_dataset_builder.py:61
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
60	    'validation_human_labels': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-human-imagelabels.csv',
61	    'validation_machine_labels': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-machine-imagelabels.csv',
62	    'train-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/train/train-annotations-bbox.csv',
63	    'test-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/test/test-annotations-bbox.csv',
64	    'validation-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-bbox.csv',
65	}
66	# pylint: enable=line-too-long
67	
68	_Object = collections.namedtuple('Object', ['label', 'confidence', 'source'])
69	_Bbox = collections.namedtuple(
70	    'Bbox',
71	    [
72	        'label',
73	        'source',
74	        'bbox',
75	        'is_occluded',
76	        'is_truncated',
77	        'is_group_of',
78	        'is_depiction',
79	        'is_inside',
80	    ],
81	)
82	
83	IMAGE_LEVEL_SOURCES = [
84	    'verification',
85	    'crowdsource-verification',  # human labels
86	    'machine',
87	]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/open_images_v4/open_images_v4_dataset_builder.py:62
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
61	    'validation_machine_labels': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-machine-imagelabels.csv',
62	    'train-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/train/train-annotations-bbox.csv',
63	    'test-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/test/test-annotations-bbox.csv',
64	    'validation-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-bbox.csv',
65	}
66	# pylint: enable=line-too-long
67	
68	_Object = collections.namedtuple('Object', ['label', 'confidence', 'source'])
69	_Bbox = collections.namedtuple(
70	    'Bbox',
71	    [
72	        'label',
73	        'source',
74	        'bbox',
75	        'is_occluded',
76	        'is_truncated',
77	        'is_group_of',
78	        'is_depiction',
79	        'is_inside',
80	    ],
81	)
82	
83	IMAGE_LEVEL_SOURCES = [
84	    'verification',
85	    'crowdsource-verification',  # human labels
86	    'machine',
87	]
88	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/open_images_v4/open_images_v4_dataset_builder.py:63
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
62	    'train-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/train/train-annotations-bbox.csv',
63	    'test-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/test/test-annotations-bbox.csv',
64	    'validation-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-bbox.csv',
65	}
66	# pylint: enable=line-too-long
67	
68	_Object = collections.namedtuple('Object', ['label', 'confidence', 'source'])
69	_Bbox = collections.namedtuple(
70	    'Bbox',
71	    [
72	        'label',
73	        'source',
74	        'bbox',
75	        'is_occluded',
76	        'is_truncated',
77	        'is_group_of',
78	        'is_depiction',
79	        'is_inside',
80	    ],
81	)
82	
83	IMAGE_LEVEL_SOURCES = [
84	    'verification',
85	    'crowdsource-verification',  # human labels
86	    'machine',
87	]
88	
89	BBOX_SOURCES = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/open_images_v4/open_images_v4_dataset_builder.py:64
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
63	    'test-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/test/test-annotations-bbox.csv',
64	    'validation-annotations-bbox': 'https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-bbox.csv',
65	}
66	# pylint: enable=line-too-long
67	
68	_Object = collections.namedtuple('Object', ['label', 'confidence', 'source'])
69	_Bbox = collections.namedtuple(
70	    'Bbox',
71	    [
72	        'label',
73	        'source',
74	        'bbox',
75	        'is_occluded',
76	        'is_truncated',
77	        'is_group_of',
78	        'is_depiction',
79	        'is_inside',
80	    ],
81	)
82	
83	IMAGE_LEVEL_SOURCES = [
84	    'verification',
85	    'crowdsource-verification',  # human labels
86	    'machine',
87	]
88	
89	BBOX_SOURCES = [
90	    'freeform',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/open_images_v4/open_images_v4_dataset_builder.py:109
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
108	    kwargs['release_notes'] = {
109	        '2.0.0': 'New split API (https://tensorflow.org/datasets/splits)',
110	    }
111	    super(OpenImagesV4Config, self).__init__(**kwargs)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/open_images_v4/open_images_v4_dataset_builder.py:189
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
188	        }),
189	        homepage='https://storage.googleapis.com/openimages/web/index.html',
190	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/open_images_v4/open_images_v4_dataset_builder.py:295
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
294	  image = cv2.imdecode(
295	      np.frombuffer(image_fobj.read(), dtype=np.uint8), flags=3
296	  )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/openbookqa/openbookqa_dataset_builder.py:52
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
51	        # Homepage of the dataset for documentation
52	        homepage='https://leaderboard.allenai.org/open_book_qa/submissions/get-started',
53	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/openbookqa/openbookqa_dataset_builder.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
57	
58	    download_url = 'https://s3-us-west-2.amazonaws.com/ai2-website/data/OpenBookQA-V1-Sep2018.zip'
59	    dl_dir = dl_manager.download_and_extract(download_url)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opinion_abstracts/opinion_abstracts_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_URL = "https://web.eecs.umich.edu/~wangluxy/datasets/opinion_abstracts.zip"
29	
30	
31	class OpinionAbstractsConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opinion_abstracts/opinion_abstracts_dataset_builder.py:92
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
91	        supervised_keys=(config.opinions_key, config.summary_key),
92	        homepage="https://web.eecs.umich.edu/~wangluxy/data.html",
93	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opinosis/opinosis_dataset_builder.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_URL = "https://github.com/kavgan/opinosis-summarization/raw/master/OpinosisDataset1.0_0.zip"
24	
25	_REVIEW_SENTS = "review_sents"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opinosis/opinosis_dataset_builder.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	        supervised_keys=(_REVIEW_SENTS, _SUMMARIES),
41	        homepage="http://kavita-ganesan.com/opinosis/",
42	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opinosis/opinosis_dataset_builder.py:62
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
61	      with tf.io.gfile.GFile(file_path, "rb") as src_f:
62	        input_data = src_f.read()
63	      summaries_path = os.path.join(path, "summaries-gold", topic_name)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opinosis/opinosis_dataset_builder.py:68
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
67	        with tf.io.gfile.GFile(file_path, "rb") as tgt_f:
68	          data = tgt_f.read().strip()
69	          summary_lst.append(data)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:62
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
61	            ),
62	            homepage="http://opus.nlpl.eu/EMEA.php",
63	            url="http://opus.nlpl.eu/download.php?f=EMEA/v3/moses/",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:63
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
62	            homepage="http://opus.nlpl.eu/EMEA.php",
63	            url="http://opus.nlpl.eu/download.php?f=EMEA/v3/moses/",
64	            languages=[

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:96
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
95	            ),
96	            homepage="http://opus.nlpl.eu/JRC-Acquis.php",
97	            url="http://opus.nlpl.eu/download.php?f=JRC-Acquis/",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:97
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
96	            homepage="http://opus.nlpl.eu/JRC-Acquis.php",
97	            url="http://opus.nlpl.eu/download.php?f=JRC-Acquis/",
98	            languages=[

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:129
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
128	            ),
129	            homepage="http://opus.nlpl.eu/Tanzil.php",
130	            url="http://opus.nlpl.eu/download.php?f=Tanzil/v1/moses/",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:130
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
129	            homepage="http://opus.nlpl.eu/Tanzil.php",
130	            url="http://opus.nlpl.eu/download.php?f=Tanzil/v1/moses/",
131	            languages=[

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:179
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
178	            description=(
179	                "A parallel corpus of GNOME localization files. Source:"
180	                " https://l10n.gnome.org"
181	            ),
182	            homepage="http://opus.nlpl.eu/GNOME.php",
183	            url="http://opus.nlpl.eu/download.php?f=GNOME/v1/moses/",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:182
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
181	            ),
182	            homepage="http://opus.nlpl.eu/GNOME.php",
183	            url="http://opus.nlpl.eu/download.php?f=GNOME/v1/moses/",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:183
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
182	            homepage="http://opus.nlpl.eu/GNOME.php",
183	            url="http://opus.nlpl.eu/download.php?f=GNOME/v1/moses/",
184	            languages=[

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:377
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
376	            description="A parallel corpus of KDE4 localization files (v.2).",
377	            homepage="http://opus.nlpl.eu/KDE4.php",
378	            url="http://opus.nlpl.eu/download.php?f=KDE4/v2/moses/",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:378
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
377	            homepage="http://opus.nlpl.eu/KDE4.php",
378	            url="http://opus.nlpl.eu/download.php?f=KDE4/v2/moses/",
379	            languages=[

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:477
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
476	            description=(
477	                "A parallel corpus originally extracted from"
478	                " http://se.php.net/download-docs.php."
479	            ),
480	            homepage="http://opus.nlpl.eu/PHP.php",
481	            url="http://opus.nlpl.eu/download.php?f=PHP/v1/moses/",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:480
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
479	            ),
480	            homepage="http://opus.nlpl.eu/PHP.php",
481	            url="http://opus.nlpl.eu/download.php?f=PHP/v1/moses/",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:481
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
480	            homepage="http://opus.nlpl.eu/PHP.php",
481	            url="http://opus.nlpl.eu/download.php?f=PHP/v1/moses/",
482	            languages=[

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:511
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
510	            description=(
511	                "A parallel corpus of Ubuntu localization files. Source:"
512	                " https://translations.launchpad.net"
513	            ),
514	            homepage="http://opus.nlpl.eu/Ubuntu.php",
515	            url="http://opus.nlpl.eu/download.php?f=Ubuntu/v14.10/moses/",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:514
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
513	            ),
514	            homepage="http://opus.nlpl.eu/Ubuntu.php",
515	            url="http://opus.nlpl.eu/download.php?f=Ubuntu/v14.10/moses/",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:515
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
514	            homepage="http://opus.nlpl.eu/Ubuntu.php",
515	            url="http://opus.nlpl.eu/download.php?f=Ubuntu/v14.10/moses/",
516	            languages=[

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:766
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
765	            description=(
766	                "A collection of documents from http://www.openoffice.org/."
767	            ),
768	            homepage="http://opus.nlpl.eu/OpenOffice-v2.php",
769	            url="http://opus.nlpl.eu/download.php?f=OpenOffice/v2/moses/",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:768
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
767	            ),
768	            homepage="http://opus.nlpl.eu/OpenOffice-v2.php",
769	            url="http://opus.nlpl.eu/download.php?f=OpenOffice/v2/moses/",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:769
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
768	            homepage="http://opus.nlpl.eu/OpenOffice-v2.php",
769	            url="http://opus.nlpl.eu/download.php?f=OpenOffice/v2/moses/",
770	            languages=["de", "en", "es", "fr", "jp", "sv"],

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:775
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
774	            description=(
775	                "A new collection of translated movie subtitles from"
776	                " http://www.opensubtitles.org/"
777	            ),
778	            homepage="http://opus.nlpl.eu/OpenSubtitles-v2018.php",
779	            url="http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/moses/",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:778
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
777	            ),
778	            homepage="http://opus.nlpl.eu/OpenSubtitles-v2018.php",
779	            url="http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/moses/",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:779
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
778	            homepage="http://opus.nlpl.eu/OpenSubtitles-v2018.php",
779	            url="http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/moses/",
780	            languages=[

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/opus/opus_dataset_builder.py:925
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
924	        supervised_keys=(src, target),
925	        homepage="http://opus.nlpl.eu/",
926	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/oxford_flowers102/oxford_flowers102_dataset_builder.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_BASE_URL = "https://www.robots.ox.ac.uk/~vgg/data/flowers/102/"
24	
25	_NAMES = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/oxford_iiit_pet/oxford_iiit_pet_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	
25	_BASE_URL = "https://thor.robots.ox.ac.uk/~vgg/data/pets"
26	
27	_LABEL_CLASSES = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/oxford_iiit_pet/oxford_iiit_pet_dataset_builder.py:132
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
131	        supervised_keys=("image", "label"),
132	        homepage="http://www.robots.ox.ac.uk/~vgg/data/pets/",
133	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/oxford_iiit_pet/oxford_iiit_pet_dataset_builder.py:179
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
178	        with epath.Path(image_path).open("rb") as image_file:
179	          img_data = image_file.read()
180	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/para_crawl/para_crawl_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_BENCHMARK_URL = "https://paracrawl.eu/releases.html"
25	
26	_BASE_DATA_URL_FORMAT_STR = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/para_crawl/para_crawl_dataset_builder.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	_BASE_DATA_URL_FORMAT_STR = (
27	    "https://s3.amazonaws.com/web-language-models/"
28	    "paracrawl/release4/en-{target_lang}.bicleaner07."
29	    "txt.gz"
30	)
31	
32	
33	@utils.memoize()
34	def _target_languages():
35	  """Create the sorted dictionary of language codes, and language names.

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/pass/pass_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	        tfds.download.Resource(  # pylint:disable=g-complex-comprehension
26	            url='https://zenodo.org/record/6615455/files/PASS.%s.tar' % i_,
27	            extract_method=tfds.download.ExtractMethod.TAR,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/pass/pass_dataset_builder.py:32
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
31	    'meta_data': tfds.download.Resource(
32	        url='https://zenodo.org/record/6615455/files/pass_metadata.csv'
33	    ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/pass/pass_dataset_builder.py:74
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
73	        supervised_keys=None,
74	        homepage='https://www.robots.ox.ac.uk/~vgg/data/pass/',
75	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/patch_camelyon/patch_camelyon_dataset_builder.py:20
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
19	
20	_URL = 'https://patchcamelyon.grand-challenge.org/'
21	
22	
23	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/patch_camelyon/patch_camelyon_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	  RELEASE_NOTES = {
28	      '2.0.0': 'New split API (https://tensorflow.org/datasets/splits)',
29	  }
30	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/patch_camelyon/patch_camelyon_dataset_builder.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	  def _split_generators(self, dl_manager):
45	    base_url = 'https://zenodo.org/record/2546921/files/'
46	    resources = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/paws_wiki/paws_wiki_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	
25	_HOMEPAGE_URL = "https://github.com/google-research-datasets/paws"
26	_LABELED_FINAL = "labeled_final"  # default subset

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/paws_wiki/paws_wiki_dataset_builder.py:31
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
30	_DOWNLOAD_URLS = {
31	    _LABELED_FINAL: "https://storage.googleapis.com/paws/english/paws_wiki_labeled_final.tar.gz",
32	    _LABELED_SWAP: "https://storage.googleapis.com/paws/english/paws_wiki_labeled_swap.tar.gz",
33	    _UNLABELED_FINAL: "https://storage.googleapis.com/paws/english/paws_wiki_unlabeled_final.tar.gz",
34	    _RAW_MAPPING: "https://storage.googleapis.com/paws/english/wiki_raw_and_mapping.tar.gz",
35	}
36	_EXTRACTED_FOLDERS = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/paws_wiki/paws_wiki_dataset_builder.py:32
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
31	    _LABELED_FINAL: "https://storage.googleapis.com/paws/english/paws_wiki_labeled_final.tar.gz",
32	    _LABELED_SWAP: "https://storage.googleapis.com/paws/english/paws_wiki_labeled_swap.tar.gz",
33	    _UNLABELED_FINAL: "https://storage.googleapis.com/paws/english/paws_wiki_unlabeled_final.tar.gz",
34	    _RAW_MAPPING: "https://storage.googleapis.com/paws/english/wiki_raw_and_mapping.tar.gz",
35	}
36	_EXTRACTED_FOLDERS = {
37	    _LABELED_FINAL: "final",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/paws_wiki/paws_wiki_dataset_builder.py:33
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
32	    _LABELED_SWAP: "https://storage.googleapis.com/paws/english/paws_wiki_labeled_swap.tar.gz",
33	    _UNLABELED_FINAL: "https://storage.googleapis.com/paws/english/paws_wiki_unlabeled_final.tar.gz",
34	    _RAW_MAPPING: "https://storage.googleapis.com/paws/english/wiki_raw_and_mapping.tar.gz",
35	}
36	_EXTRACTED_FOLDERS = {
37	    _LABELED_FINAL: "final",
38	    _LABELED_SWAP: "swap",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/paws_wiki/paws_wiki_dataset_builder.py:34
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
33	    _UNLABELED_FINAL: "https://storage.googleapis.com/paws/english/paws_wiki_unlabeled_final.tar.gz",
34	    _RAW_MAPPING: "https://storage.googleapis.com/paws/english/wiki_raw_and_mapping.tar.gz",
35	}
36	_EXTRACTED_FOLDERS = {
37	    _LABELED_FINAL: "final",
38	    _LABELED_SWAP: "swap",
39	    _UNLABELED_FINAL: "unlabeled/final",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/paws_x_wiki/paws_x_wiki_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	_HOMEPAGE_URL = (
25	    "https://github.com/google-research-datasets/paws/tree/master/pawsx"
26	)
27	_DOWNLOAD_URL = "https://storage.googleapis.com/paws/pawsx/x-final.tar.gz"
28	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/paws_x_wiki/paws_x_wiki_dataset_builder.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	)
27	_DOWNLOAD_URL = "https://storage.googleapis.com/paws/pawsx/x-final.tar.gz"
28	
29	_CLASS_LABELS = ["different_meaning", "paraphrase"]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/penguins/penguins_dataset_builder.py:31
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
30	_PENGUINS_PATH = (
31	    'https://storage.googleapis.com/download.tensorflow.org/'
32	    'data/palmer_penguins/'
33	)
34	
35	if typing.TYPE_CHECKING:
36	  FeatureType = Union[type_utils.TfdsDType, tfds.core.features.FeatureConnector]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/penguins/penguins_dataset_builder.py:185
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
184	        supervised_keys=supervised_keys,
185	        homepage='https://allisonhorst.github.io/palmerpenguins/',
186	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/pet_finder/pet_finder_dataset_builder.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_URL = "https://storage.googleapis.com/petfinder_dataset/"
28	_DATA_OPTIONS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/pet_finder/pet_finder_dataset_builder.py:90
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
89	        supervised_keys=("attributes", "label"),
90	        homepage="https://www.kaggle.com/c/petfinder-adoption-prediction/data",
91	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/pg19/pg19_dataset_builder.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	        supervised_keys=None,
44	        homepage='https://github.com/deepmind/pg19',
45	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/pg19/pg19_dataset_builder.py:53
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
52	    metadata_path = os.path.join(_DATA_DIR, 'metadata.csv')
53	    metadata = tf.io.gfile.GFile(metadata_path).read().splitlines()
54	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/pg19/pg19_dataset_builder.py:91
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
90	      with tf.io.gfile.GFile(path, 'r') as f:
91	        text = f.read().strip()
92	        yield book_id, {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/piqa/piqa_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_PIQA_URL = "https://storage.googleapis.com/ai2-mosaic/public/physicaliqa/physicaliqa-train-dev.zip"
25	
26	
27	def _read_json(json_path):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/piqa/piqa_dataset_builder.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	        }),
50	        homepage="https://leaderboard.allenai.org/physicaliqa/submissions/get-started",
51	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/places365_small/places365_small_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_BASE_URL = "http://data.csail.mit.edu/places/places365/"
25	_TRAIN_URL = "train_256_places365standard.tar"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/places365_small/places365_small_dataset_builder.py:52
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
51	        supervised_keys=("image", "label", "filename"),
52	        homepage="http://places2.csail.mit.edu/",
53	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/placesfull/placesfull_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_BASE_URL = "http://data.csail.mit.edu/places/places365/"
25	_TRAIN_URL = "Images256.tar"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/placesfull/placesfull_dataset_builder.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	        supervised_keys=("image", "label", "filename"),
47	        homepage="http://places2.csail.mit.edu/",
48	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/plant_leaves/plant_leaves_dataset_builder.py:48
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
47	
48	_DOWNLOAD_URL = "https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/hb74ynkjcn-1.zip"
49	
50	
51	class DownloadRetryLimitReachedError(Exception):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/plant_leaves/plant_leaves_dataset_builder.py:73
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
72	        supervised_keys=("image", "label"),
73	        homepage="https://data.mendeley.com/datasets/hb74ynkjcn/1",
74	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/plant_village/plant_village_dataset_builder.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_URL = "https://data.mendeley.com/public-files/datasets/tywbtsjrjv/files/d5652a28-c1d8-4b76-97f3-72fb80f94efc/file_downloaded"
24	_LABELS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/plant_village/plant_village_dataset_builder.py:79
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
78	        supervised_keys=("image", "label"),
79	        homepage="https://arxiv.org/abs/1511.08060",
80	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/plantae_k/plantae_k_dataset_builder.py:67
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
66	        supervised_keys=("image", "label"),
67	        homepage="https://data.mendeley.com/datasets/t6j2h22jpx/1",
68	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/pneumonia_mnist/pneumonia_mnist_dataset_builder.py:39
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
38	        supervised_keys=('image', 'label'),
39	        homepage='https://medmnist.com//',
40	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/pneumonia_mnist/pneumonia_mnist_dataset_builder.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	    npz_path = dl_manager.download(
45	        'https://zenodo.org/records/10519652/files/pneumoniamnist.npz'
46	    )
47	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/protein_net/protein_net_dataset_builder.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	_ExampleIterator = Iterator[Tuple[str, _Example]]
30	_PROTEINNET_HOMEPAGE = 'https://github.com/aqlaboratory/proteinnet'
31	
32	_LINES_PER_ENTRY = 33

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/protein_net/protein_net_dataset_builder.py:96
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
95	  URL = (
96	      'https://sharehost.hms.harvard.edu/sysbio/alquraishi/proteinnet'
97	      '/human_readable/'
98	  )
99	  FILES = {
100	      'casp7': 'casp7.tar.gz',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/qa4mre/qa4mre_dataset_builder.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_BASE_URL = 'http://nlp.uned.es/clef-qa/repository/js/scripts/downloadFile.php?file=/var/www/html/nlp/clef-qa/repository/resources/QA4MRE/'
28	
29	PATHS = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/qa4mre/qa4mre_dataset_builder.py:242
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
241	        supervised_keys=None,
242	        homepage='http://nlp.uned.es/clef-qa/repository/pastCampaigns.php',
243	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/qasc/qasc_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_DOWNLOAD_URL = "http://data.allenai.org/downloads/qasc/qasc_dataset.tar.gz"
25	_HOMEPAGE_URL = "https://allenai.org/data/qasc"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/qasc/qasc_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	_DOWNLOAD_URL = "http://data.allenai.org/downloads/qasc/qasc_dataset.tar.gz"
25	_HOMEPAGE_URL = "https://allenai.org/data/qasc"
26	
27	
28	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/qm9/qm9_dataset_builder.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	
37	_HOMEPAGE = 'https://doi.org/10.6084/m9.figshare.c.978904.v5'
38	
39	_ATOMREF_URL = 'https://figshare.com/ndownloader/files/3195395'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/qm9/qm9_dataset_builder.py:39
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
38	
39	_ATOMREF_URL = 'https://figshare.com/ndownloader/files/3195395'
40	_UNCHARACTERIZED_URL = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/qm9/qm9_dataset_builder.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	_UNCHARACTERIZED_URL = (
41	    'https://springernature.figshare.com/ndownloader/files/3195404'
42	)
43	_MOLECULES_URL = 'https://springernature.figshare.com/ndownloader/files/3195389'
44	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/qm9/qm9_dataset_builder.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	)
43	_MOLECULES_URL = 'https://springernature.figshare.com/ndownloader/files/3195389'
44	
45	_SIZE = 133_885

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/qm9/qm9_dataset_builder.py:207
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
206	          description=(
207	              'Dataset split used by Cormorant. 100,000 train,'
208	              ' 17,748 validation, and 13,083 test samples.'
209	              ' Splitting happens after shuffling with seed 0.'
210	              ' Paper: https://arxiv.org/abs/1906.04015.'
211	              ' Split:'
212	              ' https://github.com/risilab/cormorant/blob/master/src/cormorant/data/prepare/qm9.py'
213	          ),
214	          permutation_seed=0,
215	          train_size=100_000,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/qm9/qm9_dataset_builder.py:221
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
220	          description=(
221	              'Dataset split used by DimeNet. 110,000 train, 10,000 validation,'
222	              ' and 10,831 test samples.'
223	              ' Splitting happens after shuffling with seed 42.'
224	              ' Paper: https://arxiv.org/abs/2003.03123.'
225	              ' Split:'
226	              ' https://github.com/gasteigerjo/dimenet/blob/master/dimenet/training/data_provider.py'
227	          ),
228	          permutation_seed=42,
229	          train_size=110_000,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/quac/quac_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_DATA_URL = "https://s3.amazonaws.com/my89public/quac/"
27	
28	_MODULE = "QuAC"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/quac/quac_dataset_builder.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
57	        }),
58	        homepage="https://quac.ai/",
59	        supervised_keys=("context", "answers"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/quality/quality_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	_DOWNLOAD_URL = (
24	    'https://github.com/nyu-mll/quality/raw/main/data/QuALITY.v0.9.zip'
25	)
26	
27	# Fields that are straight text copies from raw example to processed example.
28	_ONE2ONE_FIELDS = (
29	    'article',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/quality/quality_dataset_builder.py:84
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
83	        supervised_keys=None,
84	        homepage='https://github.com/nyu-mll/quality',
85	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/quickdraw_bitmap/quickdraw_bitmap_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	_QUICKDRAW_BASE_URL = (
26	    "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap"  # pylint: disable=line-too-long
27	)
28	_QUICKDRAW_LABELS_FNAME = "datasets/quickdraw_bitmap/labels.txt"
29	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/quickdraw_bitmap/quickdraw_bitmap_dataset_builder.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	
30	_URL = "https://github.com/googlecreativelab/quickdraw-dataset"
31	
32	
33	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/quickdraw_bitmap/quickdraw_bitmap_dataset_builder.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	  RELEASE_NOTES = {
43	      "3.0.0": "New split API (https://tensorflow.org/datasets/splits)",
44	  }
45	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/race/race_dataset_builder.py:57
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
56	        supervised_keys=None,  # Set to `None` to disable
57	        homepage="https://www.cs.cmu.edu/~glai1/data/race/",
58	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/race/race_dataset_builder.py:63
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
62	    path = dl_manager.download_and_extract(
63	        "http://www.cs.cmu.edu/~glai1/data/race/RACE.tar.gz"
64	    )
65	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/radon/radon_dataset_builder.py:31
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
30	
31	BASE_URL = 'http://www.stat.columbia.edu/~gelman/arm/examples/radon/'
32	
33	
34	def convert_to_int(d):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/radon/radon_dataset_builder.py:92
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
91	        supervised_keys=('features', 'activity'),
92	        homepage='http://www.stat.columbia.edu/~gelman/arm/examples/radon/',
93	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/real_toxicity_prompts/real_toxicity_prompts_dataset_builder.py:59
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
58	        supervised_keys=None,
59	        homepage="https://github.com/allenai/real-toxicity-prompts",
60	        license="https://github.com/allenai/real-toxicity-prompts/blob/master/LICENSE",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/real_toxicity_prompts/real_toxicity_prompts_dataset_builder.py:60
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
59	        homepage="https://github.com/allenai/real-toxicity-prompts",
60	        license="https://github.com/allenai/real-toxicity-prompts/blob/master/LICENSE",
61	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/real_toxicity_prompts/real_toxicity_prompts_dataset_builder.py:66
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
65	    path = dl_manager.download_and_extract(
66	        "https://ai2-public-datasets.s3.amazonaws.com/realtoxicityprompts/realtoxicityprompts-data.tar.gz"
67	    )
68	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/reddit/reddit_dataset_builder.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_URL = "https://zenodo.org/record/1043504/files/corpus-webis-tldr-17.zip?download=1"
28	
29	_DOCUMENT = "content"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/reddit/reddit_dataset_builder.py:52
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
51	        supervised_keys=(_DOCUMENT, _SUMMARY),
52	        homepage="https://github.com/webis-de/webis-tldr-17-corpus",
53	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/reddit_disentanglement/reddit_disentanglement_dataset_builder.py:77
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
76	  VERSION = tfds.core.Version("2.0.0")
77	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
78	  Download https://github.com/henghuiz/MaskedHierarchicalTransformer, decompress
79	  raw_data.zip and run generate_dataset.py with your reddit api credentials.
80	  Then put train.csv, val.csv and test.csv from the output directory into the
81	  manual folder.
82	  """
83	
84	  def _info(self):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/reddit_disentanglement/reddit_disentanglement_dataset_builder.py:100
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
99	        ),
100	        homepage="https://github.com/henghuiz/MaskedHierarchicalTransformer",
101	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/reddit_tifu/reddit_tifu_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_URL = "https://drive.google.com/uc?export=download&id=1ffWfITKFMJeqjT8loC8aiCLRNJpc_XnF"
25	_LONG_SPLIT = "https://storage.googleapis.com/tfds-data/downloads/reddit_tifu/reddit_tifu_long_splits.json"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/reddit_tifu/reddit_tifu_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	_URL = "https://drive.google.com/uc?export=download&id=1ffWfITKFMJeqjT8loC8aiCLRNJpc_XnF"
25	_LONG_SPLIT = "https://storage.googleapis.com/tfds-data/downloads/reddit_tifu/reddit_tifu_long_splits.json"
26	
27	_DOCUMENT = "documents"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/reddit_tifu/reddit_tifu_dataset_builder.py:56
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
55	      "1.1.1": (
56	          "Add train, dev and test (80/10/10) splits which are used in "
57	          "PEGASUS (https://arxiv.org/abs/1912.08777) in a separate config. "
58	          "These were created randomly using the tfds split function and are "
59	          "being released to ensure that results on Reddit Tifu Long are "
60	          "reproducible and comparable."
61	          "Also add `id` to the datapoints."
62	      ),
63	      "1.1.2": "Corrected splits uploaded.",
64	  }
65	  BUILDER_CONFIGS = [
66	      RedditTifuConfig(
67	          name="short",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/reddit_tifu/reddit_tifu_dataset_builder.py:94
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
93	        supervised_keys=(_DOCUMENT, self.builder_config.summary_key),
94	        homepage="https://github.com/ctr4si/MMN",
95	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ref_coco/ref_coco_dataset_builder.py:159
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
158	
159	  MANUAL_DOWNLOAD_INSTRUCTIONS = """
160	  1. Follow the instructions in https://github.com/lichengunc/refer and
161	  download the annotations and the images, matching the data/ directory
162	  specified in the repo.
163	
164	  2. Follow the instructions of PythonAPI in
165	  https://github.com/cocodataset/cocoapi to get pycocotools and the
166	  instances_train2014 annotations file from https://cocodataset.org/#download
167	
168	  3. Add both refer.py from (1) and pycocotools from (2) to your PYTHONPATH.
169	
170	  4. Run manual_download_process.py to generate refcoco.json, replacing
171	  `ref_data_root`, `coco_annotations_file`, and `out_file` with the values
172	  corresponding to where you have downloaded / want to save these files.
173	  Note that manual_download_process.py can be found in the TFDS repository.
174	
175	  5. Download the COCO training set from https://cocodataset.org/#download
176	  and stick it into a folder called `coco_train2014/`. Move `refcoco.json`
177	  to the same level as `coco_train2014`.
178	
179	  6. Follow the standard manual download instructions.
180	  """
181	
182	  BUILDER_CONFIGS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ref_coco/ref_coco_dataset_builder.py:192
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
191	    return self.dataset_info_from_configs(
192	        homepage='https://github.com/lichengunc/refer',
193	        features=tfds.features.FeaturesDict({

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/resisc45/resisc45_dataset_builder.py:71
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
70	
71	_URL = 'http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.html'
72	
73	
74	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/resisc45/resisc45_dataset_builder.py:79
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
78	
79	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
80	  Dataset can be downloaded from OneDrive:
81	  https://1drv.ms/u/s!AmgKYzARBl5ca3HNaHIlzp_IXjs
82	  After downloading the rar file, please extract it to the manual_dir.
83	  """
84	
85	  def _info(self):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/robonet/robonet_dataset_builder.py:33
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
32	DATA_URL_SAMPLE = (
33	    'https://drive.google.com/uc?export=download&'
34	    'id=1YX2TgT8IKSn9V4wGCwdzbRnS53yicV2P'
35	)
36	DATA_URL = (
37	    'https://drive.google.com/uc?export=download&'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/robonet/robonet_dataset_builder.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	DATA_URL = (
37	    'https://drive.google.com/uc?export=download&'
38	    'id=1BkqHzfRkfzgzCfc73NbNnPMK_rg3i1n9'
39	)
40	
41	STATES_DIM = 5
42	ACTIONS_DIM = 5

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/robonet/robonet_dataset_builder.py:148
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
147	        features=features,
148	        homepage='https://www.robonet.wiki/',
149	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/rock_paper_scissors/rock_paper_scissors_dataset_builder.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	_TRAIN_URL = (
22	    "https://storage.googleapis.com/download.tensorflow.org/data/rps.zip"
23	)
24	_TEST_URL = "https://storage.googleapis.com/download.tensorflow.org/data/rps-test-set.zip"
25	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/rock_paper_scissors/rock_paper_scissors_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	)
24	_TEST_URL = "https://storage.googleapis.com/download.tensorflow.org/data/rps-test-set.zip"
25	
26	_IMAGE_SIZE = 300

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/rock_paper_scissors/rock_paper_scissors_dataset_builder.py:39
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
38	  RELEASE_NOTES = {
39	      "3.0.0": "New split API (https://tensorflow.org/datasets/splits)",
40	  }
41	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/rock_paper_scissors/rock_paper_scissors_dataset_builder.py:51
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
50	        supervised_keys=("image", "label"),
51	        homepage="http://laurencemoroney.com/rock-paper-scissors-dataset",
52	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/rock_you/rock_you_dataset_builder.py:21
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
20	
21	_DOWNLOAD_URL = "https://github.com/brannondorsey/naive-hashcat/releases/download/data/rockyou.txt"
22	
23	
24	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/rock_you/rock_you_dataset_builder.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	        supervised_keys=None,
37	        homepage="https://wiki.skullsecurity.org/Passwords",
38	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/s3o4d/s3o4d_dataset_builder.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	        supervised_keys=None,
44	        homepage='https://github.com/deepmind/deepmind-research/tree/master/geomancer#stanford-3d-objects-for-disentangling-s3o4d',
45	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/s3o4d/s3o4d_dataset_builder.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
57	                '_'.join([a, b, c]),
58	                f'https://storage.googleapis.com/dm_s3o4d/{a}/{b}_{suffices[c]}',
59	            )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/samsum/samsum_dataset_builder.py:34
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
33	  VERSION = tfds.core.Version("1.0.0")
34	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
35	  Download https://arxiv.org/src/1911.12237v2/anc/corpus.7z, decompress and
36	  place train.json, val.json and test.json in the manual follder.
37	  """
38	
39	  def _info(self) -> tfds.core.DatasetInfo:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/samsum/samsum_dataset_builder.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	        supervised_keys=(_DOCUMENT, _SUMMARY),
47	        homepage="https://arxiv.org/src/1911.12237v2/anc",
48	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/savee/savee_dataset_builder.py:119
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
118	
119	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
120	  manual_dir should contain the file AudioData.zip. This file should be under
121	  Data/Zip/AudioData.zip in the dataset folder provided upon registration.
122	  You need to register at
123	  http://personal.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/Register.html in
124	  order to get the link to download the dataset.
125	  """
126	
127	  def _info(self):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/savee/savee_dataset_builder.py:135
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
134	        supervised_keys=('audio', 'label'),
135	        homepage='http://kahlan.eps.surrey.ac.uk/savee/',
136	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/scan/scan_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_DATA_URL = 'https://github.com/brendenlake/SCAN/archive/master.zip'
25	_MCD_SPLITS_URL = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/scan/scan_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	_MCD_SPLITS_URL = (
26	    'https://storage.googleapis.com/cfq_dataset/scan-splits.tar.gz'
27	)
28	
29	
30	class ScanConfig(tfds.core.BuilderConfig):
31	  """BuilderConfig for SCAN.

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/scan/scan_dataset_builder.py:98
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
97	        supervised_keys=(_COMMANDS, _ACTIONS),
98	        homepage='https://github.com/brendenlake/SCAN',
99	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/scene_parse150/scene_parse150_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	    "images": (
25	        "http://sceneparsing.csail.mit.edu/data/ChallengeData2017/images.tar"
26	    ),
27	    "annotations": "http://sceneparsing.csail.mit.edu/data/ChallengeData2017/annotations_instance.tar",
28	}
29	
30	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/scene_parse150/scene_parse150_dataset_builder.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	    ),
27	    "annotations": "http://sceneparsing.csail.mit.edu/data/ChallengeData2017/annotations_instance.tar",
28	}
29	
30	
31	class Builder(tfds.core.GeneratorBasedBuilder):
32	  """MIT Scene Parsing Benchmark dataset."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/scene_parse150/scene_parse150_dataset_builder.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	        supervised_keys=("image", "annotation"),
43	        homepage="http://sceneparsing.csail.mit.edu/",
44	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/schema_guided_dialogue/schema_guided_dialogue_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	_DATA_URL = (
26	    "https://github.com/google-research-datasets/dstc8-schema-guided-dialogue"
27	)
28	
29	
30	class Builder(tfds.core.GeneratorBasedBuilder):
31	  """DatasetBuilder for schema_guided_dialogue dataset."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/sci_tail/sci_tail_dataset_builder.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_URL = 'http://data.allenai.org.s3.amazonaws.com/downloads/SciTailV1.1.zip'
24	_SCITAIL_DIR = 'SciTailV1.1'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/sci_tail/sci_tail_dataset_builder.py:51
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
50	        supervised_keys=None,  # Set to `None` to disable
51	        homepage='https://allenai.org/data/scitail',
52	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/scicite/scicite_dataset_builder.py:68
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
67	        # Homepage of the dataset for documentation
68	        homepage="https://github.com/allenai/scicite",
69	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/scicite/scicite_dataset_builder.py:75
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
74	        {
75	            "scicite": "https://s3-us-west-2.amazonaws.com/ai2-s2-research/scicite/scicite.tar.gz",
76	        }
77	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/scientific_papers/scientific_papers_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	_URLS = {
28	    "arxiv": "https://drive.google.com/uc?id=1b3rmCSIoh6VhD4HKWjI4HOW-cSwcwbeC&export=download",
29	    "pubmed": "https://drive.google.com/uc?id=1lvsqvsFi3W-pE1SqNZI0s8NR9rC1tsja&export=download",
30	}
31	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/scientific_papers/scientific_papers_dataset_builder.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	    "arxiv": "https://drive.google.com/uc?id=1b3rmCSIoh6VhD4HKWjI4HOW-cSwcwbeC&export=download",
29	    "pubmed": "https://drive.google.com/uc?id=1lvsqvsFi3W-pE1SqNZI0s8NR9rC1tsja&export=download",
30	}
31	
32	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/scientific_papers/scientific_papers_dataset_builder.py:72
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
71	        supervised_keys=(_DOCUMENT, _SUMMARY),
72	        homepage="https://github.com/armancohan/long-summarization",
73	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/segment_anything/segment_anything_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_HOMEPAGE_URL = 'https://ai.facebook.com/datasets/segment-anything-downloads'
27	_LOCAL_LINKS_FILE_NAME = 'segment_anything_links.txt'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/sentiment140/sentiment140_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_DOWNLOAD_URL = "http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip"
29	_HOMEPAGE_URL = "http://help.sentiment140.com/home"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/sentiment140/sentiment140_dataset_builder.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	_DOWNLOAD_URL = "http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip"
29	_HOMEPAGE_URL = "http://help.sentiment140.com/home"
30	
31	
32	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/shapes3d/shapes3d_dataset_builder.py:21
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
20	
21	_URL = "https://storage.googleapis.com/3d-shapes/3dshapes.h5"
22	
23	
24	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/shapes3d/shapes3d_dataset_builder.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	  RELEASE_NOTES = {
29	      "2.0.0": "New split API (https://tensorflow.org/datasets/splits)",
30	  }
31	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/shapes3d/shapes3d_dataset_builder.py:53
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
52	        }),
53	        homepage="https://github.com/deepmind/3d-shapes",
54	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/sift1m/sift1m_dataset_builder.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_URL = 'http://ann-benchmarks.com/sift-128-euclidean.hdf5'
24	
25	
26	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/sift1m/sift1m_dataset_builder.py:57
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
56	        }),
57	        homepage='http://corpus-texmex.irisa.fr/',
58	        disable_shuffling=True,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/simpte/simpte_dataset_builder.py:94
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
93	        ),
94	        homepage='https://rdrr.io/cran/uplift/man/sim_pte.html',
95	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/siscore/siscore_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_BASE_URL = "https://s3.us-east-1.amazonaws.com/si-score-dataset"
29	
30	_VARIANT_EXPANDED_DIR_NAMES = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/siscore/siscore_dataset_builder.py:78
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
77	        # Homepage of the dataset for documentation
78	        homepage="https://github.com/google-research/si-score",
79	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/smallnorb/smallnorb_dataset_builder.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	_TRAINING_URL_TEMPLATE = (
23	    "https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/"
24	    "smallnorb-5x46789x9x18x6x2x96x96-training-{type}.mat.gz"
25	)
26	_TESTING_URL_TEMPLATE = (
27	    "https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/smallnorb/smallnorb_dataset_builder.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	_TESTING_URL_TEMPLATE = (
27	    "https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/"
28	    "smallnorb-5x01235x9x18x6x2x96x96-testing-{type}.mat.gz"
29	)
30	
31	
32	class Builder(tfds.core.GeneratorBasedBuilder):
33	  """Smallnorb data set."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/smallnorb/smallnorb_dataset_builder.py:40
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
39	  RELEASE_NOTES = {
40	      "2.0.0": "New split API (https://tensorflow.org/datasets/splits)",
41	  }
42	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/smallnorb/smallnorb_dataset_builder.py:65
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
64	        features=tfds.features.FeaturesDict(features_dict),
65	        homepage="https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/",
66	        supervised_keys=("image", "label_category"),

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/smart_buildings/controller_reader.py:190
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
189	        # Read size as a varint
190	        size_bytes = f.read(4)
191	        if not size_bytes:

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/smart_buildings/controller_reader.py:196
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
195	        # Read serialized data of the protobuf
196	        serialized_data = f.read(size)
197	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/smart_buildings/controller_reader.py:214
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
213	        # Read size as a varint
214	        size_bytes = f.read(4)
215	        if not size_bytes:

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/smart_buildings/controller_reader.py:220
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
219	        # Read serialized data of the protobuf
220	        serialized_data = f.read(size)
221	        variable = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/smart_buildings/smart_buildings_dataset_builder.py:166
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
165	        }),
166	        homepage='https://github.com/google/sbsim',
167	        disable_shuffling=True,  # our dataset needs to be in order

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/smart_buildings/smart_buildings_dataset_builder.py:178
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
177	    path_by_year: dict[int, epath.Path] = dl_manager.download_and_extract({
178	        year: f'https://storage.googleapis.com/gresearch/smart_buildings_dataset/{building_upper}/{building_upper}_{year}.zip'
179	        for year in YEARS

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/smartwatch_gestures/smartwatch_gestures_dataset_builder.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	        supervised_keys=('features', 'gesture'),
54	        homepage='https://tev.fbk.eu/resources/smartwatch',
55	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/smartwatch_gestures/smartwatch_gestures_dataset_builder.py:60
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
59	    path = dl_manager.download_and_extract(
60	        'https://drive.google.com/uc?export=download&'
61	        'id=1nEs-JlAQv6xpuSIqahTKK68TgK37GirP'
62	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/snli/snli_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_DATA_URL = 'https://nlp.stanford.edu/projects/snli/snli_1.0.zip'
25	
26	
27	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/snli/snli_dataset_builder.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	        supervised_keys=None,
44	        homepage='https://nlp.stanford.edu/projects/snli/',
45	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/so2sat/so2sat_dataset_builder.py:67
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
66	        release_notes={
67	            '2.0.0': 'New split API (https://tensorflow.org/datasets/splits)',
68	            '2.1.0': 'Using updated optical channels calibration factor.',
69	        },
70	        **kwargs,  # pytype: disable=wrong-arg-types  # gen-stub-imports

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/so2sat/so2sat_dataset_builder.py:112
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
111	        supervised_keys=supervised_keys,
112	        homepage='http://doi.org/10.14459/2018MP1454690',
113	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/speech_commands/speech_commands_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	_DOWNLOAD_PATH = (
25	    'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'
26	)
27	_TEST_DOWNLOAD_PATH_ = (
28	    'http://download.tensorflow.org/data/speech_commands_test_set_v0.02.tar.gz'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/speech_commands/speech_commands_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	_TEST_DOWNLOAD_PATH_ = (
28	    'http://download.tensorflow.org/data/speech_commands_test_set_v0.02.tar.gz'
29	)
30	
31	_SPLITS = ['train', 'valid', 'test']
32	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/speech_commands/speech_commands_dataset_builder.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
57	        # Homepage of the dataset for documentation
58	        homepage='https://arxiv.org/abs/1804.03209',
59	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/speech_commands/speech_commands_dataset_builder.py:154
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
153	      if 'testing_list.txt' in path:
154	        train_test_paths = file_obj.read().strip().splitlines()
155	        train_test_paths = [p.decode('ascii') for p in train_test_paths]

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/speech_commands/speech_commands_dataset_builder.py:157
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
156	      elif 'validation_list.txt' in path:
157	        validation_paths = file_obj.read().strip().splitlines()
158	        validation_paths = [p.decode('ascii') for p in validation_paths]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/spoken_digit/spoken_digit_dataset_builder.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_DOWNLOAD_URL = "https://github.com/Jakobovski/free-spoken-digit-dataset/archive/v1.0.9.tar.gz"
24	_HOMEPAGE_URL = "https://github.com/Jakobovski/free-spoken-digit-dataset"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/spoken_digit/spoken_digit_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	_DOWNLOAD_URL = "https://github.com/Jakobovski/free-spoken-digit-dataset/archive/v1.0.9.tar.gz"
24	_HOMEPAGE_URL = "https://github.com/Jakobovski/free-spoken-digit-dataset"
25	
26	
27	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/squad/squad_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_URL = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
29	_HOMEPAGE_URL = "https://rajpurkar.github.io/SQuAD-explorer/"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/squad/squad_dataset_builder.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	_URL = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
29	_HOMEPAGE_URL = "https://rajpurkar.github.io/SQuAD-explorer/"
30	
31	
32	def _v2_features():

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/stanford_dogs/stanford_dogs_dataset_builder.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_URL = "http://vision.stanford.edu/aditya86/ImageNetDogs/main.html"
27	
28	_IMAGES_URL = "http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/stanford_dogs/stanford_dogs_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_IMAGES_URL = "http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar"
29	_SPLIT_URL = "http://vision.stanford.edu/aditya86/ImageNetDogs/lists.tar"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/stanford_dogs/stanford_dogs_dataset_builder.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	_IMAGES_URL = "http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar"
29	_SPLIT_URL = "http://vision.stanford.edu/aditya86/ImageNetDogs/lists.tar"
30	_ANNOTATIONS_URL = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/stanford_dogs/stanford_dogs_dataset_builder.py:31
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
30	_ANNOTATIONS_URL = (
31	    "http://vision.stanford.edu/aditya86/ImageNetDogs/annotation.tar"
32	)
33	_NAME_RE = re.compile(r"([\w-]*[/\\])*([\w]*.jpg)$")
34	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/stanford_online_products/stanford_online_products_dataset_builder.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	    return self.dataset_info_from_configs(
50	        homepage="http://cvgl.stanford.edu/projects/lifted_struct/",
51	        features=tfds.features.FeaturesDict({

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/star_cfq/star_cfq_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	
25	_DATA_URL = 'https://storage.googleapis.com/star_cfq_dataset'
26	
27	_RANDOM_SEEDS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/star_cfq/star_cfq_dataset_builder.py:311
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
310	        supervised_keys=(_QUESTION, _QUERY),
311	        homepage='https://github.com/google-research/google-research/tree/master/star-cfq',
312	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/star_cfq/star_cfq_dataset_builder.py:371
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
370	        logging.info('Reading json from %s into memory...', samples_path)
371	        samples = json.loads(samples_file.read())
372	        logging.info('%d samples loaded', len(samples))

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/star_cfq/star_cfq_dataset_builder.py:374
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
373	        with epath.Path(split_path).open() as split_file:
374	          splits = json.loads(split_file.read())
375	          for idx in splits[split_id]:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/starcraft_video/starcraft_video_dataset_builder.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	
25	DATA_URL_DIR = "https://storage.googleapis.com/scv_dataset/data/"
26	
27	
28	class StarcraftVideoConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/starcraft_video/starcraft_video_dataset_builder.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	        release_notes={
35	            "1.0.0": "New split API (https://tensorflow.org/datasets/splits)",
36	        },
37	        **kwargs,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/starcraft_video/starcraft_video_dataset_builder.py:121
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
120	        features=features,
121	        homepage="https://storage.googleapis.com/scv_dataset/README.html",
122	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/stl10/stl10_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	URL = "http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz"
25	UNLABELLED = tfds.Split("unlabelled")

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/stl10/stl10_dataset_builder.py:40
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
39	        supervised_keys=("image", "label"),
40	        homepage="http://ai.stanford.edu/~acoates/stl10/",
41	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/stl10/stl10_dataset_builder.py:93
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
92	    with tf.io.gfile.GFile(image_path, "rb") as f:
93	      images = np.frombuffer(f.read(), dtype=np.uint8)
94	      images = np.reshape(images, (-1, 3, 96, 96))

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/stl10/stl10_dataset_builder.py:99
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
98	      with tf.io.gfile.GFile(label_path, "rb") as f:
99	        labels = np.copy(np.frombuffer(f.read(), dtype=np.uint8))
100	        # Switch to zero-based indexing.

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/story_cloze/story_cloze_dataset_builder.py:39
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
38	  ]
39	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
40	  Visit https://www.cs.rochester.edu/nlp/rocstories/ and fill out the google
41	  form to obtain the datasets. You will receive an email with the link to
42	  download the datasets. For the 2016 data, the validation and test file needs
43	  to be renamed to cloze_test_val__spring2016.csv and
44	  cloze_test_test__spring2016.csv respectively. For 2018 version, the validation
45	  and test file needs to be renamed to cloze_test_val__winter2018.csv and
46	  to cloze_test_test__winter2018.csv respectively. Move both these files
47	  to the manual directory.
48	  """
49	
50	  def _info(self) -> tfds.core.DatasetInfo:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/story_cloze/story_cloze_dataset_builder.py:59
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
58	        supervised_keys=None,  # e.g. ('image', 'label')
59	        homepage='https://www.cs.rochester.edu/nlp/rocstories/',
60	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/summscreen/summscreen_dataset_builder.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	    # pylint: disable=line-too-long
27	    'tokenized': 'https://drive.google.com/uc?export=download&id=1BvdIllGBo9d2-bzXQRzWuJXB04XPVmfF',
28	    'untokenized': 'https://drive.google.com/uc?export=download&id=1tFpt32USOO2i1FWhtFTsyYyFzuRm2k36',
29	    # pylint: enable=line-too-long
30	}
31	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/summscreen/summscreen_dataset_builder.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	    'tokenized': 'https://drive.google.com/uc?export=download&id=1BvdIllGBo9d2-bzXQRzWuJXB04XPVmfF',
28	    'untokenized': 'https://drive.google.com/uc?export=download&id=1tFpt32USOO2i1FWhtFTsyYyFzuRm2k36',
29	    # pylint: enable=line-too-long
30	}
31	
32	_RECAP = 'recap'

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/summscreen/summscreen_dataset_builder.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
42	  with tf.io.gfile.GFile(path, 'r') as f:
43	    return f.read()
44	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/summscreen/summscreen_dataset_builder.py:146
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
145	        supervised_keys=(_TRANSCRIPT, _RECAP),
146	        homepage='https://github.com/mingdachen/SummScreen',
147	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/sun397/sun397_dataset_builder.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_SUN397_URL = "https://vision.princeton.edu/projects/2010/SUN/"
28	
29	# These images are badly encoded and cannot be decoded correctly (TF), or the
30	# decoding is not deterministic (PIL).
31	_SUN397_IGNORE_IMAGES = [

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/sun397/sun397_dataset_builder.py:64
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
63	
64	  buf = fobj.read()
65	  image = tfds.core.lazy_imports.cv2.imdecode(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/svhn_cropped/svhn_cropped_dataset_builder.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	URL = "http://ufldl.stanford.edu/housenumbers/"
24	
25	
26	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/svhn_cropped/svhn_cropped_dataset_builder.py:34
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
33	  RELEASE_NOTES = {
34	      "3.1.0": "New split API (https://tensorflow.org/datasets/splits)",
35	  }
36	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/symmetric_solids/symmetric_solids_dataset_builder.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	_DATA_PATH = (
22	    'https://storage.googleapis.com/gresearch/implicit-pdf/symsol_dataset.zip'
23	)
24	_IMAGE_DIMENSIONS = (224, 224, 3)
25	_SHAPE_NAMES = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/symmetric_solids/symmetric_solids_dataset_builder.py:60
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
59	        supervised_keys=('image', 'rotation'),
60	        homepage='https://implicit-pdf.github.io',
61	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/tatoeba/tatoeba_dataset_builder.py:61
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
60	
61	_DATA_URLS = "https://raw.githubusercontent.com/facebookresearch/LASER/main/data/tatoeba/v1"
62	
63	
64	class TatoebaConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/tatoeba/tatoeba_dataset_builder.py:101
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
100	        supervised_keys=None,
101	        homepage="http://opus.nlpl.eu/Tatoeba.php",
102	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ted_hrlr_translate/ted_hrlr_translate_dataset_builder.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_DATA_URL = "http://www.phontron.com/data/qi18naacl-dataset.tar.gz"
24	
25	_VALID_LANGUAGE_PAIRS = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ted_hrlr_translate/ted_hrlr_translate_dataset_builder.py:93
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
92	          release_notes={
93	              "1.0.0": "New split API (https://tensorflow.org/datasets/splits)",
94	          },
95	      )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ted_hrlr_translate/ted_hrlr_translate_dataset_builder.py:104
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
103	        ),
104	        homepage="https://github.com/neulab/word-embeddings-for-nmt",
105	        supervised_keys=self.builder_config.language_pair,

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ted_hrlr_translate/ted_hrlr_translate_dataset_builder.py:149
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
148	    with epath.Path(source_file).open() as f:
149	      source_sentences = f.read().split("\n")
150	    with epath.Path(target_file).open() as f:

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ted_hrlr_translate/ted_hrlr_translate_dataset_builder.py:151
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
150	    with epath.Path(target_file).open() as f:
151	      target_sentences = f.read().split("\n")
152	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ted_multi_translate/ted_multi_translate_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_DATA_URL = 'http://phontron.com/data/ted_talks.tar.gz'
25	
26	_LANGUAGES = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/ted_multi_translate/ted_multi_translate_dataset_builder.py:103
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
102	        }),
103	        homepage='https://github.com/neulab/word-embeddings-for-nmt',
104	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/tedlium/tedlium_dataset_builder.py:46
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
45	      name="release1",
46	      description="""\
47	        The TED-LIUM corpus is English-language TED talks, with transcriptions,
48	        sampled at 16kHz. It contains about 118 hours of speech.
49	
50	        This is the TED-LIUM corpus release 1,
51	        licensed under Creative Commons BY-NC-ND 3.0
52	        (http://creativecommons.org/licenses/by-nc-nd/3.0/deed.en).
53	        """,
54	      citation="""\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/tedlium/tedlium_dataset_builder.py:63
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
62	        """,
63	      url="https://www.openslr.org/7/",
64	      download_url="http://www.openslr.org/resources/7/TEDLIUM_release1.tar.gz",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/tedlium/tedlium_dataset_builder.py:64
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
63	      url="https://www.openslr.org/7/",
64	      download_url="http://www.openslr.org/resources/7/TEDLIUM_release1.tar.gz",
65	      split_paths=[

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/tedlium/tedlium_dataset_builder.py:74
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
73	      name="release2",
74	      description="""\
75	        This is the TED-LIUM corpus release 2,
76	        licensed under Creative Commons BY-NC-ND 3.0
77	        (http://creativecommons.org/licenses/by-nc-nd/3.0/deed.en).
78	
79	        All talks and text are property of TED Conferences LLC.
80	
81	        The TED-LIUM corpus was made from audio talks and their transcriptions
82	        available on the TED website. We have prepared and filtered these data
83	        in order to train acoustic models to participate to the International
84	        Workshop on Spoken Language Translation 2011 (the LIUM English/French
85	        SLT system reached the first rank in the SLT task).
86	
87	        Contains 1495 talks and transcripts.
88	        """,
89	      citation="""\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/tedlium/tedlium_dataset_builder.py:97
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
96	        """,
97	      url="https://www.openslr.org/19/",
98	      download_url=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/tedlium/tedlium_dataset_builder.py:99
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
98	      download_url=(
99	          "http://www.openslr.org/resources/19/TEDLIUM_release2.tar.gz"
100	      ),
101	      split_paths=[
102	          (tfds.Split.TRAIN, os.path.join("TEDLIUM_release2", "train")),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/tedlium/tedlium_dataset_builder.py:151
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
150	        """,
151	      url="https://www.openslr.org/51/",
152	      download_url="http://www.openslr.org/resources/51/TEDLIUM_release-3.tgz",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/tedlium/tedlium_dataset_builder.py:152
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
151	      url="https://www.openslr.org/51/",
152	      download_url="http://www.openslr.org/resources/51/TEDLIUM_release-3.tgz",
153	      split_paths=[

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/tf_flowers/tf_flowers_dataset_builder.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	
22	_URL = "http://download.tensorflow.org/example_images/flower_photos.tgz"
23	
24	
25	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/tf_flowers/tf_flowers_dataset_builder.py:39
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
38	        supervised_keys=("image", "label"),
39	        homepage="https://www.tensorflow.org/tutorials/load_data/images",
40	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/the300w_lp/the300w_lp_dataset_builder.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_DATASET_URL = "https://drive.google.com/uc?export=download&id=0B7OEHD3T4eCkVGs0TkhUWFN6N1k"
25	
26	_PROJECT_URL = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/the300w_lp/the300w_lp_dataset_builder.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	_PROJECT_URL = (
27	    "http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm"
28	)
29	
30	
31	class Builder(tfds.core.GeneratorBasedBuilder):
32	  """300W-LP dataset."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/tiny_shakespeare/tiny_shakespeare_dataset_builder.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	        supervised_keys=None,
35	        homepage='https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt',
36	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/tiny_shakespeare/tiny_shakespeare_dataset_builder.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	    download_path = dl_manager.download(
41	        'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'
42	    )
43	    if tf.io.gfile.isdir(download_path):

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/tiny_shakespeare/tiny_shakespeare_dataset_builder.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
48	    with tf.io.gfile.GFile(txt_path, 'r') as f:
49	      text = f.read()
50	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/titanic/titanic_dataset_builder.py:100
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
99	
100	_URL = "https://www.openml.org/data/get_csv/16826755/phpMYEkMl"
101	
102	
103	class Builder(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/titanic/titanic_dataset_builder.py:115
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
114	      ),
115	      "2.0.0": "New split API (https://tensorflow.org/datasets/splits)",
116	  }
117	
118	  def _info(self):
119	    supervised_features = _feature_dict()
120	    survived_feature, unused_func = supervised_features.pop("survived")
121	
122	    if self.version >= "3.0.0":
123	      supervised_keys = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/titanic/titanic_dataset_builder.py:141
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
140	        supervised_keys=supervised_keys,
141	        homepage="https://www.openml.org/d/40945",
142	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/trec/trec_dataset_builder.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	_URLs = {
22	    "train": "http://cogcomp.org/Data/QA/QC/train_5500.label",
23	    "test": "http://cogcomp.org/Data/QA/QC/TREC_10.label",
24	}
25	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/trec/trec_dataset_builder.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	    "train": "http://cogcomp.org/Data/QA/QC/train_5500.label",
23	    "test": "http://cogcomp.org/Data/QA/QC/TREC_10.label",
24	}
25	
26	_COARSE_LABELS = ["DESC", "ENTY", "ABBR", "HUM", "NUM", "LOC"]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/trec/trec_dataset_builder.py:96
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
95	        # Homepage of the dataset for documentation
96	        homepage="https://cogcomp.seas.upenn.edu/Data/QA/QC/",
97	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/trivia_qa/trivia_qa_dataset_builder.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	_DOWNLOAD_URL_TMPL = (
29	    "http://nlp.cs.washington.edu/triviaqa/data/triviaqa-{}.tar.gz"
30	)
31	_TRAIN_FILE_FORMAT = "*-train.json"
32	_VALIDATION_FILE_FORMAT = "*-dev.json"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/trivia_qa/trivia_qa_dataset_builder.py:135
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
134	        supervised_keys=None,
135	        homepage="http://nlp.cs.washington.edu/triviaqa/",
136	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/trivia_qa/trivia_qa_dataset_builder.py:244
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
243	            with tf.io.gfile.GFile(os.path.join(file_dir, fname)) as f:
244	              new_item[context_field] = f.read()
245	          except (IOError, tf.errors.NotFoundError):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_dataset_builder.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	
22	_DATA_URL = "https://raw.githubusercontent.com/UniversalDependencies/"
23	
24	
25	class Builder(tfds.dataset_builders.ConllUDatasetBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_dataset_builder.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
48	    return self.create_dataset_info(
49	        homepage="https://universaldependencies.org/",
50	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	    "af_afribooms": (
28	        "UD Afrikaans-AfriBooms is a conversion of the AfriBooms Dependency "
29	        "Treebank, originally annotated with a simplified PoS set and "
30	        "dependency relations according to a subset of the Stanford tag set. "
31	        "The corpus consists of public government documents. The dataset was "
32	        "proposed in 'AfriBooms: An Online Treebank for Afrikaans' by "
33	        "Augustinus et al. (2016); "
34	        "https://www.aclweb.org/anthology/L16-1107.pdf."
35	    ),
36	    "akk_pisandub": (
37	        "A small set of sentences from Babylonian royal inscriptions."
38	    ),
39	    "akk_riao": (
40	        "UD_Akkadian-RIAO is a small treebank which consists of 22 277 words "
41	        "and 1845 sentences. This represents an intact subset of a total of "
42	        "2211 sentences from the early Neo-Assyrian royal inscriptions  of the"
43	        " tenth and ninth centuries BCE. These royal inscriptions were "
44	        "extracted from Oracc (Open Richly Annotated Cuneiform Corpus; "
45	        "http://oracc.museum.upenn.edu/riao/), where all Neo-Assyrian royal "
46	        "inscriptions are lemmatized word-for-word. The language of the corpus"
47	        " is Standard Babylonian, with occasional Assyrianisms, whereas "
48	        "Akkadian is the umbrella term for both Assyrian and Babylonian. The"
49	        " treebank was manually annotated following the UD annotation "
50	        "guidelines."
51	    ),
52	    "aqz_tudet": (
53	        "UD_Akuntsu-TuDeT is a collection of annotated texts in Akunts. "
54	        "Together with UD_Tupinamba-TuDeT and UD_Munduruku-TuDeT, "
55	        "UD_Akuntsu-TuDeT is part of the TuLaR project.  The sentences are "
56	        "being annotated by Carolina Aragon and Fabrcio Ferraz Gerardi."
57	    ),
58	    "sq_tsa": (
59	        "The UD Treebank for Standard Albanian (TSA) is a small treebank that "
60	        "consists of 60 sentences corresponding to 922 tokens. The data was "
61	        "collected from different Wikipedia entries. This treebank was created"
62	        " mainly manually following the Universal Dependencies guidelines. The"
63	        " lemmatization was performed using the lemmatizer "
64	        "https://bitbucket.org/timarkh/uniparser-albanian-grammar/src/master/ "
65	        "developed by the Albanian National Corpus team (Maria Morozova, "
66	        "Alexander Rusakov, Timofey Arkhangelskiy). Tagging and Morphological "
67	        "Analysis were semi-automated through python scripts and corrected "
68	        "manually, whereas Dependency relations were assigned fully manually. "
69	        "We encourage any initiatives to increase the size and/or improve the "
70	        "overall quality of the Treebank."
71	    ),
72	    "am_att": (
73	        "UD_Amharic-ATT is a manually annotated Treebanks. It is annotated for"
74	        " POS tag, morphological information and dependency relations. Since "
75	        "Amharic is a morphologically-rich, pro-drop, and languages having a "
76	        "feature of clitic doubling, clitics have been segmented manually."
77	    ),
78	    "grc_perseus": (
79	        "This Universal Dependencies Ancient Greek Treebank consists of an "
80	        "automatic conversion of a selection of passages from the Ancient "
81	        "Greek and Latin Dependency Treebank 2.1"
82	    ),
83	    "grc_proiel": (
84	        "The Ancient Greek PROIEL treebank is based on the Ancient Greek data "
85	        "from the PROIEL treebank, which is maintained at the Department of "
86	        "Philosophy, Classics, History of Arts and Ideas at the University of "
87	        "Oslo. The conversion is based on the 20180408 release of the PROIEL "
88	        "treebank available from "
89	        "https://github.com/proiel/proiel-treebank/releases. The original "
90	        "annotators are acknowledged in the files available there. The "
91	        "conversion code is available in the Rubygem proiel-cli, "
92	        "https://github.com/proiel/proiel-cli."
93	    ),
94	    "apu_ufpa": (
95	        "The initial release contains 70 annotated sentences. This is the "
96	        "first treebank in a language from the Arawak family. The original "
97	        "interlinear glosses are included in the tree bank, and their "
98	        "conversion into a full UD annotation is an ongoing process. The "
99	        "sent_id values (e.g.: FernandaM2017:Texto-6-19) are representative of"
100	        " the collector, year of publication, text identifier and the number "
101	        "of the sentence in order from the original text."
102	    ),
103	    "hbo_ptnk": (
104	        "UD Ancient Hebrew PTNK contains portions of the Biblia Hebraic "
105	        "Stuttgartensia with morphological annotations from ETCBC."
106	    ),
107	    "ar_nyuad": (
108	        "The treebank consists of 19,738 sentences (738889 tokens), and its "
109	        "domain is mainly newswire. The annotation is licensed under the terms"
110	        " of CC BY-SA 4.0, and the original PATB can be obtained from the "
111	        "LDCs official website."
112	    ),
113	    "ar_padt": (
114	        "The Arabic-PADT UD treebank is based on the Prague Arabic Dependency "
115	        "Treebank (PADT), created at the Charles University in Prague."
116	    ),
117	    "ar_pud": (
118	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
119	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
120	        "Raw Text to Universal Dependencies."
121	    ),
122	    "aii_as": (
123	        "The Uppsala Assyrian Treebank is a small treebank for Modern Standard"
124	        " Assyrian. The corpus is collected and annotated manually. The data "
125	        "was randomly collected from different textbooks and a short "
126	        "translation of The Merchant of Venice."
127	    ),
128	    "bm_crb": (
129	        "The UD Bambara treebank is a section of the Corpus Rfrence du "
130	        "Bambara annotated natively with Universal Dependencies."
131	    ),
132	    "eu_bdt": (
133	        "The Basque UD treebank is based on a automatic conversion from part "
134	        "of the Basque Dependency Treebank (BDT), created at the University of"
135	        " of the Basque Country by the IXA NLP research group. The treebank "
136	        "consists of 8.993 sentences (121.443 tokens) and covers mainly "
137	        "literary and journalistic texts."
138	    ),
139	    "bej_nsc": (
140	        "A Universal Dependencies corpus for Beja, North-Cushitic branch of "
141	        "the Afro-Asiatic phylum mainly spoken in Sudan, Egypt and Eritrea."
142	    ),
143	    "be_hse": (
144	        "The Belarusian UD treebank is based on a sample of the news texts "
145	        "included in the Belarusian-Russian parallel subcorpus of the Russian "
146	        "National Corpus, online search available at: "
147	        "http://ruscorpora.ru/search-para-be.html."
148	    ),
149	    "bn_bru": (
150	        "The BRU Bengali treebank has been created at Begum Rokeya University,"
151	        " Rangpur, by the members of Semantics Lab."
152	    ),
153	    "bho_bhtb": (
154	        "The Bhojpuri UD Treebank (BHTB) v2.6 consists of 6,664 tokens(357 "
155	        "sentences). This Treebank is a part of the Universal Dependency "
156	        "treebank project. Initially, it was initiated by me (Atul) at "
157	        "Jawaharlal Nehru University, New Delhi during the doctoral research "
158	        "work. BHTB data contains syntactic annotation according to "
159	        "dependency-constituency schema, as well as morphological tags and "
160	        "lemmas. In this data, XPOS is annotated  according to Bureau of "
161	        "Indian Standards (BIS) Part Of Speech (POS) tagset."
162	    ),
163	    "br_keb": (
164	        "UD Breton-KEB is a treebank of Breton that has been manually "
165	        "annotated according to the Universal Dependencies guidelines. The "
166	        "tokenisation guidelines and morphological annotation comes from a "
167	        "finite-state morphological analyser of Breton released as part of the"
168	        " Apertium project."
169	    ),
170	    "bg_btb": (
171	        "UD_Bulgarian-BTB is based on the HPSG-based BulTreeBank, created at "
172	        "the Institute of Information and Communication Technologies, "
173	        "Bulgarian Academy of Sciences. The original consists of 215,000 "
174	        "tokens (over 15,000 sentences)."
175	    ),
176	    "bxr_bdt": (
177	        "The UD Buryat treebank was annotated manually natively in UD and "
178	        "contains grammar book sentences, along with news and some fiction."
179	    ),
180	    "yue_hk": (
181	        "A Cantonese treebank (in Traditional Chinese characters) of film "
182	        "subtitles and of legislative proceedings of Hong Kong, parallel with "
183	        "the Chinese-HK treebank."
184	    ),
185	    "ca_ancora": "Catalan data from the AnCora corpus.",
186	    "ceb_gja": (
187	        "UD_Cebuano_GJA is a collection of annotated Cebuano sample sentences "
188	        "randomly taken from three different sources: community-contributed "
189	        "samples from the website Tatoeba, a Cebuano grammar book by Bunye & "
190	        "Yap (1971) and Tanangkinsing's reference grammar on Cebuano (2011). "
191	        "This project is currently work in progress."
192	    ),
193	    "zh_cfl": (
194	        "The Chinese-CFL UD treebank is manually annotated by Keying Li with "
195	        "minor manual revisions by Herman Leung and John Lee at City "
196	        "University of Hong Kong, based on essays written by learners of "
197	        "Mandarin Chinese as a foreign language. The data is in Simplified "
198	        "Chinese."
199	    ),
200	    "zh_gsd": (
201	        "Traditional Chinese Universal Dependencies Treebank annotated and "
202	        "converted by Google."
203	    ),
204	    "zh_gsdsimp": (
205	        "Simplified Chinese Universal Dependencies dataset converted from the "
206	        "GSD (traditional) dataset with manual corrections."
207	    ),
208	    "zh_hk": (
209	        "A Traditional Chinese treebank of film subtitles and of legislative "
210	        "proceedings of Hong Kong, parallel with the Cantonese-HK treebank."
211	    ),
212	    "zh_pud": (
213	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
214	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
215	        "Raw Text to Universal Dependencies."
216	    ),
217	    "ckt_hse": (
218	        "This data is a manual annotation of the corpus from multimedia "
219	        "annotated corpus of the Chuklang project, a dialectal corpus of the "
220	        "Amguema variant of Chukchi."
221	    ),
222	    "lzh_kyoto": (
223	        "Classical Chinese Universal Dependencies Treebank annotated and "
224	        "converted by Institute for Research in Humanities, Kyoto University."
225	    ),
226	    "cop_scriptorium": (
227	        "UD Coptic contains manually annotated Sahidic Coptic texts, including"
228	        " Biblical texts, sermons, letters, and hagiography."
229	    ),
230	    "hr_set": (
231	        "The Croatian UD treebank is based on the extension of the SETimes-HR "
232	        "corpus, the hr500k corpus."
233	    ),
234	    "cs_cac": (
235	        "The UD_Czech-CAC treebank is based on the Czech Academic Corpus 2.0 "
236	        "(CAC; esk akademick korpus; AK), created at Charles University in"
237	        " Prague."
238	    ),
239	    "cs_cltt": (
240	        "The UD_Czech-CLTT treebank is based on the Czech Legal Text Treebank "
241	        "1.0, created at Charles University in Prague."
242	    ),
243	    "cs_fictree": (
244	        "FicTree is a treebank of Czech fiction, automatically converted into "
245	        "the UD format. The treebank was built at Charles University in "
246	        "Prague."
247	    ),
248	    "cs_pdt": (
249	        "The Czech-PDT UD treebank is based on the Prague Dependency Treebank "
250	        "3.0 (PDT), created at the Charles University in Prague."
251	    ),
252	    "cs_pud": (
253	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
254	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
255	        "Raw Text to Universal Dependencies."
256	    ),
257	    "da_ddt": (
258	        "The Danish UD treebank is a conversion of the Danish Dependency "
259	        "Treebank."
260	    ),
261	    "nl_alpino": (
262	        "This corpus consists of samples from various treebanks annotated at "
263	        "the University of Groningen using the Alpino annotation tools and "
264	        "guidelines."
265	    ),
266	    "nl_lassysmall": (
267	        "This corpus contains sentences from the Wikipedia section of the "
268	        "Lassy Small Treebank. Universal Dependency annotation was generated "
269	        "automatically from the original annotation in Lassy."
270	    ),
271	    "en_esl": (
272	        "UD English-ESL / Treebank of Learner English (TLE) contains manual "
273	        "POS tag and dependency annotations for 5,124 English as a Second "
274	        "Language (ESL) sentences drawn from the Cambridge Learner Corpus "
275	        "First Certificate in English (FCE) dataset."
276	    ),
277	    "en_ewt": (
278	        "A Gold Standard Universal Dependencies Corpus for English, built over"
279	        " the source material of the English Web Treebank LDC2012T13 "
280	        "(https://catalog.ldc.upenn.edu/LDC2012T13)."
281	    ),
282	    "en_gum": (
283	        "Universal Dependencies syntax annotations from the GUM corpus "
284	        "(https://corpling.uis.georgetown.edu/gum/)."
285	    ),
286	    "en_gumreddit": (
287	        "Universal Dependencies syntax annotations from the Reddit portion of "
288	        "the GUM corpus (https://corpling.uis.georgetown.edu/gum/) "
289	    ),
290	    "en_lines": (
291	        "UD English_LinES is the English half of the LinES Parallel Treebank "
292	        "with the original dependency annotation first automatically converted"
293	        " into Universal Dependencies and then partially reviewed. Its "
294	        "contents cover literature, an online manual and Europarl data."
295	    ),
296	    "en_atis": (
297	        "UD Atis Treebank is a manually annotated treebank consisting of the "
298	        "sentences in the Atis (Airline Travel Informations) dataset which "
299	        "includes the human speech transcriptions of people asking for flight "
300	        "information on the automated inquiry systems."
301	    ),
302	    "en_partut": (
303	        "UD_English-ParTUT is a conversion of a multilingual parallel treebank"
304	        " developed at the University of Turin, and consisting of a variety of"
305	        " text genres, including talks, legal texts and Wikipedia articles, "
306	        "among others."
307	    ),
308	    "en_pronouns": (
309	        "UD English-Pronouns is dataset created to make pronoun identification"
310	        " more accurate and with a more balanced distribution across genders. "
311	        "The dataset is initially targeting the Independent Genitive pronouns,"
312	        " 'hers', (independent) 'his', (singular) 'theirs', 'mine', and "
313	        "(singular) 'yours'."
314	    ),
315	    "en_pud": (
316	        "This is the English portion of the Parallel Universal Dependencies "
317	        "(PUD) treebanks created for the CoNLL 2017 shared task on "
318	        "Multilingual Parsing from Raw Text to Universal Dependencies "
319	        "(http://universaldependencies.org/conll17/)."
320	    ),
321	    "myv_jr": (
322	        "UD Erzya is the original annotation (CoNLL-U) for texts in the Erzya "
323	        "language, it originally consists of a sample from a number of fiction"
324	        " authors writing originals in Erzya."
325	    ),
326	    "et_edt": (
327	        "UD Estonian is a converted version of the Estonian Dependency "
328	        "Treebank (EDT), originally annotated in the Constraint Grammar (CG) "
329	        "annotation scheme, and consisting of genres of fiction, newspaper "
330	        "texts and scientific texts. The treebank contains 30,972 trees, "
331	        "437,769 tokens."
332	    ),
333	    "et_ewt": (
334	        "UD EWT treebank consists of different genres of new media. The "
335	        "treebank contains 4,493 trees, 56,399 tokens."
336	    ),
337	    "fo_farpahc": (
338	        "UD_Icelandic-FarPaHC is a conversion of the Faroese Parsed Historical"
339	        " Corpus (FarPaHC) to the Universal Dependencies scheme. The "
340	        "conversion was done using UDConverter."
341	    ),
342	    "fo_oft": "This is a treebank of Faroese based on the Faroese Wikipedia.",
343	    "fi_ftb": (
344	        "FinnTreeBank 1 consists of manually annotated grammatical examples "
345	        "from VISK. The UD version of FinnTreeBank 1 was converted from a "
346	        "native annotation model with a script and later manually revised."
347	    ),
348	    "fi_ood": (
349	        "Finnish-OOD is an external out-of-domain test set for Finnish-TDT "
350	        "annotated natively into UD scheme."
351	    ),
352	    "fi_pud": (
353	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
354	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
355	        "Raw Text to Universal Dependencies."
356	    ),
357	    "fi_tdt": (
358	        "UD_Finnish-TDT is based on the Turku Dependency Treebank (TDT), a "
359	        "broad-coverage dependency treebank of general Finnish covering "
360	        "numerous genres. The conversion to UD was followed by extensive "
361	        "manual checks and corrections, and the treebank closely adheres to "
362	        "the UD guidelines."
363	    ),
364	    "fr_fqb": (
365	        "The corpus **UD_French-FQB** is an automatic conversion of the French"
366	        " QuestionBank v1, a corpus entirely made of questions."
367	    ),
368	    "fr_ftb": (
369	        "The Universal Dependency version of the French Treebank (Abeill et "
370	        "al., 2003), hereafter UD_French-FTB, is a treebank of sentences from "
371	        "the newspaper Le Monde, initially manually annotated with "
372	        "morphological information and phrase-structure and then converted to "
373	        "the Universal Dependencies annotation scheme."
374	    ),
375	    "fr_gsd": (
376	        "The **UD_French-GSD** was converted in 2015 from the content head "
377	        "version of the universal dependency treebank v2.0 "
378	        "(https://github.com/ryanmcd/uni-dep-tb). It is updated since 2015 "
379	        "independently from the previous source."
380	    ),
381	    "fr_partut": (
382	        "UD_French-ParTUT is a conversion of a multilingual parallel treebank "
383	        "developed at the University of Turin, and consisting of a variety of "
384	        "text genres, including talks, legal texts and Wikipedia articles, "
385	        "among others."
386	    ),
387	    "fr_rhapsodie": "A Universal Dependencies corpus for spoken French.",
388	    "fr_parisstories": (
389	        "Paris Stories is a corpus of oral French collected and transcribed by"
390	        " Linguistics students from Sorbonne Nouvelle and corrected by "
391	        "students from the Plurital Master's Degree of Computational "
392	        "Linguistics ( Inalco, Paris Nanterre, Sorbonne Nouvelle) between 2017"
393	        " and 2021. It contains monologues and dialogues from speakers living "
394	        "in the Parisian region."
395	    ),
396	    "fr_pud": (
397	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
398	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
399	        "Raw Text to Universal Dependencies."
400	    ),
401	    "fr_sequoia": (
402	        "UD_French-Sequoia is an automatic conversion of the Sequoia Treebank "
403	        "corpus French Sequoia corpus."
404	    ),
405	    "gl_ctg": (
406	        "The Galician UD treebank is based on the automatic parsing of the "
407	        "Galician Technical Corpus (http://sli.uvigo.gal/CTG) created at the "
408	        "University of Vigo by the the TALG NLP research group."
409	    ),
410	    "gl_treegal": (
411	        "The Galician-TreeGal is a treebank for Galician developed at LyS "
412	        "Group (Universidade da Corua)."
413	    ),
414	    "de_gsd": (
415	        "The German UD is converted from the content head version of the "
416	        "universal dependency treebank v2.0 (legacy)."
417	    ),
418	    "de_hdt": (
419	        "UD German-HDT is a conversion of the Hamburg Dependency Treebank, "
420	        "created at the University of Hamburg through manual annotation in "
421	        "conjunction with a standard for morphologically and syntactically "
422	        "annotating sentences as well as a constraint-based parser."
423	    ),
424	    "de_lit": (
425	        "This treebank aims at gathering texts of the German literary history."
426	        " Currently, it hosts Fragments of the early Romanticism, i.e. "
427	        "aphorism-like texts mainly dealing with philosophical issues "
428	        "concerning art, beauty and related topics."
429	    ),
430	    "de_pud": (
431	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
432	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
433	        "Raw Text to Universal Dependencies."
434	    ),
435	    "got_proiel": (
436	        "The UD Gothic treebank is based on the Gothic data from the PROIEL "
437	        "treebank, and consists of Wulfila's Bible translation."
438	    ),
439	    "el_gdt": (
440	        "The Greek UD treebank (UD_Greek-GDT) is derived from the Greek "
441	        "Dependency Treebank (http://gdt.ilsp.gr), a resource developed and "
442	        "maintained by researchers at the Institute for Language and Speech "
443	        "Processing/Athena R.C. (http://www.ilsp.gr)."
444	    ),
445	    "gub_tudet": (
446	        "UD_Guajajara-TuDeT is a collection of annotated sentences in "
447	        "Guajajara. Sentences stem from multiple sources such as descriptions "
448	        "of the language, short stories, dictionaries and translations from "
449	        "the New Testament. Sentence annotation and documentation by Lorena "
450	        "Martn Rodrguez and Fabrcio Ferraz Gerardi."
451	    ),
452	    "gn_oldtudet": (
453	        "UD_Guarani-OldTuDeT is a collection of annotated texts in Old "
454	        "Guaran. All known sources in this language are being annotated: "
455	        "cathesisms, grammars (seventeenth and eighteenth century), sentences "
456	        "from dictionaries, and other texts. Sentence annotation and "
457	        "documentation by Fabrcio Ferraz Gerardi and Lorena Martn Rodrguez."
458	    ),
459	    "he_htb": "A Universal Dependencies Corpus for Hebrew.",
460	    "he_iahltwiki": (
461	        "Publicly available subset of the IAHLT UD Hebrew Treebank's Wikipedia"
462	        " section (https://www.iahlt.org/)"
463	    ),
464	    "qfn_fame": (
465	        "UD_Frisian_Dutch-Fame is a selection of 400 sentences from the FAME! "
466	        "speech corpus by Yilmaz et al. (2016a, 2016b). The treebank is "
467	        "manually annotated using the UD scheme."
468	    ),
469	    "qhe_hiencs": (
470	        "The Hindi-English Code-switching treebank is based on code-switching "
471	        "tweets of Hindi and English multilingual speakers (mostly Indian) on "
472	        "Twitter. The treebank is manually annotated using UD sceheme. The "
473	        "training and evaluations sets were seperately annotated by different "
474	        "annotators using UD v2 and v1 guidelines respectively. The evaluation"
475	        " sets are automatically converted from UD v1 to v2."
476	    ),
477	    "hi_hdtb": (
478	        "The Hindi UD treebank is based on the Hindi Dependency Treebank "
479	        "(HDTB), created at IIIT Hyderabad, India."
480	    ),
481	    "hi_pud": (
482	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
483	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
484	        "Raw Text to Universal Dependencies."
485	    ),
486	    "hu_szeged": (
487	        "The Hungarian UD treebank is derived from the Szeged Dependency "
488	        "Treebank (Vincze et al. 2010)."
489	    ),
490	    "is_modern": (
491	        "UD_Icelandic-Modern is a conversion of the modern additions to the "
492	        "Icelandic Parsed Historical Corpus (IcePaHC) to the Universal "
493	        "Dependencies scheme."
494	    ),
495	    "is_icepahc": (
496	        "UD_Icelandic-IcePaHC is a conversion of the Icelandic Parsed "
497	        "Historical Corpus (IcePaHC) to the Universal Dependencies scheme. The"
498	        " conversion was done using UDConverter."
499	    ),
500	    "is_pud": (
501	        "Icelandic-PUD is the Icelandic part of the Parallel Universal "
502	        "Dependencies (PUD) treebanks."
503	    ),
504	    "id_csui": (
505	        "UD Indonesian-CSUI is a conversion from an Indonesian constituency "
506	        "treebank in the Penn Treebank format named Kethu that was also a "
507	        "conversion from a constituency treebank built by Dinakaramani et al. "
508	        "(2015). We named this treebank Indonesian-CSUI, since all the three "
509	        "versions of the treebanks were built at Faculty of Computer Science, "
510	        "Universitas Indonesia."
511	    ),
512	    "id_gsd": (
513	        "The Indonesian UD is converted from the content head version of the "
514	        "universal dependency treebank v2.0 (legacy)."
515	    ),
516	    "id_pud": (
517	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
518	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
519	        "Raw Text to Universal Dependencies."
520	    ),
521	    "ga_idt": (
522	        "A Universal Dependencies 4910-sentence treebank for modern Irish."
523	    ),
524	    "ga_twittirish": (
525	        "A Universal Dependencies treebank of 866 tweets in modern Irish."
526	    ),
527	    "it_isdt": (
528	        "The Italian corpus annotated according to the UD annotation scheme "
529	        "was obtained by conversion from ISDT (Italian Stanford Dependency "
530	        "Treebank), released for the dependency parsing shared task of "
531	        "Evalita-2014 (Bosco et al. 2014)."
532	    ),
533	    "it_partut": (
534	        "UD_Italian-ParTUT is a conversion of a multilingual parallel treebank"
535	        " developed at the University of Turin, and consisting of a variety of"
536	        " text genres, including talks, legal texts and Wikipedia articles, "
537	        "among others."
538	    ),
539	    "it_postwita": (
540	        "PoSTWITA-UD is a collection of Italian tweets annotated in Universal "
541	        "Dependencies that can be exploited for the training of NLP systems to"
542	        " enhance their performance on social media texts."
543	    ),
544	    "it_markit": (
545	        "It is MarkIT That is New: An Italian Treebank of Marked "
546	        "Constructions. Teresa Paccosi, Alessio Palmero Aprosio and Sara "
547	        "Tonelli, To appear in Proceedings of the Eighth Italian Conference on"
548	        " Computational Linguistics 2022 (CLIC-it 2021)"
549	    ),
550	    "it_valico": (
551	        "Manually corrected Treebank of Learner Italian drawn from the Valico "
552	        "corpus and correspondent corrected sentences."
553	    ),
554	    "it_pud": (
555	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
556	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
557	        "Raw Text to Universal Dependencies."
558	    ),
559	    "it_twittiro": (
560	        "TWITTIR-UD is a collection of ironic Italian tweets annotated in "
561	        "Universal Dependencies. The treebank can be exploited for the "
562	        "training of NLP systems to enhance their performance on social media "
563	        "texts, and in particular, for irony detection purposes."
564	    ),
565	    "it_vit": (
566	        "The UD_Italian-VIT corpus was obtained by conversion from VIT (Venice"
567	        " Italian Treebank), developed at the Laboratory of Computational "
568	        "Linguistics of the Universit Ca' Foscari in Venice (Delmonte et al. "
569	        "2007; Delmonte 2009; "
570	        "http://rondelmo.it/resource/VIT/Browser-VIT/index.htm)."
571	    ),
572	    "ja_pudluw": (
573	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
574	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
575	        "Raw Text to Universal Dependencies."
576	    ),
577	    "ja_bccwjluw": (
578	        "This Universal Dependencies (UD) Japanese treebank is based on the "
579	        "definition of UD Japanese convention described in the UD "
580	        "documentation. The original sentences are from `Balanced Corpus of "
581	        "Contemporary Written Japanese'(BCCWJ)."
582	    ),
583	    "ja_gsdluw": (
584	        "This Universal Dependencies (UD) Japanese treebank is based on the "
585	        "definition of UD Japanese convention described in the UD "
586	        "documentation. The original sentences are from Google UDT 2.0."
587	    ),
588	    "ja_bccwj": (
589	        "This Universal Dependencies (UD) Japanese treebank is based on the "
590	        "definition of UD Japanese convention described in the UD "
591	        "documentation. The original sentences are from `Balanced Corpus of "
592	        "Contemporary Written Japanese'(BCCWJ)."
593	    ),
594	    "ja_gsd": (
595	        "This Universal Dependencies (UD) Japanese treebank is based on the "
596	        "definition of UD Japanese convention described in the UD "
597	        "documentation.  The original sentences are from Google UDT 2.0."
598	    ),
599	    "ja_modern": (
600	        "This Universal Dependencies (UD) Japanese treebank is based on the "
601	        "definition of UD Japanese convention described in the UD "
602	        "documentation. The original sentences are from `Corpus of Historical "
603	        "Japanese' (CHJ)."
604	    ),
605	    "ja_pud": (
606	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
607	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
608	        " Raw Text to Universal "
609	        "Dependencies](http://universaldependencies.org/conll17/)."
610	    ),
611	    "jv_csui": (
612	        "UD Javanese-CSUI is a dependency treebank in Javanese, a regional "
613	        "language in Indonesia with more than 60 million users. The original "
614	        "sentences were taken from OPUS, especially from the WikiMatrix v1 "
615	        "corpus. We revised the sentences that contained more Indonesian words"
616	        " than Javanese words and manually annotated them."
617	    ),
618	    "urb_tudet": (
619	        "UD_Kaapor-TuDeT is a collection of annotated sentences in Ka'apor. "
620	        "The project is a work in progress and the treebank is being updated "
621	        "on a regular basis."
622	    ),
623	    "xnr_kdtb": (
624	        "The Kangri UD Treebank (KDTB) is a part of the Universal Dependency "
625	        "treebank project."
626	    ),
627	    "krl_kkpp": (
628	        "UD Karelian-KKPP is a manually annotated new corpus of Karelian made "
629	        "in Universal dependencies annotation scheme. The data is collected "
630	        "from VepKar corpora and consists of mostly modern news texts but also"
631	        " some stories and educational texts."
632	    ),
633	    "kk_ktb": (
634	        "The UD Kazakh treebank is a combination of text from various sources "
635	        "including Wikipedia, some folk tales, sentences from the UDHR, news "
636	        "and phrasebook sentences. Sentences IDs include partial document "
637	        "identifiers."
638	    ),
639	    "arr_tudet": (
640	        "UD_Karo-TuDeT is a collection of annotated sentences in Karo. The "
641	        "sentences stem from the only grammatical description of the language "
642	        "(Gabas, 1999) and from the sentences in the dictionary by the same "
643	        "author (Gabas, 2007). Sentence annotation and documentation by "
644	        "Fabrcio Ferraz Gerardi."
645	    ),
646	    "kfm_aha": (
647	        "The AHA Khunsari Treebank is a small treebank for contemporary "
648	        "Khunsari. Its corpus is collected and annotated manually. We have "
649	        "prepared this treebank based on interviews with Khunsari speakers."
650	    ),
651	    "quc_iu": (
652	        "UD Kiche-IU is a treebank consisting of sentences from a variety of"
653	        " text domains but principally dictionary example sentences and "
654	        "linguistic examples."
655	    ),
656	    "koi_uh": (
657	        "This is a Komi-Permyak literary language treebank consisting of "
658	        "original and translated texts."
659	    ),
660	    "kpv_ikdp": (
661	        "This treebank consists of dialectal transcriptions of spoken "
662	        "Komi-Zyrian. The current texts are short recorded segments from "
663	        "different areas where the Iva dialect of Komi language is spoken."
664	    ),
665	    "kpv_lattice": (
666	        "UD Komi-Zyrian Lattice is a treebank of written standard Komi-Zyrian."
667	    ),
668	    "ko_gsd": (
669	        "The Google Korean Universal Dependency Treebank is first converted "
670	        "from the Universal Dependency Treebank v2.0 (legacy), and then "
671	        "enhanced by Chun et al., 2018."
672	    ),
673	    "ko_kaist": (
674	        "The KAIST Korean Universal Dependency Treebank is generated by Chun "
675	        "et al., 2018 from the constituency trees in the KAIST Tree-Tagging "
676	        "Corpus."
677	    ),
678	    "ko_pud": (
679	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
680	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
681	        "Raw Text to Universal Dependencies."
682	    ),
683	    "kmr_mg": (
684	        "The UD Kurmanji corpus is a corpus of Kurmanji Kurdish. It contains "
685	        "fiction and encyclopaedic texts in roughly equal measure. It has been"
686	        " annotated natively in accordance with the UD annotation scheme."
687	    ),
688	    "la_ittb": (
689	        "Latin data from the _Index Thomisticus_ Treebank. Data are taken from"
690	        " the _Index Thomisticus_ corpus by Roberto Busa SJ, which contains "
691	        "the complete work by Thomas Aquinas (12251274; Medieval Latin) and "
692	        "by 61 other authors related to Thomas."
693	    ),
694	    "la_udante": (
695	        "The UDante treebank is based on the Latin texts of Dante Alighieri, "
696	        "taken from the DanteSearch corpus, originally created at the "
697	        "University of Pisa, Italy. It is a treebank of Latin language, more "
698	        "precisely of literary Medieval Latin (XIVth century)."
699	    ),
700	    "la_llct": (
701	        "This Universal Dependencies version of the LLCT (Late Latin Charter "
702	        "Treebank) consists of an automated conversion of the LLCT2 treebank "
703	        "from the Latin Dependency Treebank (LDT) format into the Universal "
704	        "Dependencies standard."
705	    ),
706	    "la_perseus": (
707	        "This Universal Dependencies Latin Treebank consists of an automatic "
708	        "conversion of a selection of passages from the Ancient Greek and "
709	        "Latin Dependency Treebank 2.1"
710	    ),
711	    "la_proiel": (
712	        "The Latin PROIEL treebank is based on the Latin data from the PROIEL "
713	        "treebank, and contains most of the Vulgate New Testament translations"
714	        " plus selections from Caesar's Gallic War, Cicero's Letters to "
715	        "Atticus, Palladius' Opus Agriculturae and the first book of Cicero's "
716	        "De officiis."
717	    ),
718	    "lv_lvtb": (
719	        "Latvian UD Treebank is based on Latvian Treebank (LVTB), being "
720	        "created at University of Latvia, Institute of Mathematics and "
721	        "Computer Science, Artificial Intelligence Laboratory."
722	    ),
723	    "lij_glt": (
724	        "The Genoese Ligurian Treebank is a small, manually annotated "
725	        "collection of contemporary Ligurian prose. The focus of the treebank "
726	        "is written Genoese, the koin variety of Ligurian which is associated"
727	        " with today's literary, journalistic and academic ligurophone sphere."
728	    ),
729	    "lt_alksnis": (
730	        "The Lithuanian dependency treebank ALKSNIS v3.0 (Vytautas Magnus "
731	        "University)."
732	    ),
733	    "lt_hse": (
734	        "Lithuanian treebank annotated manually (dependencies) using the "
735	        "Morphological Annotator by CCL, Vytautas Magnus University "
736	        "(http://tekstynas.vdu.lt/) and manual disambiguation. A pilot version"
737	        " which includes news and an essay by Tomas Venclova is available "
738	        "here."
739	    ),
740	    "olo_kkpp": (
741	        "UD Livvi-KKPP is a manually annotated new corpus of Livvi-Karelian "
742	        "made directly in the Universal dependencies annotation scheme. The "
743	        "data is collected from VepKar corpora and consists of mostly modern "
744	        "news texts but also some stories and educational texts."
745	    ),
746	    "nds_lsdc": (
747	        "The UD Low Saxon LSDC dataset consists of sentences in 18 Low Saxon "
748	        "dialects from both Germany and the Netherlands. These sentences are "
749	        "(or are to become) part of the LSDC dataset and represent the "
750	        "language from the 19th and early 20th century in genres such as short"
751	        " stories, novels, speeches, letters and fairytales."
752	    ),
753	    "mt_mudt": (
754	        "MUDT (Maltese Universal Dependencies Treebank) is a manually "
755	        "annotated treebank of Maltese, a Semitic language of Malta descended "
756	        "from North African Arabic with a significant amount of Italo-Romance "
757	        "influence. MUDT was designed as a balanced corpus with four major "
758	        "genres (see Splitting below) represented roughly equally."
759	    ),
760	    "gv_cadhan": (
761	        "This is the Cadhan Aonair UD treebank for Manx Gaelic, created by "
762	        "Kevin Scannell."
763	    ),
764	    "mr_ufal": (
765	        "UD Marathi is a manually annotated treebank consisting primarily of "
766	        "stories from Wikisource, and parts of an article on Wikipedia."
767	    ),
768	    "gun_dooley": (
769	        "UD Mbya_Guarani-Dooley is a corpus of narratives written in Mby "
770	        "Guaran (Tupian) in Brazil, and collected by Robert Dooley. Due to "
771	        "copyright restrictions, the corpus that is distributed as part of UD "
772	        "only contains the annotation (tags, features, relations) while the "
773	        "FORM and LEMMA columns are empty."
774	    ),
775	    "gun_thomas": (
776	        "UD Mbya_Guarani-Thomas is a corpus of Mby Guaran (Tupian) texts "
777	        "collected by Guillaume Thomas. The current version of the corpus "
778	        "consists of three speeches by Paulina Kerechu Nez Romero, a Mby "
779	        "Guaran speaker from Ytu, Caazap Department, Paraguay."
780	    ),
781	    "mdf_jr": (
782	        "Erme Universal Dependencies annotated texts Moksha are the origin of "
783	        "UD_Moksha-JR with annotation (CoNLL-U) for texts in the Moksha "
784	        "language, it originally consists of a sample from a number of fiction"
785	        " authors writing originals in Moksha."
786	    ),
787	    "myu_tudet": (
788	        "UD_Munduruku-TuDeT is a collection of annotated sentences in "
789	        "Munduruk. Together with UD_Akuntsu-TuDeT and UD_Tupinamba-TuDeT, "
790	        "UD_Munduruku-TuDeT is part of the TuLaR project."
791	    ),
792	    "pcm_nsc": (
793	        "A Universal Dependencies corpus for spoken Naija (Nigerian Pidgin)."
794	    ),
795	    "nyq_aha": (
796	        "The AHA Nayini Treebank is a small treebank for contemporary Nayini. "
797	        "Its corpus is collected and annotated manually. We have prepared this"
798	        " treebank based on interviews with Nayini speakers."
799	    ),
800	    "sme_giella": (
801	        "This is a North Smi treebank based on a manually disambiguated and "
802	        "function-labelled gold-standard corpus of North Smi produced by the "
803	        "Giellatekno team at UiT Norgga rktala universitehta."
804	    ),
805	    "no_bokmaal": (
806	        "The Norwegian UD treebank is based on the Bokml section of the "
807	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
808	        " Norwegian. NDT has been automatically converted to the UD scheme by "
809	        "Lilja vrelid at the University of Oslo."
810	    ),
811	    "no_nynorsk": (
812	        "The Norwegian UD treebank is based on the Nynorsk section of the "
813	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
814	        " Norwegian.  NDT has been automatically converted to the UD scheme by"
815	        " Lilja vrelid at the University of Oslo."
816	    ),
817	    "no_nynorsklia": (
818	        "This Norwegian treebank is based on the LIA treebank of transcribed "
819	        "spoken Norwegian dialects. The treebank has been automatically "
820	        "converted to the UD scheme by Lilja vrelid at the University of "
821	        "Oslo."
822	    ),
823	    "cu_proiel": (
824	        "The Old Church Slavonic (OCS) UD treebank is based on the Old Church "
825	        "Slavonic data from the PROIEL treebank and contains the text of the "
826	        "Codex Marianus New Testament translation."
827	    ),
828	    "fro_srcmf": (
829	        "UD_Old_French-SRCMF is a conversion of (part of) the SRCMF corpus "
830	        "(Syntactic Reference Corpus of Medieval French srcmf.org)."
831	    ),
832	    "orv_birchbark": (
833	        "UD Old_East_Slavic-Birchbark is based on the RNC Corpus of Birchbark "
834	        "Letters and includes documents written in 1025-1500 in an East Slavic"
835	        " vernacular (letters, household and business records, records for "
836	        "church services, spell against diseases, and other short "
837	        "inscriptions). The treebank is manually syntactically annotated in "
838	        "the UD 2.0 scheme, morphological and lexical annotation is a "
839	        "conversion of the original RNC annotation."
840	    ),
841	    "orv_rnc": (
842	        "`UD_Old_Russian-RNC` is a sample of the Middle Russian corpus "
843	        "(1300-1700), a part of the Russian National Corpus. The data were "
844	        "originally annotated according to the RNC and extended UD-Russian "
845	        "morphological schemas and UD 2.4 dependency schema."
846	    ),
847	    "orv_torot": (
848	        "UD_Old_Russian-TOROT is a conversion of a selection of the Old East "
849	        "Slavonic and Middle Russian data in the Troms Old Russian and OCS "
850	        "Treebank (TOROT), which was originally annotated in PROIEL dependency"
851	        " format."
852	    ),
853	    "otk_tonqq": (
854	        "`UD_Old_Turkish-Tonqq` is an Old Turkish treebank built upon Turkic "
855	        "script texts or sentences that are trivially convertible."
856	    ),
857	    "fa_perdt": (
858	        "The Persian Universal Dependency Treebank (PerUDT) is the result of "
859	        "automatic coversion of Persian Dependency Treebank (PerDT) with "
860	        "extensive manual corrections. Please refer to the follwoing work, if "
861	        "you use this data: Mohammad Sadegh Rasooli, Pegah Safari, Amirsaeid "
862	        "Moloodi, and Alireza Nourian. 'The Persian Dependency Treebank Made "
863	        "Universal'. 2020 (to appear)."
864	    ),
865	    "fa_seraji": (
866	        "The Persian Universal Dependency Treebank (Persian UD) is based on "
867	        "Uppsala Persian Dependency Treebank (UPDT). The conversion of the "
868	        "UPDT to the Universal Dependencies was performed semi-automatically "
869	        "with extensive manual checks and corrections."
870	    ),
871	    "pl_lfg": (
872	        "The LFG Enhanced UD treebank of Polish is based on a corpus of LFG "
873	        "(Lexical Functional Grammar) syntactic structures generated by an LFG"
874	        " grammar of Polish, POLFIE, and manually disambiguated by human "
875	        "annotators."
876	    ),
877	    "pl_pdb": (
878	        "The Polish PDB-UD treebank is based on the Polish Dependency Bank 2.0"
879	        " (PDB 2.0), created at the Institute of Computer Science, Polish "
880	        "Academy of Sciences in Warsaw. The PDB-UD treebank is an extended and"
881	        " corrected version of the Polish SZ-UD treebank (the release 1.2 to "
882	        "2.3)."
883	    ),
884	    "pl_pud": (
885	        "This is the Polish portion of the Parallel Universal Dependencies "
886	        "(PUD) treebanks, created at the Institute of Computer Science, Polish"
887	        " Academy of Sciences in Warsaw.Re"
888	    ),
889	    "pt_bosque": (
890	        "This Universal Dependencies (UD) Portuguese treebank is based on the "
891	        "Constraint Grammar converted version of the Bosque, which is part of "
892	        "the Floresta Sint(c)tica treebank. It contains both European "
893	        "(CETEMPblico) and Brazilian (CETENFolha) variants."
894	    ),
895	    "pt_gsd": (
896	        "The Brazilian Portuguese UD is converted from the Google Universal "
897	        "Dependency Treebank v2.0 (legacy)."
898	    ),
899	    "pt_pud": (
900	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
901	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
902	        "Raw Text to Universal Dependencies."
903	    ),
904	    "ro_art": (
905	        "The UD treebank ArT is a treebank of the Aromanian dialect of the "
906	        "Romanian language in UD format."
907	    ),
908	    "ro_nonstandard": (
909	        "The Romanian Non-standard UD treebank (called UAIC-RoDia) is based on"
910	        " UAIC-RoDia Treebank. UAIC-RoDia = ISLRN 156-635-615-024-0"
911	    ),
912	    "ro_rrt": (
913	        "The Romanian UD treebank (called RoRefTrees) (Barbu Mititelu et al., "
914	        "2016) is the reference treebank in UD format for standard Romanian."
915	    ),
916	    "ro_simonero": "SiMoNERo is a medical corpus of contemporary Romanian.",
917	    "ru_gsd": (
918	        "Russian Universal Dependencies Treebank annotated and converted by "
919	        "Google."
920	    ),
921	    "ru_pud": (
922	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
923	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
924	        "Raw Text to Universal Dependencies."
925	    ),
926	    "ru_syntagrus": "Russian data from the SynTagRus corpus.",
927	    "ru_taiga": (
928	        "Universal Dependencies treebank is based on data samples extracted "
929	        "from Taiga Corpus and MorphoRuEval-2017 and GramEval-2020 shared "
930	        "tasks collections."
931	    ),
932	    "sa_ufal": (
933	        "A small Sanskrit treebank of sentences from Pacatantra, an ancient "
934	        "Indian collection of interrelated fables by Vishnu Sharma."
935	    ),
936	    "sa_vedic": (
937	        "The Treebank of Vedic Sanskrit contains 4,000 sentences with 27,000 "
938	        "words chosen from metrical and prose passages of the gveda (RV), the"
939	        " aunaka recension of the Atharvaveda (S), the Maitryasahit "
940	        "(MS), and the Aitareya- (AB) and atapatha-Brhmaas (B). Lexical "
941	        "and morpho-syntactic information has been generated using a tagging "
942	        "software and manually validated. POS tags have been induced "
943	        "automatically from the morpho-sytactic information of each word."
944	    ),
945	    "gd_arcosg": (
946	        "A treebank of Scottish Gaelic based on the Annotated Reference Corpus"
947	        " Of Scottish Gaelic (ARCOSG)."
948	    ),
949	    "sr_set": (
950	        "The Serbian UD treebank is based on the "
951	        "[SETimes-SR](http://hdl.handle.net/11356/1200) corpus and additional "
952	        "news documents from the Serbian web."
953	    ),
954	    "sms_giellagas": (
955	        "The UD Skolt Sami Giellagas treebank is based almost entirely on "
956	        "spoken Skolt Sami corpora."
957	    ),
958	    "sk_snk": (
959	        "The Slovak UD treebank is based on data originally annotated as part "
960	        "of the Slovak National Corpus, following the annotation style of the "
961	        "Prague Dependency Treebank."
962	    ),
963	    "sl_ssj": (
964	        "The Slovenian UD Treebank is a rule-based conversion of the ssj500k "
965	        "treebank, the largest collection of manually syntactically annotated "
966	        "data in Slovenian, originally annotated in the JOS annotation scheme."
967	    ),
968	    "sl_sst": (
969	        "The Spoken Slovenian UD Treebank (SST) is the first syntactically "
970	        "annotated corpus of spoken Slovenian, based on a sample of the "
971	        "reference GOS corpus, a collection of transcribed audio recordings of"
972	        " monologic, dialogic and multi-party spontaneous speech in different "
973	        "everyday situations."
974	    ),
975	    "soj_aha": (
976	        "The AHA Soi Treebank is a small treebank for contemporary Soi. Its "
977	        "corpus is collected and annotated manually. We have prepared this "
978	        "treebank based on interviews with Soi speakers."
979	    ),
980	    "ajp_madar": (
981	        "The South_Levantine_Arabic-MADAR treebank consists of 100 "
982	        "manually-annotated sentences taken from the "
983	        "[MADAR](https://camel.abudhabi.nyu.edu/madar/) (Multi-Arabic Dialect "
984	        "Applications and Resources) project. "
985	    ),
986	    "es_ancora": "Spanish data from the AnCora corpus.",
987	    "es_gsd": (
988	        "The Spanish UD is converted from the content head version of the "
989	        "universal dependency treebank v2.0 (legacy)."
990	    ),
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:40
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
39	    "akk_riao": (
40	        "UD_Akkadian-RIAO is a small treebank which consists of 22 277 words "
41	        "and 1845 sentences. This represents an intact subset of a total of "
42	        "2211 sentences from the early Neo-Assyrian royal inscriptions  of the"
43	        " tenth and ninth centuries BCE. These royal inscriptions were "
44	        "extracted from Oracc (Open Richly Annotated Cuneiform Corpus; "
45	        "http://oracc.museum.upenn.edu/riao/), where all Neo-Assyrian royal "
46	        "inscriptions are lemmatized word-for-word. The language of the corpus"
47	        " is Standard Babylonian, with occasional Assyrianisms, whereas "
48	        "Akkadian is the umbrella term for both Assyrian and Babylonian. The"
49	        " treebank was manually annotated following the UD annotation "
50	        "guidelines."
51	    ),
52	    "aqz_tudet": (
53	        "UD_Akuntsu-TuDeT is a collection of annotated texts in Akunts. "
54	        "Together with UD_Tupinamba-TuDeT and UD_Munduruku-TuDeT, "
55	        "UD_Akuntsu-TuDeT is part of the TuLaR project.  The sentences are "
56	        "being annotated by Carolina Aragon and Fabrcio Ferraz Gerardi."
57	    ),
58	    "sq_tsa": (
59	        "The UD Treebank for Standard Albanian (TSA) is a small treebank that "
60	        "consists of 60 sentences corresponding to 922 tokens. The data was "
61	        "collected from different Wikipedia entries. This treebank was created"
62	        " mainly manually following the Universal Dependencies guidelines. The"
63	        " lemmatization was performed using the lemmatizer "
64	        "https://bitbucket.org/timarkh/uniparser-albanian-grammar/src/master/ "
65	        "developed by the Albanian National Corpus team (Maria Morozova, "
66	        "Alexander Rusakov, Timofey Arkhangelskiy). Tagging and Morphological "
67	        "Analysis were semi-automated through python scripts and corrected "
68	        "manually, whereas Dependency relations were assigned fully manually. "
69	        "We encourage any initiatives to increase the size and/or improve the "
70	        "overall quality of the Treebank."
71	    ),
72	    "am_att": (
73	        "UD_Amharic-ATT is a manually annotated Treebanks. It is annotated for"
74	        " POS tag, morphological information and dependency relations. Since "
75	        "Amharic is a morphologically-rich, pro-drop, and languages having a "
76	        "feature of clitic doubling, clitics have been segmented manually."
77	    ),
78	    "grc_perseus": (
79	        "This Universal Dependencies Ancient Greek Treebank consists of an "
80	        "automatic conversion of a selection of passages from the Ancient "
81	        "Greek and Latin Dependency Treebank 2.1"
82	    ),
83	    "grc_proiel": (
84	        "The Ancient Greek PROIEL treebank is based on the Ancient Greek data "
85	        "from the PROIEL treebank, which is maintained at the Department of "
86	        "Philosophy, Classics, History of Arts and Ideas at the University of "
87	        "Oslo. The conversion is based on the 20180408 release of the PROIEL "
88	        "treebank available from "
89	        "https://github.com/proiel/proiel-treebank/releases. The original "
90	        "annotators are acknowledged in the files available there. The "
91	        "conversion code is available in the Rubygem proiel-cli, "
92	        "https://github.com/proiel/proiel-cli."
93	    ),
94	    "apu_ufpa": (
95	        "The initial release contains 70 annotated sentences. This is the "
96	        "first treebank in a language from the Arawak family. The original "
97	        "interlinear glosses are included in the tree bank, and their "
98	        "conversion into a full UD annotation is an ongoing process. The "
99	        "sent_id values (e.g.: FernandaM2017:Texto-6-19) are representative of"
100	        " the collector, year of publication, text identifier and the number "
101	        "of the sentence in order from the original text."
102	    ),
103	    "hbo_ptnk": (
104	        "UD Ancient Hebrew PTNK contains portions of the Biblia Hebraic "
105	        "Stuttgartensia with morphological annotations from ETCBC."
106	    ),
107	    "ar_nyuad": (
108	        "The treebank consists of 19,738 sentences (738889 tokens), and its "
109	        "domain is mainly newswire. The annotation is licensed under the terms"
110	        " of CC BY-SA 4.0, and the original PATB can be obtained from the "
111	        "LDCs official website."
112	    ),
113	    "ar_padt": (
114	        "The Arabic-PADT UD treebank is based on the Prague Arabic Dependency "
115	        "Treebank (PADT), created at the Charles University in Prague."
116	    ),
117	    "ar_pud": (
118	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
119	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
120	        "Raw Text to Universal Dependencies."
121	    ),
122	    "aii_as": (
123	        "The Uppsala Assyrian Treebank is a small treebank for Modern Standard"
124	        " Assyrian. The corpus is collected and annotated manually. The data "
125	        "was randomly collected from different textbooks and a short "
126	        "translation of The Merchant of Venice."
127	    ),
128	    "bm_crb": (
129	        "The UD Bambara treebank is a section of the Corpus Rfrence du "
130	        "Bambara annotated natively with Universal Dependencies."
131	    ),
132	    "eu_bdt": (
133	        "The Basque UD treebank is based on a automatic conversion from part "
134	        "of the Basque Dependency Treebank (BDT), created at the University of"
135	        " of the Basque Country by the IXA NLP research group. The treebank "
136	        "consists of 8.993 sentences (121.443 tokens) and covers mainly "
137	        "literary and journalistic texts."
138	    ),
139	    "bej_nsc": (
140	        "A Universal Dependencies corpus for Beja, North-Cushitic branch of "
141	        "the Afro-Asiatic phylum mainly spoken in Sudan, Egypt and Eritrea."
142	    ),
143	    "be_hse": (
144	        "The Belarusian UD treebank is based on a sample of the news texts "
145	        "included in the Belarusian-Russian parallel subcorpus of the Russian "
146	        "National Corpus, online search available at: "
147	        "http://ruscorpora.ru/search-para-be.html."
148	    ),
149	    "bn_bru": (
150	        "The BRU Bengali treebank has been created at Begum Rokeya University,"
151	        " Rangpur, by the members of Semantics Lab."
152	    ),
153	    "bho_bhtb": (
154	        "The Bhojpuri UD Treebank (BHTB) v2.6 consists of 6,664 tokens(357 "
155	        "sentences). This Treebank is a part of the Universal Dependency "
156	        "treebank project. Initially, it was initiated by me (Atul) at "
157	        "Jawaharlal Nehru University, New Delhi during the doctoral research "
158	        "work. BHTB data contains syntactic annotation according to "
159	        "dependency-constituency schema, as well as morphological tags and "
160	        "lemmas. In this data, XPOS is annotated  according to Bureau of "
161	        "Indian Standards (BIS) Part Of Speech (POS) tagset."
162	    ),
163	    "br_keb": (
164	        "UD Breton-KEB is a treebank of Breton that has been manually "
165	        "annotated according to the Universal Dependencies guidelines. The "
166	        "tokenisation guidelines and morphological annotation comes from a "
167	        "finite-state morphological analyser of Breton released as part of the"
168	        " Apertium project."
169	    ),
170	    "bg_btb": (
171	        "UD_Bulgarian-BTB is based on the HPSG-based BulTreeBank, created at "
172	        "the Institute of Information and Communication Technologies, "
173	        "Bulgarian Academy of Sciences. The original consists of 215,000 "
174	        "tokens (over 15,000 sentences)."
175	    ),
176	    "bxr_bdt": (
177	        "The UD Buryat treebank was annotated manually natively in UD and "
178	        "contains grammar book sentences, along with news and some fiction."
179	    ),
180	    "yue_hk": (
181	        "A Cantonese treebank (in Traditional Chinese characters) of film "
182	        "subtitles and of legislative proceedings of Hong Kong, parallel with "
183	        "the Chinese-HK treebank."
184	    ),
185	    "ca_ancora": "Catalan data from the AnCora corpus.",
186	    "ceb_gja": (
187	        "UD_Cebuano_GJA is a collection of annotated Cebuano sample sentences "
188	        "randomly taken from three different sources: community-contributed "
189	        "samples from the website Tatoeba, a Cebuano grammar book by Bunye & "
190	        "Yap (1971) and Tanangkinsing's reference grammar on Cebuano (2011). "
191	        "This project is currently work in progress."
192	    ),
193	    "zh_cfl": (
194	        "The Chinese-CFL UD treebank is manually annotated by Keying Li with "
195	        "minor manual revisions by Herman Leung and John Lee at City "
196	        "University of Hong Kong, based on essays written by learners of "
197	        "Mandarin Chinese as a foreign language. The data is in Simplified "
198	        "Chinese."
199	    ),
200	    "zh_gsd": (
201	        "Traditional Chinese Universal Dependencies Treebank annotated and "
202	        "converted by Google."
203	    ),
204	    "zh_gsdsimp": (
205	        "Simplified Chinese Universal Dependencies dataset converted from the "
206	        "GSD (traditional) dataset with manual corrections."
207	    ),
208	    "zh_hk": (
209	        "A Traditional Chinese treebank of film subtitles and of legislative "
210	        "proceedings of Hong Kong, parallel with the Cantonese-HK treebank."
211	    ),
212	    "zh_pud": (
213	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
214	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
215	        "Raw Text to Universal Dependencies."
216	    ),
217	    "ckt_hse": (
218	        "This data is a manual annotation of the corpus from multimedia "
219	        "annotated corpus of the Chuklang project, a dialectal corpus of the "
220	        "Amguema variant of Chukchi."
221	    ),
222	    "lzh_kyoto": (
223	        "Classical Chinese Universal Dependencies Treebank annotated and "
224	        "converted by Institute for Research in Humanities, Kyoto University."
225	    ),
226	    "cop_scriptorium": (
227	        "UD Coptic contains manually annotated Sahidic Coptic texts, including"
228	        " Biblical texts, sermons, letters, and hagiography."
229	    ),
230	    "hr_set": (
231	        "The Croatian UD treebank is based on the extension of the SETimes-HR "
232	        "corpus, the hr500k corpus."
233	    ),
234	    "cs_cac": (
235	        "The UD_Czech-CAC treebank is based on the Czech Academic Corpus 2.0 "
236	        "(CAC; esk akademick korpus; AK), created at Charles University in"
237	        " Prague."
238	    ),
239	    "cs_cltt": (
240	        "The UD_Czech-CLTT treebank is based on the Czech Legal Text Treebank "
241	        "1.0, created at Charles University in Prague."
242	    ),
243	    "cs_fictree": (
244	        "FicTree is a treebank of Czech fiction, automatically converted into "
245	        "the UD format. The treebank was built at Charles University in "
246	        "Prague."
247	    ),
248	    "cs_pdt": (
249	        "The Czech-PDT UD treebank is based on the Prague Dependency Treebank "
250	        "3.0 (PDT), created at the Charles University in Prague."
251	    ),
252	    "cs_pud": (
253	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
254	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
255	        "Raw Text to Universal Dependencies."
256	    ),
257	    "da_ddt": (
258	        "The Danish UD treebank is a conversion of the Danish Dependency "
259	        "Treebank."
260	    ),
261	    "nl_alpino": (
262	        "This corpus consists of samples from various treebanks annotated at "
263	        "the University of Groningen using the Alpino annotation tools and "
264	        "guidelines."
265	    ),
266	    "nl_lassysmall": (
267	        "This corpus contains sentences from the Wikipedia section of the "
268	        "Lassy Small Treebank. Universal Dependency annotation was generated "
269	        "automatically from the original annotation in Lassy."
270	    ),
271	    "en_esl": (
272	        "UD English-ESL / Treebank of Learner English (TLE) contains manual "
273	        "POS tag and dependency annotations for 5,124 English as a Second "
274	        "Language (ESL) sentences drawn from the Cambridge Learner Corpus "
275	        "First Certificate in English (FCE) dataset."
276	    ),
277	    "en_ewt": (
278	        "A Gold Standard Universal Dependencies Corpus for English, built over"
279	        " the source material of the English Web Treebank LDC2012T13 "
280	        "(https://catalog.ldc.upenn.edu/LDC2012T13)."
281	    ),
282	    "en_gum": (
283	        "Universal Dependencies syntax annotations from the GUM corpus "
284	        "(https://corpling.uis.georgetown.edu/gum/)."
285	    ),
286	    "en_gumreddit": (
287	        "Universal Dependencies syntax annotations from the Reddit portion of "
288	        "the GUM corpus (https://corpling.uis.georgetown.edu/gum/) "
289	    ),
290	    "en_lines": (
291	        "UD English_LinES is the English half of the LinES Parallel Treebank "
292	        "with the original dependency annotation first automatically converted"
293	        " into Universal Dependencies and then partially reviewed. Its "
294	        "contents cover literature, an online manual and Europarl data."
295	    ),
296	    "en_atis": (
297	        "UD Atis Treebank is a manually annotated treebank consisting of the "
298	        "sentences in the Atis (Airline Travel Informations) dataset which "
299	        "includes the human speech transcriptions of people asking for flight "
300	        "information on the automated inquiry systems."
301	    ),
302	    "en_partut": (
303	        "UD_English-ParTUT is a conversion of a multilingual parallel treebank"
304	        " developed at the University of Turin, and consisting of a variety of"
305	        " text genres, including talks, legal texts and Wikipedia articles, "
306	        "among others."
307	    ),
308	    "en_pronouns": (
309	        "UD English-Pronouns is dataset created to make pronoun identification"
310	        " more accurate and with a more balanced distribution across genders. "
311	        "The dataset is initially targeting the Independent Genitive pronouns,"
312	        " 'hers', (independent) 'his', (singular) 'theirs', 'mine', and "
313	        "(singular) 'yours'."
314	    ),
315	    "en_pud": (
316	        "This is the English portion of the Parallel Universal Dependencies "
317	        "(PUD) treebanks created for the CoNLL 2017 shared task on "
318	        "Multilingual Parsing from Raw Text to Universal Dependencies "
319	        "(http://universaldependencies.org/conll17/)."
320	    ),
321	    "myv_jr": (
322	        "UD Erzya is the original annotation (CoNLL-U) for texts in the Erzya "
323	        "language, it originally consists of a sample from a number of fiction"
324	        " authors writing originals in Erzya."
325	    ),
326	    "et_edt": (
327	        "UD Estonian is a converted version of the Estonian Dependency "
328	        "Treebank (EDT), originally annotated in the Constraint Grammar (CG) "
329	        "annotation scheme, and consisting of genres of fiction, newspaper "
330	        "texts and scientific texts. The treebank contains 30,972 trees, "
331	        "437,769 tokens."
332	    ),
333	    "et_ewt": (
334	        "UD EWT treebank consists of different genres of new media. The "
335	        "treebank contains 4,493 trees, 56,399 tokens."
336	    ),
337	    "fo_farpahc": (
338	        "UD_Icelandic-FarPaHC is a conversion of the Faroese Parsed Historical"
339	        " Corpus (FarPaHC) to the Universal Dependencies scheme. The "
340	        "conversion was done using UDConverter."
341	    ),
342	    "fo_oft": "This is a treebank of Faroese based on the Faroese Wikipedia.",
343	    "fi_ftb": (
344	        "FinnTreeBank 1 consists of manually annotated grammatical examples "
345	        "from VISK. The UD version of FinnTreeBank 1 was converted from a "
346	        "native annotation model with a script and later manually revised."
347	    ),
348	    "fi_ood": (
349	        "Finnish-OOD is an external out-of-domain test set for Finnish-TDT "
350	        "annotated natively into UD scheme."
351	    ),
352	    "fi_pud": (
353	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
354	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
355	        "Raw Text to Universal Dependencies."
356	    ),
357	    "fi_tdt": (
358	        "UD_Finnish-TDT is based on the Turku Dependency Treebank (TDT), a "
359	        "broad-coverage dependency treebank of general Finnish covering "
360	        "numerous genres. The conversion to UD was followed by extensive "
361	        "manual checks and corrections, and the treebank closely adheres to "
362	        "the UD guidelines."
363	    ),
364	    "fr_fqb": (
365	        "The corpus **UD_French-FQB** is an automatic conversion of the French"
366	        " QuestionBank v1, a corpus entirely made of questions."
367	    ),
368	    "fr_ftb": (
369	        "The Universal Dependency version of the French Treebank (Abeill et "
370	        "al., 2003), hereafter UD_French-FTB, is a treebank of sentences from "
371	        "the newspaper Le Monde, initially manually annotated with "
372	        "morphological information and phrase-structure and then converted to "
373	        "the Universal Dependencies annotation scheme."
374	    ),
375	    "fr_gsd": (
376	        "The **UD_French-GSD** was converted in 2015 from the content head "
377	        "version of the universal dependency treebank v2.0 "
378	        "(https://github.com/ryanmcd/uni-dep-tb). It is updated since 2015 "
379	        "independently from the previous source."
380	    ),
381	    "fr_partut": (
382	        "UD_French-ParTUT is a conversion of a multilingual parallel treebank "
383	        "developed at the University of Turin, and consisting of a variety of "
384	        "text genres, including talks, legal texts and Wikipedia articles, "
385	        "among others."
386	    ),
387	    "fr_rhapsodie": "A Universal Dependencies corpus for spoken French.",
388	    "fr_parisstories": (
389	        "Paris Stories is a corpus of oral French collected and transcribed by"
390	        " Linguistics students from Sorbonne Nouvelle and corrected by "
391	        "students from the Plurital Master's Degree of Computational "
392	        "Linguistics ( Inalco, Paris Nanterre, Sorbonne Nouvelle) between 2017"
393	        " and 2021. It contains monologues and dialogues from speakers living "
394	        "in the Parisian region."
395	    ),
396	    "fr_pud": (
397	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
398	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
399	        "Raw Text to Universal Dependencies."
400	    ),
401	    "fr_sequoia": (
402	        "UD_French-Sequoia is an automatic conversion of the Sequoia Treebank "
403	        "corpus French Sequoia corpus."
404	    ),
405	    "gl_ctg": (
406	        "The Galician UD treebank is based on the automatic parsing of the "
407	        "Galician Technical Corpus (http://sli.uvigo.gal/CTG) created at the "
408	        "University of Vigo by the the TALG NLP research group."
409	    ),
410	    "gl_treegal": (
411	        "The Galician-TreeGal is a treebank for Galician developed at LyS "
412	        "Group (Universidade da Corua)."
413	    ),
414	    "de_gsd": (
415	        "The German UD is converted from the content head version of the "
416	        "universal dependency treebank v2.0 (legacy)."
417	    ),
418	    "de_hdt": (
419	        "UD German-HDT is a conversion of the Hamburg Dependency Treebank, "
420	        "created at the University of Hamburg through manual annotation in "
421	        "conjunction with a standard for morphologically and syntactically "
422	        "annotating sentences as well as a constraint-based parser."
423	    ),
424	    "de_lit": (
425	        "This treebank aims at gathering texts of the German literary history."
426	        " Currently, it hosts Fragments of the early Romanticism, i.e. "
427	        "aphorism-like texts mainly dealing with philosophical issues "
428	        "concerning art, beauty and related topics."
429	    ),
430	    "de_pud": (
431	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
432	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
433	        "Raw Text to Universal Dependencies."
434	    ),
435	    "got_proiel": (
436	        "The UD Gothic treebank is based on the Gothic data from the PROIEL "
437	        "treebank, and consists of Wulfila's Bible translation."
438	    ),
439	    "el_gdt": (
440	        "The Greek UD treebank (UD_Greek-GDT) is derived from the Greek "
441	        "Dependency Treebank (http://gdt.ilsp.gr), a resource developed and "
442	        "maintained by researchers at the Institute for Language and Speech "
443	        "Processing/Athena R.C. (http://www.ilsp.gr)."
444	    ),
445	    "gub_tudet": (
446	        "UD_Guajajara-TuDeT is a collection of annotated sentences in "
447	        "Guajajara. Sentences stem from multiple sources such as descriptions "
448	        "of the language, short stories, dictionaries and translations from "
449	        "the New Testament. Sentence annotation and documentation by Lorena "
450	        "Martn Rodrguez and Fabrcio Ferraz Gerardi."
451	    ),
452	    "gn_oldtudet": (
453	        "UD_Guarani-OldTuDeT is a collection of annotated texts in Old "
454	        "Guaran. All known sources in this language are being annotated: "
455	        "cathesisms, grammars (seventeenth and eighteenth century), sentences "
456	        "from dictionaries, and other texts. Sentence annotation and "
457	        "documentation by Fabrcio Ferraz Gerardi and Lorena Martn Rodrguez."
458	    ),
459	    "he_htb": "A Universal Dependencies Corpus for Hebrew.",
460	    "he_iahltwiki": (
461	        "Publicly available subset of the IAHLT UD Hebrew Treebank's Wikipedia"
462	        " section (https://www.iahlt.org/)"
463	    ),
464	    "qfn_fame": (
465	        "UD_Frisian_Dutch-Fame is a selection of 400 sentences from the FAME! "
466	        "speech corpus by Yilmaz et al. (2016a, 2016b). The treebank is "
467	        "manually annotated using the UD scheme."
468	    ),
469	    "qhe_hiencs": (
470	        "The Hindi-English Code-switching treebank is based on code-switching "
471	        "tweets of Hindi and English multilingual speakers (mostly Indian) on "
472	        "Twitter. The treebank is manually annotated using UD sceheme. The "
473	        "training and evaluations sets were seperately annotated by different "
474	        "annotators using UD v2 and v1 guidelines respectively. The evaluation"
475	        " sets are automatically converted from UD v1 to v2."
476	    ),
477	    "hi_hdtb": (
478	        "The Hindi UD treebank is based on the Hindi Dependency Treebank "
479	        "(HDTB), created at IIIT Hyderabad, India."
480	    ),
481	    "hi_pud": (
482	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
483	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
484	        "Raw Text to Universal Dependencies."
485	    ),
486	    "hu_szeged": (
487	        "The Hungarian UD treebank is derived from the Szeged Dependency "
488	        "Treebank (Vincze et al. 2010)."
489	    ),
490	    "is_modern": (
491	        "UD_Icelandic-Modern is a conversion of the modern additions to the "
492	        "Icelandic Parsed Historical Corpus (IcePaHC) to the Universal "
493	        "Dependencies scheme."
494	    ),
495	    "is_icepahc": (
496	        "UD_Icelandic-IcePaHC is a conversion of the Icelandic Parsed "
497	        "Historical Corpus (IcePaHC) to the Universal Dependencies scheme. The"
498	        " conversion was done using UDConverter."
499	    ),
500	    "is_pud": (
501	        "Icelandic-PUD is the Icelandic part of the Parallel Universal "
502	        "Dependencies (PUD) treebanks."
503	    ),
504	    "id_csui": (
505	        "UD Indonesian-CSUI is a conversion from an Indonesian constituency "
506	        "treebank in the Penn Treebank format named Kethu that was also a "
507	        "conversion from a constituency treebank built by Dinakaramani et al. "
508	        "(2015). We named this treebank Indonesian-CSUI, since all the three "
509	        "versions of the treebanks were built at Faculty of Computer Science, "
510	        "Universitas Indonesia."
511	    ),
512	    "id_gsd": (
513	        "The Indonesian UD is converted from the content head version of the "
514	        "universal dependency treebank v2.0 (legacy)."
515	    ),
516	    "id_pud": (
517	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
518	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
519	        "Raw Text to Universal Dependencies."
520	    ),
521	    "ga_idt": (
522	        "A Universal Dependencies 4910-sentence treebank for modern Irish."
523	    ),
524	    "ga_twittirish": (
525	        "A Universal Dependencies treebank of 866 tweets in modern Irish."
526	    ),
527	    "it_isdt": (
528	        "The Italian corpus annotated according to the UD annotation scheme "
529	        "was obtained by conversion from ISDT (Italian Stanford Dependency "
530	        "Treebank), released for the dependency parsing shared task of "
531	        "Evalita-2014 (Bosco et al. 2014)."
532	    ),
533	    "it_partut": (
534	        "UD_Italian-ParTUT is a conversion of a multilingual parallel treebank"
535	        " developed at the University of Turin, and consisting of a variety of"
536	        " text genres, including talks, legal texts and Wikipedia articles, "
537	        "among others."
538	    ),
539	    "it_postwita": (
540	        "PoSTWITA-UD is a collection of Italian tweets annotated in Universal "
541	        "Dependencies that can be exploited for the training of NLP systems to"
542	        " enhance their performance on social media texts."
543	    ),
544	    "it_markit": (
545	        "It is MarkIT That is New: An Italian Treebank of Marked "
546	        "Constructions. Teresa Paccosi, Alessio Palmero Aprosio and Sara "
547	        "Tonelli, To appear in Proceedings of the Eighth Italian Conference on"
548	        " Computational Linguistics 2022 (CLIC-it 2021)"
549	    ),
550	    "it_valico": (
551	        "Manually corrected Treebank of Learner Italian drawn from the Valico "
552	        "corpus and correspondent corrected sentences."
553	    ),
554	    "it_pud": (
555	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
556	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
557	        "Raw Text to Universal Dependencies."
558	    ),
559	    "it_twittiro": (
560	        "TWITTIR-UD is a collection of ironic Italian tweets annotated in "
561	        "Universal Dependencies. The treebank can be exploited for the "
562	        "training of NLP systems to enhance their performance on social media "
563	        "texts, and in particular, for irony detection purposes."
564	    ),
565	    "it_vit": (
566	        "The UD_Italian-VIT corpus was obtained by conversion from VIT (Venice"
567	        " Italian Treebank), developed at the Laboratory of Computational "
568	        "Linguistics of the Universit Ca' Foscari in Venice (Delmonte et al. "
569	        "2007; Delmonte 2009; "
570	        "http://rondelmo.it/resource/VIT/Browser-VIT/index.htm)."
571	    ),
572	    "ja_pudluw": (
573	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
574	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
575	        "Raw Text to Universal Dependencies."
576	    ),
577	    "ja_bccwjluw": (
578	        "This Universal Dependencies (UD) Japanese treebank is based on the "
579	        "definition of UD Japanese convention described in the UD "
580	        "documentation. The original sentences are from `Balanced Corpus of "
581	        "Contemporary Written Japanese'(BCCWJ)."
582	    ),
583	    "ja_gsdluw": (
584	        "This Universal Dependencies (UD) Japanese treebank is based on the "
585	        "definition of UD Japanese convention described in the UD "
586	        "documentation. The original sentences are from Google UDT 2.0."
587	    ),
588	    "ja_bccwj": (
589	        "This Universal Dependencies (UD) Japanese treebank is based on the "
590	        "definition of UD Japanese convention described in the UD "
591	        "documentation. The original sentences are from `Balanced Corpus of "
592	        "Contemporary Written Japanese'(BCCWJ)."
593	    ),
594	    "ja_gsd": (
595	        "This Universal Dependencies (UD) Japanese treebank is based on the "
596	        "definition of UD Japanese convention described in the UD "
597	        "documentation.  The original sentences are from Google UDT 2.0."
598	    ),
599	    "ja_modern": (
600	        "This Universal Dependencies (UD) Japanese treebank is based on the "
601	        "definition of UD Japanese convention described in the UD "
602	        "documentation. The original sentences are from `Corpus of Historical "
603	        "Japanese' (CHJ)."
604	    ),
605	    "ja_pud": (
606	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
607	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
608	        " Raw Text to Universal "
609	        "Dependencies](http://universaldependencies.org/conll17/)."
610	    ),
611	    "jv_csui": (
612	        "UD Javanese-CSUI is a dependency treebank in Javanese, a regional "
613	        "language in Indonesia with more than 60 million users. The original "
614	        "sentences were taken from OPUS, especially from the WikiMatrix v1 "
615	        "corpus. We revised the sentences that contained more Indonesian words"
616	        " than Javanese words and manually annotated them."
617	    ),
618	    "urb_tudet": (
619	        "UD_Kaapor-TuDeT is a collection of annotated sentences in Ka'apor. "
620	        "The project is a work in progress and the treebank is being updated "
621	        "on a regular basis."
622	    ),
623	    "xnr_kdtb": (
624	        "The Kangri UD Treebank (KDTB) is a part of the Universal Dependency "
625	        "treebank project."
626	    ),
627	    "krl_kkpp": (
628	        "UD Karelian-KKPP is a manually annotated new corpus of Karelian made "
629	        "in Universal dependencies annotation scheme. The data is collected "
630	        "from VepKar corpora and consists of mostly modern news texts but also"
631	        " some stories and educational texts."
632	    ),
633	    "kk_ktb": (
634	        "The UD Kazakh treebank is a combination of text from various sources "
635	        "including Wikipedia, some folk tales, sentences from the UDHR, news "
636	        "and phrasebook sentences. Sentences IDs include partial document "
637	        "identifiers."
638	    ),
639	    "arr_tudet": (
640	        "UD_Karo-TuDeT is a collection of annotated sentences in Karo. The "
641	        "sentences stem from the only grammatical description of the language "
642	        "(Gabas, 1999) and from the sentences in the dictionary by the same "
643	        "author (Gabas, 2007). Sentence annotation and documentation by "
644	        "Fabrcio Ferraz Gerardi."
645	    ),
646	    "kfm_aha": (
647	        "The AHA Khunsari Treebank is a small treebank for contemporary "
648	        "Khunsari. Its corpus is collected and annotated manually. We have "
649	        "prepared this treebank based on interviews with Khunsari speakers."
650	    ),
651	    "quc_iu": (
652	        "UD Kiche-IU is a treebank consisting of sentences from a variety of"
653	        " text domains but principally dictionary example sentences and "
654	        "linguistic examples."
655	    ),
656	    "koi_uh": (
657	        "This is a Komi-Permyak literary language treebank consisting of "
658	        "original and translated texts."
659	    ),
660	    "kpv_ikdp": (
661	        "This treebank consists of dialectal transcriptions of spoken "
662	        "Komi-Zyrian. The current texts are short recorded segments from "
663	        "different areas where the Iva dialect of Komi language is spoken."
664	    ),
665	    "kpv_lattice": (
666	        "UD Komi-Zyrian Lattice is a treebank of written standard Komi-Zyrian."
667	    ),
668	    "ko_gsd": (
669	        "The Google Korean Universal Dependency Treebank is first converted "
670	        "from the Universal Dependency Treebank v2.0 (legacy), and then "
671	        "enhanced by Chun et al., 2018."
672	    ),
673	    "ko_kaist": (
674	        "The KAIST Korean Universal Dependency Treebank is generated by Chun "
675	        "et al., 2018 from the constituency trees in the KAIST Tree-Tagging "
676	        "Corpus."
677	    ),
678	    "ko_pud": (
679	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
680	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
681	        "Raw Text to Universal Dependencies."
682	    ),
683	    "kmr_mg": (
684	        "The UD Kurmanji corpus is a corpus of Kurmanji Kurdish. It contains "
685	        "fiction and encyclopaedic texts in roughly equal measure. It has been"
686	        " annotated natively in accordance with the UD annotation scheme."
687	    ),
688	    "la_ittb": (
689	        "Latin data from the _Index Thomisticus_ Treebank. Data are taken from"
690	        " the _Index Thomisticus_ corpus by Roberto Busa SJ, which contains "
691	        "the complete work by Thomas Aquinas (12251274; Medieval Latin) and "
692	        "by 61 other authors related to Thomas."
693	    ),
694	    "la_udante": (
695	        "The UDante treebank is based on the Latin texts of Dante Alighieri, "
696	        "taken from the DanteSearch corpus, originally created at the "
697	        "University of Pisa, Italy. It is a treebank of Latin language, more "
698	        "precisely of literary Medieval Latin (XIVth century)."
699	    ),
700	    "la_llct": (
701	        "This Universal Dependencies version of the LLCT (Late Latin Charter "
702	        "Treebank) consists of an automated conversion of the LLCT2 treebank "
703	        "from the Latin Dependency Treebank (LDT) format into the Universal "
704	        "Dependencies standard."
705	    ),
706	    "la_perseus": (
707	        "This Universal Dependencies Latin Treebank consists of an automatic "
708	        "conversion of a selection of passages from the Ancient Greek and "
709	        "Latin Dependency Treebank 2.1"
710	    ),
711	    "la_proiel": (
712	        "The Latin PROIEL treebank is based on the Latin data from the PROIEL "
713	        "treebank, and contains most of the Vulgate New Testament translations"
714	        " plus selections from Caesar's Gallic War, Cicero's Letters to "
715	        "Atticus, Palladius' Opus Agriculturae and the first book of Cicero's "
716	        "De officiis."
717	    ),
718	    "lv_lvtb": (
719	        "Latvian UD Treebank is based on Latvian Treebank (LVTB), being "
720	        "created at University of Latvia, Institute of Mathematics and "
721	        "Computer Science, Artificial Intelligence Laboratory."
722	    ),
723	    "lij_glt": (
724	        "The Genoese Ligurian Treebank is a small, manually annotated "
725	        "collection of contemporary Ligurian prose. The focus of the treebank "
726	        "is written Genoese, the koin variety of Ligurian which is associated"
727	        " with today's literary, journalistic and academic ligurophone sphere."
728	    ),
729	    "lt_alksnis": (
730	        "The Lithuanian dependency treebank ALKSNIS v3.0 (Vytautas Magnus "
731	        "University)."
732	    ),
733	    "lt_hse": (
734	        "Lithuanian treebank annotated manually (dependencies) using the "
735	        "Morphological Annotator by CCL, Vytautas Magnus University "
736	        "(http://tekstynas.vdu.lt/) and manual disambiguation. A pilot version"
737	        " which includes news and an essay by Tomas Venclova is available "
738	        "here."
739	    ),
740	    "olo_kkpp": (
741	        "UD Livvi-KKPP is a manually annotated new corpus of Livvi-Karelian "
742	        "made directly in the Universal dependencies annotation scheme. The "
743	        "data is collected from VepKar corpora and consists of mostly modern "
744	        "news texts but also some stories and educational texts."
745	    ),
746	    "nds_lsdc": (
747	        "The UD Low Saxon LSDC dataset consists of sentences in 18 Low Saxon "
748	        "dialects from both Germany and the Netherlands. These sentences are "
749	        "(or are to become) part of the LSDC dataset and represent the "
750	        "language from the 19th and early 20th century in genres such as short"
751	        " stories, novels, speeches, letters and fairytales."
752	    ),
753	    "mt_mudt": (
754	        "MUDT (Maltese Universal Dependencies Treebank) is a manually "
755	        "annotated treebank of Maltese, a Semitic language of Malta descended "
756	        "from North African Arabic with a significant amount of Italo-Romance "
757	        "influence. MUDT was designed as a balanced corpus with four major "
758	        "genres (see Splitting below) represented roughly equally."
759	    ),
760	    "gv_cadhan": (
761	        "This is the Cadhan Aonair UD treebank for Manx Gaelic, created by "
762	        "Kevin Scannell."
763	    ),
764	    "mr_ufal": (
765	        "UD Marathi is a manually annotated treebank consisting primarily of "
766	        "stories from Wikisource, and parts of an article on Wikipedia."
767	    ),
768	    "gun_dooley": (
769	        "UD Mbya_Guarani-Dooley is a corpus of narratives written in Mby "
770	        "Guaran (Tupian) in Brazil, and collected by Robert Dooley. Due to "
771	        "copyright restrictions, the corpus that is distributed as part of UD "
772	        "only contains the annotation (tags, features, relations) while the "
773	        "FORM and LEMMA columns are empty."
774	    ),
775	    "gun_thomas": (
776	        "UD Mbya_Guarani-Thomas is a corpus of Mby Guaran (Tupian) texts "
777	        "collected by Guillaume Thomas. The current version of the corpus "
778	        "consists of three speeches by Paulina Kerechu Nez Romero, a Mby "
779	        "Guaran speaker from Ytu, Caazap Department, Paraguay."
780	    ),
781	    "mdf_jr": (
782	        "Erme Universal Dependencies annotated texts Moksha are the origin of "
783	        "UD_Moksha-JR with annotation (CoNLL-U) for texts in the Moksha "
784	        "language, it originally consists of a sample from a number of fiction"
785	        " authors writing originals in Moksha."
786	    ),
787	    "myu_tudet": (
788	        "UD_Munduruku-TuDeT is a collection of annotated sentences in "
789	        "Munduruk. Together with UD_Akuntsu-TuDeT and UD_Tupinamba-TuDeT, "
790	        "UD_Munduruku-TuDeT is part of the TuLaR project."
791	    ),
792	    "pcm_nsc": (
793	        "A Universal Dependencies corpus for spoken Naija (Nigerian Pidgin)."
794	    ),
795	    "nyq_aha": (
796	        "The AHA Nayini Treebank is a small treebank for contemporary Nayini. "
797	        "Its corpus is collected and annotated manually. We have prepared this"
798	        " treebank based on interviews with Nayini speakers."
799	    ),
800	    "sme_giella": (
801	        "This is a North Smi treebank based on a manually disambiguated and "
802	        "function-labelled gold-standard corpus of North Smi produced by the "
803	        "Giellatekno team at UiT Norgga rktala universitehta."
804	    ),
805	    "no_bokmaal": (
806	        "The Norwegian UD treebank is based on the Bokml section of the "
807	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
808	        " Norwegian. NDT has been automatically converted to the UD scheme by "
809	        "Lilja vrelid at the University of Oslo."
810	    ),
811	    "no_nynorsk": (
812	        "The Norwegian UD treebank is based on the Nynorsk section of the "
813	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
814	        " Norwegian.  NDT has been automatically converted to the UD scheme by"
815	        " Lilja vrelid at the University of Oslo."
816	    ),
817	    "no_nynorsklia": (
818	        "This Norwegian treebank is based on the LIA treebank of transcribed "
819	        "spoken Norwegian dialects. The treebank has been automatically "
820	        "converted to the UD scheme by Lilja vrelid at the University of "
821	        "Oslo."
822	    ),
823	    "cu_proiel": (
824	        "The Old Church Slavonic (OCS) UD treebank is based on the Old Church "
825	        "Slavonic data from the PROIEL treebank and contains the text of the "
826	        "Codex Marianus New Testament translation."
827	    ),
828	    "fro_srcmf": (
829	        "UD_Old_French-SRCMF is a conversion of (part of) the SRCMF corpus "
830	        "(Syntactic Reference Corpus of Medieval French srcmf.org)."
831	    ),
832	    "orv_birchbark": (
833	        "UD Old_East_Slavic-Birchbark is based on the RNC Corpus of Birchbark "
834	        "Letters and includes documents written in 1025-1500 in an East Slavic"
835	        " vernacular (letters, household and business records, records for "
836	        "church services, spell against diseases, and other short "
837	        "inscriptions). The treebank is manually syntactically annotated in "
838	        "the UD 2.0 scheme, morphological and lexical annotation is a "
839	        "conversion of the original RNC annotation."
840	    ),
841	    "orv_rnc": (
842	        "`UD_Old_Russian-RNC` is a sample of the Middle Russian corpus "
843	        "(1300-1700), a part of the Russian National Corpus. The data were "
844	        "originally annotated according to the RNC and extended UD-Russian "
845	        "morphological schemas and UD 2.4 dependency schema."
846	    ),
847	    "orv_torot": (
848	        "UD_Old_Russian-TOROT is a conversion of a selection of the Old East "
849	        "Slavonic and Middle Russian data in the Troms Old Russian and OCS "
850	        "Treebank (TOROT), which was originally annotated in PROIEL dependency"
851	        " format."
852	    ),
853	    "otk_tonqq": (
854	        "`UD_Old_Turkish-Tonqq` is an Old Turkish treebank built upon Turkic "
855	        "script texts or sentences that are trivially convertible."
856	    ),
857	    "fa_perdt": (
858	        "The Persian Universal Dependency Treebank (PerUDT) is the result of "
859	        "automatic coversion of Persian Dependency Treebank (PerDT) with "
860	        "extensive manual corrections. Please refer to the follwoing work, if "
861	        "you use this data: Mohammad Sadegh Rasooli, Pegah Safari, Amirsaeid "
862	        "Moloodi, and Alireza Nourian. 'The Persian Dependency Treebank Made "
863	        "Universal'. 2020 (to appear)."
864	    ),
865	    "fa_seraji": (
866	        "The Persian Universal Dependency Treebank (Persian UD) is based on "
867	        "Uppsala Persian Dependency Treebank (UPDT). The conversion of the "
868	        "UPDT to the Universal Dependencies was performed semi-automatically "
869	        "with extensive manual checks and corrections."
870	    ),
871	    "pl_lfg": (
872	        "The LFG Enhanced UD treebank of Polish is based on a corpus of LFG "
873	        "(Lexical Functional Grammar) syntactic structures generated by an LFG"
874	        " grammar of Polish, POLFIE, and manually disambiguated by human "
875	        "annotators."
876	    ),
877	    "pl_pdb": (
878	        "The Polish PDB-UD treebank is based on the Polish Dependency Bank 2.0"
879	        " (PDB 2.0), created at the Institute of Computer Science, Polish "
880	        "Academy of Sciences in Warsaw. The PDB-UD treebank is an extended and"
881	        " corrected version of the Polish SZ-UD treebank (the release 1.2 to "
882	        "2.3)."
883	    ),
884	    "pl_pud": (
885	        "This is the Polish portion of the Parallel Universal Dependencies "
886	        "(PUD) treebanks, created at the Institute of Computer Science, Polish"
887	        " Academy of Sciences in Warsaw.Re"
888	    ),
889	    "pt_bosque": (
890	        "This Universal Dependencies (UD) Portuguese treebank is based on the "
891	        "Constraint Grammar converted version of the Bosque, which is part of "
892	        "the Floresta Sint(c)tica treebank. It contains both European "
893	        "(CETEMPblico) and Brazilian (CETENFolha) variants."
894	    ),
895	    "pt_gsd": (
896	        "The Brazilian Portuguese UD is converted from the Google Universal "
897	        "Dependency Treebank v2.0 (legacy)."
898	    ),
899	    "pt_pud": (
900	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
901	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
902	        "Raw Text to Universal Dependencies."
903	    ),
904	    "ro_art": (
905	        "The UD treebank ArT is a treebank of the Aromanian dialect of the "
906	        "Romanian language in UD format."
907	    ),
908	    "ro_nonstandard": (
909	        "The Romanian Non-standard UD treebank (called UAIC-RoDia) is based on"
910	        " UAIC-RoDia Treebank. UAIC-RoDia = ISLRN 156-635-615-024-0"
911	    ),
912	    "ro_rrt": (
913	        "The Romanian UD treebank (called RoRefTrees) (Barbu Mititelu et al., "
914	        "2016) is the reference treebank in UD format for standard Romanian."
915	    ),
916	    "ro_simonero": "SiMoNERo is a medical corpus of contemporary Romanian.",
917	    "ru_gsd": (
918	        "Russian Universal Dependencies Treebank annotated and converted by "
919	        "Google."
920	    ),
921	    "ru_pud": (
922	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
923	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
924	        "Raw Text to Universal Dependencies."
925	    ),
926	    "ru_syntagrus": "Russian data from the SynTagRus corpus.",
927	    "ru_taiga": (
928	        "Universal Dependencies treebank is based on data samples extracted "
929	        "from Taiga Corpus and MorphoRuEval-2017 and GramEval-2020 shared "
930	        "tasks collections."
931	    ),
932	    "sa_ufal": (
933	        "A small Sanskrit treebank of sentences from Pacatantra, an ancient "
934	        "Indian collection of interrelated fables by Vishnu Sharma."
935	    ),
936	    "sa_vedic": (
937	        "The Treebank of Vedic Sanskrit contains 4,000 sentences with 27,000 "
938	        "words chosen from metrical and prose passages of the gveda (RV), the"
939	        " aunaka recension of the Atharvaveda (S), the Maitryasahit "
940	        "(MS), and the Aitareya- (AB) and atapatha-Brhmaas (B). Lexical "
941	        "and morpho-syntactic information has been generated using a tagging "
942	        "software and manually validated. POS tags have been induced "
943	        "automatically from the morpho-sytactic information of each word."
944	    ),
945	    "gd_arcosg": (
946	        "A treebank of Scottish Gaelic based on the Annotated Reference Corpus"
947	        " Of Scottish Gaelic (ARCOSG)."
948	    ),
949	    "sr_set": (
950	        "The Serbian UD treebank is based on the "
951	        "[SETimes-SR](http://hdl.handle.net/11356/1200) corpus and additional "
952	        "news documents from the Serbian web."
953	    ),
954	    "sms_giellagas": (
955	        "The UD Skolt Sami Giellagas treebank is based almost entirely on "
956	        "spoken Skolt Sami corpora."
957	    ),
958	    "sk_snk": (
959	        "The Slovak UD treebank is based on data originally annotated as part "
960	        "of the Slovak National Corpus, following the annotation style of the "
961	        "Prague Dependency Treebank."
962	    ),
963	    "sl_ssj": (
964	        "The Slovenian UD Treebank is a rule-based conversion of the ssj500k "
965	        "treebank, the largest collection of manually syntactically annotated "
966	        "data in Slovenian, originally annotated in the JOS annotation scheme."
967	    ),
968	    "sl_sst": (
969	        "The Spoken Slovenian UD Treebank (SST) is the first syntactically "
970	        "annotated corpus of spoken Slovenian, based on a sample of the "
971	        "reference GOS corpus, a collection of transcribed audio recordings of"
972	        " monologic, dialogic and multi-party spontaneous speech in different "
973	        "everyday situations."
974	    ),
975	    "soj_aha": (
976	        "The AHA Soi Treebank is a small treebank for contemporary Soi. Its "
977	        "corpus is collected and annotated manually. We have prepared this "
978	        "treebank based on interviews with Soi speakers."
979	    ),
980	    "ajp_madar": (
981	        "The South_Levantine_Arabic-MADAR treebank consists of 100 "
982	        "manually-annotated sentences taken from the "
983	        "[MADAR](https://camel.abudhabi.nyu.edu/madar/) (Multi-Arabic Dialect "
984	        "Applications and Resources) project. "
985	    ),
986	    "es_ancora": "Spanish data from the AnCora corpus.",
987	    "es_gsd": (
988	        "The Spanish UD is converted from the content head version of the "
989	        "universal dependency treebank v2.0 (legacy)."
990	    ),
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:59
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
58	    "sq_tsa": (
59	        "The UD Treebank for Standard Albanian (TSA) is a small treebank that "
60	        "consists of 60 sentences corresponding to 922 tokens. The data was "
61	        "collected from different Wikipedia entries. This treebank was created"
62	        " mainly manually following the Universal Dependencies guidelines. The"
63	        " lemmatization was performed using the lemmatizer "
64	        "https://bitbucket.org/timarkh/uniparser-albanian-grammar/src/master/ "
65	        "developed by the Albanian National Corpus team (Maria Morozova, "
66	        "Alexander Rusakov, Timofey Arkhangelskiy). Tagging and Morphological "
67	        "Analysis were semi-automated through python scripts and corrected "
68	        "manually, whereas Dependency relations were assigned fully manually. "
69	        "We encourage any initiatives to increase the size and/or improve the "
70	        "overall quality of the Treebank."
71	    ),
72	    "am_att": (
73	        "UD_Amharic-ATT is a manually annotated Treebanks. It is annotated for"
74	        " POS tag, morphological information and dependency relations. Since "
75	        "Amharic is a morphologically-rich, pro-drop, and languages having a "
76	        "feature of clitic doubling, clitics have been segmented manually."
77	    ),
78	    "grc_perseus": (
79	        "This Universal Dependencies Ancient Greek Treebank consists of an "
80	        "automatic conversion of a selection of passages from the Ancient "
81	        "Greek and Latin Dependency Treebank 2.1"
82	    ),
83	    "grc_proiel": (
84	        "The Ancient Greek PROIEL treebank is based on the Ancient Greek data "
85	        "from the PROIEL treebank, which is maintained at the Department of "
86	        "Philosophy, Classics, History of Arts and Ideas at the University of "
87	        "Oslo. The conversion is based on the 20180408 release of the PROIEL "
88	        "treebank available from "
89	        "https://github.com/proiel/proiel-treebank/releases. The original "
90	        "annotators are acknowledged in the files available there. The "
91	        "conversion code is available in the Rubygem proiel-cli, "
92	        "https://github.com/proiel/proiel-cli."
93	    ),
94	    "apu_ufpa": (
95	        "The initial release contains 70 annotated sentences. This is the "
96	        "first treebank in a language from the Arawak family. The original "
97	        "interlinear glosses are included in the tree bank, and their "
98	        "conversion into a full UD annotation is an ongoing process. The "
99	        "sent_id values (e.g.: FernandaM2017:Texto-6-19) are representative of"
100	        " the collector, year of publication, text identifier and the number "
101	        "of the sentence in order from the original text."
102	    ),
103	    "hbo_ptnk": (
104	        "UD Ancient Hebrew PTNK contains portions of the Biblia Hebraic "
105	        "Stuttgartensia with morphological annotations from ETCBC."
106	    ),
107	    "ar_nyuad": (
108	        "The treebank consists of 19,738 sentences (738889 tokens), and its "
109	        "domain is mainly newswire. The annotation is licensed under the terms"
110	        " of CC BY-SA 4.0, and the original PATB can be obtained from the "
111	        "LDCs official website."
112	    ),
113	    "ar_padt": (
114	        "The Arabic-PADT UD treebank is based on the Prague Arabic Dependency "
115	        "Treebank (PADT), created at the Charles University in Prague."
116	    ),
117	    "ar_pud": (
118	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
119	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
120	        "Raw Text to Universal Dependencies."
121	    ),
122	    "aii_as": (
123	        "The Uppsala Assyrian Treebank is a small treebank for Modern Standard"
124	        " Assyrian. The corpus is collected and annotated manually. The data "
125	        "was randomly collected from different textbooks and a short "
126	        "translation of The Merchant of Venice."
127	    ),
128	    "bm_crb": (
129	        "The UD Bambara treebank is a section of the Corpus Rfrence du "
130	        "Bambara annotated natively with Universal Dependencies."
131	    ),
132	    "eu_bdt": (
133	        "The Basque UD treebank is based on a automatic conversion from part "
134	        "of the Basque Dependency Treebank (BDT), created at the University of"
135	        " of the Basque Country by the IXA NLP research group. The treebank "
136	        "consists of 8.993 sentences (121.443 tokens) and covers mainly "
137	        "literary and journalistic texts."
138	    ),
139	    "bej_nsc": (
140	        "A Universal Dependencies corpus for Beja, North-Cushitic branch of "
141	        "the Afro-Asiatic phylum mainly spoken in Sudan, Egypt and Eritrea."
142	    ),
143	    "be_hse": (
144	        "The Belarusian UD treebank is based on a sample of the news texts "
145	        "included in the Belarusian-Russian parallel subcorpus of the Russian "
146	        "National Corpus, online search available at: "
147	        "http://ruscorpora.ru/search-para-be.html."
148	    ),
149	    "bn_bru": (
150	        "The BRU Bengali treebank has been created at Begum Rokeya University,"
151	        " Rangpur, by the members of Semantics Lab."
152	    ),
153	    "bho_bhtb": (
154	        "The Bhojpuri UD Treebank (BHTB) v2.6 consists of 6,664 tokens(357 "
155	        "sentences). This Treebank is a part of the Universal Dependency "
156	        "treebank project. Initially, it was initiated by me (Atul) at "
157	        "Jawaharlal Nehru University, New Delhi during the doctoral research "
158	        "work. BHTB data contains syntactic annotation according to "
159	        "dependency-constituency schema, as well as morphological tags and "
160	        "lemmas. In this data, XPOS is annotated  according to Bureau of "
161	        "Indian Standards (BIS) Part Of Speech (POS) tagset."
162	    ),
163	    "br_keb": (
164	        "UD Breton-KEB is a treebank of Breton that has been manually "
165	        "annotated according to the Universal Dependencies guidelines. The "
166	        "tokenisation guidelines and morphological annotation comes from a "
167	        "finite-state morphological analyser of Breton released as part of the"
168	        " Apertium project."
169	    ),
170	    "bg_btb": (
171	        "UD_Bulgarian-BTB is based on the HPSG-based BulTreeBank, created at "
172	        "the Institute of Information and Communication Technologies, "
173	        "Bulgarian Academy of Sciences. The original consists of 215,000 "
174	        "tokens (over 15,000 sentences)."
175	    ),
176	    "bxr_bdt": (
177	        "The UD Buryat treebank was annotated manually natively in UD and "
178	        "contains grammar book sentences, along with news and some fiction."
179	    ),
180	    "yue_hk": (
181	        "A Cantonese treebank (in Traditional Chinese characters) of film "
182	        "subtitles and of legislative proceedings of Hong Kong, parallel with "
183	        "the Chinese-HK treebank."
184	    ),
185	    "ca_ancora": "Catalan data from the AnCora corpus.",
186	    "ceb_gja": (
187	        "UD_Cebuano_GJA is a collection of annotated Cebuano sample sentences "
188	        "randomly taken from three different sources: community-contributed "
189	        "samples from the website Tatoeba, a Cebuano grammar book by Bunye & "
190	        "Yap (1971) and Tanangkinsing's reference grammar on Cebuano (2011). "
191	        "This project is currently work in progress."
192	    ),
193	    "zh_cfl": (
194	        "The Chinese-CFL UD treebank is manually annotated by Keying Li with "
195	        "minor manual revisions by Herman Leung and John Lee at City "
196	        "University of Hong Kong, based on essays written by learners of "
197	        "Mandarin Chinese as a foreign language. The data is in Simplified "
198	        "Chinese."
199	    ),
200	    "zh_gsd": (
201	        "Traditional Chinese Universal Dependencies Treebank annotated and "
202	        "converted by Google."
203	    ),
204	    "zh_gsdsimp": (
205	        "Simplified Chinese Universal Dependencies dataset converted from the "
206	        "GSD (traditional) dataset with manual corrections."
207	    ),
208	    "zh_hk": (
209	        "A Traditional Chinese treebank of film subtitles and of legislative "
210	        "proceedings of Hong Kong, parallel with the Cantonese-HK treebank."
211	    ),
212	    "zh_pud": (
213	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
214	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
215	        "Raw Text to Universal Dependencies."
216	    ),
217	    "ckt_hse": (
218	        "This data is a manual annotation of the corpus from multimedia "
219	        "annotated corpus of the Chuklang project, a dialectal corpus of the "
220	        "Amguema variant of Chukchi."
221	    ),
222	    "lzh_kyoto": (
223	        "Classical Chinese Universal Dependencies Treebank annotated and "
224	        "converted by Institute for Research in Humanities, Kyoto University."
225	    ),
226	    "cop_scriptorium": (
227	        "UD Coptic contains manually annotated Sahidic Coptic texts, including"
228	        " Biblical texts, sermons, letters, and hagiography."
229	    ),
230	    "hr_set": (
231	        "The Croatian UD treebank is based on the extension of the SETimes-HR "
232	        "corpus, the hr500k corpus."
233	    ),
234	    "cs_cac": (
235	        "The UD_Czech-CAC treebank is based on the Czech Academic Corpus 2.0 "
236	        "(CAC; esk akademick korpus; AK), created at Charles University in"
237	        " Prague."
238	    ),
239	    "cs_cltt": (
240	        "The UD_Czech-CLTT treebank is based on the Czech Legal Text Treebank "
241	        "1.0, created at Charles University in Prague."
242	    ),
243	    "cs_fictree": (
244	        "FicTree is a treebank of Czech fiction, automatically converted into "
245	        "the UD format. The treebank was built at Charles University in "
246	        "Prague."
247	    ),
248	    "cs_pdt": (
249	        "The Czech-PDT UD treebank is based on the Prague Dependency Treebank "
250	        "3.0 (PDT), created at the Charles University in Prague."
251	    ),
252	    "cs_pud": (
253	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
254	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
255	        "Raw Text to Universal Dependencies."
256	    ),
257	    "da_ddt": (
258	        "The Danish UD treebank is a conversion of the Danish Dependency "
259	        "Treebank."
260	    ),
261	    "nl_alpino": (
262	        "This corpus consists of samples from various treebanks annotated at "
263	        "the University of Groningen using the Alpino annotation tools and "
264	        "guidelines."
265	    ),
266	    "nl_lassysmall": (
267	        "This corpus contains sentences from the Wikipedia section of the "
268	        "Lassy Small Treebank. Universal Dependency annotation was generated "
269	        "automatically from the original annotation in Lassy."
270	    ),
271	    "en_esl": (
272	        "UD English-ESL / Treebank of Learner English (TLE) contains manual "
273	        "POS tag and dependency annotations for 5,124 English as a Second "
274	        "Language (ESL) sentences drawn from the Cambridge Learner Corpus "
275	        "First Certificate in English (FCE) dataset."
276	    ),
277	    "en_ewt": (
278	        "A Gold Standard Universal Dependencies Corpus for English, built over"
279	        " the source material of the English Web Treebank LDC2012T13 "
280	        "(https://catalog.ldc.upenn.edu/LDC2012T13)."
281	    ),
282	    "en_gum": (
283	        "Universal Dependencies syntax annotations from the GUM corpus "
284	        "(https://corpling.uis.georgetown.edu/gum/)."
285	    ),
286	    "en_gumreddit": (
287	        "Universal Dependencies syntax annotations from the Reddit portion of "
288	        "the GUM corpus (https://corpling.uis.georgetown.edu/gum/) "
289	    ),
290	    "en_lines": (
291	        "UD English_LinES is the English half of the LinES Parallel Treebank "
292	        "with the original dependency annotation first automatically converted"
293	        " into Universal Dependencies and then partially reviewed. Its "
294	        "contents cover literature, an online manual and Europarl data."
295	    ),
296	    "en_atis": (
297	        "UD Atis Treebank is a manually annotated treebank consisting of the "
298	        "sentences in the Atis (Airline Travel Informations) dataset which "
299	        "includes the human speech transcriptions of people asking for flight "
300	        "information on the automated inquiry systems."
301	    ),
302	    "en_partut": (
303	        "UD_English-ParTUT is a conversion of a multilingual parallel treebank"
304	        " developed at the University of Turin, and consisting of a variety of"
305	        " text genres, including talks, legal texts and Wikipedia articles, "
306	        "among others."
307	    ),
308	    "en_pronouns": (
309	        "UD English-Pronouns is dataset created to make pronoun identification"
310	        " more accurate and with a more balanced distribution across genders. "
311	        "The dataset is initially targeting the Independent Genitive pronouns,"
312	        " 'hers', (independent) 'his', (singular) 'theirs', 'mine', and "
313	        "(singular) 'yours'."
314	    ),
315	    "en_pud": (
316	        "This is the English portion of the Parallel Universal Dependencies "
317	        "(PUD) treebanks created for the CoNLL 2017 shared task on "
318	        "Multilingual Parsing from Raw Text to Universal Dependencies "
319	        "(http://universaldependencies.org/conll17/)."
320	    ),
321	    "myv_jr": (
322	        "UD Erzya is the original annotation (CoNLL-U) for texts in the Erzya "
323	        "language, it originally consists of a sample from a number of fiction"
324	        " authors writing originals in Erzya."
325	    ),
326	    "et_edt": (
327	        "UD Estonian is a converted version of the Estonian Dependency "
328	        "Treebank (EDT), originally annotated in the Constraint Grammar (CG) "
329	        "annotation scheme, and consisting of genres of fiction, newspaper "
330	        "texts and scientific texts. The treebank contains 30,972 trees, "
331	        "437,769 tokens."
332	    ),
333	    "et_ewt": (
334	        "UD EWT treebank consists of different genres of new media. The "
335	        "treebank contains 4,493 trees, 56,399 tokens."
336	    ),
337	    "fo_farpahc": (
338	        "UD_Icelandic-FarPaHC is a conversion of the Faroese Parsed Historical"
339	        " Corpus (FarPaHC) to the Universal Dependencies scheme. The "
340	        "conversion was done using UDConverter."
341	    ),
342	    "fo_oft": "This is a treebank of Faroese based on the Faroese Wikipedia.",
343	    "fi_ftb": (
344	        "FinnTreeBank 1 consists of manually annotated grammatical examples "
345	        "from VISK. The UD version of FinnTreeBank 1 was converted from a "
346	        "native annotation model with a script and later manually revised."
347	    ),
348	    "fi_ood": (
349	        "Finnish-OOD is an external out-of-domain test set for Finnish-TDT "
350	        "annotated natively into UD scheme."
351	    ),
352	    "fi_pud": (
353	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
354	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
355	        "Raw Text to Universal Dependencies."
356	    ),
357	    "fi_tdt": (
358	        "UD_Finnish-TDT is based on the Turku Dependency Treebank (TDT), a "
359	        "broad-coverage dependency treebank of general Finnish covering "
360	        "numerous genres. The conversion to UD was followed by extensive "
361	        "manual checks and corrections, and the treebank closely adheres to "
362	        "the UD guidelines."
363	    ),
364	    "fr_fqb": (
365	        "The corpus **UD_French-FQB** is an automatic conversion of the French"
366	        " QuestionBank v1, a corpus entirely made of questions."
367	    ),
368	    "fr_ftb": (
369	        "The Universal Dependency version of the French Treebank (Abeill et "
370	        "al., 2003), hereafter UD_French-FTB, is a treebank of sentences from "
371	        "the newspaper Le Monde, initially manually annotated with "
372	        "morphological information and phrase-structure and then converted to "
373	        "the Universal Dependencies annotation scheme."
374	    ),
375	    "fr_gsd": (
376	        "The **UD_French-GSD** was converted in 2015 from the content head "
377	        "version of the universal dependency treebank v2.0 "
378	        "(https://github.com/ryanmcd/uni-dep-tb). It is updated since 2015 "
379	        "independently from the previous source."
380	    ),
381	    "fr_partut": (
382	        "UD_French-ParTUT is a conversion of a multilingual parallel treebank "
383	        "developed at the University of Turin, and consisting of a variety of "
384	        "text genres, including talks, legal texts and Wikipedia articles, "
385	        "among others."
386	    ),
387	    "fr_rhapsodie": "A Universal Dependencies corpus for spoken French.",
388	    "fr_parisstories": (
389	        "Paris Stories is a corpus of oral French collected and transcribed by"
390	        " Linguistics students from Sorbonne Nouvelle and corrected by "
391	        "students from the Plurital Master's Degree of Computational "
392	        "Linguistics ( Inalco, Paris Nanterre, Sorbonne Nouvelle) between 2017"
393	        " and 2021. It contains monologues and dialogues from speakers living "
394	        "in the Parisian region."
395	    ),
396	    "fr_pud": (
397	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
398	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
399	        "Raw Text to Universal Dependencies."
400	    ),
401	    "fr_sequoia": (
402	        "UD_French-Sequoia is an automatic conversion of the Sequoia Treebank "
403	        "corpus French Sequoia corpus."
404	    ),
405	    "gl_ctg": (
406	        "The Galician UD treebank is based on the automatic parsing of the "
407	        "Galician Technical Corpus (http://sli.uvigo.gal/CTG) created at the "
408	        "University of Vigo by the the TALG NLP research group."
409	    ),
410	    "gl_treegal": (
411	        "The Galician-TreeGal is a treebank for Galician developed at LyS "
412	        "Group (Universidade da Corua)."
413	    ),
414	    "de_gsd": (
415	        "The German UD is converted from the content head version of the "
416	        "universal dependency treebank v2.0 (legacy)."
417	    ),
418	    "de_hdt": (
419	        "UD German-HDT is a conversion of the Hamburg Dependency Treebank, "
420	        "created at the University of Hamburg through manual annotation in "
421	        "conjunction with a standard for morphologically and syntactically "
422	        "annotating sentences as well as a constraint-based parser."
423	    ),
424	    "de_lit": (
425	        "This treebank aims at gathering texts of the German literary history."
426	        " Currently, it hosts Fragments of the early Romanticism, i.e. "
427	        "aphorism-like texts mainly dealing with philosophical issues "
428	        "concerning art, beauty and related topics."
429	    ),
430	    "de_pud": (
431	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
432	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
433	        "Raw Text to Universal Dependencies."
434	    ),
435	    "got_proiel": (
436	        "The UD Gothic treebank is based on the Gothic data from the PROIEL "
437	        "treebank, and consists of Wulfila's Bible translation."
438	    ),
439	    "el_gdt": (
440	        "The Greek UD treebank (UD_Greek-GDT) is derived from the Greek "
441	        "Dependency Treebank (http://gdt.ilsp.gr), a resource developed and "
442	        "maintained by researchers at the Institute for Language and Speech "
443	        "Processing/Athena R.C. (http://www.ilsp.gr)."
444	    ),
445	    "gub_tudet": (
446	        "UD_Guajajara-TuDeT is a collection of annotated sentences in "
447	        "Guajajara. Sentences stem from multiple sources such as descriptions "
448	        "of the language, short stories, dictionaries and translations from "
449	        "the New Testament. Sentence annotation and documentation by Lorena "
450	        "Martn Rodrguez and Fabrcio Ferraz Gerardi."
451	    ),
452	    "gn_oldtudet": (
453	        "UD_Guarani-OldTuDeT is a collection of annotated texts in Old "
454	        "Guaran. All known sources in this language are being annotated: "
455	        "cathesisms, grammars (seventeenth and eighteenth century), sentences "
456	        "from dictionaries, and other texts. Sentence annotation and "
457	        "documentation by Fabrcio Ferraz Gerardi and Lorena Martn Rodrguez."
458	    ),
459	    "he_htb": "A Universal Dependencies Corpus for Hebrew.",
460	    "he_iahltwiki": (
461	        "Publicly available subset of the IAHLT UD Hebrew Treebank's Wikipedia"
462	        " section (https://www.iahlt.org/)"
463	    ),
464	    "qfn_fame": (
465	        "UD_Frisian_Dutch-Fame is a selection of 400 sentences from the FAME! "
466	        "speech corpus by Yilmaz et al. (2016a, 2016b). The treebank is "
467	        "manually annotated using the UD scheme."
468	    ),
469	    "qhe_hiencs": (
470	        "The Hindi-English Code-switching treebank is based on code-switching "
471	        "tweets of Hindi and English multilingual speakers (mostly Indian) on "
472	        "Twitter. The treebank is manually annotated using UD sceheme. The "
473	        "training and evaluations sets were seperately annotated by different "
474	        "annotators using UD v2 and v1 guidelines respectively. The evaluation"
475	        " sets are automatically converted from UD v1 to v2."
476	    ),
477	    "hi_hdtb": (
478	        "The Hindi UD treebank is based on the Hindi Dependency Treebank "
479	        "(HDTB), created at IIIT Hyderabad, India."
480	    ),
481	    "hi_pud": (
482	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
483	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
484	        "Raw Text to Universal Dependencies."
485	    ),
486	    "hu_szeged": (
487	        "The Hungarian UD treebank is derived from the Szeged Dependency "
488	        "Treebank (Vincze et al. 2010)."
489	    ),
490	    "is_modern": (
491	        "UD_Icelandic-Modern is a conversion of the modern additions to the "
492	        "Icelandic Parsed Historical Corpus (IcePaHC) to the Universal "
493	        "Dependencies scheme."
494	    ),
495	    "is_icepahc": (
496	        "UD_Icelandic-IcePaHC is a conversion of the Icelandic Parsed "
497	        "Historical Corpus (IcePaHC) to the Universal Dependencies scheme. The"
498	        " conversion was done using UDConverter."
499	    ),
500	    "is_pud": (
501	        "Icelandic-PUD is the Icelandic part of the Parallel Universal "
502	        "Dependencies (PUD) treebanks."
503	    ),
504	    "id_csui": (
505	        "UD Indonesian-CSUI is a conversion from an Indonesian constituency "
506	        "treebank in the Penn Treebank format named Kethu that was also a "
507	        "conversion from a constituency treebank built by Dinakaramani et al. "
508	        "(2015). We named this treebank Indonesian-CSUI, since all the three "
509	        "versions of the treebanks were built at Faculty of Computer Science, "
510	        "Universitas Indonesia."
511	    ),
512	    "id_gsd": (
513	        "The Indonesian UD is converted from the content head version of the "
514	        "universal dependency treebank v2.0 (legacy)."
515	    ),
516	    "id_pud": (
517	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
518	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
519	        "Raw Text to Universal Dependencies."
520	    ),
521	    "ga_idt": (
522	        "A Universal Dependencies 4910-sentence treebank for modern Irish."
523	    ),
524	    "ga_twittirish": (
525	        "A Universal Dependencies treebank of 866 tweets in modern Irish."
526	    ),
527	    "it_isdt": (
528	        "The Italian corpus annotated according to the UD annotation scheme "
529	        "was obtained by conversion from ISDT (Italian Stanford Dependency "
530	        "Treebank), released for the dependency parsing shared task of "
531	        "Evalita-2014 (Bosco et al. 2014)."
532	    ),
533	    "it_partut": (
534	        "UD_Italian-ParTUT is a conversion of a multilingual parallel treebank"
535	        " developed at the University of Turin, and consisting of a variety of"
536	        " text genres, including talks, legal texts and Wikipedia articles, "
537	        "among others."
538	    ),
539	    "it_postwita": (
540	        "PoSTWITA-UD is a collection of Italian tweets annotated in Universal "
541	        "Dependencies that can be exploited for the training of NLP systems to"
542	        " enhance their performance on social media texts."
543	    ),
544	    "it_markit": (
545	        "It is MarkIT That is New: An Italian Treebank of Marked "
546	        "Constructions. Teresa Paccosi, Alessio Palmero Aprosio and Sara "
547	        "Tonelli, To appear in Proceedings of the Eighth Italian Conference on"
548	        " Computational Linguistics 2022 (CLIC-it 2021)"
549	    ),
550	    "it_valico": (
551	        "Manually corrected Treebank of Learner Italian drawn from the Valico "
552	        "corpus and correspondent corrected sentences."
553	    ),
554	    "it_pud": (
555	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
556	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
557	        "Raw Text to Universal Dependencies."
558	    ),
559	    "it_twittiro": (
560	        "TWITTIR-UD is a collection of ironic Italian tweets annotated in "
561	        "Universal Dependencies. The treebank can be exploited for the "
562	        "training of NLP systems to enhance their performance on social media "
563	        "texts, and in particular, for irony detection purposes."
564	    ),
565	    "it_vit": (
566	        "The UD_Italian-VIT corpus was obtained by conversion from VIT (Venice"
567	        " Italian Treebank), developed at the Laboratory of Computational "
568	        "Linguistics of the Universit Ca' Foscari in Venice (Delmonte et al. "
569	        "2007; Delmonte 2009; "
570	        "http://rondelmo.it/resource/VIT/Browser-VIT/index.htm)."
571	    ),
572	    "ja_pudluw": (
573	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
574	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
575	        "Raw Text to Universal Dependencies."
576	    ),
577	    "ja_bccwjluw": (
578	        "This Universal Dependencies (UD) Japanese treebank is based on the "
579	        "definition of UD Japanese convention described in the UD "
580	        "documentation. The original sentences are from `Balanced Corpus of "
581	        "Contemporary Written Japanese'(BCCWJ)."
582	    ),
583	    "ja_gsdluw": (
584	        "This Universal Dependencies (UD) Japanese treebank is based on the "
585	        "definition of UD Japanese convention described in the UD "
586	        "documentation. The original sentences are from Google UDT 2.0."
587	    ),
588	    "ja_bccwj": (
589	        "This Universal Dependencies (UD) Japanese treebank is based on the "
590	        "definition of UD Japanese convention described in the UD "
591	        "documentation. The original sentences are from `Balanced Corpus of "
592	        "Contemporary Written Japanese'(BCCWJ)."
593	    ),
594	    "ja_gsd": (
595	        "This Universal Dependencies (UD) Japanese treebank is based on the "
596	        "definition of UD Japanese convention described in the UD "
597	        "documentation.  The original sentences are from Google UDT 2.0."
598	    ),
599	    "ja_modern": (
600	        "This Universal Dependencies (UD) Japanese treebank is based on the "
601	        "definition of UD Japanese convention described in the UD "
602	        "documentation. The original sentences are from `Corpus of Historical "
603	        "Japanese' (CHJ)."
604	    ),
605	    "ja_pud": (
606	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
607	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
608	        " Raw Text to Universal "
609	        "Dependencies](http://universaldependencies.org/conll17/)."
610	    ),
611	    "jv_csui": (
612	        "UD Javanese-CSUI is a dependency treebank in Javanese, a regional "
613	        "language in Indonesia with more than 60 million users. The original "
614	        "sentences were taken from OPUS, especially from the WikiMatrix v1 "
615	        "corpus. We revised the sentences that contained more Indonesian words"
616	        " than Javanese words and manually annotated them."
617	    ),
618	    "urb_tudet": (
619	        "UD_Kaapor-TuDeT is a collection of annotated sentences in Ka'apor. "
620	        "The project is a work in progress and the treebank is being updated "
621	        "on a regular basis."
622	    ),
623	    "xnr_kdtb": (
624	        "The Kangri UD Treebank (KDTB) is a part of the Universal Dependency "
625	        "treebank project."
626	    ),
627	    "krl_kkpp": (
628	        "UD Karelian-KKPP is a manually annotated new corpus of Karelian made "
629	        "in Universal dependencies annotation scheme. The data is collected "
630	        "from VepKar corpora and consists of mostly modern news texts but also"
631	        " some stories and educational texts."
632	    ),
633	    "kk_ktb": (
634	        "The UD Kazakh treebank is a combination of text from various sources "
635	        "including Wikipedia, some folk tales, sentences from the UDHR, news "
636	        "and phrasebook sentences. Sentences IDs include partial document "
637	        "identifiers."
638	    ),
639	    "arr_tudet": (
640	        "UD_Karo-TuDeT is a collection of annotated sentences in Karo. The "
641	        "sentences stem from the only grammatical description of the language "
642	        "(Gabas, 1999) and from the sentences in the dictionary by the same "
643	        "author (Gabas, 2007). Sentence annotation and documentation by "
644	        "Fabrcio Ferraz Gerardi."
645	    ),
646	    "kfm_aha": (
647	        "The AHA Khunsari Treebank is a small treebank for contemporary "
648	        "Khunsari. Its corpus is collected and annotated manually. We have "
649	        "prepared this treebank based on interviews with Khunsari speakers."
650	    ),
651	    "quc_iu": (
652	        "UD Kiche-IU is a treebank consisting of sentences from a variety of"
653	        " text domains but principally dictionary example sentences and "
654	        "linguistic examples."
655	    ),
656	    "koi_uh": (
657	        "This is a Komi-Permyak literary language treebank consisting of "
658	        "original and translated texts."
659	    ),
660	    "kpv_ikdp": (
661	        "This treebank consists of dialectal transcriptions of spoken "
662	        "Komi-Zyrian. The current texts are short recorded segments from "
663	        "different areas where the Iva dialect of Komi language is spoken."
664	    ),
665	    "kpv_lattice": (
666	        "UD Komi-Zyrian Lattice is a treebank of written standard Komi-Zyrian."
667	    ),
668	    "ko_gsd": (
669	        "The Google Korean Universal Dependency Treebank is first converted "
670	        "from the Universal Dependency Treebank v2.0 (legacy), and then "
671	        "enhanced by Chun et al., 2018."
672	    ),
673	    "ko_kaist": (
674	        "The KAIST Korean Universal Dependency Treebank is generated by Chun "
675	        "et al., 2018 from the constituency trees in the KAIST Tree-Tagging "
676	        "Corpus."
677	    ),
678	    "ko_pud": (
679	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
680	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
681	        "Raw Text to Universal Dependencies."
682	    ),
683	    "kmr_mg": (
684	        "The UD Kurmanji corpus is a corpus of Kurmanji Kurdish. It contains "
685	        "fiction and encyclopaedic texts in roughly equal measure. It has been"
686	        " annotated natively in accordance with the UD annotation scheme."
687	    ),
688	    "la_ittb": (
689	        "Latin data from the _Index Thomisticus_ Treebank. Data are taken from"
690	        " the _Index Thomisticus_ corpus by Roberto Busa SJ, which contains "
691	        "the complete work by Thomas Aquinas (12251274; Medieval Latin) and "
692	        "by 61 other authors related to Thomas."
693	    ),
694	    "la_udante": (
695	        "The UDante treebank is based on the Latin texts of Dante Alighieri, "
696	        "taken from the DanteSearch corpus, originally created at the "
697	        "University of Pisa, Italy. It is a treebank of Latin language, more "
698	        "precisely of literary Medieval Latin (XIVth century)."
699	    ),
700	    "la_llct": (
701	        "This Universal Dependencies version of the LLCT (Late Latin Charter "
702	        "Treebank) consists of an automated conversion of the LLCT2 treebank "
703	        "from the Latin Dependency Treebank (LDT) format into the Universal "
704	        "Dependencies standard."
705	    ),
706	    "la_perseus": (
707	        "This Universal Dependencies Latin Treebank consists of an automatic "
708	        "conversion of a selection of passages from the Ancient Greek and "
709	        "Latin Dependency Treebank 2.1"
710	    ),
711	    "la_proiel": (
712	        "The Latin PROIEL treebank is based on the Latin data from the PROIEL "
713	        "treebank, and contains most of the Vulgate New Testament translations"
714	        " plus selections from Caesar's Gallic War, Cicero's Letters to "
715	        "Atticus, Palladius' Opus Agriculturae and the first book of Cicero's "
716	        "De officiis."
717	    ),
718	    "lv_lvtb": (
719	        "Latvian UD Treebank is based on Latvian Treebank (LVTB), being "
720	        "created at University of Latvia, Institute of Mathematics and "
721	        "Computer Science, Artificial Intelligence Laboratory."
722	    ),
723	    "lij_glt": (
724	        "The Genoese Ligurian Treebank is a small, manually annotated "
725	        "collection of contemporary Ligurian prose. The focus of the treebank "
726	        "is written Genoese, the koin variety of Ligurian which is associated"
727	        " with today's literary, journalistic and academic ligurophone sphere."
728	    ),
729	    "lt_alksnis": (
730	        "The Lithuanian dependency treebank ALKSNIS v3.0 (Vytautas Magnus "
731	        "University)."
732	    ),
733	    "lt_hse": (
734	        "Lithuanian treebank annotated manually (dependencies) using the "
735	        "Morphological Annotator by CCL, Vytautas Magnus University "
736	        "(http://tekstynas.vdu.lt/) and manual disambiguation. A pilot version"
737	        " which includes news and an essay by Tomas Venclova is available "
738	        "here."
739	    ),
740	    "olo_kkpp": (
741	        "UD Livvi-KKPP is a manually annotated new corpus of Livvi-Karelian "
742	        "made directly in the Universal dependencies annotation scheme. The "
743	        "data is collected from VepKar corpora and consists of mostly modern "
744	        "news texts but also some stories and educational texts."
745	    ),
746	    "nds_lsdc": (
747	        "The UD Low Saxon LSDC dataset consists of sentences in 18 Low Saxon "
748	        "dialects from both Germany and the Netherlands. These sentences are "
749	        "(or are to become) part of the LSDC dataset and represent the "
750	        "language from the 19th and early 20th century in genres such as short"
751	        " stories, novels, speeches, letters and fairytales."
752	    ),
753	    "mt_mudt": (
754	        "MUDT (Maltese Universal Dependencies Treebank) is a manually "
755	        "annotated treebank of Maltese, a Semitic language of Malta descended "
756	        "from North African Arabic with a significant amount of Italo-Romance "
757	        "influence. MUDT was designed as a balanced corpus with four major "
758	        "genres (see Splitting below) represented roughly equally."
759	    ),
760	    "gv_cadhan": (
761	        "This is the Cadhan Aonair UD treebank for Manx Gaelic, created by "
762	        "Kevin Scannell."
763	    ),
764	    "mr_ufal": (
765	        "UD Marathi is a manually annotated treebank consisting primarily of "
766	        "stories from Wikisource, and parts of an article on Wikipedia."
767	    ),
768	    "gun_dooley": (
769	        "UD Mbya_Guarani-Dooley is a corpus of narratives written in Mby "
770	        "Guaran (Tupian) in Brazil, and collected by Robert Dooley. Due to "
771	        "copyright restrictions, the corpus that is distributed as part of UD "
772	        "only contains the annotation (tags, features, relations) while the "
773	        "FORM and LEMMA columns are empty."
774	    ),
775	    "gun_thomas": (
776	        "UD Mbya_Guarani-Thomas is a corpus of Mby Guaran (Tupian) texts "
777	        "collected by Guillaume Thomas. The current version of the corpus "
778	        "consists of three speeches by Paulina Kerechu Nez Romero, a Mby "
779	        "Guaran speaker from Ytu, Caazap Department, Paraguay."
780	    ),
781	    "mdf_jr": (
782	        "Erme Universal Dependencies annotated texts Moksha are the origin of "
783	        "UD_Moksha-JR with annotation (CoNLL-U) for texts in the Moksha "
784	        "language, it originally consists of a sample from a number of fiction"
785	        " authors writing originals in Moksha."
786	    ),
787	    "myu_tudet": (
788	        "UD_Munduruku-TuDeT is a collection of annotated sentences in "
789	        "Munduruk. Together with UD_Akuntsu-TuDeT and UD_Tupinamba-TuDeT, "
790	        "UD_Munduruku-TuDeT is part of the TuLaR project."
791	    ),
792	    "pcm_nsc": (
793	        "A Universal Dependencies corpus for spoken Naija (Nigerian Pidgin)."
794	    ),
795	    "nyq_aha": (
796	        "The AHA Nayini Treebank is a small treebank for contemporary Nayini. "
797	        "Its corpus is collected and annotated manually. We have prepared this"
798	        " treebank based on interviews with Nayini speakers."
799	    ),
800	    "sme_giella": (
801	        "This is a North Smi treebank based on a manually disambiguated and "
802	        "function-labelled gold-standard corpus of North Smi produced by the "
803	        "Giellatekno team at UiT Norgga rktala universitehta."
804	    ),
805	    "no_bokmaal": (
806	        "The Norwegian UD treebank is based on the Bokml section of the "
807	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
808	        " Norwegian. NDT has been automatically converted to the UD scheme by "
809	        "Lilja vrelid at the University of Oslo."
810	    ),
811	    "no_nynorsk": (
812	        "The Norwegian UD treebank is based on the Nynorsk section of the "
813	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
814	        " Norwegian.  NDT has been automatically converted to the UD scheme by"
815	        " Lilja vrelid at the University of Oslo."
816	    ),
817	    "no_nynorsklia": (
818	        "This Norwegian treebank is based on the LIA treebank of transcribed "
819	        "spoken Norwegian dialects. The treebank has been automatically "
820	        "converted to the UD scheme by Lilja vrelid at the University of "
821	        "Oslo."
822	    ),
823	    "cu_proiel": (
824	        "The Old Church Slavonic (OCS) UD treebank is based on the Old Church "
825	        "Slavonic data from the PROIEL treebank and contains the text of the "
826	        "Codex Marianus New Testament translation."
827	    ),
828	    "fro_srcmf": (
829	        "UD_Old_French-SRCMF is a conversion of (part of) the SRCMF corpus "
830	        "(Syntactic Reference Corpus of Medieval French srcmf.org)."
831	    ),
832	    "orv_birchbark": (
833	        "UD Old_East_Slavic-Birchbark is based on the RNC Corpus of Birchbark "
834	        "Letters and includes documents written in 1025-1500 in an East Slavic"
835	        " vernacular (letters, household and business records, records for "
836	        "church services, spell against diseases, and other short "
837	        "inscriptions). The treebank is manually syntactically annotated in "
838	        "the UD 2.0 scheme, morphological and lexical annotation is a "
839	        "conversion of the original RNC annotation."
840	    ),
841	    "orv_rnc": (
842	        "`UD_Old_Russian-RNC` is a sample of the Middle Russian corpus "
843	        "(1300-1700), a part of the Russian National Corpus. The data were "
844	        "originally annotated according to the RNC and extended UD-Russian "
845	        "morphological schemas and UD 2.4 dependency schema."
846	    ),
847	    "orv_torot": (
848	        "UD_Old_Russian-TOROT is a conversion of a selection of the Old East "
849	        "Slavonic and Middle Russian data in the Troms Old Russian and OCS "
850	        "Treebank (TOROT), which was originally annotated in PROIEL dependency"
851	        " format."
852	    ),
853	    "otk_tonqq": (
854	        "`UD_Old_Turkish-Tonqq` is an Old Turkish treebank built upon Turkic "
855	        "script texts or sentences that are trivially convertible."
856	    ),
857	    "fa_perdt": (
858	        "The Persian Universal Dependency Treebank (PerUDT) is the result of "
859	        "automatic coversion of Persian Dependency Treebank (PerDT) with "
860	        "extensive manual corrections. Please refer to the follwoing work, if "
861	        "you use this data: Mohammad Sadegh Rasooli, Pegah Safari, Amirsaeid "
862	        "Moloodi, and Alireza Nourian. 'The Persian Dependency Treebank Made "
863	        "Universal'. 2020 (to appear)."
864	    ),
865	    "fa_seraji": (
866	        "The Persian Universal Dependency Treebank (Persian UD) is based on "
867	        "Uppsala Persian Dependency Treebank (UPDT). The conversion of the "
868	        "UPDT to the Universal Dependencies was performed semi-automatically "
869	        "with extensive manual checks and corrections."
870	    ),
871	    "pl_lfg": (
872	        "The LFG Enhanced UD treebank of Polish is based on a corpus of LFG "
873	        "(Lexical Functional Grammar) syntactic structures generated by an LFG"
874	        " grammar of Polish, POLFIE, and manually disambiguated by human "
875	        "annotators."
876	    ),
877	    "pl_pdb": (
878	        "The Polish PDB-UD treebank is based on the Polish Dependency Bank 2.0"
879	        " (PDB 2.0), created at the Institute of Computer Science, Polish "
880	        "Academy of Sciences in Warsaw. The PDB-UD treebank is an extended and"
881	        " corrected version of the Polish SZ-UD treebank (the release 1.2 to "
882	        "2.3)."
883	    ),
884	    "pl_pud": (
885	        "This is the Polish portion of the Parallel Universal Dependencies "
886	        "(PUD) treebanks, created at the Institute of Computer Science, Polish"
887	        " Academy of Sciences in Warsaw.Re"
888	    ),
889	    "pt_bosque": (
890	        "This Universal Dependencies (UD) Portuguese treebank is based on the "
891	        "Constraint Grammar converted version of the Bosque, which is part of "
892	        "the Floresta Sint(c)tica treebank. It contains both European "
893	        "(CETEMPblico) and Brazilian (CETENFolha) variants."
894	    ),
895	    "pt_gsd": (
896	        "The Brazilian Portuguese UD is converted from the Google Universal "
897	        "Dependency Treebank v2.0 (legacy)."
898	    ),
899	    "pt_pud": (
900	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
901	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
902	        "Raw Text to Universal Dependencies."
903	    ),
904	    "ro_art": (
905	        "The UD treebank ArT is a treebank of the Aromanian dialect of the "
906	        "Romanian language in UD format."
907	    ),
908	    "ro_nonstandard": (
909	        "The Romanian Non-standard UD treebank (called UAIC-RoDia) is based on"
910	        " UAIC-RoDia Treebank. UAIC-RoDia = ISLRN 156-635-615-024-0"
911	    ),
912	    "ro_rrt": (
913	        "The Romanian UD treebank (called RoRefTrees) (Barbu Mititelu et al., "
914	        "2016) is the reference treebank in UD format for standard Romanian."
915	    ),
916	    "ro_simonero": "SiMoNERo is a medical corpus of contemporary Romanian.",
917	    "ru_gsd": (
918	        "Russian Universal Dependencies Treebank annotated and converted by "
919	        "Google."
920	    ),
921	    "ru_pud": (
922	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
923	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
924	        "Raw Text to Universal Dependencies."
925	    ),
926	    "ru_syntagrus": "Russian data from the SynTagRus corpus.",
927	    "ru_taiga": (
928	        "Universal Dependencies treebank is based on data samples extracted "
929	        "from Taiga Corpus and MorphoRuEval-2017 and GramEval-2020 shared "
930	        "tasks collections."
931	    ),
932	    "sa_ufal": (
933	        "A small Sanskrit treebank of sentences from Pacatantra, an ancient "
934	        "Indian collection of interrelated fables by Vishnu Sharma."
935	    ),
936	    "sa_vedic": (
937	        "The Treebank of Vedic Sanskrit contains 4,000 sentences with 27,000 "
938	        "words chosen from metrical and prose passages of the gveda (RV), the"
939	        " aunaka recension of the Atharvaveda (S), the Maitryasahit "
940	        "(MS), and the Aitareya- (AB) and atapatha-Brhmaas (B). Lexical "
941	        "and morpho-syntactic information has been generated using a tagging "
942	        "software and manually validated. POS tags have been induced "
943	        "automatically from the morpho-sytactic information of each word."
944	    ),
945	    "gd_arcosg": (
946	        "A treebank of Scottish Gaelic based on the Annotated Reference Corpus"
947	        " Of Scottish Gaelic (ARCOSG)."
948	    ),
949	    "sr_set": (
950	        "The Serbian UD treebank is based on the "
951	        "[SETimes-SR](http://hdl.handle.net/11356/1200) corpus and additional "
952	        "news documents from the Serbian web."
953	    ),
954	    "sms_giellagas": (
955	        "The UD Skolt Sami Giellagas treebank is based almost entirely on "
956	        "spoken Skolt Sami corpora."
957	    ),
958	    "sk_snk": (
959	        "The Slovak UD treebank is based on data originally annotated as part "
960	        "of the Slovak National Corpus, following the annotation style of the "
961	        "Prague Dependency Treebank."
962	    ),
963	    "sl_ssj": (
964	        "The Slovenian UD Treebank is a rule-based conversion of the ssj500k "
965	        "treebank, the largest collection of manually syntactically annotated "
966	        "data in Slovenian, originally annotated in the JOS annotation scheme."
967	    ),
968	    "sl_sst": (
969	        "The Spoken Slovenian UD Treebank (SST) is the first syntactically "
970	        "annotated corpus of spoken Slovenian, based on a sample of the "
971	        "reference GOS corpus, a collection of transcribed audio recordings of"
972	        " monologic, dialogic and multi-party spontaneous speech in different "
973	        "everyday situations."
974	    ),
975	    "soj_aha": (
976	        "The AHA Soi Treebank is a small treebank for contemporary Soi. Its "
977	        "corpus is collected and annotated manually. We have prepared this "
978	        "treebank based on interviews with Soi speakers."
979	    ),
980	    "ajp_madar": (
981	        "The South_Levantine_Arabic-MADAR treebank consists of 100 "
982	        "manually-annotated sentences taken from the "
983	        "[MADAR](https://camel.abudhabi.nyu.edu/madar/) (Multi-Arabic Dialect "
984	        "Applications and Resources) project. "
985	    ),
986	    "es_ancora": "Spanish data from the AnCora corpus.",
987	    "es_gsd": (
988	        "The Spanish UD is converted from the content head version of the "
989	        "universal dependency treebank v2.0 (legacy)."
990	    ),
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:84
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
83	    "grc_proiel": (
84	        "The Ancient Greek PROIEL treebank is based on the Ancient Greek data "
85	        "from the PROIEL treebank, which is maintained at the Department of "
86	        "Philosophy, Classics, History of Arts and Ideas at the University of "
87	        "Oslo. The conversion is based on the 20180408 release of the PROIEL "
88	        "treebank available from "
89	        "https://github.com/proiel/proiel-treebank/releases. The original "
90	        "annotators are acknowledged in the files available there. The "
91	        "conversion code is available in the Rubygem proiel-cli, "
92	        "https://github.com/proiel/proiel-cli."
93	    ),
94	    "apu_ufpa": (
95	        "The initial release contains 70 annotated sentences. This is the "
96	        "first treebank in a language from the Arawak family. The original "
97	        "interlinear glosses are included in the tree bank, and their "
98	        "conversion into a full UD annotation is an ongoing process. The "
99	        "sent_id values (e.g.: FernandaM2017:Texto-6-19) are representative of"
100	        " the collector, year of publication, text identifier and the number "
101	        "of the sentence in order from the original text."
102	    ),
103	    "hbo_ptnk": (
104	        "UD Ancient Hebrew PTNK contains portions of the Biblia Hebraic "
105	        "Stuttgartensia with morphological annotations from ETCBC."
106	    ),
107	    "ar_nyuad": (
108	        "The treebank consists of 19,738 sentences (738889 tokens), and its "
109	        "domain is mainly newswire. The annotation is licensed under the terms"
110	        " of CC BY-SA 4.0, and the original PATB can be obtained from the "
111	        "LDCs official website."
112	    ),
113	    "ar_padt": (
114	        "The Arabic-PADT UD treebank is based on the Prague Arabic Dependency "
115	        "Treebank (PADT), created at the Charles University in Prague."
116	    ),
117	    "ar_pud": (
118	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
119	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
120	        "Raw Text to Universal Dependencies."
121	    ),
122	    "aii_as": (
123	        "The Uppsala Assyrian Treebank is a small treebank for Modern Standard"
124	        " Assyrian. The corpus is collected and annotated manually. The data "
125	        "was randomly collected from different textbooks and a short "
126	        "translation of The Merchant of Venice."
127	    ),
128	    "bm_crb": (
129	        "The UD Bambara treebank is a section of the Corpus Rfrence du "
130	        "Bambara annotated natively with Universal Dependencies."
131	    ),
132	    "eu_bdt": (
133	        "The Basque UD treebank is based on a automatic conversion from part "
134	        "of the Basque Dependency Treebank (BDT), created at the University of"
135	        " of the Basque Country by the IXA NLP research group. The treebank "
136	        "consists of 8.993 sentences (121.443 tokens) and covers mainly "
137	        "literary and journalistic texts."
138	    ),
139	    "bej_nsc": (
140	        "A Universal Dependencies corpus for Beja, North-Cushitic branch of "
141	        "the Afro-Asiatic phylum mainly spoken in Sudan, Egypt and Eritrea."
142	    ),
143	    "be_hse": (
144	        "The Belarusian UD treebank is based on a sample of the news texts "
145	        "included in the Belarusian-Russian parallel subcorpus of the Russian "
146	        "National Corpus, online search available at: "
147	        "http://ruscorpora.ru/search-para-be.html."
148	    ),
149	    "bn_bru": (
150	        "The BRU Bengali treebank has been created at Begum Rokeya University,"
151	        " Rangpur, by the members of Semantics Lab."
152	    ),
153	    "bho_bhtb": (
154	        "The Bhojpuri UD Treebank (BHTB) v2.6 consists of 6,664 tokens(357 "
155	        "sentences). This Treebank is a part of the Universal Dependency "
156	        "treebank project. Initially, it was initiated by me (Atul) at "
157	        "Jawaharlal Nehru University, New Delhi during the doctoral research "
158	        "work. BHTB data contains syntactic annotation according to "
159	        "dependency-constituency schema, as well as morphological tags and "
160	        "lemmas. In this data, XPOS is annotated  according to Bureau of "
161	        "Indian Standards (BIS) Part Of Speech (POS) tagset."
162	    ),
163	    "br_keb": (
164	        "UD Breton-KEB is a treebank of Breton that has been manually "
165	        "annotated according to the Universal Dependencies guidelines. The "
166	        "tokenisation guidelines and morphological annotation comes from a "
167	        "finite-state morphological analyser of Breton released as part of the"
168	        " Apertium project."
169	    ),
170	    "bg_btb": (
171	        "UD_Bulgarian-BTB is based on the HPSG-based BulTreeBank, created at "
172	        "the Institute of Information and Communication Technologies, "
173	        "Bulgarian Academy of Sciences. The original consists of 215,000 "
174	        "tokens (over 15,000 sentences)."
175	    ),
176	    "bxr_bdt": (
177	        "The UD Buryat treebank was annotated manually natively in UD and "
178	        "contains grammar book sentences, along with news and some fiction."
179	    ),
180	    "yue_hk": (
181	        "A Cantonese treebank (in Traditional Chinese characters) of film "
182	        "subtitles and of legislative proceedings of Hong Kong, parallel with "
183	        "the Chinese-HK treebank."
184	    ),
185	    "ca_ancora": "Catalan data from the AnCora corpus.",
186	    "ceb_gja": (
187	        "UD_Cebuano_GJA is a collection of annotated Cebuano sample sentences "
188	        "randomly taken from three different sources: community-contributed "
189	        "samples from the website Tatoeba, a Cebuano grammar book by Bunye & "
190	        "Yap (1971) and Tanangkinsing's reference grammar on Cebuano (2011). "
191	        "This project is currently work in progress."
192	    ),
193	    "zh_cfl": (
194	        "The Chinese-CFL UD treebank is manually annotated by Keying Li with "
195	        "minor manual revisions by Herman Leung and John Lee at City "
196	        "University of Hong Kong, based on essays written by learners of "
197	        "Mandarin Chinese as a foreign language. The data is in Simplified "
198	        "Chinese."
199	    ),
200	    "zh_gsd": (
201	        "Traditional Chinese Universal Dependencies Treebank annotated and "
202	        "converted by Google."
203	    ),
204	    "zh_gsdsimp": (
205	        "Simplified Chinese Universal Dependencies dataset converted from the "
206	        "GSD (traditional) dataset with manual corrections."
207	    ),
208	    "zh_hk": (
209	        "A Traditional Chinese treebank of film subtitles and of legislative "
210	        "proceedings of Hong Kong, parallel with the Cantonese-HK treebank."
211	    ),
212	    "zh_pud": (
213	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
214	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
215	        "Raw Text to Universal Dependencies."
216	    ),
217	    "ckt_hse": (
218	        "This data is a manual annotation of the corpus from multimedia "
219	        "annotated corpus of the Chuklang project, a dialectal corpus of the "
220	        "Amguema variant of Chukchi."
221	    ),
222	    "lzh_kyoto": (
223	        "Classical Chinese Universal Dependencies Treebank annotated and "
224	        "converted by Institute for Research in Humanities, Kyoto University."
225	    ),
226	    "cop_scriptorium": (
227	        "UD Coptic contains manually annotated Sahidic Coptic texts, including"
228	        " Biblical texts, sermons, letters, and hagiography."
229	    ),
230	    "hr_set": (
231	        "The Croatian UD treebank is based on the extension of the SETimes-HR "
232	        "corpus, the hr500k corpus."
233	    ),
234	    "cs_cac": (
235	        "The UD_Czech-CAC treebank is based on the Czech Academic Corpus 2.0 "
236	        "(CAC; esk akademick korpus; AK), created at Charles University in"
237	        " Prague."
238	    ),
239	    "cs_cltt": (
240	        "The UD_Czech-CLTT treebank is based on the Czech Legal Text Treebank "
241	        "1.0, created at Charles University in Prague."
242	    ),
243	    "cs_fictree": (
244	        "FicTree is a treebank of Czech fiction, automatically converted into "
245	        "the UD format. The treebank was built at Charles University in "
246	        "Prague."
247	    ),
248	    "cs_pdt": (
249	        "The Czech-PDT UD treebank is based on the Prague Dependency Treebank "
250	        "3.0 (PDT), created at the Charles University in Prague."
251	    ),
252	    "cs_pud": (
253	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
254	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
255	        "Raw Text to Universal Dependencies."
256	    ),
257	    "da_ddt": (
258	        "The Danish UD treebank is a conversion of the Danish Dependency "
259	        "Treebank."
260	    ),
261	    "nl_alpino": (
262	        "This corpus consists of samples from various treebanks annotated at "
263	        "the University of Groningen using the Alpino annotation tools and "
264	        "guidelines."
265	    ),
266	    "nl_lassysmall": (
267	        "This corpus contains sentences from the Wikipedia section of the "
268	        "Lassy Small Treebank. Universal Dependency annotation was generated "
269	        "automatically from the original annotation in Lassy."
270	    ),
271	    "en_esl": (
272	        "UD English-ESL / Treebank of Learner English (TLE) contains manual "
273	        "POS tag and dependency annotations for 5,124 English as a Second "
274	        "Language (ESL) sentences drawn from the Cambridge Learner Corpus "
275	        "First Certificate in English (FCE) dataset."
276	    ),
277	    "en_ewt": (
278	        "A Gold Standard Universal Dependencies Corpus for English, built over"
279	        " the source material of the English Web Treebank LDC2012T13 "
280	        "(https://catalog.ldc.upenn.edu/LDC2012T13)."
281	    ),
282	    "en_gum": (
283	        "Universal Dependencies syntax annotations from the GUM corpus "
284	        "(https://corpling.uis.georgetown.edu/gum/)."
285	    ),
286	    "en_gumreddit": (
287	        "Universal Dependencies syntax annotations from the Reddit portion of "
288	        "the GUM corpus (https://corpling.uis.georgetown.edu/gum/) "
289	    ),
290	    "en_lines": (
291	        "UD English_LinES is the English half of the LinES Parallel Treebank "
292	        "with the original dependency annotation first automatically converted"
293	        " into Universal Dependencies and then partially reviewed. Its "
294	        "contents cover literature, an online manual and Europarl data."
295	    ),
296	    "en_atis": (
297	        "UD Atis Treebank is a manually annotated treebank consisting of the "
298	        "sentences in the Atis (Airline Travel Informations) dataset which "
299	        "includes the human speech transcriptions of people asking for flight "
300	        "information on the automated inquiry systems."
301	    ),
302	    "en_partut": (
303	        "UD_English-ParTUT is a conversion of a multilingual parallel treebank"
304	        " developed at the University of Turin, and consisting of a variety of"
305	        " text genres, including talks, legal texts and Wikipedia articles, "
306	        "among others."
307	    ),
308	    "en_pronouns": (
309	        "UD English-Pronouns is dataset created to make pronoun identification"
310	        " more accurate and with a more balanced distribution across genders. "
311	        "The dataset is initially targeting the Independent Genitive pronouns,"
312	        " 'hers', (independent) 'his', (singular) 'theirs', 'mine', and "
313	        "(singular) 'yours'."
314	    ),
315	    "en_pud": (
316	        "This is the English portion of the Parallel Universal Dependencies "
317	        "(PUD) treebanks created for the CoNLL 2017 shared task on "
318	        "Multilingual Parsing from Raw Text to Universal Dependencies "
319	        "(http://universaldependencies.org/conll17/)."
320	    ),
321	    "myv_jr": (
322	        "UD Erzya is the original annotation (CoNLL-U) for texts in the Erzya "
323	        "language, it originally consists of a sample from a number of fiction"
324	        " authors writing originals in Erzya."
325	    ),
326	    "et_edt": (
327	        "UD Estonian is a converted version of the Estonian Dependency "
328	        "Treebank (EDT), originally annotated in the Constraint Grammar (CG) "
329	        "annotation scheme, and consisting of genres of fiction, newspaper "
330	        "texts and scientific texts. The treebank contains 30,972 trees, "
331	        "437,769 tokens."
332	    ),
333	    "et_ewt": (
334	        "UD EWT treebank consists of different genres of new media. The "
335	        "treebank contains 4,493 trees, 56,399 tokens."
336	    ),
337	    "fo_farpahc": (
338	        "UD_Icelandic-FarPaHC is a conversion of the Faroese Parsed Historical"
339	        " Corpus (FarPaHC) to the Universal Dependencies scheme. The "
340	        "conversion was done using UDConverter."
341	    ),
342	    "fo_oft": "This is a treebank of Faroese based on the Faroese Wikipedia.",
343	    "fi_ftb": (
344	        "FinnTreeBank 1 consists of manually annotated grammatical examples "
345	        "from VISK. The UD version of FinnTreeBank 1 was converted from a "
346	        "native annotation model with a script and later manually revised."
347	    ),
348	    "fi_ood": (
349	        "Finnish-OOD is an external out-of-domain test set for Finnish-TDT "
350	        "annotated natively into UD scheme."
351	    ),
352	    "fi_pud": (
353	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
354	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
355	        "Raw Text to Universal Dependencies."
356	    ),
357	    "fi_tdt": (
358	        "UD_Finnish-TDT is based on the Turku Dependency Treebank (TDT), a "
359	        "broad-coverage dependency treebank of general Finnish covering "
360	        "numerous genres. The conversion to UD was followed by extensive "
361	        "manual checks and corrections, and the treebank closely adheres to "
362	        "the UD guidelines."
363	    ),
364	    "fr_fqb": (
365	        "The corpus **UD_French-FQB** is an automatic conversion of the French"
366	        " QuestionBank v1, a corpus entirely made of questions."
367	    ),
368	    "fr_ftb": (
369	        "The Universal Dependency version of the French Treebank (Abeill et "
370	        "al., 2003), hereafter UD_French-FTB, is a treebank of sentences from "
371	        "the newspaper Le Monde, initially manually annotated with "
372	        "morphological information and phrase-structure and then converted to "
373	        "the Universal Dependencies annotation scheme."
374	    ),
375	    "fr_gsd": (
376	        "The **UD_French-GSD** was converted in 2015 from the content head "
377	        "version of the universal dependency treebank v2.0 "
378	        "(https://github.com/ryanmcd/uni-dep-tb). It is updated since 2015 "
379	        "independently from the previous source."
380	    ),
381	    "fr_partut": (
382	        "UD_French-ParTUT is a conversion of a multilingual parallel treebank "
383	        "developed at the University of Turin, and consisting of a variety of "
384	        "text genres, including talks, legal texts and Wikipedia articles, "
385	        "among others."
386	    ),
387	    "fr_rhapsodie": "A Universal Dependencies corpus for spoken French.",
388	    "fr_parisstories": (
389	        "Paris Stories is a corpus of oral French collected and transcribed by"
390	        " Linguistics students from Sorbonne Nouvelle and corrected by "
391	        "students from the Plurital Master's Degree of Computational "
392	        "Linguistics ( Inalco, Paris Nanterre, Sorbonne Nouvelle) between 2017"
393	        " and 2021. It contains monologues and dialogues from speakers living "
394	        "in the Parisian region."
395	    ),
396	    "fr_pud": (
397	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
398	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
399	        "Raw Text to Universal Dependencies."
400	    ),
401	    "fr_sequoia": (
402	        "UD_French-Sequoia is an automatic conversion of the Sequoia Treebank "
403	        "corpus French Sequoia corpus."
404	    ),
405	    "gl_ctg": (
406	        "The Galician UD treebank is based on the automatic parsing of the "
407	        "Galician Technical Corpus (http://sli.uvigo.gal/CTG) created at the "
408	        "University of Vigo by the the TALG NLP research group."
409	    ),
410	    "gl_treegal": (
411	        "The Galician-TreeGal is a treebank for Galician developed at LyS "
412	        "Group (Universidade da Corua)."
413	    ),
414	    "de_gsd": (
415	        "The German UD is converted from the content head version of the "
416	        "universal dependency treebank v2.0 (legacy)."
417	    ),
418	    "de_hdt": (
419	        "UD German-HDT is a conversion of the Hamburg Dependency Treebank, "
420	        "created at the University of Hamburg through manual annotation in "
421	        "conjunction with a standard for morphologically and syntactically "
422	        "annotating sentences as well as a constraint-based parser."
423	    ),
424	    "de_lit": (
425	        "This treebank aims at gathering texts of the German literary history."
426	        " Currently, it hosts Fragments of the early Romanticism, i.e. "
427	        "aphorism-like texts mainly dealing with philosophical issues "
428	        "concerning art, beauty and related topics."
429	    ),
430	    "de_pud": (
431	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
432	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
433	        "Raw Text to Universal Dependencies."
434	    ),
435	    "got_proiel": (
436	        "The UD Gothic treebank is based on the Gothic data from the PROIEL "
437	        "treebank, and consists of Wulfila's Bible translation."
438	    ),
439	    "el_gdt": (
440	        "The Greek UD treebank (UD_Greek-GDT) is derived from the Greek "
441	        "Dependency Treebank (http://gdt.ilsp.gr), a resource developed and "
442	        "maintained by researchers at the Institute for Language and Speech "
443	        "Processing/Athena R.C. (http://www.ilsp.gr)."
444	    ),
445	    "gub_tudet": (
446	        "UD_Guajajara-TuDeT is a collection of annotated sentences in "
447	        "Guajajara. Sentences stem from multiple sources such as descriptions "
448	        "of the language, short stories, dictionaries and translations from "
449	        "the New Testament. Sentence annotation and documentation by Lorena "
450	        "Martn Rodrguez and Fabrcio Ferraz Gerardi."
451	    ),
452	    "gn_oldtudet": (
453	        "UD_Guarani-OldTuDeT is a collection of annotated texts in Old "
454	        "Guaran. All known sources in this language are being annotated: "
455	        "cathesisms, grammars (seventeenth and eighteenth century), sentences "
456	        "from dictionaries, and other texts. Sentence annotation and "
457	        "documentation by Fabrcio Ferraz Gerardi and Lorena Martn Rodrguez."
458	    ),
459	    "he_htb": "A Universal Dependencies Corpus for Hebrew.",
460	    "he_iahltwiki": (
461	        "Publicly available subset of the IAHLT UD Hebrew Treebank's Wikipedia"
462	        " section (https://www.iahlt.org/)"
463	    ),
464	    "qfn_fame": (
465	        "UD_Frisian_Dutch-Fame is a selection of 400 sentences from the FAME! "
466	        "speech corpus by Yilmaz et al. (2016a, 2016b). The treebank is "
467	        "manually annotated using the UD scheme."
468	    ),
469	    "qhe_hiencs": (
470	        "The Hindi-English Code-switching treebank is based on code-switching "
471	        "tweets of Hindi and English multilingual speakers (mostly Indian) on "
472	        "Twitter. The treebank is manually annotated using UD sceheme. The "
473	        "training and evaluations sets were seperately annotated by different "
474	        "annotators using UD v2 and v1 guidelines respectively. The evaluation"
475	        " sets are automatically converted from UD v1 to v2."
476	    ),
477	    "hi_hdtb": (
478	        "The Hindi UD treebank is based on the Hindi Dependency Treebank "
479	        "(HDTB), created at IIIT Hyderabad, India."
480	    ),
481	    "hi_pud": (
482	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
483	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
484	        "Raw Text to Universal Dependencies."
485	    ),
486	    "hu_szeged": (
487	        "The Hungarian UD treebank is derived from the Szeged Dependency "
488	        "Treebank (Vincze et al. 2010)."
489	    ),
490	    "is_modern": (
491	        "UD_Icelandic-Modern is a conversion of the modern additions to the "
492	        "Icelandic Parsed Historical Corpus (IcePaHC) to the Universal "
493	        "Dependencies scheme."
494	    ),
495	    "is_icepahc": (
496	        "UD_Icelandic-IcePaHC is a conversion of the Icelandic Parsed "
497	        "Historical Corpus (IcePaHC) to the Universal Dependencies scheme. The"
498	        " conversion was done using UDConverter."
499	    ),
500	    "is_pud": (
501	        "Icelandic-PUD is the Icelandic part of the Parallel Universal "
502	        "Dependencies (PUD) treebanks."
503	    ),
504	    "id_csui": (
505	        "UD Indonesian-CSUI is a conversion from an Indonesian constituency "
506	        "treebank in the Penn Treebank format named Kethu that was also a "
507	        "conversion from a constituency treebank built by Dinakaramani et al. "
508	        "(2015). We named this treebank Indonesian-CSUI, since all the three "
509	        "versions of the treebanks were built at Faculty of Computer Science, "
510	        "Universitas Indonesia."
511	    ),
512	    "id_gsd": (
513	        "The Indonesian UD is converted from the content head version of the "
514	        "universal dependency treebank v2.0 (legacy)."
515	    ),
516	    "id_pud": (
517	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
518	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
519	        "Raw Text to Universal Dependencies."
520	    ),
521	    "ga_idt": (
522	        "A Universal Dependencies 4910-sentence treebank for modern Irish."
523	    ),
524	    "ga_twittirish": (
525	        "A Universal Dependencies treebank of 866 tweets in modern Irish."
526	    ),
527	    "it_isdt": (
528	        "The Italian corpus annotated according to the UD annotation scheme "
529	        "was obtained by conversion from ISDT (Italian Stanford Dependency "
530	        "Treebank), released for the dependency parsing shared task of "
531	        "Evalita-2014 (Bosco et al. 2014)."
532	    ),
533	    "it_partut": (
534	        "UD_Italian-ParTUT is a conversion of a multilingual parallel treebank"
535	        " developed at the University of Turin, and consisting of a variety of"
536	        " text genres, including talks, legal texts and Wikipedia articles, "
537	        "among others."
538	    ),
539	    "it_postwita": (
540	        "PoSTWITA-UD is a collection of Italian tweets annotated in Universal "
541	        "Dependencies that can be exploited for the training of NLP systems to"
542	        " enhance their performance on social media texts."
543	    ),
544	    "it_markit": (
545	        "It is MarkIT That is New: An Italian Treebank of Marked "
546	        "Constructions. Teresa Paccosi, Alessio Palmero Aprosio and Sara "
547	        "Tonelli, To appear in Proceedings of the Eighth Italian Conference on"
548	        " Computational Linguistics 2022 (CLIC-it 2021)"
549	    ),
550	    "it_valico": (
551	        "Manually corrected Treebank of Learner Italian drawn from the Valico "
552	        "corpus and correspondent corrected sentences."
553	    ),
554	    "it_pud": (
555	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
556	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
557	        "Raw Text to Universal Dependencies."
558	    ),
559	    "it_twittiro": (
560	        "TWITTIR-UD is a collection of ironic Italian tweets annotated in "
561	        "Universal Dependencies. The treebank can be exploited for the "
562	        "training of NLP systems to enhance their performance on social media "
563	        "texts, and in particular, for irony detection purposes."
564	    ),
565	    "it_vit": (
566	        "The UD_Italian-VIT corpus was obtained by conversion from VIT (Venice"
567	        " Italian Treebank), developed at the Laboratory of Computational "
568	        "Linguistics of the Universit Ca' Foscari in Venice (Delmonte et al. "
569	        "2007; Delmonte 2009; "
570	        "http://rondelmo.it/resource/VIT/Browser-VIT/index.htm)."
571	    ),
572	    "ja_pudluw": (
573	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
574	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
575	        "Raw Text to Universal Dependencies."
576	    ),
577	    "ja_bccwjluw": (
578	        "This Universal Dependencies (UD) Japanese treebank is based on the "
579	        "definition of UD Japanese convention described in the UD "
580	        "documentation. The original sentences are from `Balanced Corpus of "
581	        "Contemporary Written Japanese'(BCCWJ)."
582	    ),
583	    "ja_gsdluw": (
584	        "This Universal Dependencies (UD) Japanese treebank is based on the "
585	        "definition of UD Japanese convention described in the UD "
586	        "documentation. The original sentences are from Google UDT 2.0."
587	    ),
588	    "ja_bccwj": (
589	        "This Universal Dependencies (UD) Japanese treebank is based on the "
590	        "definition of UD Japanese convention described in the UD "
591	        "documentation. The original sentences are from `Balanced Corpus of "
592	        "Contemporary Written Japanese'(BCCWJ)."
593	    ),
594	    "ja_gsd": (
595	        "This Universal Dependencies (UD) Japanese treebank is based on the "
596	        "definition of UD Japanese convention described in the UD "
597	        "documentation.  The original sentences are from Google UDT 2.0."
598	    ),
599	    "ja_modern": (
600	        "This Universal Dependencies (UD) Japanese treebank is based on the "
601	        "definition of UD Japanese convention described in the UD "
602	        "documentation. The original sentences are from `Corpus of Historical "
603	        "Japanese' (CHJ)."
604	    ),
605	    "ja_pud": (
606	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
607	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
608	        " Raw Text to Universal "
609	        "Dependencies](http://universaldependencies.org/conll17/)."
610	    ),
611	    "jv_csui": (
612	        "UD Javanese-CSUI is a dependency treebank in Javanese, a regional "
613	        "language in Indonesia with more than 60 million users. The original "
614	        "sentences were taken from OPUS, especially from the WikiMatrix v1 "
615	        "corpus. We revised the sentences that contained more Indonesian words"
616	        " than Javanese words and manually annotated them."
617	    ),
618	    "urb_tudet": (
619	        "UD_Kaapor-TuDeT is a collection of annotated sentences in Ka'apor. "
620	        "The project is a work in progress and the treebank is being updated "
621	        "on a regular basis."
622	    ),
623	    "xnr_kdtb": (
624	        "The Kangri UD Treebank (KDTB) is a part of the Universal Dependency "
625	        "treebank project."
626	    ),
627	    "krl_kkpp": (
628	        "UD Karelian-KKPP is a manually annotated new corpus of Karelian made "
629	        "in Universal dependencies annotation scheme. The data is collected "
630	        "from VepKar corpora and consists of mostly modern news texts but also"
631	        " some stories and educational texts."
632	    ),
633	    "kk_ktb": (
634	        "The UD Kazakh treebank is a combination of text from various sources "
635	        "including Wikipedia, some folk tales, sentences from the UDHR, news "
636	        "and phrasebook sentences. Sentences IDs include partial document "
637	        "identifiers."
638	    ),
639	    "arr_tudet": (
640	        "UD_Karo-TuDeT is a collection of annotated sentences in Karo. The "
641	        "sentences stem from the only grammatical description of the language "
642	        "(Gabas, 1999) and from the sentences in the dictionary by the same "
643	        "author (Gabas, 2007). Sentence annotation and documentation by "
644	        "Fabrcio Ferraz Gerardi."
645	    ),
646	    "kfm_aha": (
647	        "The AHA Khunsari Treebank is a small treebank for contemporary "
648	        "Khunsari. Its corpus is collected and annotated manually. We have "
649	        "prepared this treebank based on interviews with Khunsari speakers."
650	    ),
651	    "quc_iu": (
652	        "UD Kiche-IU is a treebank consisting of sentences from a variety of"
653	        " text domains but principally dictionary example sentences and "
654	        "linguistic examples."
655	    ),
656	    "koi_uh": (
657	        "This is a Komi-Permyak literary language treebank consisting of "
658	        "original and translated texts."
659	    ),
660	    "kpv_ikdp": (
661	        "This treebank consists of dialectal transcriptions of spoken "
662	        "Komi-Zyrian. The current texts are short recorded segments from "
663	        "different areas where the Iva dialect of Komi language is spoken."
664	    ),
665	    "kpv_lattice": (
666	        "UD Komi-Zyrian Lattice is a treebank of written standard Komi-Zyrian."
667	    ),
668	    "ko_gsd": (
669	        "The Google Korean Universal Dependency Treebank is first converted "
670	        "from the Universal Dependency Treebank v2.0 (legacy), and then "
671	        "enhanced by Chun et al., 2018."
672	    ),
673	    "ko_kaist": (
674	        "The KAIST Korean Universal Dependency Treebank is generated by Chun "
675	        "et al., 2018 from the constituency trees in the KAIST Tree-Tagging "
676	        "Corpus."
677	    ),
678	    "ko_pud": (
679	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
680	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
681	        "Raw Text to Universal Dependencies."
682	    ),
683	    "kmr_mg": (
684	        "The UD Kurmanji corpus is a corpus of Kurmanji Kurdish. It contains "
685	        "fiction and encyclopaedic texts in roughly equal measure. It has been"
686	        " annotated natively in accordance with the UD annotation scheme."
687	    ),
688	    "la_ittb": (
689	        "Latin data from the _Index Thomisticus_ Treebank. Data are taken from"
690	        " the _Index Thomisticus_ corpus by Roberto Busa SJ, which contains "
691	        "the complete work by Thomas Aquinas (12251274; Medieval Latin) and "
692	        "by 61 other authors related to Thomas."
693	    ),
694	    "la_udante": (
695	        "The UDante treebank is based on the Latin texts of Dante Alighieri, "
696	        "taken from the DanteSearch corpus, originally created at the "
697	        "University of Pisa, Italy. It is a treebank of Latin language, more "
698	        "precisely of literary Medieval Latin (XIVth century)."
699	    ),
700	    "la_llct": (
701	        "This Universal Dependencies version of the LLCT (Late Latin Charter "
702	        "Treebank) consists of an automated conversion of the LLCT2 treebank "
703	        "from the Latin Dependency Treebank (LDT) format into the Universal "
704	        "Dependencies standard."
705	    ),
706	    "la_perseus": (
707	        "This Universal Dependencies Latin Treebank consists of an automatic "
708	        "conversion of a selection of passages from the Ancient Greek and "
709	        "Latin Dependency Treebank 2.1"
710	    ),
711	    "la_proiel": (
712	        "The Latin PROIEL treebank is based on the Latin data from the PROIEL "
713	        "treebank, and contains most of the Vulgate New Testament translations"
714	        " plus selections from Caesar's Gallic War, Cicero's Letters to "
715	        "Atticus, Palladius' Opus Agriculturae and the first book of Cicero's "
716	        "De officiis."
717	    ),
718	    "lv_lvtb": (
719	        "Latvian UD Treebank is based on Latvian Treebank (LVTB), being "
720	        "created at University of Latvia, Institute of Mathematics and "
721	        "Computer Science, Artificial Intelligence Laboratory."
722	    ),
723	    "lij_glt": (
724	        "The Genoese Ligurian Treebank is a small, manually annotated "
725	        "collection of contemporary Ligurian prose. The focus of the treebank "
726	        "is written Genoese, the koin variety of Ligurian which is associated"
727	        " with today's literary, journalistic and academic ligurophone sphere."
728	    ),
729	    "lt_alksnis": (
730	        "The Lithuanian dependency treebank ALKSNIS v3.0 (Vytautas Magnus "
731	        "University)."
732	    ),
733	    "lt_hse": (
734	        "Lithuanian treebank annotated manually (dependencies) using the "
735	        "Morphological Annotator by CCL, Vytautas Magnus University "
736	        "(http://tekstynas.vdu.lt/) and manual disambiguation. A pilot version"
737	        " which includes news and an essay by Tomas Venclova is available "
738	        "here."
739	    ),
740	    "olo_kkpp": (
741	        "UD Livvi-KKPP is a manually annotated new corpus of Livvi-Karelian "
742	        "made directly in the Universal dependencies annotation scheme. The "
743	        "data is collected from VepKar corpora and consists of mostly modern "
744	        "news texts but also some stories and educational texts."
745	    ),
746	    "nds_lsdc": (
747	        "The UD Low Saxon LSDC dataset consists of sentences in 18 Low Saxon "
748	        "dialects from both Germany and the Netherlands. These sentences are "
749	        "(or are to become) part of the LSDC dataset and represent the "
750	        "language from the 19th and early 20th century in genres such as short"
751	        " stories, novels, speeches, letters and fairytales."
752	    ),
753	    "mt_mudt": (
754	        "MUDT (Maltese Universal Dependencies Treebank) is a manually "
755	        "annotated treebank of Maltese, a Semitic language of Malta descended "
756	        "from North African Arabic with a significant amount of Italo-Romance "
757	        "influence. MUDT was designed as a balanced corpus with four major "
758	        "genres (see Splitting below) represented roughly equally."
759	    ),
760	    "gv_cadhan": (
761	        "This is the Cadhan Aonair UD treebank for Manx Gaelic, created by "
762	        "Kevin Scannell."
763	    ),
764	    "mr_ufal": (
765	        "UD Marathi is a manually annotated treebank consisting primarily of "
766	        "stories from Wikisource, and parts of an article on Wikipedia."
767	    ),
768	    "gun_dooley": (
769	        "UD Mbya_Guarani-Dooley is a corpus of narratives written in Mby "
770	        "Guaran (Tupian) in Brazil, and collected by Robert Dooley. Due to "
771	        "copyright restrictions, the corpus that is distributed as part of UD "
772	        "only contains the annotation (tags, features, relations) while the "
773	        "FORM and LEMMA columns are empty."
774	    ),
775	    "gun_thomas": (
776	        "UD Mbya_Guarani-Thomas is a corpus of Mby Guaran (Tupian) texts "
777	        "collected by Guillaume Thomas. The current version of the corpus "
778	        "consists of three speeches by Paulina Kerechu Nez Romero, a Mby "
779	        "Guaran speaker from Ytu, Caazap Department, Paraguay."
780	    ),
781	    "mdf_jr": (
782	        "Erme Universal Dependencies annotated texts Moksha are the origin of "
783	        "UD_Moksha-JR with annotation (CoNLL-U) for texts in the Moksha "
784	        "language, it originally consists of a sample from a number of fiction"
785	        " authors writing originals in Moksha."
786	    ),
787	    "myu_tudet": (
788	        "UD_Munduruku-TuDeT is a collection of annotated sentences in "
789	        "Munduruk. Together with UD_Akuntsu-TuDeT and UD_Tupinamba-TuDeT, "
790	        "UD_Munduruku-TuDeT is part of the TuLaR project."
791	    ),
792	    "pcm_nsc": (
793	        "A Universal Dependencies corpus for spoken Naija (Nigerian Pidgin)."
794	    ),
795	    "nyq_aha": (
796	        "The AHA Nayini Treebank is a small treebank for contemporary Nayini. "
797	        "Its corpus is collected and annotated manually. We have prepared this"
798	        " treebank based on interviews with Nayini speakers."
799	    ),
800	    "sme_giella": (
801	        "This is a North Smi treebank based on a manually disambiguated and "
802	        "function-labelled gold-standard corpus of North Smi produced by the "
803	        "Giellatekno team at UiT Norgga rktala universitehta."
804	    ),
805	    "no_bokmaal": (
806	        "The Norwegian UD treebank is based on the Bokml section of the "
807	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
808	        " Norwegian. NDT has been automatically converted to the UD scheme by "
809	        "Lilja vrelid at the University of Oslo."
810	    ),
811	    "no_nynorsk": (
812	        "The Norwegian UD treebank is based on the Nynorsk section of the "
813	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
814	        " Norwegian.  NDT has been automatically converted to the UD scheme by"
815	        " Lilja vrelid at the University of Oslo."
816	    ),
817	    "no_nynorsklia": (
818	        "This Norwegian treebank is based on the LIA treebank of transcribed "
819	        "spoken Norwegian dialects. The treebank has been automatically "
820	        "converted to the UD scheme by Lilja vrelid at the University of "
821	        "Oslo."
822	    ),
823	    "cu_proiel": (
824	        "The Old Church Slavonic (OCS) UD treebank is based on the Old Church "
825	        "Slavonic data from the PROIEL treebank and contains the text of the "
826	        "Codex Marianus New Testament translation."
827	    ),
828	    "fro_srcmf": (
829	        "UD_Old_French-SRCMF is a conversion of (part of) the SRCMF corpus "
830	        "(Syntactic Reference Corpus of Medieval French srcmf.org)."
831	    ),
832	    "orv_birchbark": (
833	        "UD Old_East_Slavic-Birchbark is based on the RNC Corpus of Birchbark "
834	        "Letters and includes documents written in 1025-1500 in an East Slavic"
835	        " vernacular (letters, household and business records, records for "
836	        "church services, spell against diseases, and other short "
837	        "inscriptions). The treebank is manually syntactically annotated in "
838	        "the UD 2.0 scheme, morphological and lexical annotation is a "
839	        "conversion of the original RNC annotation."
840	    ),
841	    "orv_rnc": (
842	        "`UD_Old_Russian-RNC` is a sample of the Middle Russian corpus "
843	        "(1300-1700), a part of the Russian National Corpus. The data were "
844	        "originally annotated according to the RNC and extended UD-Russian "
845	        "morphological schemas and UD 2.4 dependency schema."
846	    ),
847	    "orv_torot": (
848	        "UD_Old_Russian-TOROT is a conversion of a selection of the Old East "
849	        "Slavonic and Middle Russian data in the Troms Old Russian and OCS "
850	        "Treebank (TOROT), which was originally annotated in PROIEL dependency"
851	        " format."
852	    ),
853	    "otk_tonqq": (
854	        "`UD_Old_Turkish-Tonqq` is an Old Turkish treebank built upon Turkic "
855	        "script texts or sentences that are trivially convertible."
856	    ),
857	    "fa_perdt": (
858	        "The Persian Universal Dependency Treebank (PerUDT) is the result of "
859	        "automatic coversion of Persian Dependency Treebank (PerDT) with "
860	        "extensive manual corrections. Please refer to the follwoing work, if "
861	        "you use this data: Mohammad Sadegh Rasooli, Pegah Safari, Amirsaeid "
862	        "Moloodi, and Alireza Nourian. 'The Persian Dependency Treebank Made "
863	        "Universal'. 2020 (to appear)."
864	    ),
865	    "fa_seraji": (
866	        "The Persian Universal Dependency Treebank (Persian UD) is based on "
867	        "Uppsala Persian Dependency Treebank (UPDT). The conversion of the "
868	        "UPDT to the Universal Dependencies was performed semi-automatically "
869	        "with extensive manual checks and corrections."
870	    ),
871	    "pl_lfg": (
872	        "The LFG Enhanced UD treebank of Polish is based on a corpus of LFG "
873	        "(Lexical Functional Grammar) syntactic structures generated by an LFG"
874	        " grammar of Polish, POLFIE, and manually disambiguated by human "
875	        "annotators."
876	    ),
877	    "pl_pdb": (
878	        "The Polish PDB-UD treebank is based on the Polish Dependency Bank 2.0"
879	        " (PDB 2.0), created at the Institute of Computer Science, Polish "
880	        "Academy of Sciences in Warsaw. The PDB-UD treebank is an extended and"
881	        " corrected version of the Polish SZ-UD treebank (the release 1.2 to "
882	        "2.3)."
883	    ),
884	    "pl_pud": (
885	        "This is the Polish portion of the Parallel Universal Dependencies "
886	        "(PUD) treebanks, created at the Institute of Computer Science, Polish"
887	        " Academy of Sciences in Warsaw.Re"
888	    ),
889	    "pt_bosque": (
890	        "This Universal Dependencies (UD) Portuguese treebank is based on the "
891	        "Constraint Grammar converted version of the Bosque, which is part of "
892	        "the Floresta Sint(c)tica treebank. It contains both European "
893	        "(CETEMPblico) and Brazilian (CETENFolha) variants."
894	    ),
895	    "pt_gsd": (
896	        "The Brazilian Portuguese UD is converted from the Google Universal "
897	        "Dependency Treebank v2.0 (legacy)."
898	    ),
899	    "pt_pud": (
900	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
901	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
902	        "Raw Text to Universal Dependencies."
903	    ),
904	    "ro_art": (
905	        "The UD treebank ArT is a treebank of the Aromanian dialect of the "
906	        "Romanian language in UD format."
907	    ),
908	    "ro_nonstandard": (
909	        "The Romanian Non-standard UD treebank (called UAIC-RoDia) is based on"
910	        " UAIC-RoDia Treebank. UAIC-RoDia = ISLRN 156-635-615-024-0"
911	    ),
912	    "ro_rrt": (
913	        "The Romanian UD treebank (called RoRefTrees) (Barbu Mititelu et al., "
914	        "2016) is the reference treebank in UD format for standard Romanian."
915	    ),
916	    "ro_simonero": "SiMoNERo is a medical corpus of contemporary Romanian.",
917	    "ru_gsd": (
918	        "Russian Universal Dependencies Treebank annotated and converted by "
919	        "Google."
920	    ),
921	    "ru_pud": (
922	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
923	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
924	        "Raw Text to Universal Dependencies."
925	    ),
926	    "ru_syntagrus": "Russian data from the SynTagRus corpus.",
927	    "ru_taiga": (
928	        "Universal Dependencies treebank is based on data samples extracted "
929	        "from Taiga Corpus and MorphoRuEval-2017 and GramEval-2020 shared "
930	        "tasks collections."
931	    ),
932	    "sa_ufal": (
933	        "A small Sanskrit treebank of sentences from Pacatantra, an ancient "
934	        "Indian collection of interrelated fables by Vishnu Sharma."
935	    ),
936	    "sa_vedic": (
937	        "The Treebank of Vedic Sanskrit contains 4,000 sentences with 27,000 "
938	        "words chosen from metrical and prose passages of the gveda (RV), the"
939	        " aunaka recension of the Atharvaveda (S), the Maitryasahit "
940	        "(MS), and the Aitareya- (AB) and atapatha-Brhmaas (B). Lexical "
941	        "and morpho-syntactic information has been generated using a tagging "
942	        "software and manually validated. POS tags have been induced "
943	        "automatically from the morpho-sytactic information of each word."
944	    ),
945	    "gd_arcosg": (
946	        "A treebank of Scottish Gaelic based on the Annotated Reference Corpus"
947	        " Of Scottish Gaelic (ARCOSG)."
948	    ),
949	    "sr_set": (
950	        "The Serbian UD treebank is based on the "
951	        "[SETimes-SR](http://hdl.handle.net/11356/1200) corpus and additional "
952	        "news documents from the Serbian web."
953	    ),
954	    "sms_giellagas": (
955	        "The UD Skolt Sami Giellagas treebank is based almost entirely on "
956	        "spoken Skolt Sami corpora."
957	    ),
958	    "sk_snk": (
959	        "The Slovak UD treebank is based on data originally annotated as part "
960	        "of the Slovak National Corpus, following the annotation style of the "
961	        "Prague Dependency Treebank."
962	    ),
963	    "sl_ssj": (
964	        "The Slovenian UD Treebank is a rule-based conversion of the ssj500k "
965	        "treebank, the largest collection of manually syntactically annotated "
966	        "data in Slovenian, originally annotated in the JOS annotation scheme."
967	    ),
968	    "sl_sst": (
969	        "The Spoken Slovenian UD Treebank (SST) is the first syntactically "
970	        "annotated corpus of spoken Slovenian, based on a sample of the "
971	        "reference GOS corpus, a collection of transcribed audio recordings of"
972	        " monologic, dialogic and multi-party spontaneous speech in different "
973	        "everyday situations."
974	    ),
975	    "soj_aha": (
976	        "The AHA Soi Treebank is a small treebank for contemporary Soi. Its "
977	        "corpus is collected and annotated manually. We have prepared this "
978	        "treebank based on interviews with Soi speakers."
979	    ),
980	    "ajp_madar": (
981	        "The South_Levantine_Arabic-MADAR treebank consists of 100 "
982	        "manually-annotated sentences taken from the "
983	        "[MADAR](https://camel.abudhabi.nyu.edu/madar/) (Multi-Arabic Dialect "
984	        "Applications and Resources) project. "
985	    ),
986	    "es_ancora": "Spanish data from the AnCora corpus.",
987	    "es_gsd": (
988	        "The Spanish UD is converted from the content head version of the "
989	        "universal dependency treebank v2.0 (legacy)."
990	    ),
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:144
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
143	    "be_hse": (
144	        "The Belarusian UD treebank is based on a sample of the news texts "
145	        "included in the Belarusian-Russian parallel subcorpus of the Russian "
146	        "National Corpus, online search available at: "
147	        "http://ruscorpora.ru/search-para-be.html."
148	    ),
149	    "bn_bru": (
150	        "The BRU Bengali treebank has been created at Begum Rokeya University,"
151	        " Rangpur, by the members of Semantics Lab."
152	    ),
153	    "bho_bhtb": (
154	        "The Bhojpuri UD Treebank (BHTB) v2.6 consists of 6,664 tokens(357 "
155	        "sentences). This Treebank is a part of the Universal Dependency "
156	        "treebank project. Initially, it was initiated by me (Atul) at "
157	        "Jawaharlal Nehru University, New Delhi during the doctoral research "
158	        "work. BHTB data contains syntactic annotation according to "
159	        "dependency-constituency schema, as well as morphological tags and "
160	        "lemmas. In this data, XPOS is annotated  according to Bureau of "
161	        "Indian Standards (BIS) Part Of Speech (POS) tagset."
162	    ),
163	    "br_keb": (
164	        "UD Breton-KEB is a treebank of Breton that has been manually "
165	        "annotated according to the Universal Dependencies guidelines. The "
166	        "tokenisation guidelines and morphological annotation comes from a "
167	        "finite-state morphological analyser of Breton released as part of the"
168	        " Apertium project."
169	    ),
170	    "bg_btb": (
171	        "UD_Bulgarian-BTB is based on the HPSG-based BulTreeBank, created at "
172	        "the Institute of Information and Communication Technologies, "
173	        "Bulgarian Academy of Sciences. The original consists of 215,000 "
174	        "tokens (over 15,000 sentences)."
175	    ),
176	    "bxr_bdt": (
177	        "The UD Buryat treebank was annotated manually natively in UD and "
178	        "contains grammar book sentences, along with news and some fiction."
179	    ),
180	    "yue_hk": (
181	        "A Cantonese treebank (in Traditional Chinese characters) of film "
182	        "subtitles and of legislative proceedings of Hong Kong, parallel with "
183	        "the Chinese-HK treebank."
184	    ),
185	    "ca_ancora": "Catalan data from the AnCora corpus.",
186	    "ceb_gja": (
187	        "UD_Cebuano_GJA is a collection of annotated Cebuano sample sentences "
188	        "randomly taken from three different sources: community-contributed "
189	        "samples from the website Tatoeba, a Cebuano grammar book by Bunye & "
190	        "Yap (1971) and Tanangkinsing's reference grammar on Cebuano (2011). "
191	        "This project is currently work in progress."
192	    ),
193	    "zh_cfl": (
194	        "The Chinese-CFL UD treebank is manually annotated by Keying Li with "
195	        "minor manual revisions by Herman Leung and John Lee at City "
196	        "University of Hong Kong, based on essays written by learners of "
197	        "Mandarin Chinese as a foreign language. The data is in Simplified "
198	        "Chinese."
199	    ),
200	    "zh_gsd": (
201	        "Traditional Chinese Universal Dependencies Treebank annotated and "
202	        "converted by Google."
203	    ),
204	    "zh_gsdsimp": (
205	        "Simplified Chinese Universal Dependencies dataset converted from the "
206	        "GSD (traditional) dataset with manual corrections."
207	    ),
208	    "zh_hk": (
209	        "A Traditional Chinese treebank of film subtitles and of legislative "
210	        "proceedings of Hong Kong, parallel with the Cantonese-HK treebank."
211	    ),
212	    "zh_pud": (
213	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
214	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
215	        "Raw Text to Universal Dependencies."
216	    ),
217	    "ckt_hse": (
218	        "This data is a manual annotation of the corpus from multimedia "
219	        "annotated corpus of the Chuklang project, a dialectal corpus of the "
220	        "Amguema variant of Chukchi."
221	    ),
222	    "lzh_kyoto": (
223	        "Classical Chinese Universal Dependencies Treebank annotated and "
224	        "converted by Institute for Research in Humanities, Kyoto University."
225	    ),
226	    "cop_scriptorium": (
227	        "UD Coptic contains manually annotated Sahidic Coptic texts, including"
228	        " Biblical texts, sermons, letters, and hagiography."
229	    ),
230	    "hr_set": (
231	        "The Croatian UD treebank is based on the extension of the SETimes-HR "
232	        "corpus, the hr500k corpus."
233	    ),
234	    "cs_cac": (
235	        "The UD_Czech-CAC treebank is based on the Czech Academic Corpus 2.0 "
236	        "(CAC; esk akademick korpus; AK), created at Charles University in"
237	        " Prague."
238	    ),
239	    "cs_cltt": (
240	        "The UD_Czech-CLTT treebank is based on the Czech Legal Text Treebank "
241	        "1.0, created at Charles University in Prague."
242	    ),
243	    "cs_fictree": (
244	        "FicTree is a treebank of Czech fiction, automatically converted into "
245	        "the UD format. The treebank was built at Charles University in "
246	        "Prague."
247	    ),
248	    "cs_pdt": (
249	        "The Czech-PDT UD treebank is based on the Prague Dependency Treebank "
250	        "3.0 (PDT), created at the Charles University in Prague."
251	    ),
252	    "cs_pud": (
253	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
254	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
255	        "Raw Text to Universal Dependencies."
256	    ),
257	    "da_ddt": (
258	        "The Danish UD treebank is a conversion of the Danish Dependency "
259	        "Treebank."
260	    ),
261	    "nl_alpino": (
262	        "This corpus consists of samples from various treebanks annotated at "
263	        "the University of Groningen using the Alpino annotation tools and "
264	        "guidelines."
265	    ),
266	    "nl_lassysmall": (
267	        "This corpus contains sentences from the Wikipedia section of the "
268	        "Lassy Small Treebank. Universal Dependency annotation was generated "
269	        "automatically from the original annotation in Lassy."
270	    ),
271	    "en_esl": (
272	        "UD English-ESL / Treebank of Learner English (TLE) contains manual "
273	        "POS tag and dependency annotations for 5,124 English as a Second "
274	        "Language (ESL) sentences drawn from the Cambridge Learner Corpus "
275	        "First Certificate in English (FCE) dataset."
276	    ),
277	    "en_ewt": (
278	        "A Gold Standard Universal Dependencies Corpus for English, built over"
279	        " the source material of the English Web Treebank LDC2012T13 "
280	        "(https://catalog.ldc.upenn.edu/LDC2012T13)."
281	    ),
282	    "en_gum": (
283	        "Universal Dependencies syntax annotations from the GUM corpus "
284	        "(https://corpling.uis.georgetown.edu/gum/)."
285	    ),
286	    "en_gumreddit": (
287	        "Universal Dependencies syntax annotations from the Reddit portion of "
288	        "the GUM corpus (https://corpling.uis.georgetown.edu/gum/) "
289	    ),
290	    "en_lines": (
291	        "UD English_LinES is the English half of the LinES Parallel Treebank "
292	        "with the original dependency annotation first automatically converted"
293	        " into Universal Dependencies and then partially reviewed. Its "
294	        "contents cover literature, an online manual and Europarl data."
295	    ),
296	    "en_atis": (
297	        "UD Atis Treebank is a manually annotated treebank consisting of the "
298	        "sentences in the Atis (Airline Travel Informations) dataset which "
299	        "includes the human speech transcriptions of people asking for flight "
300	        "information on the automated inquiry systems."
301	    ),
302	    "en_partut": (
303	        "UD_English-ParTUT is a conversion of a multilingual parallel treebank"
304	        " developed at the University of Turin, and consisting of a variety of"
305	        " text genres, including talks, legal texts and Wikipedia articles, "
306	        "among others."
307	    ),
308	    "en_pronouns": (
309	        "UD English-Pronouns is dataset created to make pronoun identification"
310	        " more accurate and with a more balanced distribution across genders. "
311	        "The dataset is initially targeting the Independent Genitive pronouns,"
312	        " 'hers', (independent) 'his', (singular) 'theirs', 'mine', and "
313	        "(singular) 'yours'."
314	    ),
315	    "en_pud": (
316	        "This is the English portion of the Parallel Universal Dependencies "
317	        "(PUD) treebanks created for the CoNLL 2017 shared task on "
318	        "Multilingual Parsing from Raw Text to Universal Dependencies "
319	        "(http://universaldependencies.org/conll17/)."
320	    ),
321	    "myv_jr": (
322	        "UD Erzya is the original annotation (CoNLL-U) for texts in the Erzya "
323	        "language, it originally consists of a sample from a number of fiction"
324	        " authors writing originals in Erzya."
325	    ),
326	    "et_edt": (
327	        "UD Estonian is a converted version of the Estonian Dependency "
328	        "Treebank (EDT), originally annotated in the Constraint Grammar (CG) "
329	        "annotation scheme, and consisting of genres of fiction, newspaper "
330	        "texts and scientific texts. The treebank contains 30,972 trees, "
331	        "437,769 tokens."
332	    ),
333	    "et_ewt": (
334	        "UD EWT treebank consists of different genres of new media. The "
335	        "treebank contains 4,493 trees, 56,399 tokens."
336	    ),
337	    "fo_farpahc": (
338	        "UD_Icelandic-FarPaHC is a conversion of the Faroese Parsed Historical"
339	        " Corpus (FarPaHC) to the Universal Dependencies scheme. The "
340	        "conversion was done using UDConverter."
341	    ),
342	    "fo_oft": "This is a treebank of Faroese based on the Faroese Wikipedia.",
343	    "fi_ftb": (
344	        "FinnTreeBank 1 consists of manually annotated grammatical examples "
345	        "from VISK. The UD version of FinnTreeBank 1 was converted from a "
346	        "native annotation model with a script and later manually revised."
347	    ),
348	    "fi_ood": (
349	        "Finnish-OOD is an external out-of-domain test set for Finnish-TDT "
350	        "annotated natively into UD scheme."
351	    ),
352	    "fi_pud": (
353	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
354	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
355	        "Raw Text to Universal Dependencies."
356	    ),
357	    "fi_tdt": (
358	        "UD_Finnish-TDT is based on the Turku Dependency Treebank (TDT), a "
359	        "broad-coverage dependency treebank of general Finnish covering "
360	        "numerous genres. The conversion to UD was followed by extensive "
361	        "manual checks and corrections, and the treebank closely adheres to "
362	        "the UD guidelines."
363	    ),
364	    "fr_fqb": (
365	        "The corpus **UD_French-FQB** is an automatic conversion of the French"
366	        " QuestionBank v1, a corpus entirely made of questions."
367	    ),
368	    "fr_ftb": (
369	        "The Universal Dependency version of the French Treebank (Abeill et "
370	        "al., 2003), hereafter UD_French-FTB, is a treebank of sentences from "
371	        "the newspaper Le Monde, initially manually annotated with "
372	        "morphological information and phrase-structure and then converted to "
373	        "the Universal Dependencies annotation scheme."
374	    ),
375	    "fr_gsd": (
376	        "The **UD_French-GSD** was converted in 2015 from the content head "
377	        "version of the universal dependency treebank v2.0 "
378	        "(https://github.com/ryanmcd/uni-dep-tb). It is updated since 2015 "
379	        "independently from the previous source."
380	    ),
381	    "fr_partut": (
382	        "UD_French-ParTUT is a conversion of a multilingual parallel treebank "
383	        "developed at the University of Turin, and consisting of a variety of "
384	        "text genres, including talks, legal texts and Wikipedia articles, "
385	        "among others."
386	    ),
387	    "fr_rhapsodie": "A Universal Dependencies corpus for spoken French.",
388	    "fr_parisstories": (
389	        "Paris Stories is a corpus of oral French collected and transcribed by"
390	        " Linguistics students from Sorbonne Nouvelle and corrected by "
391	        "students from the Plurital Master's Degree of Computational "
392	        "Linguistics ( Inalco, Paris Nanterre, Sorbonne Nouvelle) between 2017"
393	        " and 2021. It contains monologues and dialogues from speakers living "
394	        "in the Parisian region."
395	    ),
396	    "fr_pud": (
397	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
398	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
399	        "Raw Text to Universal Dependencies."
400	    ),
401	    "fr_sequoia": (
402	        "UD_French-Sequoia is an automatic conversion of the Sequoia Treebank "
403	        "corpus French Sequoia corpus."
404	    ),
405	    "gl_ctg": (
406	        "The Galician UD treebank is based on the automatic parsing of the "
407	        "Galician Technical Corpus (http://sli.uvigo.gal/CTG) created at the "
408	        "University of Vigo by the the TALG NLP research group."
409	    ),
410	    "gl_treegal": (
411	        "The Galician-TreeGal is a treebank for Galician developed at LyS "
412	        "Group (Universidade da Corua)."
413	    ),
414	    "de_gsd": (
415	        "The German UD is converted from the content head version of the "
416	        "universal dependency treebank v2.0 (legacy)."
417	    ),
418	    "de_hdt": (
419	        "UD German-HDT is a conversion of the Hamburg Dependency Treebank, "
420	        "created at the University of Hamburg through manual annotation in "
421	        "conjunction with a standard for morphologically and syntactically "
422	        "annotating sentences as well as a constraint-based parser."
423	    ),
424	    "de_lit": (
425	        "This treebank aims at gathering texts of the German literary history."
426	        " Currently, it hosts Fragments of the early Romanticism, i.e. "
427	        "aphorism-like texts mainly dealing with philosophical issues "
428	        "concerning art, beauty and related topics."
429	    ),
430	    "de_pud": (
431	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
432	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
433	        "Raw Text to Universal Dependencies."
434	    ),
435	    "got_proiel": (
436	        "The UD Gothic treebank is based on the Gothic data from the PROIEL "
437	        "treebank, and consists of Wulfila's Bible translation."
438	    ),
439	    "el_gdt": (
440	        "The Greek UD treebank (UD_Greek-GDT) is derived from the Greek "
441	        "Dependency Treebank (http://gdt.ilsp.gr), a resource developed and "
442	        "maintained by researchers at the Institute for Language and Speech "
443	        "Processing/Athena R.C. (http://www.ilsp.gr)."
444	    ),
445	    "gub_tudet": (
446	        "UD_Guajajara-TuDeT is a collection of annotated sentences in "
447	        "Guajajara. Sentences stem from multiple sources such as descriptions "
448	        "of the language, short stories, dictionaries and translations from "
449	        "the New Testament. Sentence annotation and documentation by Lorena "
450	        "Martn Rodrguez and Fabrcio Ferraz Gerardi."
451	    ),
452	    "gn_oldtudet": (
453	        "UD_Guarani-OldTuDeT is a collection of annotated texts in Old "
454	        "Guaran. All known sources in this language are being annotated: "
455	        "cathesisms, grammars (seventeenth and eighteenth century), sentences "
456	        "from dictionaries, and other texts. Sentence annotation and "
457	        "documentation by Fabrcio Ferraz Gerardi and Lorena Martn Rodrguez."
458	    ),
459	    "he_htb": "A Universal Dependencies Corpus for Hebrew.",
460	    "he_iahltwiki": (
461	        "Publicly available subset of the IAHLT UD Hebrew Treebank's Wikipedia"
462	        " section (https://www.iahlt.org/)"
463	    ),
464	    "qfn_fame": (
465	        "UD_Frisian_Dutch-Fame is a selection of 400 sentences from the FAME! "
466	        "speech corpus by Yilmaz et al. (2016a, 2016b). The treebank is "
467	        "manually annotated using the UD scheme."
468	    ),
469	    "qhe_hiencs": (
470	        "The Hindi-English Code-switching treebank is based on code-switching "
471	        "tweets of Hindi and English multilingual speakers (mostly Indian) on "
472	        "Twitter. The treebank is manually annotated using UD sceheme. The "
473	        "training and evaluations sets were seperately annotated by different "
474	        "annotators using UD v2 and v1 guidelines respectively. The evaluation"
475	        " sets are automatically converted from UD v1 to v2."
476	    ),
477	    "hi_hdtb": (
478	        "The Hindi UD treebank is based on the Hindi Dependency Treebank "
479	        "(HDTB), created at IIIT Hyderabad, India."
480	    ),
481	    "hi_pud": (
482	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
483	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
484	        "Raw Text to Universal Dependencies."
485	    ),
486	    "hu_szeged": (
487	        "The Hungarian UD treebank is derived from the Szeged Dependency "
488	        "Treebank (Vincze et al. 2010)."
489	    ),
490	    "is_modern": (
491	        "UD_Icelandic-Modern is a conversion of the modern additions to the "
492	        "Icelandic Parsed Historical Corpus (IcePaHC) to the Universal "
493	        "Dependencies scheme."
494	    ),
495	    "is_icepahc": (
496	        "UD_Icelandic-IcePaHC is a conversion of the Icelandic Parsed "
497	        "Historical Corpus (IcePaHC) to the Universal Dependencies scheme. The"
498	        " conversion was done using UDConverter."
499	    ),
500	    "is_pud": (
501	        "Icelandic-PUD is the Icelandic part of the Parallel Universal "
502	        "Dependencies (PUD) treebanks."
503	    ),
504	    "id_csui": (
505	        "UD Indonesian-CSUI is a conversion from an Indonesian constituency "
506	        "treebank in the Penn Treebank format named Kethu that was also a "
507	        "conversion from a constituency treebank built by Dinakaramani et al. "
508	        "(2015). We named this treebank Indonesian-CSUI, since all the three "
509	        "versions of the treebanks were built at Faculty of Computer Science, "
510	        "Universitas Indonesia."
511	    ),
512	    "id_gsd": (
513	        "The Indonesian UD is converted from the content head version of the "
514	        "universal dependency treebank v2.0 (legacy)."
515	    ),
516	    "id_pud": (
517	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
518	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
519	        "Raw Text to Universal Dependencies."
520	    ),
521	    "ga_idt": (
522	        "A Universal Dependencies 4910-sentence treebank for modern Irish."
523	    ),
524	    "ga_twittirish": (
525	        "A Universal Dependencies treebank of 866 tweets in modern Irish."
526	    ),
527	    "it_isdt": (
528	        "The Italian corpus annotated according to the UD annotation scheme "
529	        "was obtained by conversion from ISDT (Italian Stanford Dependency "
530	        "Treebank), released for the dependency parsing shared task of "
531	        "Evalita-2014 (Bosco et al. 2014)."
532	    ),
533	    "it_partut": (
534	        "UD_Italian-ParTUT is a conversion of a multilingual parallel treebank"
535	        " developed at the University of Turin, and consisting of a variety of"
536	        " text genres, including talks, legal texts and Wikipedia articles, "
537	        "among others."
538	    ),
539	    "it_postwita": (
540	        "PoSTWITA-UD is a collection of Italian tweets annotated in Universal "
541	        "Dependencies that can be exploited for the training of NLP systems to"
542	        " enhance their performance on social media texts."
543	    ),
544	    "it_markit": (
545	        "It is MarkIT That is New: An Italian Treebank of Marked "
546	        "Constructions. Teresa Paccosi, Alessio Palmero Aprosio and Sara "
547	        "Tonelli, To appear in Proceedings of the Eighth Italian Conference on"
548	        " Computational Linguistics 2022 (CLIC-it 2021)"
549	    ),
550	    "it_valico": (
551	        "Manually corrected Treebank of Learner Italian drawn from the Valico "
552	        "corpus and correspondent corrected sentences."
553	    ),
554	    "it_pud": (
555	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
556	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
557	        "Raw Text to Universal Dependencies."
558	    ),
559	    "it_twittiro": (
560	        "TWITTIR-UD is a collection of ironic Italian tweets annotated in "
561	        "Universal Dependencies. The treebank can be exploited for the "
562	        "training of NLP systems to enhance their performance on social media "
563	        "texts, and in particular, for irony detection purposes."
564	    ),
565	    "it_vit": (
566	        "The UD_Italian-VIT corpus was obtained by conversion from VIT (Venice"
567	        " Italian Treebank), developed at the Laboratory of Computational "
568	        "Linguistics of the Universit Ca' Foscari in Venice (Delmonte et al. "
569	        "2007; Delmonte 2009; "
570	        "http://rondelmo.it/resource/VIT/Browser-VIT/index.htm)."
571	    ),
572	    "ja_pudluw": (
573	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
574	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
575	        "Raw Text to Universal Dependencies."
576	    ),
577	    "ja_bccwjluw": (
578	        "This Universal Dependencies (UD) Japanese treebank is based on the "
579	        "definition of UD Japanese convention described in the UD "
580	        "documentation. The original sentences are from `Balanced Corpus of "
581	        "Contemporary Written Japanese'(BCCWJ)."
582	    ),
583	    "ja_gsdluw": (
584	        "This Universal Dependencies (UD) Japanese treebank is based on the "
585	        "definition of UD Japanese convention described in the UD "
586	        "documentation. The original sentences are from Google UDT 2.0."
587	    ),
588	    "ja_bccwj": (
589	        "This Universal Dependencies (UD) Japanese treebank is based on the "
590	        "definition of UD Japanese convention described in the UD "
591	        "documentation. The original sentences are from `Balanced Corpus of "
592	        "Contemporary Written Japanese'(BCCWJ)."
593	    ),
594	    "ja_gsd": (
595	        "This Universal Dependencies (UD) Japanese treebank is based on the "
596	        "definition of UD Japanese convention described in the UD "
597	        "documentation.  The original sentences are from Google UDT 2.0."
598	    ),
599	    "ja_modern": (
600	        "This Universal Dependencies (UD) Japanese treebank is based on the "
601	        "definition of UD Japanese convention described in the UD "
602	        "documentation. The original sentences are from `Corpus of Historical "
603	        "Japanese' (CHJ)."
604	    ),
605	    "ja_pud": (
606	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
607	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
608	        " Raw Text to Universal "
609	        "Dependencies](http://universaldependencies.org/conll17/)."
610	    ),
611	    "jv_csui": (
612	        "UD Javanese-CSUI is a dependency treebank in Javanese, a regional "
613	        "language in Indonesia with more than 60 million users. The original "
614	        "sentences were taken from OPUS, especially from the WikiMatrix v1 "
615	        "corpus. We revised the sentences that contained more Indonesian words"
616	        " than Javanese words and manually annotated them."
617	    ),
618	    "urb_tudet": (
619	        "UD_Kaapor-TuDeT is a collection of annotated sentences in Ka'apor. "
620	        "The project is a work in progress and the treebank is being updated "
621	        "on a regular basis."
622	    ),
623	    "xnr_kdtb": (
624	        "The Kangri UD Treebank (KDTB) is a part of the Universal Dependency "
625	        "treebank project."
626	    ),
627	    "krl_kkpp": (
628	        "UD Karelian-KKPP is a manually annotated new corpus of Karelian made "
629	        "in Universal dependencies annotation scheme. The data is collected "
630	        "from VepKar corpora and consists of mostly modern news texts but also"
631	        " some stories and educational texts."
632	    ),
633	    "kk_ktb": (
634	        "The UD Kazakh treebank is a combination of text from various sources "
635	        "including Wikipedia, some folk tales, sentences from the UDHR, news "
636	        "and phrasebook sentences. Sentences IDs include partial document "
637	        "identifiers."
638	    ),
639	    "arr_tudet": (
640	        "UD_Karo-TuDeT is a collection of annotated sentences in Karo. The "
641	        "sentences stem from the only grammatical description of the language "
642	        "(Gabas, 1999) and from the sentences in the dictionary by the same "
643	        "author (Gabas, 2007). Sentence annotation and documentation by "
644	        "Fabrcio Ferraz Gerardi."
645	    ),
646	    "kfm_aha": (
647	        "The AHA Khunsari Treebank is a small treebank for contemporary "
648	        "Khunsari. Its corpus is collected and annotated manually. We have "
649	        "prepared this treebank based on interviews with Khunsari speakers."
650	    ),
651	    "quc_iu": (
652	        "UD Kiche-IU is a treebank consisting of sentences from a variety of"
653	        " text domains but principally dictionary example sentences and "
654	        "linguistic examples."
655	    ),
656	    "koi_uh": (
657	        "This is a Komi-Permyak literary language treebank consisting of "
658	        "original and translated texts."
659	    ),
660	    "kpv_ikdp": (
661	        "This treebank consists of dialectal transcriptions of spoken "
662	        "Komi-Zyrian. The current texts are short recorded segments from "
663	        "different areas where the Iva dialect of Komi language is spoken."
664	    ),
665	    "kpv_lattice": (
666	        "UD Komi-Zyrian Lattice is a treebank of written standard Komi-Zyrian."
667	    ),
668	    "ko_gsd": (
669	        "The Google Korean Universal Dependency Treebank is first converted "
670	        "from the Universal Dependency Treebank v2.0 (legacy), and then "
671	        "enhanced by Chun et al., 2018."
672	    ),
673	    "ko_kaist": (
674	        "The KAIST Korean Universal Dependency Treebank is generated by Chun "
675	        "et al., 2018 from the constituency trees in the KAIST Tree-Tagging "
676	        "Corpus."
677	    ),
678	    "ko_pud": (
679	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
680	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
681	        "Raw Text to Universal Dependencies."
682	    ),
683	    "kmr_mg": (
684	        "The UD Kurmanji corpus is a corpus of Kurmanji Kurdish. It contains "
685	        "fiction and encyclopaedic texts in roughly equal measure. It has been"
686	        " annotated natively in accordance with the UD annotation scheme."
687	    ),
688	    "la_ittb": (
689	        "Latin data from the _Index Thomisticus_ Treebank. Data are taken from"
690	        " the _Index Thomisticus_ corpus by Roberto Busa SJ, which contains "
691	        "the complete work by Thomas Aquinas (12251274; Medieval Latin) and "
692	        "by 61 other authors related to Thomas."
693	    ),
694	    "la_udante": (
695	        "The UDante treebank is based on the Latin texts of Dante Alighieri, "
696	        "taken from the DanteSearch corpus, originally created at the "
697	        "University of Pisa, Italy. It is a treebank of Latin language, more "
698	        "precisely of literary Medieval Latin (XIVth century)."
699	    ),
700	    "la_llct": (
701	        "This Universal Dependencies version of the LLCT (Late Latin Charter "
702	        "Treebank) consists of an automated conversion of the LLCT2 treebank "
703	        "from the Latin Dependency Treebank (LDT) format into the Universal "
704	        "Dependencies standard."
705	    ),
706	    "la_perseus": (
707	        "This Universal Dependencies Latin Treebank consists of an automatic "
708	        "conversion of a selection of passages from the Ancient Greek and "
709	        "Latin Dependency Treebank 2.1"
710	    ),
711	    "la_proiel": (
712	        "The Latin PROIEL treebank is based on the Latin data from the PROIEL "
713	        "treebank, and contains most of the Vulgate New Testament translations"
714	        " plus selections from Caesar's Gallic War, Cicero's Letters to "
715	        "Atticus, Palladius' Opus Agriculturae and the first book of Cicero's "
716	        "De officiis."
717	    ),
718	    "lv_lvtb": (
719	        "Latvian UD Treebank is based on Latvian Treebank (LVTB), being "
720	        "created at University of Latvia, Institute of Mathematics and "
721	        "Computer Science, Artificial Intelligence Laboratory."
722	    ),
723	    "lij_glt": (
724	        "The Genoese Ligurian Treebank is a small, manually annotated "
725	        "collection of contemporary Ligurian prose. The focus of the treebank "
726	        "is written Genoese, the koin variety of Ligurian which is associated"
727	        " with today's literary, journalistic and academic ligurophone sphere."
728	    ),
729	    "lt_alksnis": (
730	        "The Lithuanian dependency treebank ALKSNIS v3.0 (Vytautas Magnus "
731	        "University)."
732	    ),
733	    "lt_hse": (
734	        "Lithuanian treebank annotated manually (dependencies) using the "
735	        "Morphological Annotator by CCL, Vytautas Magnus University "
736	        "(http://tekstynas.vdu.lt/) and manual disambiguation. A pilot version"
737	        " which includes news and an essay by Tomas Venclova is available "
738	        "here."
739	    ),
740	    "olo_kkpp": (
741	        "UD Livvi-KKPP is a manually annotated new corpus of Livvi-Karelian "
742	        "made directly in the Universal dependencies annotation scheme. The "
743	        "data is collected from VepKar corpora and consists of mostly modern "
744	        "news texts but also some stories and educational texts."
745	    ),
746	    "nds_lsdc": (
747	        "The UD Low Saxon LSDC dataset consists of sentences in 18 Low Saxon "
748	        "dialects from both Germany and the Netherlands. These sentences are "
749	        "(or are to become) part of the LSDC dataset and represent the "
750	        "language from the 19th and early 20th century in genres such as short"
751	        " stories, novels, speeches, letters and fairytales."
752	    ),
753	    "mt_mudt": (
754	        "MUDT (Maltese Universal Dependencies Treebank) is a manually "
755	        "annotated treebank of Maltese, a Semitic language of Malta descended "
756	        "from North African Arabic with a significant amount of Italo-Romance "
757	        "influence. MUDT was designed as a balanced corpus with four major "
758	        "genres (see Splitting below) represented roughly equally."
759	    ),
760	    "gv_cadhan": (
761	        "This is the Cadhan Aonair UD treebank for Manx Gaelic, created by "
762	        "Kevin Scannell."
763	    ),
764	    "mr_ufal": (
765	        "UD Marathi is a manually annotated treebank consisting primarily of "
766	        "stories from Wikisource, and parts of an article on Wikipedia."
767	    ),
768	    "gun_dooley": (
769	        "UD Mbya_Guarani-Dooley is a corpus of narratives written in Mby "
770	        "Guaran (Tupian) in Brazil, and collected by Robert Dooley. Due to "
771	        "copyright restrictions, the corpus that is distributed as part of UD "
772	        "only contains the annotation (tags, features, relations) while the "
773	        "FORM and LEMMA columns are empty."
774	    ),
775	    "gun_thomas": (
776	        "UD Mbya_Guarani-Thomas is a corpus of Mby Guaran (Tupian) texts "
777	        "collected by Guillaume Thomas. The current version of the corpus "
778	        "consists of three speeches by Paulina Kerechu Nez Romero, a Mby "
779	        "Guaran speaker from Ytu, Caazap Department, Paraguay."
780	    ),
781	    "mdf_jr": (
782	        "Erme Universal Dependencies annotated texts Moksha are the origin of "
783	        "UD_Moksha-JR with annotation (CoNLL-U) for texts in the Moksha "
784	        "language, it originally consists of a sample from a number of fiction"
785	        " authors writing originals in Moksha."
786	    ),
787	    "myu_tudet": (
788	        "UD_Munduruku-TuDeT is a collection of annotated sentences in "
789	        "Munduruk. Together with UD_Akuntsu-TuDeT and UD_Tupinamba-TuDeT, "
790	        "UD_Munduruku-TuDeT is part of the TuLaR project."
791	    ),
792	    "pcm_nsc": (
793	        "A Universal Dependencies corpus for spoken Naija (Nigerian Pidgin)."
794	    ),
795	    "nyq_aha": (
796	        "The AHA Nayini Treebank is a small treebank for contemporary Nayini. "
797	        "Its corpus is collected and annotated manually. We have prepared this"
798	        " treebank based on interviews with Nayini speakers."
799	    ),
800	    "sme_giella": (
801	        "This is a North Smi treebank based on a manually disambiguated and "
802	        "function-labelled gold-standard corpus of North Smi produced by the "
803	        "Giellatekno team at UiT Norgga rktala universitehta."
804	    ),
805	    "no_bokmaal": (
806	        "The Norwegian UD treebank is based on the Bokml section of the "
807	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
808	        " Norwegian. NDT has been automatically converted to the UD scheme by "
809	        "Lilja vrelid at the University of Oslo."
810	    ),
811	    "no_nynorsk": (
812	        "The Norwegian UD treebank is based on the Nynorsk section of the "
813	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
814	        " Norwegian.  NDT has been automatically converted to the UD scheme by"
815	        " Lilja vrelid at the University of Oslo."
816	    ),
817	    "no_nynorsklia": (
818	        "This Norwegian treebank is based on the LIA treebank of transcribed "
819	        "spoken Norwegian dialects. The treebank has been automatically "
820	        "converted to the UD scheme by Lilja vrelid at the University of "
821	        "Oslo."
822	    ),
823	    "cu_proiel": (
824	        "The Old Church Slavonic (OCS) UD treebank is based on the Old Church "
825	        "Slavonic data from the PROIEL treebank and contains the text of the "
826	        "Codex Marianus New Testament translation."
827	    ),
828	    "fro_srcmf": (
829	        "UD_Old_French-SRCMF is a conversion of (part of) the SRCMF corpus "
830	        "(Syntactic Reference Corpus of Medieval French srcmf.org)."
831	    ),
832	    "orv_birchbark": (
833	        "UD Old_East_Slavic-Birchbark is based on the RNC Corpus of Birchbark "
834	        "Letters and includes documents written in 1025-1500 in an East Slavic"
835	        " vernacular (letters, household and business records, records for "
836	        "church services, spell against diseases, and other short "
837	        "inscriptions). The treebank is manually syntactically annotated in "
838	        "the UD 2.0 scheme, morphological and lexical annotation is a "
839	        "conversion of the original RNC annotation."
840	    ),
841	    "orv_rnc": (
842	        "`UD_Old_Russian-RNC` is a sample of the Middle Russian corpus "
843	        "(1300-1700), a part of the Russian National Corpus. The data were "
844	        "originally annotated according to the RNC and extended UD-Russian "
845	        "morphological schemas and UD 2.4 dependency schema."
846	    ),
847	    "orv_torot": (
848	        "UD_Old_Russian-TOROT is a conversion of a selection of the Old East "
849	        "Slavonic and Middle Russian data in the Troms Old Russian and OCS "
850	        "Treebank (TOROT), which was originally annotated in PROIEL dependency"
851	        " format."
852	    ),
853	    "otk_tonqq": (
854	        "`UD_Old_Turkish-Tonqq` is an Old Turkish treebank built upon Turkic "
855	        "script texts or sentences that are trivially convertible."
856	    ),
857	    "fa_perdt": (
858	        "The Persian Universal Dependency Treebank (PerUDT) is the result of "
859	        "automatic coversion of Persian Dependency Treebank (PerDT) with "
860	        "extensive manual corrections. Please refer to the follwoing work, if "
861	        "you use this data: Mohammad Sadegh Rasooli, Pegah Safari, Amirsaeid "
862	        "Moloodi, and Alireza Nourian. 'The Persian Dependency Treebank Made "
863	        "Universal'. 2020 (to appear)."
864	    ),
865	    "fa_seraji": (
866	        "The Persian Universal Dependency Treebank (Persian UD) is based on "
867	        "Uppsala Persian Dependency Treebank (UPDT). The conversion of the "
868	        "UPDT to the Universal Dependencies was performed semi-automatically "
869	        "with extensive manual checks and corrections."
870	    ),
871	    "pl_lfg": (
872	        "The LFG Enhanced UD treebank of Polish is based on a corpus of LFG "
873	        "(Lexical Functional Grammar) syntactic structures generated by an LFG"
874	        " grammar of Polish, POLFIE, and manually disambiguated by human "
875	        "annotators."
876	    ),
877	    "pl_pdb": (
878	        "The Polish PDB-UD treebank is based on the Polish Dependency Bank 2.0"
879	        " (PDB 2.0), created at the Institute of Computer Science, Polish "
880	        "Academy of Sciences in Warsaw. The PDB-UD treebank is an extended and"
881	        " corrected version of the Polish SZ-UD treebank (the release 1.2 to "
882	        "2.3)."
883	    ),
884	    "pl_pud": (
885	        "This is the Polish portion of the Parallel Universal Dependencies "
886	        "(PUD) treebanks, created at the Institute of Computer Science, Polish"
887	        " Academy of Sciences in Warsaw.Re"
888	    ),
889	    "pt_bosque": (
890	        "This Universal Dependencies (UD) Portuguese treebank is based on the "
891	        "Constraint Grammar converted version of the Bosque, which is part of "
892	        "the Floresta Sint(c)tica treebank. It contains both European "
893	        "(CETEMPblico) and Brazilian (CETENFolha) variants."
894	    ),
895	    "pt_gsd": (
896	        "The Brazilian Portuguese UD is converted from the Google Universal "
897	        "Dependency Treebank v2.0 (legacy)."
898	    ),
899	    "pt_pud": (
900	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
901	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
902	        "Raw Text to Universal Dependencies."
903	    ),
904	    "ro_art": (
905	        "The UD treebank ArT is a treebank of the Aromanian dialect of the "
906	        "Romanian language in UD format."
907	    ),
908	    "ro_nonstandard": (
909	        "The Romanian Non-standard UD treebank (called UAIC-RoDia) is based on"
910	        " UAIC-RoDia Treebank. UAIC-RoDia = ISLRN 156-635-615-024-0"
911	    ),
912	    "ro_rrt": (
913	        "The Romanian UD treebank (called RoRefTrees) (Barbu Mititelu et al., "
914	        "2016) is the reference treebank in UD format for standard Romanian."
915	    ),
916	    "ro_simonero": "SiMoNERo is a medical corpus of contemporary Romanian.",
917	    "ru_gsd": (
918	        "Russian Universal Dependencies Treebank annotated and converted by "
919	        "Google."
920	    ),
921	    "ru_pud": (
922	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
923	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
924	        "Raw Text to Universal Dependencies."
925	    ),
926	    "ru_syntagrus": "Russian data from the SynTagRus corpus.",
927	    "ru_taiga": (
928	        "Universal Dependencies treebank is based on data samples extracted "
929	        "from Taiga Corpus and MorphoRuEval-2017 and GramEval-2020 shared "
930	        "tasks collections."
931	    ),
932	    "sa_ufal": (
933	        "A small Sanskrit treebank of sentences from Pacatantra, an ancient "
934	        "Indian collection of interrelated fables by Vishnu Sharma."
935	    ),
936	    "sa_vedic": (
937	        "The Treebank of Vedic Sanskrit contains 4,000 sentences with 27,000 "
938	        "words chosen from metrical and prose passages of the gveda (RV), the"
939	        " aunaka recension of the Atharvaveda (S), the Maitryasahit "
940	        "(MS), and the Aitareya- (AB) and atapatha-Brhmaas (B). Lexical "
941	        "and morpho-syntactic information has been generated using a tagging "
942	        "software and manually validated. POS tags have been induced "
943	        "automatically from the morpho-sytactic information of each word."
944	    ),
945	    "gd_arcosg": (
946	        "A treebank of Scottish Gaelic based on the Annotated Reference Corpus"
947	        " Of Scottish Gaelic (ARCOSG)."
948	    ),
949	    "sr_set": (
950	        "The Serbian UD treebank is based on the "
951	        "[SETimes-SR](http://hdl.handle.net/11356/1200) corpus and additional "
952	        "news documents from the Serbian web."
953	    ),
954	    "sms_giellagas": (
955	        "The UD Skolt Sami Giellagas treebank is based almost entirely on "
956	        "spoken Skolt Sami corpora."
957	    ),
958	    "sk_snk": (
959	        "The Slovak UD treebank is based on data originally annotated as part "
960	        "of the Slovak National Corpus, following the annotation style of the "
961	        "Prague Dependency Treebank."
962	    ),
963	    "sl_ssj": (
964	        "The Slovenian UD Treebank is a rule-based conversion of the ssj500k "
965	        "treebank, the largest collection of manually syntactically annotated "
966	        "data in Slovenian, originally annotated in the JOS annotation scheme."
967	    ),
968	    "sl_sst": (
969	        "The Spoken Slovenian UD Treebank (SST) is the first syntactically "
970	        "annotated corpus of spoken Slovenian, based on a sample of the "
971	        "reference GOS corpus, a collection of transcribed audio recordings of"
972	        " monologic, dialogic and multi-party spontaneous speech in different "
973	        "everyday situations."
974	    ),
975	    "soj_aha": (
976	        "The AHA Soi Treebank is a small treebank for contemporary Soi. Its "
977	        "corpus is collected and annotated manually. We have prepared this "
978	        "treebank based on interviews with Soi speakers."
979	    ),
980	    "ajp_madar": (
981	        "The South_Levantine_Arabic-MADAR treebank consists of 100 "
982	        "manually-annotated sentences taken from the "
983	        "[MADAR](https://camel.abudhabi.nyu.edu/madar/) (Multi-Arabic Dialect "
984	        "Applications and Resources) project. "
985	    ),
986	    "es_ancora": "Spanish data from the AnCora corpus.",
987	    "es_gsd": (
988	        "The Spanish UD is converted from the content head version of the "
989	        "universal dependency treebank v2.0 (legacy)."
990	    ),
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {
1217	        "test": "UD_Arabic-PUD/r2.10/ar_pud-ud-test.conllu",
1218	    },
1219	    "hy_armtdp": {
1220	        "train": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-train.conllu",
1221	        "dev": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-dev.conllu",
1222	        "test": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-test.conllu",
1223	    },
1224	    "aii_as": {
1225	        "test": "UD_Assyrian-AS/r2.10/aii_as-ud-test.conllu",
1226	    },
1227	    "bm_crb": {
1228	        "test": "UD_Bambara-CRB/r2.10/bm_crb-ud-test.conllu",
1229	    },
1230	    "eu_bdt": {
1231	        "train": "UD_Basque-BDT/r2.10/eu_bdt-ud-train.conllu",
1232	        "dev": "UD_Basque-BDT/r2.10/eu_bdt-ud-dev.conllu",
1233	        "test": "UD_Basque-BDT/r2.10/eu_bdt-ud-test.conllu",
1234	    },
1235	    "bej_nsc": {
1236	        "test": "UD_Beja-NSC/r2.10/bej_nsc-ud-test.conllu",
1237	    },
1238	    "be_hse": {
1239	        "train": "UD_Belarusian-HSE/r2.10/be_hse-ud-train.conllu",
1240	        "dev": "UD_Belarusian-HSE/r2.10/be_hse-ud-dev.conllu",
1241	        "test": "UD_Belarusian-HSE/r2.10/be_hse-ud-test.conllu",
1242	    },
1243	    "bn_bru": {
1244	        "test": "UD_Bengali-BRU/r2.10/bn_bru-ud-test.conllu",
1245	    },
1246	    "bho_bhtb": {
1247	        "test": "UD_Bhojpuri-BHTB/r2.10/bho_bhtb-ud-test.conllu",
1248	    },
1249	    "br_keb": {
1250	        "test": "UD_Breton-KEB/r2.10/br_keb-ud-test.conllu",
1251	    },
1252	    "bg_btb": {
1253	        "train": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-train.conllu",
1254	        "dev": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-dev.conllu",
1255	        "test": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-test.conllu",
1256	    },
1257	    "bxr_bdt": {
1258	        "train": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-train.conllu",
1259	        "test": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-test.conllu",
1260	    },
1261	    "yue_hk": {
1262	        "test": "UD_Cantonese-HK/r2.10/yue_hk-ud-test.conllu",
1263	    },
1264	    "ca_ancora": {
1265	        "train": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-train.conllu",
1266	        "dev": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-dev.conllu",
1267	        "test": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-test.conllu",
1268	    },
1269	    "ceb_gja": {
1270	        "test": "UD_Cebuano-GJA/r2.10/ceb_gja-ud-test.conllu",
1271	    },
1272	    "zh_cfl": {
1273	        "test": "UD_Chinese-CFL/r2.10/zh_cfl-ud-test.conllu",
1274	    },
1275	    "zh_gsd": {
1276	        "train": "UD_Chinese-GSD/r2.10/zh_gsd-ud-train.conllu",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:278
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
277	    "en_ewt": (
278	        "A Gold Standard Universal Dependencies Corpus for English, built over"
279	        " the source material of the English Web Treebank LDC2012T13 "
280	        "(https://catalog.ldc.upenn.edu/LDC2012T13)."
281	    ),
282	    "en_gum": (
283	        "Universal Dependencies syntax annotations from the GUM corpus "
284	        "(https://corpling.uis.georgetown.edu/gum/)."
285	    ),
286	    "en_gumreddit": (
287	        "Universal Dependencies syntax annotations from the Reddit portion of "
288	        "the GUM corpus (https://corpling.uis.georgetown.edu/gum/) "
289	    ),
290	    "en_lines": (
291	        "UD English_LinES is the English half of the LinES Parallel Treebank "
292	        "with the original dependency annotation first automatically converted"
293	        " into Universal Dependencies and then partially reviewed. Its "
294	        "contents cover literature, an online manual and Europarl data."
295	    ),
296	    "en_atis": (
297	        "UD Atis Treebank is a manually annotated treebank consisting of the "
298	        "sentences in the Atis (Airline Travel Informations) dataset which "
299	        "includes the human speech transcriptions of people asking for flight "
300	        "information on the automated inquiry systems."
301	    ),
302	    "en_partut": (
303	        "UD_English-ParTUT is a conversion of a multilingual parallel treebank"
304	        " developed at the University of Turin, and consisting of a variety of"
305	        " text genres, including talks, legal texts and Wikipedia articles, "
306	        "among others."
307	    ),
308	    "en_pronouns": (
309	        "UD English-Pronouns is dataset created to make pronoun identification"
310	        " more accurate and with a more balanced distribution across genders. "
311	        "The dataset is initially targeting the Independent Genitive pronouns,"
312	        " 'hers', (independent) 'his', (singular) 'theirs', 'mine', and "
313	        "(singular) 'yours'."
314	    ),
315	    "en_pud": (
316	        "This is the English portion of the Parallel Universal Dependencies "
317	        "(PUD) treebanks created for the CoNLL 2017 shared task on "
318	        "Multilingual Parsing from Raw Text to Universal Dependencies "
319	        "(http://universaldependencies.org/conll17/)."
320	    ),
321	    "myv_jr": (
322	        "UD Erzya is the original annotation (CoNLL-U) for texts in the Erzya "
323	        "language, it originally consists of a sample from a number of fiction"
324	        " authors writing originals in Erzya."
325	    ),
326	    "et_edt": (
327	        "UD Estonian is a converted version of the Estonian Dependency "
328	        "Treebank (EDT), originally annotated in the Constraint Grammar (CG) "
329	        "annotation scheme, and consisting of genres of fiction, newspaper "
330	        "texts and scientific texts. The treebank contains 30,972 trees, "
331	        "437,769 tokens."
332	    ),
333	    "et_ewt": (
334	        "UD EWT treebank consists of different genres of new media. The "
335	        "treebank contains 4,493 trees, 56,399 tokens."
336	    ),
337	    "fo_farpahc": (
338	        "UD_Icelandic-FarPaHC is a conversion of the Faroese Parsed Historical"
339	        " Corpus (FarPaHC) to the Universal Dependencies scheme. The "
340	        "conversion was done using UDConverter."
341	    ),
342	    "fo_oft": "This is a treebank of Faroese based on the Faroese Wikipedia.",
343	    "fi_ftb": (
344	        "FinnTreeBank 1 consists of manually annotated grammatical examples "
345	        "from VISK. The UD version of FinnTreeBank 1 was converted from a "
346	        "native annotation model with a script and later manually revised."
347	    ),
348	    "fi_ood": (
349	        "Finnish-OOD is an external out-of-domain test set for Finnish-TDT "
350	        "annotated natively into UD scheme."
351	    ),
352	    "fi_pud": (
353	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
354	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
355	        "Raw Text to Universal Dependencies."
356	    ),
357	    "fi_tdt": (
358	        "UD_Finnish-TDT is based on the Turku Dependency Treebank (TDT), a "
359	        "broad-coverage dependency treebank of general Finnish covering "
360	        "numerous genres. The conversion to UD was followed by extensive "
361	        "manual checks and corrections, and the treebank closely adheres to "
362	        "the UD guidelines."
363	    ),
364	    "fr_fqb": (
365	        "The corpus **UD_French-FQB** is an automatic conversion of the French"
366	        " QuestionBank v1, a corpus entirely made of questions."
367	    ),
368	    "fr_ftb": (
369	        "The Universal Dependency version of the French Treebank (Abeill et "
370	        "al., 2003), hereafter UD_French-FTB, is a treebank of sentences from "
371	        "the newspaper Le Monde, initially manually annotated with "
372	        "morphological information and phrase-structure and then converted to "
373	        "the Universal Dependencies annotation scheme."
374	    ),
375	    "fr_gsd": (
376	        "The **UD_French-GSD** was converted in 2015 from the content head "
377	        "version of the universal dependency treebank v2.0 "
378	        "(https://github.com/ryanmcd/uni-dep-tb). It is updated since 2015 "
379	        "independently from the previous source."
380	    ),
381	    "fr_partut": (
382	        "UD_French-ParTUT is a conversion of a multilingual parallel treebank "
383	        "developed at the University of Turin, and consisting of a variety of "
384	        "text genres, including talks, legal texts and Wikipedia articles, "
385	        "among others."
386	    ),
387	    "fr_rhapsodie": "A Universal Dependencies corpus for spoken French.",
388	    "fr_parisstories": (
389	        "Paris Stories is a corpus of oral French collected and transcribed by"
390	        " Linguistics students from Sorbonne Nouvelle and corrected by "
391	        "students from the Plurital Master's Degree of Computational "
392	        "Linguistics ( Inalco, Paris Nanterre, Sorbonne Nouvelle) between 2017"
393	        " and 2021. It contains monologues and dialogues from speakers living "
394	        "in the Parisian region."
395	    ),
396	    "fr_pud": (
397	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
398	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
399	        "Raw Text to Universal Dependencies."
400	    ),
401	    "fr_sequoia": (
402	        "UD_French-Sequoia is an automatic conversion of the Sequoia Treebank "
403	        "corpus French Sequoia corpus."
404	    ),
405	    "gl_ctg": (
406	        "The Galician UD treebank is based on the automatic parsing of the "
407	        "Galician Technical Corpus (http://sli.uvigo.gal/CTG) created at the "
408	        "University of Vigo by the the TALG NLP research group."
409	    ),
410	    "gl_treegal": (
411	        "The Galician-TreeGal is a treebank for Galician developed at LyS "
412	        "Group (Universidade da Corua)."
413	    ),
414	    "de_gsd": (
415	        "The German UD is converted from the content head version of the "
416	        "universal dependency treebank v2.0 (legacy)."
417	    ),
418	    "de_hdt": (
419	        "UD German-HDT is a conversion of the Hamburg Dependency Treebank, "
420	        "created at the University of Hamburg through manual annotation in "
421	        "conjunction with a standard for morphologically and syntactically "
422	        "annotating sentences as well as a constraint-based parser."
423	    ),
424	    "de_lit": (
425	        "This treebank aims at gathering texts of the German literary history."
426	        " Currently, it hosts Fragments of the early Romanticism, i.e. "
427	        "aphorism-like texts mainly dealing with philosophical issues "
428	        "concerning art, beauty and related topics."
429	    ),
430	    "de_pud": (
431	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
432	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
433	        "Raw Text to Universal Dependencies."
434	    ),
435	    "got_proiel": (
436	        "The UD Gothic treebank is based on the Gothic data from the PROIEL "
437	        "treebank, and consists of Wulfila's Bible translation."
438	    ),
439	    "el_gdt": (
440	        "The Greek UD treebank (UD_Greek-GDT) is derived from the Greek "
441	        "Dependency Treebank (http://gdt.ilsp.gr), a resource developed and "
442	        "maintained by researchers at the Institute for Language and Speech "
443	        "Processing/Athena R.C. (http://www.ilsp.gr)."
444	    ),
445	    "gub_tudet": (
446	        "UD_Guajajara-TuDeT is a collection of annotated sentences in "
447	        "Guajajara. Sentences stem from multiple sources such as descriptions "
448	        "of the language, short stories, dictionaries and translations from "
449	        "the New Testament. Sentence annotation and documentation by Lorena "
450	        "Martn Rodrguez and Fabrcio Ferraz Gerardi."
451	    ),
452	    "gn_oldtudet": (
453	        "UD_Guarani-OldTuDeT is a collection of annotated texts in Old "
454	        "Guaran. All known sources in this language are being annotated: "
455	        "cathesisms, grammars (seventeenth and eighteenth century), sentences "
456	        "from dictionaries, and other texts. Sentence annotation and "
457	        "documentation by Fabrcio Ferraz Gerardi and Lorena Martn Rodrguez."
458	    ),
459	    "he_htb": "A Universal Dependencies Corpus for Hebrew.",
460	    "he_iahltwiki": (
461	        "Publicly available subset of the IAHLT UD Hebrew Treebank's Wikipedia"
462	        " section (https://www.iahlt.org/)"
463	    ),
464	    "qfn_fame": (
465	        "UD_Frisian_Dutch-Fame is a selection of 400 sentences from the FAME! "
466	        "speech corpus by Yilmaz et al. (2016a, 2016b). The treebank is "
467	        "manually annotated using the UD scheme."
468	    ),
469	    "qhe_hiencs": (
470	        "The Hindi-English Code-switching treebank is based on code-switching "
471	        "tweets of Hindi and English multilingual speakers (mostly Indian) on "
472	        "Twitter. The treebank is manually annotated using UD sceheme. The "
473	        "training and evaluations sets were seperately annotated by different "
474	        "annotators using UD v2 and v1 guidelines respectively. The evaluation"
475	        " sets are automatically converted from UD v1 to v2."
476	    ),
477	    "hi_hdtb": (
478	        "The Hindi UD treebank is based on the Hindi Dependency Treebank "
479	        "(HDTB), created at IIIT Hyderabad, India."
480	    ),
481	    "hi_pud": (
482	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
483	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
484	        "Raw Text to Universal Dependencies."
485	    ),
486	    "hu_szeged": (
487	        "The Hungarian UD treebank is derived from the Szeged Dependency "
488	        "Treebank (Vincze et al. 2010)."
489	    ),
490	    "is_modern": (
491	        "UD_Icelandic-Modern is a conversion of the modern additions to the "
492	        "Icelandic Parsed Historical Corpus (IcePaHC) to the Universal "
493	        "Dependencies scheme."
494	    ),
495	    "is_icepahc": (
496	        "UD_Icelandic-IcePaHC is a conversion of the Icelandic Parsed "
497	        "Historical Corpus (IcePaHC) to the Universal Dependencies scheme. The"
498	        " conversion was done using UDConverter."
499	    ),
500	    "is_pud": (
501	        "Icelandic-PUD is the Icelandic part of the Parallel Universal "
502	        "Dependencies (PUD) treebanks."
503	    ),
504	    "id_csui": (
505	        "UD Indonesian-CSUI is a conversion from an Indonesian constituency "
506	        "treebank in the Penn Treebank format named Kethu that was also a "
507	        "conversion from a constituency treebank built by Dinakaramani et al. "
508	        "(2015). We named this treebank Indonesian-CSUI, since all the three "
509	        "versions of the treebanks were built at Faculty of Computer Science, "
510	        "Universitas Indonesia."
511	    ),
512	    "id_gsd": (
513	        "The Indonesian UD is converted from the content head version of the "
514	        "universal dependency treebank v2.0 (legacy)."
515	    ),
516	    "id_pud": (
517	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
518	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
519	        "Raw Text to Universal Dependencies."
520	    ),
521	    "ga_idt": (
522	        "A Universal Dependencies 4910-sentence treebank for modern Irish."
523	    ),
524	    "ga_twittirish": (
525	        "A Universal Dependencies treebank of 866 tweets in modern Irish."
526	    ),
527	    "it_isdt": (
528	        "The Italian corpus annotated according to the UD annotation scheme "
529	        "was obtained by conversion from ISDT (Italian Stanford Dependency "
530	        "Treebank), released for the dependency parsing shared task of "
531	        "Evalita-2014 (Bosco et al. 2014)."
532	    ),
533	    "it_partut": (
534	        "UD_Italian-ParTUT is a conversion of a multilingual parallel treebank"
535	        " developed at the University of Turin, and consisting of a variety of"
536	        " text genres, including talks, legal texts and Wikipedia articles, "
537	        "among others."
538	    ),
539	    "it_postwita": (
540	        "PoSTWITA-UD is a collection of Italian tweets annotated in Universal "
541	        "Dependencies that can be exploited for the training of NLP systems to"
542	        " enhance their performance on social media texts."
543	    ),
544	    "it_markit": (
545	        "It is MarkIT That is New: An Italian Treebank of Marked "
546	        "Constructions. Teresa Paccosi, Alessio Palmero Aprosio and Sara "
547	        "Tonelli, To appear in Proceedings of the Eighth Italian Conference on"
548	        " Computational Linguistics 2022 (CLIC-it 2021)"
549	    ),
550	    "it_valico": (
551	        "Manually corrected Treebank of Learner Italian drawn from the Valico "
552	        "corpus and correspondent corrected sentences."
553	    ),
554	    "it_pud": (
555	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
556	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
557	        "Raw Text to Universal Dependencies."
558	    ),
559	    "it_twittiro": (
560	        "TWITTIR-UD is a collection of ironic Italian tweets annotated in "
561	        "Universal Dependencies. The treebank can be exploited for the "
562	        "training of NLP systems to enhance their performance on social media "
563	        "texts, and in particular, for irony detection purposes."
564	    ),
565	    "it_vit": (
566	        "The UD_Italian-VIT corpus was obtained by conversion from VIT (Venice"
567	        " Italian Treebank), developed at the Laboratory of Computational "
568	        "Linguistics of the Universit Ca' Foscari in Venice (Delmonte et al. "
569	        "2007; Delmonte 2009; "
570	        "http://rondelmo.it/resource/VIT/Browser-VIT/index.htm)."
571	    ),
572	    "ja_pudluw": (
573	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
574	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
575	        "Raw Text to Universal Dependencies."
576	    ),
577	    "ja_bccwjluw": (
578	        "This Universal Dependencies (UD) Japanese treebank is based on the "
579	        "definition of UD Japanese convention described in the UD "
580	        "documentation. The original sentences are from `Balanced Corpus of "
581	        "Contemporary Written Japanese'(BCCWJ)."
582	    ),
583	    "ja_gsdluw": (
584	        "This Universal Dependencies (UD) Japanese treebank is based on the "
585	        "definition of UD Japanese convention described in the UD "
586	        "documentation. The original sentences are from Google UDT 2.0."
587	    ),
588	    "ja_bccwj": (
589	        "This Universal Dependencies (UD) Japanese treebank is based on the "
590	        "definition of UD Japanese convention described in the UD "
591	        "documentation. The original sentences are from `Balanced Corpus of "
592	        "Contemporary Written Japanese'(BCCWJ)."
593	    ),
594	    "ja_gsd": (
595	        "This Universal Dependencies (UD) Japanese treebank is based on the "
596	        "definition of UD Japanese convention described in the UD "
597	        "documentation.  The original sentences are from Google UDT 2.0."
598	    ),
599	    "ja_modern": (
600	        "This Universal Dependencies (UD) Japanese treebank is based on the "
601	        "definition of UD Japanese convention described in the UD "
602	        "documentation. The original sentences are from `Corpus of Historical "
603	        "Japanese' (CHJ)."
604	    ),
605	    "ja_pud": (
606	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
607	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
608	        " Raw Text to Universal "
609	        "Dependencies](http://universaldependencies.org/conll17/)."
610	    ),
611	    "jv_csui": (
612	        "UD Javanese-CSUI is a dependency treebank in Javanese, a regional "
613	        "language in Indonesia with more than 60 million users. The original "
614	        "sentences were taken from OPUS, especially from the WikiMatrix v1 "
615	        "corpus. We revised the sentences that contained more Indonesian words"
616	        " than Javanese words and manually annotated them."
617	    ),
618	    "urb_tudet": (
619	        "UD_Kaapor-TuDeT is a collection of annotated sentences in Ka'apor. "
620	        "The project is a work in progress and the treebank is being updated "
621	        "on a regular basis."
622	    ),
623	    "xnr_kdtb": (
624	        "The Kangri UD Treebank (KDTB) is a part of the Universal Dependency "
625	        "treebank project."
626	    ),
627	    "krl_kkpp": (
628	        "UD Karelian-KKPP is a manually annotated new corpus of Karelian made "
629	        "in Universal dependencies annotation scheme. The data is collected "
630	        "from VepKar corpora and consists of mostly modern news texts but also"
631	        " some stories and educational texts."
632	    ),
633	    "kk_ktb": (
634	        "The UD Kazakh treebank is a combination of text from various sources "
635	        "including Wikipedia, some folk tales, sentences from the UDHR, news "
636	        "and phrasebook sentences. Sentences IDs include partial document "
637	        "identifiers."
638	    ),
639	    "arr_tudet": (
640	        "UD_Karo-TuDeT is a collection of annotated sentences in Karo. The "
641	        "sentences stem from the only grammatical description of the language "
642	        "(Gabas, 1999) and from the sentences in the dictionary by the same "
643	        "author (Gabas, 2007). Sentence annotation and documentation by "
644	        "Fabrcio Ferraz Gerardi."
645	    ),
646	    "kfm_aha": (
647	        "The AHA Khunsari Treebank is a small treebank for contemporary "
648	        "Khunsari. Its corpus is collected and annotated manually. We have "
649	        "prepared this treebank based on interviews with Khunsari speakers."
650	    ),
651	    "quc_iu": (
652	        "UD Kiche-IU is a treebank consisting of sentences from a variety of"
653	        " text domains but principally dictionary example sentences and "
654	        "linguistic examples."
655	    ),
656	    "koi_uh": (
657	        "This is a Komi-Permyak literary language treebank consisting of "
658	        "original and translated texts."
659	    ),
660	    "kpv_ikdp": (
661	        "This treebank consists of dialectal transcriptions of spoken "
662	        "Komi-Zyrian. The current texts are short recorded segments from "
663	        "different areas where the Iva dialect of Komi language is spoken."
664	    ),
665	    "kpv_lattice": (
666	        "UD Komi-Zyrian Lattice is a treebank of written standard Komi-Zyrian."
667	    ),
668	    "ko_gsd": (
669	        "The Google Korean Universal Dependency Treebank is first converted "
670	        "from the Universal Dependency Treebank v2.0 (legacy), and then "
671	        "enhanced by Chun et al., 2018."
672	    ),
673	    "ko_kaist": (
674	        "The KAIST Korean Universal Dependency Treebank is generated by Chun "
675	        "et al., 2018 from the constituency trees in the KAIST Tree-Tagging "
676	        "Corpus."
677	    ),
678	    "ko_pud": (
679	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
680	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
681	        "Raw Text to Universal Dependencies."
682	    ),
683	    "kmr_mg": (
684	        "The UD Kurmanji corpus is a corpus of Kurmanji Kurdish. It contains "
685	        "fiction and encyclopaedic texts in roughly equal measure. It has been"
686	        " annotated natively in accordance with the UD annotation scheme."
687	    ),
688	    "la_ittb": (
689	        "Latin data from the _Index Thomisticus_ Treebank. Data are taken from"
690	        " the _Index Thomisticus_ corpus by Roberto Busa SJ, which contains "
691	        "the complete work by Thomas Aquinas (12251274; Medieval Latin) and "
692	        "by 61 other authors related to Thomas."
693	    ),
694	    "la_udante": (
695	        "The UDante treebank is based on the Latin texts of Dante Alighieri, "
696	        "taken from the DanteSearch corpus, originally created at the "
697	        "University of Pisa, Italy. It is a treebank of Latin language, more "
698	        "precisely of literary Medieval Latin (XIVth century)."
699	    ),
700	    "la_llct": (
701	        "This Universal Dependencies version of the LLCT (Late Latin Charter "
702	        "Treebank) consists of an automated conversion of the LLCT2 treebank "
703	        "from the Latin Dependency Treebank (LDT) format into the Universal "
704	        "Dependencies standard."
705	    ),
706	    "la_perseus": (
707	        "This Universal Dependencies Latin Treebank consists of an automatic "
708	        "conversion of a selection of passages from the Ancient Greek and "
709	        "Latin Dependency Treebank 2.1"
710	    ),
711	    "la_proiel": (
712	        "The Latin PROIEL treebank is based on the Latin data from the PROIEL "
713	        "treebank, and contains most of the Vulgate New Testament translations"
714	        " plus selections from Caesar's Gallic War, Cicero's Letters to "
715	        "Atticus, Palladius' Opus Agriculturae and the first book of Cicero's "
716	        "De officiis."
717	    ),
718	    "lv_lvtb": (
719	        "Latvian UD Treebank is based on Latvian Treebank (LVTB), being "
720	        "created at University of Latvia, Institute of Mathematics and "
721	        "Computer Science, Artificial Intelligence Laboratory."
722	    ),
723	    "lij_glt": (
724	        "The Genoese Ligurian Treebank is a small, manually annotated "
725	        "collection of contemporary Ligurian prose. The focus of the treebank "
726	        "is written Genoese, the koin variety of Ligurian which is associated"
727	        " with today's literary, journalistic and academic ligurophone sphere."
728	    ),
729	    "lt_alksnis": (
730	        "The Lithuanian dependency treebank ALKSNIS v3.0 (Vytautas Magnus "
731	        "University)."
732	    ),
733	    "lt_hse": (
734	        "Lithuanian treebank annotated manually (dependencies) using the "
735	        "Morphological Annotator by CCL, Vytautas Magnus University "
736	        "(http://tekstynas.vdu.lt/) and manual disambiguation. A pilot version"
737	        " which includes news and an essay by Tomas Venclova is available "
738	        "here."
739	    ),
740	    "olo_kkpp": (
741	        "UD Livvi-KKPP is a manually annotated new corpus of Livvi-Karelian "
742	        "made directly in the Universal dependencies annotation scheme. The "
743	        "data is collected from VepKar corpora and consists of mostly modern "
744	        "news texts but also some stories and educational texts."
745	    ),
746	    "nds_lsdc": (
747	        "The UD Low Saxon LSDC dataset consists of sentences in 18 Low Saxon "
748	        "dialects from both Germany and the Netherlands. These sentences are "
749	        "(or are to become) part of the LSDC dataset and represent the "
750	        "language from the 19th and early 20th century in genres such as short"
751	        " stories, novels, speeches, letters and fairytales."
752	    ),
753	    "mt_mudt": (
754	        "MUDT (Maltese Universal Dependencies Treebank) is a manually "
755	        "annotated treebank of Maltese, a Semitic language of Malta descended "
756	        "from North African Arabic with a significant amount of Italo-Romance "
757	        "influence. MUDT was designed as a balanced corpus with four major "
758	        "genres (see Splitting below) represented roughly equally."
759	    ),
760	    "gv_cadhan": (
761	        "This is the Cadhan Aonair UD treebank for Manx Gaelic, created by "
762	        "Kevin Scannell."
763	    ),
764	    "mr_ufal": (
765	        "UD Marathi is a manually annotated treebank consisting primarily of "
766	        "stories from Wikisource, and parts of an article on Wikipedia."
767	    ),
768	    "gun_dooley": (
769	        "UD Mbya_Guarani-Dooley is a corpus of narratives written in Mby "
770	        "Guaran (Tupian) in Brazil, and collected by Robert Dooley. Due to "
771	        "copyright restrictions, the corpus that is distributed as part of UD "
772	        "only contains the annotation (tags, features, relations) while the "
773	        "FORM and LEMMA columns are empty."
774	    ),
775	    "gun_thomas": (
776	        "UD Mbya_Guarani-Thomas is a corpus of Mby Guaran (Tupian) texts "
777	        "collected by Guillaume Thomas. The current version of the corpus "
778	        "consists of three speeches by Paulina Kerechu Nez Romero, a Mby "
779	        "Guaran speaker from Ytu, Caazap Department, Paraguay."
780	    ),
781	    "mdf_jr": (
782	        "Erme Universal Dependencies annotated texts Moksha are the origin of "
783	        "UD_Moksha-JR with annotation (CoNLL-U) for texts in the Moksha "
784	        "language, it originally consists of a sample from a number of fiction"
785	        " authors writing originals in Moksha."
786	    ),
787	    "myu_tudet": (
788	        "UD_Munduruku-TuDeT is a collection of annotated sentences in "
789	        "Munduruk. Together with UD_Akuntsu-TuDeT and UD_Tupinamba-TuDeT, "
790	        "UD_Munduruku-TuDeT is part of the TuLaR project."
791	    ),
792	    "pcm_nsc": (
793	        "A Universal Dependencies corpus for spoken Naija (Nigerian Pidgin)."
794	    ),
795	    "nyq_aha": (
796	        "The AHA Nayini Treebank is a small treebank for contemporary Nayini. "
797	        "Its corpus is collected and annotated manually. We have prepared this"
798	        " treebank based on interviews with Nayini speakers."
799	    ),
800	    "sme_giella": (
801	        "This is a North Smi treebank based on a manually disambiguated and "
802	        "function-labelled gold-standard corpus of North Smi produced by the "
803	        "Giellatekno team at UiT Norgga rktala universitehta."
804	    ),
805	    "no_bokmaal": (
806	        "The Norwegian UD treebank is based on the Bokml section of the "
807	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
808	        " Norwegian. NDT has been automatically converted to the UD scheme by "
809	        "Lilja vrelid at the University of Oslo."
810	    ),
811	    "no_nynorsk": (
812	        "The Norwegian UD treebank is based on the Nynorsk section of the "
813	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
814	        " Norwegian.  NDT has been automatically converted to the UD scheme by"
815	        " Lilja vrelid at the University of Oslo."
816	    ),
817	    "no_nynorsklia": (
818	        "This Norwegian treebank is based on the LIA treebank of transcribed "
819	        "spoken Norwegian dialects. The treebank has been automatically "
820	        "converted to the UD scheme by Lilja vrelid at the University of "
821	        "Oslo."
822	    ),
823	    "cu_proiel": (
824	        "The Old Church Slavonic (OCS) UD treebank is based on the Old Church "
825	        "Slavonic data from the PROIEL treebank and contains the text of the "
826	        "Codex Marianus New Testament translation."
827	    ),
828	    "fro_srcmf": (
829	        "UD_Old_French-SRCMF is a conversion of (part of) the SRCMF corpus "
830	        "(Syntactic Reference Corpus of Medieval French srcmf.org)."
831	    ),
832	    "orv_birchbark": (
833	        "UD Old_East_Slavic-Birchbark is based on the RNC Corpus of Birchbark "
834	        "Letters and includes documents written in 1025-1500 in an East Slavic"
835	        " vernacular (letters, household and business records, records for "
836	        "church services, spell against diseases, and other short "
837	        "inscriptions). The treebank is manually syntactically annotated in "
838	        "the UD 2.0 scheme, morphological and lexical annotation is a "
839	        "conversion of the original RNC annotation."
840	    ),
841	    "orv_rnc": (
842	        "`UD_Old_Russian-RNC` is a sample of the Middle Russian corpus "
843	        "(1300-1700), a part of the Russian National Corpus. The data were "
844	        "originally annotated according to the RNC and extended UD-Russian "
845	        "morphological schemas and UD 2.4 dependency schema."
846	    ),
847	    "orv_torot": (
848	        "UD_Old_Russian-TOROT is a conversion of a selection of the Old East "
849	        "Slavonic and Middle Russian data in the Troms Old Russian and OCS "
850	        "Treebank (TOROT), which was originally annotated in PROIEL dependency"
851	        " format."
852	    ),
853	    "otk_tonqq": (
854	        "`UD_Old_Turkish-Tonqq` is an Old Turkish treebank built upon Turkic "
855	        "script texts or sentences that are trivially convertible."
856	    ),
857	    "fa_perdt": (
858	        "The Persian Universal Dependency Treebank (PerUDT) is the result of "
859	        "automatic coversion of Persian Dependency Treebank (PerDT) with "
860	        "extensive manual corrections. Please refer to the follwoing work, if "
861	        "you use this data: Mohammad Sadegh Rasooli, Pegah Safari, Amirsaeid "
862	        "Moloodi, and Alireza Nourian. 'The Persian Dependency Treebank Made "
863	        "Universal'. 2020 (to appear)."
864	    ),
865	    "fa_seraji": (
866	        "The Persian Universal Dependency Treebank (Persian UD) is based on "
867	        "Uppsala Persian Dependency Treebank (UPDT). The conversion of the "
868	        "UPDT to the Universal Dependencies was performed semi-automatically "
869	        "with extensive manual checks and corrections."
870	    ),
871	    "pl_lfg": (
872	        "The LFG Enhanced UD treebank of Polish is based on a corpus of LFG "
873	        "(Lexical Functional Grammar) syntactic structures generated by an LFG"
874	        " grammar of Polish, POLFIE, and manually disambiguated by human "
875	        "annotators."
876	    ),
877	    "pl_pdb": (
878	        "The Polish PDB-UD treebank is based on the Polish Dependency Bank 2.0"
879	        " (PDB 2.0), created at the Institute of Computer Science, Polish "
880	        "Academy of Sciences in Warsaw. The PDB-UD treebank is an extended and"
881	        " corrected version of the Polish SZ-UD treebank (the release 1.2 to "
882	        "2.3)."
883	    ),
884	    "pl_pud": (
885	        "This is the Polish portion of the Parallel Universal Dependencies "
886	        "(PUD) treebanks, created at the Institute of Computer Science, Polish"
887	        " Academy of Sciences in Warsaw.Re"
888	    ),
889	    "pt_bosque": (
890	        "This Universal Dependencies (UD) Portuguese treebank is based on the "
891	        "Constraint Grammar converted version of the Bosque, which is part of "
892	        "the Floresta Sint(c)tica treebank. It contains both European "
893	        "(CETEMPblico) and Brazilian (CETENFolha) variants."
894	    ),
895	    "pt_gsd": (
896	        "The Brazilian Portuguese UD is converted from the Google Universal "
897	        "Dependency Treebank v2.0 (legacy)."
898	    ),
899	    "pt_pud": (
900	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
901	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
902	        "Raw Text to Universal Dependencies."
903	    ),
904	    "ro_art": (
905	        "The UD treebank ArT is a treebank of the Aromanian dialect of the "
906	        "Romanian language in UD format."
907	    ),
908	    "ro_nonstandard": (
909	        "The Romanian Non-standard UD treebank (called UAIC-RoDia) is based on"
910	        " UAIC-RoDia Treebank. UAIC-RoDia = ISLRN 156-635-615-024-0"
911	    ),
912	    "ro_rrt": (
913	        "The Romanian UD treebank (called RoRefTrees) (Barbu Mititelu et al., "
914	        "2016) is the reference treebank in UD format for standard Romanian."
915	    ),
916	    "ro_simonero": "SiMoNERo is a medical corpus of contemporary Romanian.",
917	    "ru_gsd": (
918	        "Russian Universal Dependencies Treebank annotated and converted by "
919	        "Google."
920	    ),
921	    "ru_pud": (
922	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
923	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
924	        "Raw Text to Universal Dependencies."
925	    ),
926	    "ru_syntagrus": "Russian data from the SynTagRus corpus.",
927	    "ru_taiga": (
928	        "Universal Dependencies treebank is based on data samples extracted "
929	        "from Taiga Corpus and MorphoRuEval-2017 and GramEval-2020 shared "
930	        "tasks collections."
931	    ),
932	    "sa_ufal": (
933	        "A small Sanskrit treebank of sentences from Pacatantra, an ancient "
934	        "Indian collection of interrelated fables by Vishnu Sharma."
935	    ),
936	    "sa_vedic": (
937	        "The Treebank of Vedic Sanskrit contains 4,000 sentences with 27,000 "
938	        "words chosen from metrical and prose passages of the gveda (RV), the"
939	        " aunaka recension of the Atharvaveda (S), the Maitryasahit "
940	        "(MS), and the Aitareya- (AB) and atapatha-Brhmaas (B). Lexical "
941	        "and morpho-syntactic information has been generated using a tagging "
942	        "software and manually validated. POS tags have been induced "
943	        "automatically from the morpho-sytactic information of each word."
944	    ),
945	    "gd_arcosg": (
946	        "A treebank of Scottish Gaelic based on the Annotated Reference Corpus"
947	        " Of Scottish Gaelic (ARCOSG)."
948	    ),
949	    "sr_set": (
950	        "The Serbian UD treebank is based on the "
951	        "[SETimes-SR](http://hdl.handle.net/11356/1200) corpus and additional "
952	        "news documents from the Serbian web."
953	    ),
954	    "sms_giellagas": (
955	        "The UD Skolt Sami Giellagas treebank is based almost entirely on "
956	        "spoken Skolt Sami corpora."
957	    ),
958	    "sk_snk": (
959	        "The Slovak UD treebank is based on data originally annotated as part "
960	        "of the Slovak National Corpus, following the annotation style of the "
961	        "Prague Dependency Treebank."
962	    ),
963	    "sl_ssj": (
964	        "The Slovenian UD Treebank is a rule-based conversion of the ssj500k "
965	        "treebank, the largest collection of manually syntactically annotated "
966	        "data in Slovenian, originally annotated in the JOS annotation scheme."
967	    ),
968	    "sl_sst": (
969	        "The Spoken Slovenian UD Treebank (SST) is the first syntactically "
970	        "annotated corpus of spoken Slovenian, based on a sample of the "
971	        "reference GOS corpus, a collection of transcribed audio recordings of"
972	        " monologic, dialogic and multi-party spontaneous speech in different "
973	        "everyday situations."
974	    ),
975	    "soj_aha": (
976	        "The AHA Soi Treebank is a small treebank for contemporary Soi. Its "
977	        "corpus is collected and annotated manually. We have prepared this "
978	        "treebank based on interviews with Soi speakers."
979	    ),
980	    "ajp_madar": (
981	        "The South_Levantine_Arabic-MADAR treebank consists of 100 "
982	        "manually-annotated sentences taken from the "
983	        "[MADAR](https://camel.abudhabi.nyu.edu/madar/) (Multi-Arabic Dialect "
984	        "Applications and Resources) project. "
985	    ),
986	    "es_ancora": "Spanish data from the AnCora corpus.",
987	    "es_gsd": (
988	        "The Spanish UD is converted from the content head version of the "
989	        "universal dependency treebank v2.0 (legacy)."
990	    ),
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {
1217	        "test": "UD_Arabic-PUD/r2.10/ar_pud-ud-test.conllu",
1218	    },
1219	    "hy_armtdp": {
1220	        "train": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-train.conllu",
1221	        "dev": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-dev.conllu",
1222	        "test": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-test.conllu",
1223	    },
1224	    "aii_as": {
1225	        "test": "UD_Assyrian-AS/r2.10/aii_as-ud-test.conllu",
1226	    },
1227	    "bm_crb": {
1228	        "test": "UD_Bambara-CRB/r2.10/bm_crb-ud-test.conllu",
1229	    },
1230	    "eu_bdt": {
1231	        "train": "UD_Basque-BDT/r2.10/eu_bdt-ud-train.conllu",
1232	        "dev": "UD_Basque-BDT/r2.10/eu_bdt-ud-dev.conllu",
1233	        "test": "UD_Basque-BDT/r2.10/eu_bdt-ud-test.conllu",
1234	    },
1235	    "bej_nsc": {
1236	        "test": "UD_Beja-NSC/r2.10/bej_nsc-ud-test.conllu",
1237	    },
1238	    "be_hse": {
1239	        "train": "UD_Belarusian-HSE/r2.10/be_hse-ud-train.conllu",
1240	        "dev": "UD_Belarusian-HSE/r2.10/be_hse-ud-dev.conllu",
1241	        "test": "UD_Belarusian-HSE/r2.10/be_hse-ud-test.conllu",
1242	    },
1243	    "bn_bru": {
1244	        "test": "UD_Bengali-BRU/r2.10/bn_bru-ud-test.conllu",
1245	    },
1246	    "bho_bhtb": {
1247	        "test": "UD_Bhojpuri-BHTB/r2.10/bho_bhtb-ud-test.conllu",
1248	    },
1249	    "br_keb": {
1250	        "test": "UD_Breton-KEB/r2.10/br_keb-ud-test.conllu",
1251	    },
1252	    "bg_btb": {
1253	        "train": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-train.conllu",
1254	        "dev": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-dev.conllu",
1255	        "test": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-test.conllu",
1256	    },
1257	    "bxr_bdt": {
1258	        "train": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-train.conllu",
1259	        "test": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-test.conllu",
1260	    },
1261	    "yue_hk": {
1262	        "test": "UD_Cantonese-HK/r2.10/yue_hk-ud-test.conllu",
1263	    },
1264	    "ca_ancora": {
1265	        "train": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-train.conllu",
1266	        "dev": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-dev.conllu",
1267	        "test": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-test.conllu",
1268	    },
1269	    "ceb_gja": {
1270	        "test": "UD_Cebuano-GJA/r2.10/ceb_gja-ud-test.conllu",
1271	    },
1272	    "zh_cfl": {
1273	        "test": "UD_Chinese-CFL/r2.10/zh_cfl-ud-test.conllu",
1274	    },
1275	    "zh_gsd": {
1276	        "train": "UD_Chinese-GSD/r2.10/zh_gsd-ud-train.conllu",
1277	        "dev": "UD_Chinese-GSD/r2.10/zh_gsd-ud-dev.conllu",
1278	        "test": "UD_Chinese-GSD/r2.10/zh_gsd-ud-test.conllu",
1279	    },
1280	    "zh_gsdsimp": {
1281	        "train": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-train.conllu",
1282	        "dev": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-dev.conllu",
1283	        "test": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-test.conllu",
1284	    },
1285	    "zh_hk": {
1286	        "test": "UD_Chinese-HK/r2.10/zh_hk-ud-test.conllu",
1287	    },
1288	    "zh_pud": {
1289	        "test": "UD_Chinese-PUD/r2.10/zh_pud-ud-test.conllu",
1290	    },
1291	    "ckt_hse": {
1292	        "test": "UD_Chukchi-HSE/r2.10/ckt_hse-ud-test.conllu",
1293	    },
1294	    "lzh_kyoto": {
1295	        "train": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-train.conllu",
1296	        "dev": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-dev.conllu",
1297	        "test": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-test.conllu",
1298	    },
1299	    "cop_scriptorium": {
1300	        "train": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-train.conllu",
1301	        "dev": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-dev.conllu",
1302	        "test": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-test.conllu",
1303	    },
1304	    "hr_set": {
1305	        "train": "UD_Croatian-SET/r2.10/hr_set-ud-train.conllu",
1306	        "dev": "UD_Croatian-SET/r2.10/hr_set-ud-dev.conllu",
1307	        "test": "UD_Croatian-SET/r2.10/hr_set-ud-test.conllu",
1308	    },
1309	    "cs_cac": {
1310	        "train": "UD_Czech-CAC/r2.10/cs_cac-ud-train.conllu",
1311	        "dev": "UD_Czech-CAC/r2.10/cs_cac-ud-dev.conllu",
1312	        "test": "UD_Czech-CAC/r2.10/cs_cac-ud-test.conllu",
1313	    },
1314	    "cs_cltt": {
1315	        "train": "UD_Czech-CLTT/r2.10/cs_cltt-ud-train.conllu",
1316	        "dev": "UD_Czech-CLTT/r2.10/cs_cltt-ud-dev.conllu",
1317	        "test": "UD_Czech-CLTT/r2.10/cs_cltt-ud-test.conllu",
1318	    },
1319	    "cs_fictree": {
1320	        "train": "UD_Czech-FicTree/r2.10/cs_fictree-ud-train.conllu",
1321	        "dev": "UD_Czech-FicTree/r2.10/cs_fictree-ud-dev.conllu",
1322	        "test": "UD_Czech-FicTree/r2.10/cs_fictree-ud-test.conllu",
1323	    },
1324	    "cs_pdt": {
1325	        "train": [
1326	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-l.conllu",
1327	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-m.conllu",
1328	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-c.conllu",
1329	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-v.conllu",
1330	        ],
1331	        "dev": "UD_Czech-PDT/r2.10/cs_pdt-ud-dev.conllu",
1332	        "test": "UD_Czech-PDT/r2.10/cs_pdt-ud-test.conllu",
1333	    },
1334	    "cs_pud": {
1335	        "test": "UD_Czech-PUD/r2.10/cs_pud-ud-test.conllu",
1336	    },
1337	    "da_ddt": {
1338	        "train": "UD_Danish-DDT/r2.10/da_ddt-ud-train.conllu",
1339	        "dev": "UD_Danish-DDT/r2.10/da_ddt-ud-dev.conllu",
1340	        "test": "UD_Danish-DDT/r2.10/da_ddt-ud-test.conllu",
1341	    },
1342	    "nl_alpino": {
1343	        "train": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-train.conllu",
1344	        "dev": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-dev.conllu",
1345	        "test": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-test.conllu",
1346	    },
1347	    "nl_lassysmall": {
1348	        "train": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-train.conllu",
1349	        "dev": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-dev.conllu",
1350	        "test": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-test.conllu",
1351	    },
1352	    "en_atis": {
1353	        "train": "UD_English-Atis/r2.10/en_atis-ud-train.conllu",
1354	        "dev": "UD_English-Atis/r2.10/en_atis-ud-dev.conllu",
1355	        "test": "UD_English-Atis/r2.10/en_atis-ud-test.conllu",
1356	    },
1357	    "en_esl": {
1358	        "train": "UD_English-ESL/r2.10/en_esl-ud-train.conllu",
1359	        "dev": "UD_English-ESL/r2.10/en_esl-ud-dev.conllu",
1360	        "test": "UD_English-ESL/r2.10/en_esl-ud-test.conllu",
1361	    },
1362	    "en_ewt": {
1363	        "train": "UD_English-EWT/r2.10/en_ewt-ud-train.conllu",
1364	        "dev": "UD_English-EWT/r2.10/en_ewt-ud-dev.conllu",
1365	        "test": "UD_English-EWT/r2.10/en_ewt-ud-test.conllu",
1366	    },
1367	    "en_gum": {
1368	        "train": "UD_English-GUM/r2.10/en_gum-ud-train.conllu",
1369	        "dev": "UD_English-GUM/r2.10/en_gum-ud-dev.conllu",
1370	        "test": "UD_English-GUM/r2.10/en_gum-ud-test.conllu",
1371	    },
1372	    "en_gumreddit": {
1373	        "train": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-train.conllu",
1374	        "dev": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-dev.conllu",
1375	        "test": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-test.conllu",
1376	    },
1377	    "en_lines": {
1378	        "train": "UD_English-LinES/r2.10/en_lines-ud-train.conllu",
1379	        "dev": "UD_English-LinES/r2.10/en_lines-ud-dev.conllu",
1380	        "test": "UD_English-LinES/r2.10/en_lines-ud-test.conllu",
1381	    },
1382	    "en_partut": {
1383	        "train": "UD_English-ParTUT/r2.10/en_partut-ud-train.conllu",
1384	        "dev": "UD_English-ParTUT/r2.10/en_partut-ud-dev.conllu",
1385	        "test": "UD_English-ParTUT/r2.10/en_partut-ud-test.conllu",
1386	    },
1387	    "en_pronouns": {
1388	        "test": "UD_English-Pronouns/r2.10/en_pronouns-ud-test.conllu",
1389	    },
1390	    "en_pud": {
1391	        "test": "UD_English-PUD/r2.10/en_pud-ud-test.conllu",
1392	    },
1393	    "myv_jr": {
1394	        "test": "UD_Erzya-JR/r2.10/myv_jr-ud-test.conllu",
1395	    },
1396	    "et_edt": {
1397	        "train": "UD_Estonian-EDT/r2.10/et_edt-ud-train.conllu",
1398	        "dev": "UD_Estonian-EDT/r2.10/et_edt-ud-dev.conllu",
1399	        "test": "UD_Estonian-EDT/r2.10/et_edt-ud-test.conllu",
1400	    },
1401	    "et_ewt": {
1402	        "train": "UD_Estonian-EWT/r2.10/et_ewt-ud-train.conllu",
1403	        "dev": "UD_Estonian-EWT/r2.10/et_ewt-ud-dev.conllu",
1404	        "test": "UD_Estonian-EWT/r2.10/et_ewt-ud-test.conllu",
1405	    },
1406	    "fo_farpahc": {
1407	        "train": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-train.conllu",
1408	        "dev": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-dev.conllu",
1409	        "test": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-test.conllu",
1410	    },

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:283
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
282	    "en_gum": (
283	        "Universal Dependencies syntax annotations from the GUM corpus "
284	        "(https://corpling.uis.georgetown.edu/gum/)."
285	    ),
286	    "en_gumreddit": (
287	        "Universal Dependencies syntax annotations from the Reddit portion of "
288	        "the GUM corpus (https://corpling.uis.georgetown.edu/gum/) "
289	    ),
290	    "en_lines": (
291	        "UD English_LinES is the English half of the LinES Parallel Treebank "
292	        "with the original dependency annotation first automatically converted"
293	        " into Universal Dependencies and then partially reviewed. Its "
294	        "contents cover literature, an online manual and Europarl data."
295	    ),
296	    "en_atis": (
297	        "UD Atis Treebank is a manually annotated treebank consisting of the "
298	        "sentences in the Atis (Airline Travel Informations) dataset which "
299	        "includes the human speech transcriptions of people asking for flight "
300	        "information on the automated inquiry systems."
301	    ),
302	    "en_partut": (
303	        "UD_English-ParTUT is a conversion of a multilingual parallel treebank"
304	        " developed at the University of Turin, and consisting of a variety of"
305	        " text genres, including talks, legal texts and Wikipedia articles, "
306	        "among others."
307	    ),
308	    "en_pronouns": (
309	        "UD English-Pronouns is dataset created to make pronoun identification"
310	        " more accurate and with a more balanced distribution across genders. "
311	        "The dataset is initially targeting the Independent Genitive pronouns,"
312	        " 'hers', (independent) 'his', (singular) 'theirs', 'mine', and "
313	        "(singular) 'yours'."
314	    ),
315	    "en_pud": (
316	        "This is the English portion of the Parallel Universal Dependencies "
317	        "(PUD) treebanks created for the CoNLL 2017 shared task on "
318	        "Multilingual Parsing from Raw Text to Universal Dependencies "
319	        "(http://universaldependencies.org/conll17/)."
320	    ),
321	    "myv_jr": (
322	        "UD Erzya is the original annotation (CoNLL-U) for texts in the Erzya "
323	        "language, it originally consists of a sample from a number of fiction"
324	        " authors writing originals in Erzya."
325	    ),
326	    "et_edt": (
327	        "UD Estonian is a converted version of the Estonian Dependency "
328	        "Treebank (EDT), originally annotated in the Constraint Grammar (CG) "
329	        "annotation scheme, and consisting of genres of fiction, newspaper "
330	        "texts and scientific texts. The treebank contains 30,972 trees, "
331	        "437,769 tokens."
332	    ),
333	    "et_ewt": (
334	        "UD EWT treebank consists of different genres of new media. The "
335	        "treebank contains 4,493 trees, 56,399 tokens."
336	    ),
337	    "fo_farpahc": (
338	        "UD_Icelandic-FarPaHC is a conversion of the Faroese Parsed Historical"
339	        " Corpus (FarPaHC) to the Universal Dependencies scheme. The "
340	        "conversion was done using UDConverter."
341	    ),
342	    "fo_oft": "This is a treebank of Faroese based on the Faroese Wikipedia.",
343	    "fi_ftb": (
344	        "FinnTreeBank 1 consists of manually annotated grammatical examples "
345	        "from VISK. The UD version of FinnTreeBank 1 was converted from a "
346	        "native annotation model with a script and later manually revised."
347	    ),
348	    "fi_ood": (
349	        "Finnish-OOD is an external out-of-domain test set for Finnish-TDT "
350	        "annotated natively into UD scheme."
351	    ),
352	    "fi_pud": (
353	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
354	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
355	        "Raw Text to Universal Dependencies."
356	    ),
357	    "fi_tdt": (
358	        "UD_Finnish-TDT is based on the Turku Dependency Treebank (TDT), a "
359	        "broad-coverage dependency treebank of general Finnish covering "
360	        "numerous genres. The conversion to UD was followed by extensive "
361	        "manual checks and corrections, and the treebank closely adheres to "
362	        "the UD guidelines."
363	    ),
364	    "fr_fqb": (
365	        "The corpus **UD_French-FQB** is an automatic conversion of the French"
366	        " QuestionBank v1, a corpus entirely made of questions."
367	    ),
368	    "fr_ftb": (
369	        "The Universal Dependency version of the French Treebank (Abeill et "
370	        "al., 2003), hereafter UD_French-FTB, is a treebank of sentences from "
371	        "the newspaper Le Monde, initially manually annotated with "
372	        "morphological information and phrase-structure and then converted to "
373	        "the Universal Dependencies annotation scheme."
374	    ),
375	    "fr_gsd": (
376	        "The **UD_French-GSD** was converted in 2015 from the content head "
377	        "version of the universal dependency treebank v2.0 "
378	        "(https://github.com/ryanmcd/uni-dep-tb). It is updated since 2015 "
379	        "independently from the previous source."
380	    ),
381	    "fr_partut": (
382	        "UD_French-ParTUT is a conversion of a multilingual parallel treebank "
383	        "developed at the University of Turin, and consisting of a variety of "
384	        "text genres, including talks, legal texts and Wikipedia articles, "
385	        "among others."
386	    ),
387	    "fr_rhapsodie": "A Universal Dependencies corpus for spoken French.",
388	    "fr_parisstories": (
389	        "Paris Stories is a corpus of oral French collected and transcribed by"
390	        " Linguistics students from Sorbonne Nouvelle and corrected by "
391	        "students from the Plurital Master's Degree of Computational "
392	        "Linguistics ( Inalco, Paris Nanterre, Sorbonne Nouvelle) between 2017"
393	        " and 2021. It contains monologues and dialogues from speakers living "
394	        "in the Parisian region."
395	    ),
396	    "fr_pud": (
397	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
398	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
399	        "Raw Text to Universal Dependencies."
400	    ),
401	    "fr_sequoia": (
402	        "UD_French-Sequoia is an automatic conversion of the Sequoia Treebank "
403	        "corpus French Sequoia corpus."
404	    ),
405	    "gl_ctg": (
406	        "The Galician UD treebank is based on the automatic parsing of the "
407	        "Galician Technical Corpus (http://sli.uvigo.gal/CTG) created at the "
408	        "University of Vigo by the the TALG NLP research group."
409	    ),
410	    "gl_treegal": (
411	        "The Galician-TreeGal is a treebank for Galician developed at LyS "
412	        "Group (Universidade da Corua)."
413	    ),
414	    "de_gsd": (
415	        "The German UD is converted from the content head version of the "
416	        "universal dependency treebank v2.0 (legacy)."
417	    ),
418	    "de_hdt": (
419	        "UD German-HDT is a conversion of the Hamburg Dependency Treebank, "
420	        "created at the University of Hamburg through manual annotation in "
421	        "conjunction with a standard for morphologically and syntactically "
422	        "annotating sentences as well as a constraint-based parser."
423	    ),
424	    "de_lit": (
425	        "This treebank aims at gathering texts of the German literary history."
426	        " Currently, it hosts Fragments of the early Romanticism, i.e. "
427	        "aphorism-like texts mainly dealing with philosophical issues "
428	        "concerning art, beauty and related topics."
429	    ),
430	    "de_pud": (
431	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
432	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
433	        "Raw Text to Universal Dependencies."
434	    ),
435	    "got_proiel": (
436	        "The UD Gothic treebank is based on the Gothic data from the PROIEL "
437	        "treebank, and consists of Wulfila's Bible translation."
438	    ),
439	    "el_gdt": (
440	        "The Greek UD treebank (UD_Greek-GDT) is derived from the Greek "
441	        "Dependency Treebank (http://gdt.ilsp.gr), a resource developed and "
442	        "maintained by researchers at the Institute for Language and Speech "
443	        "Processing/Athena R.C. (http://www.ilsp.gr)."
444	    ),
445	    "gub_tudet": (
446	        "UD_Guajajara-TuDeT is a collection of annotated sentences in "
447	        "Guajajara. Sentences stem from multiple sources such as descriptions "
448	        "of the language, short stories, dictionaries and translations from "
449	        "the New Testament. Sentence annotation and documentation by Lorena "
450	        "Martn Rodrguez and Fabrcio Ferraz Gerardi."
451	    ),
452	    "gn_oldtudet": (
453	        "UD_Guarani-OldTuDeT is a collection of annotated texts in Old "
454	        "Guaran. All known sources in this language are being annotated: "
455	        "cathesisms, grammars (seventeenth and eighteenth century), sentences "
456	        "from dictionaries, and other texts. Sentence annotation and "
457	        "documentation by Fabrcio Ferraz Gerardi and Lorena Martn Rodrguez."
458	    ),
459	    "he_htb": "A Universal Dependencies Corpus for Hebrew.",
460	    "he_iahltwiki": (
461	        "Publicly available subset of the IAHLT UD Hebrew Treebank's Wikipedia"
462	        " section (https://www.iahlt.org/)"
463	    ),
464	    "qfn_fame": (
465	        "UD_Frisian_Dutch-Fame is a selection of 400 sentences from the FAME! "
466	        "speech corpus by Yilmaz et al. (2016a, 2016b). The treebank is "
467	        "manually annotated using the UD scheme."
468	    ),
469	    "qhe_hiencs": (
470	        "The Hindi-English Code-switching treebank is based on code-switching "
471	        "tweets of Hindi and English multilingual speakers (mostly Indian) on "
472	        "Twitter. The treebank is manually annotated using UD sceheme. The "
473	        "training and evaluations sets were seperately annotated by different "
474	        "annotators using UD v2 and v1 guidelines respectively. The evaluation"
475	        " sets are automatically converted from UD v1 to v2."
476	    ),
477	    "hi_hdtb": (
478	        "The Hindi UD treebank is based on the Hindi Dependency Treebank "
479	        "(HDTB), created at IIIT Hyderabad, India."
480	    ),
481	    "hi_pud": (
482	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
483	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
484	        "Raw Text to Universal Dependencies."
485	    ),
486	    "hu_szeged": (
487	        "The Hungarian UD treebank is derived from the Szeged Dependency "
488	        "Treebank (Vincze et al. 2010)."
489	    ),
490	    "is_modern": (
491	        "UD_Icelandic-Modern is a conversion of the modern additions to the "
492	        "Icelandic Parsed Historical Corpus (IcePaHC) to the Universal "
493	        "Dependencies scheme."
494	    ),
495	    "is_icepahc": (
496	        "UD_Icelandic-IcePaHC is a conversion of the Icelandic Parsed "
497	        "Historical Corpus (IcePaHC) to the Universal Dependencies scheme. The"
498	        " conversion was done using UDConverter."
499	    ),
500	    "is_pud": (
501	        "Icelandic-PUD is the Icelandic part of the Parallel Universal "
502	        "Dependencies (PUD) treebanks."
503	    ),
504	    "id_csui": (
505	        "UD Indonesian-CSUI is a conversion from an Indonesian constituency "
506	        "treebank in the Penn Treebank format named Kethu that was also a "
507	        "conversion from a constituency treebank built by Dinakaramani et al. "
508	        "(2015). We named this treebank Indonesian-CSUI, since all the three "
509	        "versions of the treebanks were built at Faculty of Computer Science, "
510	        "Universitas Indonesia."
511	    ),
512	    "id_gsd": (
513	        "The Indonesian UD is converted from the content head version of the "
514	        "universal dependency treebank v2.0 (legacy)."
515	    ),
516	    "id_pud": (
517	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
518	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
519	        "Raw Text to Universal Dependencies."
520	    ),
521	    "ga_idt": (
522	        "A Universal Dependencies 4910-sentence treebank for modern Irish."
523	    ),
524	    "ga_twittirish": (
525	        "A Universal Dependencies treebank of 866 tweets in modern Irish."
526	    ),
527	    "it_isdt": (
528	        "The Italian corpus annotated according to the UD annotation scheme "
529	        "was obtained by conversion from ISDT (Italian Stanford Dependency "
530	        "Treebank), released for the dependency parsing shared task of "
531	        "Evalita-2014 (Bosco et al. 2014)."
532	    ),
533	    "it_partut": (
534	        "UD_Italian-ParTUT is a conversion of a multilingual parallel treebank"
535	        " developed at the University of Turin, and consisting of a variety of"
536	        " text genres, including talks, legal texts and Wikipedia articles, "
537	        "among others."
538	    ),
539	    "it_postwita": (
540	        "PoSTWITA-UD is a collection of Italian tweets annotated in Universal "
541	        "Dependencies that can be exploited for the training of NLP systems to"
542	        " enhance their performance on social media texts."
543	    ),
544	    "it_markit": (
545	        "It is MarkIT That is New: An Italian Treebank of Marked "
546	        "Constructions. Teresa Paccosi, Alessio Palmero Aprosio and Sara "
547	        "Tonelli, To appear in Proceedings of the Eighth Italian Conference on"
548	        " Computational Linguistics 2022 (CLIC-it 2021)"
549	    ),
550	    "it_valico": (
551	        "Manually corrected Treebank of Learner Italian drawn from the Valico "
552	        "corpus and correspondent corrected sentences."
553	    ),
554	    "it_pud": (
555	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
556	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
557	        "Raw Text to Universal Dependencies."
558	    ),
559	    "it_twittiro": (
560	        "TWITTIR-UD is a collection of ironic Italian tweets annotated in "
561	        "Universal Dependencies. The treebank can be exploited for the "
562	        "training of NLP systems to enhance their performance on social media "
563	        "texts, and in particular, for irony detection purposes."
564	    ),
565	    "it_vit": (
566	        "The UD_Italian-VIT corpus was obtained by conversion from VIT (Venice"
567	        " Italian Treebank), developed at the Laboratory of Computational "
568	        "Linguistics of the Universit Ca' Foscari in Venice (Delmonte et al. "
569	        "2007; Delmonte 2009; "
570	        "http://rondelmo.it/resource/VIT/Browser-VIT/index.htm)."
571	    ),
572	    "ja_pudluw": (
573	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
574	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
575	        "Raw Text to Universal Dependencies."
576	    ),
577	    "ja_bccwjluw": (
578	        "This Universal Dependencies (UD) Japanese treebank is based on the "
579	        "definition of UD Japanese convention described in the UD "
580	        "documentation. The original sentences are from `Balanced Corpus of "
581	        "Contemporary Written Japanese'(BCCWJ)."
582	    ),
583	    "ja_gsdluw": (
584	        "This Universal Dependencies (UD) Japanese treebank is based on the "
585	        "definition of UD Japanese convention described in the UD "
586	        "documentation. The original sentences are from Google UDT 2.0."
587	    ),
588	    "ja_bccwj": (
589	        "This Universal Dependencies (UD) Japanese treebank is based on the "
590	        "definition of UD Japanese convention described in the UD "
591	        "documentation. The original sentences are from `Balanced Corpus of "
592	        "Contemporary Written Japanese'(BCCWJ)."
593	    ),
594	    "ja_gsd": (
595	        "This Universal Dependencies (UD) Japanese treebank is based on the "
596	        "definition of UD Japanese convention described in the UD "
597	        "documentation.  The original sentences are from Google UDT 2.0."
598	    ),
599	    "ja_modern": (
600	        "This Universal Dependencies (UD) Japanese treebank is based on the "
601	        "definition of UD Japanese convention described in the UD "
602	        "documentation. The original sentences are from `Corpus of Historical "
603	        "Japanese' (CHJ)."
604	    ),
605	    "ja_pud": (
606	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
607	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
608	        " Raw Text to Universal "
609	        "Dependencies](http://universaldependencies.org/conll17/)."
610	    ),
611	    "jv_csui": (
612	        "UD Javanese-CSUI is a dependency treebank in Javanese, a regional "
613	        "language in Indonesia with more than 60 million users. The original "
614	        "sentences were taken from OPUS, especially from the WikiMatrix v1 "
615	        "corpus. We revised the sentences that contained more Indonesian words"
616	        " than Javanese words and manually annotated them."
617	    ),
618	    "urb_tudet": (
619	        "UD_Kaapor-TuDeT is a collection of annotated sentences in Ka'apor. "
620	        "The project is a work in progress and the treebank is being updated "
621	        "on a regular basis."
622	    ),
623	    "xnr_kdtb": (
624	        "The Kangri UD Treebank (KDTB) is a part of the Universal Dependency "
625	        "treebank project."
626	    ),
627	    "krl_kkpp": (
628	        "UD Karelian-KKPP is a manually annotated new corpus of Karelian made "
629	        "in Universal dependencies annotation scheme. The data is collected "
630	        "from VepKar corpora and consists of mostly modern news texts but also"
631	        " some stories and educational texts."
632	    ),
633	    "kk_ktb": (
634	        "The UD Kazakh treebank is a combination of text from various sources "
635	        "including Wikipedia, some folk tales, sentences from the UDHR, news "
636	        "and phrasebook sentences. Sentences IDs include partial document "
637	        "identifiers."
638	    ),
639	    "arr_tudet": (
640	        "UD_Karo-TuDeT is a collection of annotated sentences in Karo. The "
641	        "sentences stem from the only grammatical description of the language "
642	        "(Gabas, 1999) and from the sentences in the dictionary by the same "
643	        "author (Gabas, 2007). Sentence annotation and documentation by "
644	        "Fabrcio Ferraz Gerardi."
645	    ),
646	    "kfm_aha": (
647	        "The AHA Khunsari Treebank is a small treebank for contemporary "
648	        "Khunsari. Its corpus is collected and annotated manually. We have "
649	        "prepared this treebank based on interviews with Khunsari speakers."
650	    ),
651	    "quc_iu": (
652	        "UD Kiche-IU is a treebank consisting of sentences from a variety of"
653	        " text domains but principally dictionary example sentences and "
654	        "linguistic examples."
655	    ),
656	    "koi_uh": (
657	        "This is a Komi-Permyak literary language treebank consisting of "
658	        "original and translated texts."
659	    ),
660	    "kpv_ikdp": (
661	        "This treebank consists of dialectal transcriptions of spoken "
662	        "Komi-Zyrian. The current texts are short recorded segments from "
663	        "different areas where the Iva dialect of Komi language is spoken."
664	    ),
665	    "kpv_lattice": (
666	        "UD Komi-Zyrian Lattice is a treebank of written standard Komi-Zyrian."
667	    ),
668	    "ko_gsd": (
669	        "The Google Korean Universal Dependency Treebank is first converted "
670	        "from the Universal Dependency Treebank v2.0 (legacy), and then "
671	        "enhanced by Chun et al., 2018."
672	    ),
673	    "ko_kaist": (
674	        "The KAIST Korean Universal Dependency Treebank is generated by Chun "
675	        "et al., 2018 from the constituency trees in the KAIST Tree-Tagging "
676	        "Corpus."
677	    ),
678	    "ko_pud": (
679	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
680	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
681	        "Raw Text to Universal Dependencies."
682	    ),
683	    "kmr_mg": (
684	        "The UD Kurmanji corpus is a corpus of Kurmanji Kurdish. It contains "
685	        "fiction and encyclopaedic texts in roughly equal measure. It has been"
686	        " annotated natively in accordance with the UD annotation scheme."
687	    ),
688	    "la_ittb": (
689	        "Latin data from the _Index Thomisticus_ Treebank. Data are taken from"
690	        " the _Index Thomisticus_ corpus by Roberto Busa SJ, which contains "
691	        "the complete work by Thomas Aquinas (12251274; Medieval Latin) and "
692	        "by 61 other authors related to Thomas."
693	    ),
694	    "la_udante": (
695	        "The UDante treebank is based on the Latin texts of Dante Alighieri, "
696	        "taken from the DanteSearch corpus, originally created at the "
697	        "University of Pisa, Italy. It is a treebank of Latin language, more "
698	        "precisely of literary Medieval Latin (XIVth century)."
699	    ),
700	    "la_llct": (
701	        "This Universal Dependencies version of the LLCT (Late Latin Charter "
702	        "Treebank) consists of an automated conversion of the LLCT2 treebank "
703	        "from the Latin Dependency Treebank (LDT) format into the Universal "
704	        "Dependencies standard."
705	    ),
706	    "la_perseus": (
707	        "This Universal Dependencies Latin Treebank consists of an automatic "
708	        "conversion of a selection of passages from the Ancient Greek and "
709	        "Latin Dependency Treebank 2.1"
710	    ),
711	    "la_proiel": (
712	        "The Latin PROIEL treebank is based on the Latin data from the PROIEL "
713	        "treebank, and contains most of the Vulgate New Testament translations"
714	        " plus selections from Caesar's Gallic War, Cicero's Letters to "
715	        "Atticus, Palladius' Opus Agriculturae and the first book of Cicero's "
716	        "De officiis."
717	    ),
718	    "lv_lvtb": (
719	        "Latvian UD Treebank is based on Latvian Treebank (LVTB), being "
720	        "created at University of Latvia, Institute of Mathematics and "
721	        "Computer Science, Artificial Intelligence Laboratory."
722	    ),
723	    "lij_glt": (
724	        "The Genoese Ligurian Treebank is a small, manually annotated "
725	        "collection of contemporary Ligurian prose. The focus of the treebank "
726	        "is written Genoese, the koin variety of Ligurian which is associated"
727	        " with today's literary, journalistic and academic ligurophone sphere."
728	    ),
729	    "lt_alksnis": (
730	        "The Lithuanian dependency treebank ALKSNIS v3.0 (Vytautas Magnus "
731	        "University)."
732	    ),
733	    "lt_hse": (
734	        "Lithuanian treebank annotated manually (dependencies) using the "
735	        "Morphological Annotator by CCL, Vytautas Magnus University "
736	        "(http://tekstynas.vdu.lt/) and manual disambiguation. A pilot version"
737	        " which includes news and an essay by Tomas Venclova is available "
738	        "here."
739	    ),
740	    "olo_kkpp": (
741	        "UD Livvi-KKPP is a manually annotated new corpus of Livvi-Karelian "
742	        "made directly in the Universal dependencies annotation scheme. The "
743	        "data is collected from VepKar corpora and consists of mostly modern "
744	        "news texts but also some stories and educational texts."
745	    ),
746	    "nds_lsdc": (
747	        "The UD Low Saxon LSDC dataset consists of sentences in 18 Low Saxon "
748	        "dialects from both Germany and the Netherlands. These sentences are "
749	        "(or are to become) part of the LSDC dataset and represent the "
750	        "language from the 19th and early 20th century in genres such as short"
751	        " stories, novels, speeches, letters and fairytales."
752	    ),
753	    "mt_mudt": (
754	        "MUDT (Maltese Universal Dependencies Treebank) is a manually "
755	        "annotated treebank of Maltese, a Semitic language of Malta descended "
756	        "from North African Arabic with a significant amount of Italo-Romance "
757	        "influence. MUDT was designed as a balanced corpus with four major "
758	        "genres (see Splitting below) represented roughly equally."
759	    ),
760	    "gv_cadhan": (
761	        "This is the Cadhan Aonair UD treebank for Manx Gaelic, created by "
762	        "Kevin Scannell."
763	    ),
764	    "mr_ufal": (
765	        "UD Marathi is a manually annotated treebank consisting primarily of "
766	        "stories from Wikisource, and parts of an article on Wikipedia."
767	    ),
768	    "gun_dooley": (
769	        "UD Mbya_Guarani-Dooley is a corpus of narratives written in Mby "
770	        "Guaran (Tupian) in Brazil, and collected by Robert Dooley. Due to "
771	        "copyright restrictions, the corpus that is distributed as part of UD "
772	        "only contains the annotation (tags, features, relations) while the "
773	        "FORM and LEMMA columns are empty."
774	    ),
775	    "gun_thomas": (
776	        "UD Mbya_Guarani-Thomas is a corpus of Mby Guaran (Tupian) texts "
777	        "collected by Guillaume Thomas. The current version of the corpus "
778	        "consists of three speeches by Paulina Kerechu Nez Romero, a Mby "
779	        "Guaran speaker from Ytu, Caazap Department, Paraguay."
780	    ),
781	    "mdf_jr": (
782	        "Erme Universal Dependencies annotated texts Moksha are the origin of "
783	        "UD_Moksha-JR with annotation (CoNLL-U) for texts in the Moksha "
784	        "language, it originally consists of a sample from a number of fiction"
785	        " authors writing originals in Moksha."
786	    ),
787	    "myu_tudet": (
788	        "UD_Munduruku-TuDeT is a collection of annotated sentences in "
789	        "Munduruk. Together with UD_Akuntsu-TuDeT and UD_Tupinamba-TuDeT, "
790	        "UD_Munduruku-TuDeT is part of the TuLaR project."
791	    ),
792	    "pcm_nsc": (
793	        "A Universal Dependencies corpus for spoken Naija (Nigerian Pidgin)."
794	    ),
795	    "nyq_aha": (
796	        "The AHA Nayini Treebank is a small treebank for contemporary Nayini. "
797	        "Its corpus is collected and annotated manually. We have prepared this"
798	        " treebank based on interviews with Nayini speakers."
799	    ),
800	    "sme_giella": (
801	        "This is a North Smi treebank based on a manually disambiguated and "
802	        "function-labelled gold-standard corpus of North Smi produced by the "
803	        "Giellatekno team at UiT Norgga rktala universitehta."
804	    ),
805	    "no_bokmaal": (
806	        "The Norwegian UD treebank is based on the Bokml section of the "
807	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
808	        " Norwegian. NDT has been automatically converted to the UD scheme by "
809	        "Lilja vrelid at the University of Oslo."
810	    ),
811	    "no_nynorsk": (
812	        "The Norwegian UD treebank is based on the Nynorsk section of the "
813	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
814	        " Norwegian.  NDT has been automatically converted to the UD scheme by"
815	        " Lilja vrelid at the University of Oslo."
816	    ),
817	    "no_nynorsklia": (
818	        "This Norwegian treebank is based on the LIA treebank of transcribed "
819	        "spoken Norwegian dialects. The treebank has been automatically "
820	        "converted to the UD scheme by Lilja vrelid at the University of "
821	        "Oslo."
822	    ),
823	    "cu_proiel": (
824	        "The Old Church Slavonic (OCS) UD treebank is based on the Old Church "
825	        "Slavonic data from the PROIEL treebank and contains the text of the "
826	        "Codex Marianus New Testament translation."
827	    ),
828	    "fro_srcmf": (
829	        "UD_Old_French-SRCMF is a conversion of (part of) the SRCMF corpus "
830	        "(Syntactic Reference Corpus of Medieval French srcmf.org)."
831	    ),
832	    "orv_birchbark": (
833	        "UD Old_East_Slavic-Birchbark is based on the RNC Corpus of Birchbark "
834	        "Letters and includes documents written in 1025-1500 in an East Slavic"
835	        " vernacular (letters, household and business records, records for "
836	        "church services, spell against diseases, and other short "
837	        "inscriptions). The treebank is manually syntactically annotated in "
838	        "the UD 2.0 scheme, morphological and lexical annotation is a "
839	        "conversion of the original RNC annotation."
840	    ),
841	    "orv_rnc": (
842	        "`UD_Old_Russian-RNC` is a sample of the Middle Russian corpus "
843	        "(1300-1700), a part of the Russian National Corpus. The data were "
844	        "originally annotated according to the RNC and extended UD-Russian "
845	        "morphological schemas and UD 2.4 dependency schema."
846	    ),
847	    "orv_torot": (
848	        "UD_Old_Russian-TOROT is a conversion of a selection of the Old East "
849	        "Slavonic and Middle Russian data in the Troms Old Russian and OCS "
850	        "Treebank (TOROT), which was originally annotated in PROIEL dependency"
851	        " format."
852	    ),
853	    "otk_tonqq": (
854	        "`UD_Old_Turkish-Tonqq` is an Old Turkish treebank built upon Turkic "
855	        "script texts or sentences that are trivially convertible."
856	    ),
857	    "fa_perdt": (
858	        "The Persian Universal Dependency Treebank (PerUDT) is the result of "
859	        "automatic coversion of Persian Dependency Treebank (PerDT) with "
860	        "extensive manual corrections. Please refer to the follwoing work, if "
861	        "you use this data: Mohammad Sadegh Rasooli, Pegah Safari, Amirsaeid "
862	        "Moloodi, and Alireza Nourian. 'The Persian Dependency Treebank Made "
863	        "Universal'. 2020 (to appear)."
864	    ),
865	    "fa_seraji": (
866	        "The Persian Universal Dependency Treebank (Persian UD) is based on "
867	        "Uppsala Persian Dependency Treebank (UPDT). The conversion of the "
868	        "UPDT to the Universal Dependencies was performed semi-automatically "
869	        "with extensive manual checks and corrections."
870	    ),
871	    "pl_lfg": (
872	        "The LFG Enhanced UD treebank of Polish is based on a corpus of LFG "
873	        "(Lexical Functional Grammar) syntactic structures generated by an LFG"
874	        " grammar of Polish, POLFIE, and manually disambiguated by human "
875	        "annotators."
876	    ),
877	    "pl_pdb": (
878	        "The Polish PDB-UD treebank is based on the Polish Dependency Bank 2.0"
879	        " (PDB 2.0), created at the Institute of Computer Science, Polish "
880	        "Academy of Sciences in Warsaw. The PDB-UD treebank is an extended and"
881	        " corrected version of the Polish SZ-UD treebank (the release 1.2 to "
882	        "2.3)."
883	    ),
884	    "pl_pud": (
885	        "This is the Polish portion of the Parallel Universal Dependencies "
886	        "(PUD) treebanks, created at the Institute of Computer Science, Polish"
887	        " Academy of Sciences in Warsaw.Re"
888	    ),
889	    "pt_bosque": (
890	        "This Universal Dependencies (UD) Portuguese treebank is based on the "
891	        "Constraint Grammar converted version of the Bosque, which is part of "
892	        "the Floresta Sint(c)tica treebank. It contains both European "
893	        "(CETEMPblico) and Brazilian (CETENFolha) variants."
894	    ),
895	    "pt_gsd": (
896	        "The Brazilian Portuguese UD is converted from the Google Universal "
897	        "Dependency Treebank v2.0 (legacy)."
898	    ),
899	    "pt_pud": (
900	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
901	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
902	        "Raw Text to Universal Dependencies."
903	    ),
904	    "ro_art": (
905	        "The UD treebank ArT is a treebank of the Aromanian dialect of the "
906	        "Romanian language in UD format."
907	    ),
908	    "ro_nonstandard": (
909	        "The Romanian Non-standard UD treebank (called UAIC-RoDia) is based on"
910	        " UAIC-RoDia Treebank. UAIC-RoDia = ISLRN 156-635-615-024-0"
911	    ),
912	    "ro_rrt": (
913	        "The Romanian UD treebank (called RoRefTrees) (Barbu Mititelu et al., "
914	        "2016) is the reference treebank in UD format for standard Romanian."
915	    ),
916	    "ro_simonero": "SiMoNERo is a medical corpus of contemporary Romanian.",
917	    "ru_gsd": (
918	        "Russian Universal Dependencies Treebank annotated and converted by "
919	        "Google."
920	    ),
921	    "ru_pud": (
922	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
923	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
924	        "Raw Text to Universal Dependencies."
925	    ),
926	    "ru_syntagrus": "Russian data from the SynTagRus corpus.",
927	    "ru_taiga": (
928	        "Universal Dependencies treebank is based on data samples extracted "
929	        "from Taiga Corpus and MorphoRuEval-2017 and GramEval-2020 shared "
930	        "tasks collections."
931	    ),
932	    "sa_ufal": (
933	        "A small Sanskrit treebank of sentences from Pacatantra, an ancient "
934	        "Indian collection of interrelated fables by Vishnu Sharma."
935	    ),
936	    "sa_vedic": (
937	        "The Treebank of Vedic Sanskrit contains 4,000 sentences with 27,000 "
938	        "words chosen from metrical and prose passages of the gveda (RV), the"
939	        " aunaka recension of the Atharvaveda (S), the Maitryasahit "
940	        "(MS), and the Aitareya- (AB) and atapatha-Brhmaas (B). Lexical "
941	        "and morpho-syntactic information has been generated using a tagging "
942	        "software and manually validated. POS tags have been induced "
943	        "automatically from the morpho-sytactic information of each word."
944	    ),
945	    "gd_arcosg": (
946	        "A treebank of Scottish Gaelic based on the Annotated Reference Corpus"
947	        " Of Scottish Gaelic (ARCOSG)."
948	    ),
949	    "sr_set": (
950	        "The Serbian UD treebank is based on the "
951	        "[SETimes-SR](http://hdl.handle.net/11356/1200) corpus and additional "
952	        "news documents from the Serbian web."
953	    ),
954	    "sms_giellagas": (
955	        "The UD Skolt Sami Giellagas treebank is based almost entirely on "
956	        "spoken Skolt Sami corpora."
957	    ),
958	    "sk_snk": (
959	        "The Slovak UD treebank is based on data originally annotated as part "
960	        "of the Slovak National Corpus, following the annotation style of the "
961	        "Prague Dependency Treebank."
962	    ),
963	    "sl_ssj": (
964	        "The Slovenian UD Treebank is a rule-based conversion of the ssj500k "
965	        "treebank, the largest collection of manually syntactically annotated "
966	        "data in Slovenian, originally annotated in the JOS annotation scheme."
967	    ),
968	    "sl_sst": (
969	        "The Spoken Slovenian UD Treebank (SST) is the first syntactically "
970	        "annotated corpus of spoken Slovenian, based on a sample of the "
971	        "reference GOS corpus, a collection of transcribed audio recordings of"
972	        " monologic, dialogic and multi-party spontaneous speech in different "
973	        "everyday situations."
974	    ),
975	    "soj_aha": (
976	        "The AHA Soi Treebank is a small treebank for contemporary Soi. Its "
977	        "corpus is collected and annotated manually. We have prepared this "
978	        "treebank based on interviews with Soi speakers."
979	    ),
980	    "ajp_madar": (
981	        "The South_Levantine_Arabic-MADAR treebank consists of 100 "
982	        "manually-annotated sentences taken from the "
983	        "[MADAR](https://camel.abudhabi.nyu.edu/madar/) (Multi-Arabic Dialect "
984	        "Applications and Resources) project. "
985	    ),
986	    "es_ancora": "Spanish data from the AnCora corpus.",
987	    "es_gsd": (
988	        "The Spanish UD is converted from the content head version of the "
989	        "universal dependency treebank v2.0 (legacy)."
990	    ),
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {
1217	        "test": "UD_Arabic-PUD/r2.10/ar_pud-ud-test.conllu",
1218	    },
1219	    "hy_armtdp": {
1220	        "train": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-train.conllu",
1221	        "dev": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-dev.conllu",
1222	        "test": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-test.conllu",
1223	    },
1224	    "aii_as": {
1225	        "test": "UD_Assyrian-AS/r2.10/aii_as-ud-test.conllu",
1226	    },
1227	    "bm_crb": {
1228	        "test": "UD_Bambara-CRB/r2.10/bm_crb-ud-test.conllu",
1229	    },
1230	    "eu_bdt": {
1231	        "train": "UD_Basque-BDT/r2.10/eu_bdt-ud-train.conllu",
1232	        "dev": "UD_Basque-BDT/r2.10/eu_bdt-ud-dev.conllu",
1233	        "test": "UD_Basque-BDT/r2.10/eu_bdt-ud-test.conllu",
1234	    },
1235	    "bej_nsc": {
1236	        "test": "UD_Beja-NSC/r2.10/bej_nsc-ud-test.conllu",
1237	    },
1238	    "be_hse": {
1239	        "train": "UD_Belarusian-HSE/r2.10/be_hse-ud-train.conllu",
1240	        "dev": "UD_Belarusian-HSE/r2.10/be_hse-ud-dev.conllu",
1241	        "test": "UD_Belarusian-HSE/r2.10/be_hse-ud-test.conllu",
1242	    },
1243	    "bn_bru": {
1244	        "test": "UD_Bengali-BRU/r2.10/bn_bru-ud-test.conllu",
1245	    },
1246	    "bho_bhtb": {
1247	        "test": "UD_Bhojpuri-BHTB/r2.10/bho_bhtb-ud-test.conllu",
1248	    },
1249	    "br_keb": {
1250	        "test": "UD_Breton-KEB/r2.10/br_keb-ud-test.conllu",
1251	    },
1252	    "bg_btb": {
1253	        "train": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-train.conllu",
1254	        "dev": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-dev.conllu",
1255	        "test": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-test.conllu",
1256	    },
1257	    "bxr_bdt": {
1258	        "train": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-train.conllu",
1259	        "test": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-test.conllu",
1260	    },
1261	    "yue_hk": {
1262	        "test": "UD_Cantonese-HK/r2.10/yue_hk-ud-test.conllu",
1263	    },
1264	    "ca_ancora": {
1265	        "train": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-train.conllu",
1266	        "dev": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-dev.conllu",
1267	        "test": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-test.conllu",
1268	    },
1269	    "ceb_gja": {
1270	        "test": "UD_Cebuano-GJA/r2.10/ceb_gja-ud-test.conllu",
1271	    },
1272	    "zh_cfl": {
1273	        "test": "UD_Chinese-CFL/r2.10/zh_cfl-ud-test.conllu",
1274	    },
1275	    "zh_gsd": {
1276	        "train": "UD_Chinese-GSD/r2.10/zh_gsd-ud-train.conllu",
1277	        "dev": "UD_Chinese-GSD/r2.10/zh_gsd-ud-dev.conllu",
1278	        "test": "UD_Chinese-GSD/r2.10/zh_gsd-ud-test.conllu",
1279	    },
1280	    "zh_gsdsimp": {
1281	        "train": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-train.conllu",
1282	        "dev": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-dev.conllu",
1283	        "test": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-test.conllu",
1284	    },
1285	    "zh_hk": {
1286	        "test": "UD_Chinese-HK/r2.10/zh_hk-ud-test.conllu",
1287	    },
1288	    "zh_pud": {
1289	        "test": "UD_Chinese-PUD/r2.10/zh_pud-ud-test.conllu",
1290	    },
1291	    "ckt_hse": {
1292	        "test": "UD_Chukchi-HSE/r2.10/ckt_hse-ud-test.conllu",
1293	    },
1294	    "lzh_kyoto": {
1295	        "train": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-train.conllu",
1296	        "dev": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-dev.conllu",
1297	        "test": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-test.conllu",
1298	    },
1299	    "cop_scriptorium": {
1300	        "train": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-train.conllu",
1301	        "dev": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-dev.conllu",
1302	        "test": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-test.conllu",
1303	    },
1304	    "hr_set": {
1305	        "train": "UD_Croatian-SET/r2.10/hr_set-ud-train.conllu",
1306	        "dev": "UD_Croatian-SET/r2.10/hr_set-ud-dev.conllu",
1307	        "test": "UD_Croatian-SET/r2.10/hr_set-ud-test.conllu",
1308	    },
1309	    "cs_cac": {
1310	        "train": "UD_Czech-CAC/r2.10/cs_cac-ud-train.conllu",
1311	        "dev": "UD_Czech-CAC/r2.10/cs_cac-ud-dev.conllu",
1312	        "test": "UD_Czech-CAC/r2.10/cs_cac-ud-test.conllu",
1313	    },
1314	    "cs_cltt": {
1315	        "train": "UD_Czech-CLTT/r2.10/cs_cltt-ud-train.conllu",
1316	        "dev": "UD_Czech-CLTT/r2.10/cs_cltt-ud-dev.conllu",
1317	        "test": "UD_Czech-CLTT/r2.10/cs_cltt-ud-test.conllu",
1318	    },
1319	    "cs_fictree": {
1320	        "train": "UD_Czech-FicTree/r2.10/cs_fictree-ud-train.conllu",
1321	        "dev": "UD_Czech-FicTree/r2.10/cs_fictree-ud-dev.conllu",
1322	        "test": "UD_Czech-FicTree/r2.10/cs_fictree-ud-test.conllu",
1323	    },
1324	    "cs_pdt": {
1325	        "train": [
1326	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-l.conllu",
1327	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-m.conllu",
1328	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-c.conllu",
1329	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-v.conllu",
1330	        ],
1331	        "dev": "UD_Czech-PDT/r2.10/cs_pdt-ud-dev.conllu",
1332	        "test": "UD_Czech-PDT/r2.10/cs_pdt-ud-test.conllu",
1333	    },
1334	    "cs_pud": {
1335	        "test": "UD_Czech-PUD/r2.10/cs_pud-ud-test.conllu",
1336	    },
1337	    "da_ddt": {
1338	        "train": "UD_Danish-DDT/r2.10/da_ddt-ud-train.conllu",
1339	        "dev": "UD_Danish-DDT/r2.10/da_ddt-ud-dev.conllu",
1340	        "test": "UD_Danish-DDT/r2.10/da_ddt-ud-test.conllu",
1341	    },
1342	    "nl_alpino": {
1343	        "train": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-train.conllu",
1344	        "dev": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-dev.conllu",
1345	        "test": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-test.conllu",
1346	    },
1347	    "nl_lassysmall": {
1348	        "train": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-train.conllu",
1349	        "dev": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-dev.conllu",
1350	        "test": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-test.conllu",
1351	    },
1352	    "en_atis": {
1353	        "train": "UD_English-Atis/r2.10/en_atis-ud-train.conllu",
1354	        "dev": "UD_English-Atis/r2.10/en_atis-ud-dev.conllu",
1355	        "test": "UD_English-Atis/r2.10/en_atis-ud-test.conllu",
1356	    },
1357	    "en_esl": {
1358	        "train": "UD_English-ESL/r2.10/en_esl-ud-train.conllu",
1359	        "dev": "UD_English-ESL/r2.10/en_esl-ud-dev.conllu",
1360	        "test": "UD_English-ESL/r2.10/en_esl-ud-test.conllu",
1361	    },
1362	    "en_ewt": {
1363	        "train": "UD_English-EWT/r2.10/en_ewt-ud-train.conllu",
1364	        "dev": "UD_English-EWT/r2.10/en_ewt-ud-dev.conllu",
1365	        "test": "UD_English-EWT/r2.10/en_ewt-ud-test.conllu",
1366	    },
1367	    "en_gum": {
1368	        "train": "UD_English-GUM/r2.10/en_gum-ud-train.conllu",
1369	        "dev": "UD_English-GUM/r2.10/en_gum-ud-dev.conllu",
1370	        "test": "UD_English-GUM/r2.10/en_gum-ud-test.conllu",
1371	    },
1372	    "en_gumreddit": {
1373	        "train": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-train.conllu",
1374	        "dev": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-dev.conllu",
1375	        "test": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-test.conllu",
1376	    },
1377	    "en_lines": {
1378	        "train": "UD_English-LinES/r2.10/en_lines-ud-train.conllu",
1379	        "dev": "UD_English-LinES/r2.10/en_lines-ud-dev.conllu",
1380	        "test": "UD_English-LinES/r2.10/en_lines-ud-test.conllu",
1381	    },
1382	    "en_partut": {
1383	        "train": "UD_English-ParTUT/r2.10/en_partut-ud-train.conllu",
1384	        "dev": "UD_English-ParTUT/r2.10/en_partut-ud-dev.conllu",
1385	        "test": "UD_English-ParTUT/r2.10/en_partut-ud-test.conllu",
1386	    },
1387	    "en_pronouns": {
1388	        "test": "UD_English-Pronouns/r2.10/en_pronouns-ud-test.conllu",
1389	    },
1390	    "en_pud": {
1391	        "test": "UD_English-PUD/r2.10/en_pud-ud-test.conllu",
1392	    },
1393	    "myv_jr": {
1394	        "test": "UD_Erzya-JR/r2.10/myv_jr-ud-test.conllu",
1395	    },
1396	    "et_edt": {
1397	        "train": "UD_Estonian-EDT/r2.10/et_edt-ud-train.conllu",
1398	        "dev": "UD_Estonian-EDT/r2.10/et_edt-ud-dev.conllu",
1399	        "test": "UD_Estonian-EDT/r2.10/et_edt-ud-test.conllu",
1400	    },
1401	    "et_ewt": {
1402	        "train": "UD_Estonian-EWT/r2.10/et_ewt-ud-train.conllu",
1403	        "dev": "UD_Estonian-EWT/r2.10/et_ewt-ud-dev.conllu",
1404	        "test": "UD_Estonian-EWT/r2.10/et_ewt-ud-test.conllu",
1405	    },
1406	    "fo_farpahc": {
1407	        "train": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-train.conllu",
1408	        "dev": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-dev.conllu",
1409	        "test": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-test.conllu",
1410	    },
1411	    "fo_oft": {
1412	        "test": "UD_Faroese-OFT/r2.10/fo_oft-ud-test.conllu",
1413	    },
1414	    "fi_ftb": {
1415	        "train": "UD_Finnish-FTB/r2.10/fi_ftb-ud-train.conllu",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:287
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
286	    "en_gumreddit": (
287	        "Universal Dependencies syntax annotations from the Reddit portion of "
288	        "the GUM corpus (https://corpling.uis.georgetown.edu/gum/) "
289	    ),
290	    "en_lines": (
291	        "UD English_LinES is the English half of the LinES Parallel Treebank "
292	        "with the original dependency annotation first automatically converted"
293	        " into Universal Dependencies and then partially reviewed. Its "
294	        "contents cover literature, an online manual and Europarl data."
295	    ),
296	    "en_atis": (
297	        "UD Atis Treebank is a manually annotated treebank consisting of the "
298	        "sentences in the Atis (Airline Travel Informations) dataset which "
299	        "includes the human speech transcriptions of people asking for flight "
300	        "information on the automated inquiry systems."
301	    ),
302	    "en_partut": (
303	        "UD_English-ParTUT is a conversion of a multilingual parallel treebank"
304	        " developed at the University of Turin, and consisting of a variety of"
305	        " text genres, including talks, legal texts and Wikipedia articles, "
306	        "among others."
307	    ),
308	    "en_pronouns": (
309	        "UD English-Pronouns is dataset created to make pronoun identification"
310	        " more accurate and with a more balanced distribution across genders. "
311	        "The dataset is initially targeting the Independent Genitive pronouns,"
312	        " 'hers', (independent) 'his', (singular) 'theirs', 'mine', and "
313	        "(singular) 'yours'."
314	    ),
315	    "en_pud": (
316	        "This is the English portion of the Parallel Universal Dependencies "
317	        "(PUD) treebanks created for the CoNLL 2017 shared task on "
318	        "Multilingual Parsing from Raw Text to Universal Dependencies "
319	        "(http://universaldependencies.org/conll17/)."
320	    ),
321	    "myv_jr": (
322	        "UD Erzya is the original annotation (CoNLL-U) for texts in the Erzya "
323	        "language, it originally consists of a sample from a number of fiction"
324	        " authors writing originals in Erzya."
325	    ),
326	    "et_edt": (
327	        "UD Estonian is a converted version of the Estonian Dependency "
328	        "Treebank (EDT), originally annotated in the Constraint Grammar (CG) "
329	        "annotation scheme, and consisting of genres of fiction, newspaper "
330	        "texts and scientific texts. The treebank contains 30,972 trees, "
331	        "437,769 tokens."
332	    ),
333	    "et_ewt": (
334	        "UD EWT treebank consists of different genres of new media. The "
335	        "treebank contains 4,493 trees, 56,399 tokens."
336	    ),
337	    "fo_farpahc": (
338	        "UD_Icelandic-FarPaHC is a conversion of the Faroese Parsed Historical"
339	        " Corpus (FarPaHC) to the Universal Dependencies scheme. The "
340	        "conversion was done using UDConverter."
341	    ),
342	    "fo_oft": "This is a treebank of Faroese based on the Faroese Wikipedia.",
343	    "fi_ftb": (
344	        "FinnTreeBank 1 consists of manually annotated grammatical examples "
345	        "from VISK. The UD version of FinnTreeBank 1 was converted from a "
346	        "native annotation model with a script and later manually revised."
347	    ),
348	    "fi_ood": (
349	        "Finnish-OOD is an external out-of-domain test set for Finnish-TDT "
350	        "annotated natively into UD scheme."
351	    ),
352	    "fi_pud": (
353	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
354	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
355	        "Raw Text to Universal Dependencies."
356	    ),
357	    "fi_tdt": (
358	        "UD_Finnish-TDT is based on the Turku Dependency Treebank (TDT), a "
359	        "broad-coverage dependency treebank of general Finnish covering "
360	        "numerous genres. The conversion to UD was followed by extensive "
361	        "manual checks and corrections, and the treebank closely adheres to "
362	        "the UD guidelines."
363	    ),
364	    "fr_fqb": (
365	        "The corpus **UD_French-FQB** is an automatic conversion of the French"
366	        " QuestionBank v1, a corpus entirely made of questions."
367	    ),
368	    "fr_ftb": (
369	        "The Universal Dependency version of the French Treebank (Abeill et "
370	        "al., 2003), hereafter UD_French-FTB, is a treebank of sentences from "
371	        "the newspaper Le Monde, initially manually annotated with "
372	        "morphological information and phrase-structure and then converted to "
373	        "the Universal Dependencies annotation scheme."
374	    ),
375	    "fr_gsd": (
376	        "The **UD_French-GSD** was converted in 2015 from the content head "
377	        "version of the universal dependency treebank v2.0 "
378	        "(https://github.com/ryanmcd/uni-dep-tb). It is updated since 2015 "
379	        "independently from the previous source."
380	    ),
381	    "fr_partut": (
382	        "UD_French-ParTUT is a conversion of a multilingual parallel treebank "
383	        "developed at the University of Turin, and consisting of a variety of "
384	        "text genres, including talks, legal texts and Wikipedia articles, "
385	        "among others."
386	    ),
387	    "fr_rhapsodie": "A Universal Dependencies corpus for spoken French.",
388	    "fr_parisstories": (
389	        "Paris Stories is a corpus of oral French collected and transcribed by"
390	        " Linguistics students from Sorbonne Nouvelle and corrected by "
391	        "students from the Plurital Master's Degree of Computational "
392	        "Linguistics ( Inalco, Paris Nanterre, Sorbonne Nouvelle) between 2017"
393	        " and 2021. It contains monologues and dialogues from speakers living "
394	        "in the Parisian region."
395	    ),
396	    "fr_pud": (
397	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
398	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
399	        "Raw Text to Universal Dependencies."
400	    ),
401	    "fr_sequoia": (
402	        "UD_French-Sequoia is an automatic conversion of the Sequoia Treebank "
403	        "corpus French Sequoia corpus."
404	    ),
405	    "gl_ctg": (
406	        "The Galician UD treebank is based on the automatic parsing of the "
407	        "Galician Technical Corpus (http://sli.uvigo.gal/CTG) created at the "
408	        "University of Vigo by the the TALG NLP research group."
409	    ),
410	    "gl_treegal": (
411	        "The Galician-TreeGal is a treebank for Galician developed at LyS "
412	        "Group (Universidade da Corua)."
413	    ),
414	    "de_gsd": (
415	        "The German UD is converted from the content head version of the "
416	        "universal dependency treebank v2.0 (legacy)."
417	    ),
418	    "de_hdt": (
419	        "UD German-HDT is a conversion of the Hamburg Dependency Treebank, "
420	        "created at the University of Hamburg through manual annotation in "
421	        "conjunction with a standard for morphologically and syntactically "
422	        "annotating sentences as well as a constraint-based parser."
423	    ),
424	    "de_lit": (
425	        "This treebank aims at gathering texts of the German literary history."
426	        " Currently, it hosts Fragments of the early Romanticism, i.e. "
427	        "aphorism-like texts mainly dealing with philosophical issues "
428	        "concerning art, beauty and related topics."
429	    ),
430	    "de_pud": (
431	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
432	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
433	        "Raw Text to Universal Dependencies."
434	    ),
435	    "got_proiel": (
436	        "The UD Gothic treebank is based on the Gothic data from the PROIEL "
437	        "treebank, and consists of Wulfila's Bible translation."
438	    ),
439	    "el_gdt": (
440	        "The Greek UD treebank (UD_Greek-GDT) is derived from the Greek "
441	        "Dependency Treebank (http://gdt.ilsp.gr), a resource developed and "
442	        "maintained by researchers at the Institute for Language and Speech "
443	        "Processing/Athena R.C. (http://www.ilsp.gr)."
444	    ),
445	    "gub_tudet": (
446	        "UD_Guajajara-TuDeT is a collection of annotated sentences in "
447	        "Guajajara. Sentences stem from multiple sources such as descriptions "
448	        "of the language, short stories, dictionaries and translations from "
449	        "the New Testament. Sentence annotation and documentation by Lorena "
450	        "Martn Rodrguez and Fabrcio Ferraz Gerardi."
451	    ),
452	    "gn_oldtudet": (
453	        "UD_Guarani-OldTuDeT is a collection of annotated texts in Old "
454	        "Guaran. All known sources in this language are being annotated: "
455	        "cathesisms, grammars (seventeenth and eighteenth century), sentences "
456	        "from dictionaries, and other texts. Sentence annotation and "
457	        "documentation by Fabrcio Ferraz Gerardi and Lorena Martn Rodrguez."
458	    ),
459	    "he_htb": "A Universal Dependencies Corpus for Hebrew.",
460	    "he_iahltwiki": (
461	        "Publicly available subset of the IAHLT UD Hebrew Treebank's Wikipedia"
462	        " section (https://www.iahlt.org/)"
463	    ),
464	    "qfn_fame": (
465	        "UD_Frisian_Dutch-Fame is a selection of 400 sentences from the FAME! "
466	        "speech corpus by Yilmaz et al. (2016a, 2016b). The treebank is "
467	        "manually annotated using the UD scheme."
468	    ),
469	    "qhe_hiencs": (
470	        "The Hindi-English Code-switching treebank is based on code-switching "
471	        "tweets of Hindi and English multilingual speakers (mostly Indian) on "
472	        "Twitter. The treebank is manually annotated using UD sceheme. The "
473	        "training and evaluations sets were seperately annotated by different "
474	        "annotators using UD v2 and v1 guidelines respectively. The evaluation"
475	        " sets are automatically converted from UD v1 to v2."
476	    ),
477	    "hi_hdtb": (
478	        "The Hindi UD treebank is based on the Hindi Dependency Treebank "
479	        "(HDTB), created at IIIT Hyderabad, India."
480	    ),
481	    "hi_pud": (
482	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
483	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
484	        "Raw Text to Universal Dependencies."
485	    ),
486	    "hu_szeged": (
487	        "The Hungarian UD treebank is derived from the Szeged Dependency "
488	        "Treebank (Vincze et al. 2010)."
489	    ),
490	    "is_modern": (
491	        "UD_Icelandic-Modern is a conversion of the modern additions to the "
492	        "Icelandic Parsed Historical Corpus (IcePaHC) to the Universal "
493	        "Dependencies scheme."
494	    ),
495	    "is_icepahc": (
496	        "UD_Icelandic-IcePaHC is a conversion of the Icelandic Parsed "
497	        "Historical Corpus (IcePaHC) to the Universal Dependencies scheme. The"
498	        " conversion was done using UDConverter."
499	    ),
500	    "is_pud": (
501	        "Icelandic-PUD is the Icelandic part of the Parallel Universal "
502	        "Dependencies (PUD) treebanks."
503	    ),
504	    "id_csui": (
505	        "UD Indonesian-CSUI is a conversion from an Indonesian constituency "
506	        "treebank in the Penn Treebank format named Kethu that was also a "
507	        "conversion from a constituency treebank built by Dinakaramani et al. "
508	        "(2015). We named this treebank Indonesian-CSUI, since all the three "
509	        "versions of the treebanks were built at Faculty of Computer Science, "
510	        "Universitas Indonesia."
511	    ),
512	    "id_gsd": (
513	        "The Indonesian UD is converted from the content head version of the "
514	        "universal dependency treebank v2.0 (legacy)."
515	    ),
516	    "id_pud": (
517	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
518	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
519	        "Raw Text to Universal Dependencies."
520	    ),
521	    "ga_idt": (
522	        "A Universal Dependencies 4910-sentence treebank for modern Irish."
523	    ),
524	    "ga_twittirish": (
525	        "A Universal Dependencies treebank of 866 tweets in modern Irish."
526	    ),
527	    "it_isdt": (
528	        "The Italian corpus annotated according to the UD annotation scheme "
529	        "was obtained by conversion from ISDT (Italian Stanford Dependency "
530	        "Treebank), released for the dependency parsing shared task of "
531	        "Evalita-2014 (Bosco et al. 2014)."
532	    ),
533	    "it_partut": (
534	        "UD_Italian-ParTUT is a conversion of a multilingual parallel treebank"
535	        " developed at the University of Turin, and consisting of a variety of"
536	        " text genres, including talks, legal texts and Wikipedia articles, "
537	        "among others."
538	    ),
539	    "it_postwita": (
540	        "PoSTWITA-UD is a collection of Italian tweets annotated in Universal "
541	        "Dependencies that can be exploited for the training of NLP systems to"
542	        " enhance their performance on social media texts."
543	    ),
544	    "it_markit": (
545	        "It is MarkIT That is New: An Italian Treebank of Marked "
546	        "Constructions. Teresa Paccosi, Alessio Palmero Aprosio and Sara "
547	        "Tonelli, To appear in Proceedings of the Eighth Italian Conference on"
548	        " Computational Linguistics 2022 (CLIC-it 2021)"
549	    ),
550	    "it_valico": (
551	        "Manually corrected Treebank of Learner Italian drawn from the Valico "
552	        "corpus and correspondent corrected sentences."
553	    ),
554	    "it_pud": (
555	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
556	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
557	        "Raw Text to Universal Dependencies."
558	    ),
559	    "it_twittiro": (
560	        "TWITTIR-UD is a collection of ironic Italian tweets annotated in "
561	        "Universal Dependencies. The treebank can be exploited for the "
562	        "training of NLP systems to enhance their performance on social media "
563	        "texts, and in particular, for irony detection purposes."
564	    ),
565	    "it_vit": (
566	        "The UD_Italian-VIT corpus was obtained by conversion from VIT (Venice"
567	        " Italian Treebank), developed at the Laboratory of Computational "
568	        "Linguistics of the Universit Ca' Foscari in Venice (Delmonte et al. "
569	        "2007; Delmonte 2009; "
570	        "http://rondelmo.it/resource/VIT/Browser-VIT/index.htm)."
571	    ),
572	    "ja_pudluw": (
573	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
574	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
575	        "Raw Text to Universal Dependencies."
576	    ),
577	    "ja_bccwjluw": (
578	        "This Universal Dependencies (UD) Japanese treebank is based on the "
579	        "definition of UD Japanese convention described in the UD "
580	        "documentation. The original sentences are from `Balanced Corpus of "
581	        "Contemporary Written Japanese'(BCCWJ)."
582	    ),
583	    "ja_gsdluw": (
584	        "This Universal Dependencies (UD) Japanese treebank is based on the "
585	        "definition of UD Japanese convention described in the UD "
586	        "documentation. The original sentences are from Google UDT 2.0."
587	    ),
588	    "ja_bccwj": (
589	        "This Universal Dependencies (UD) Japanese treebank is based on the "
590	        "definition of UD Japanese convention described in the UD "
591	        "documentation. The original sentences are from `Balanced Corpus of "
592	        "Contemporary Written Japanese'(BCCWJ)."
593	    ),
594	    "ja_gsd": (
595	        "This Universal Dependencies (UD) Japanese treebank is based on the "
596	        "definition of UD Japanese convention described in the UD "
597	        "documentation.  The original sentences are from Google UDT 2.0."
598	    ),
599	    "ja_modern": (
600	        "This Universal Dependencies (UD) Japanese treebank is based on the "
601	        "definition of UD Japanese convention described in the UD "
602	        "documentation. The original sentences are from `Corpus of Historical "
603	        "Japanese' (CHJ)."
604	    ),
605	    "ja_pud": (
606	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
607	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
608	        " Raw Text to Universal "
609	        "Dependencies](http://universaldependencies.org/conll17/)."
610	    ),
611	    "jv_csui": (
612	        "UD Javanese-CSUI is a dependency treebank in Javanese, a regional "
613	        "language in Indonesia with more than 60 million users. The original "
614	        "sentences were taken from OPUS, especially from the WikiMatrix v1 "
615	        "corpus. We revised the sentences that contained more Indonesian words"
616	        " than Javanese words and manually annotated them."
617	    ),
618	    "urb_tudet": (
619	        "UD_Kaapor-TuDeT is a collection of annotated sentences in Ka'apor. "
620	        "The project is a work in progress and the treebank is being updated "
621	        "on a regular basis."
622	    ),
623	    "xnr_kdtb": (
624	        "The Kangri UD Treebank (KDTB) is a part of the Universal Dependency "
625	        "treebank project."
626	    ),
627	    "krl_kkpp": (
628	        "UD Karelian-KKPP is a manually annotated new corpus of Karelian made "
629	        "in Universal dependencies annotation scheme. The data is collected "
630	        "from VepKar corpora and consists of mostly modern news texts but also"
631	        " some stories and educational texts."
632	    ),
633	    "kk_ktb": (
634	        "The UD Kazakh treebank is a combination of text from various sources "
635	        "including Wikipedia, some folk tales, sentences from the UDHR, news "
636	        "and phrasebook sentences. Sentences IDs include partial document "
637	        "identifiers."
638	    ),
639	    "arr_tudet": (
640	        "UD_Karo-TuDeT is a collection of annotated sentences in Karo. The "
641	        "sentences stem from the only grammatical description of the language "
642	        "(Gabas, 1999) and from the sentences in the dictionary by the same "
643	        "author (Gabas, 2007). Sentence annotation and documentation by "
644	        "Fabrcio Ferraz Gerardi."
645	    ),
646	    "kfm_aha": (
647	        "The AHA Khunsari Treebank is a small treebank for contemporary "
648	        "Khunsari. Its corpus is collected and annotated manually. We have "
649	        "prepared this treebank based on interviews with Khunsari speakers."
650	    ),
651	    "quc_iu": (
652	        "UD Kiche-IU is a treebank consisting of sentences from a variety of"
653	        " text domains but principally dictionary example sentences and "
654	        "linguistic examples."
655	    ),
656	    "koi_uh": (
657	        "This is a Komi-Permyak literary language treebank consisting of "
658	        "original and translated texts."
659	    ),
660	    "kpv_ikdp": (
661	        "This treebank consists of dialectal transcriptions of spoken "
662	        "Komi-Zyrian. The current texts are short recorded segments from "
663	        "different areas where the Iva dialect of Komi language is spoken."
664	    ),
665	    "kpv_lattice": (
666	        "UD Komi-Zyrian Lattice is a treebank of written standard Komi-Zyrian."
667	    ),
668	    "ko_gsd": (
669	        "The Google Korean Universal Dependency Treebank is first converted "
670	        "from the Universal Dependency Treebank v2.0 (legacy), and then "
671	        "enhanced by Chun et al., 2018."
672	    ),
673	    "ko_kaist": (
674	        "The KAIST Korean Universal Dependency Treebank is generated by Chun "
675	        "et al., 2018 from the constituency trees in the KAIST Tree-Tagging "
676	        "Corpus."
677	    ),
678	    "ko_pud": (
679	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
680	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
681	        "Raw Text to Universal Dependencies."
682	    ),
683	    "kmr_mg": (
684	        "The UD Kurmanji corpus is a corpus of Kurmanji Kurdish. It contains "
685	        "fiction and encyclopaedic texts in roughly equal measure. It has been"
686	        " annotated natively in accordance with the UD annotation scheme."
687	    ),
688	    "la_ittb": (
689	        "Latin data from the _Index Thomisticus_ Treebank. Data are taken from"
690	        " the _Index Thomisticus_ corpus by Roberto Busa SJ, which contains "
691	        "the complete work by Thomas Aquinas (12251274; Medieval Latin) and "
692	        "by 61 other authors related to Thomas."
693	    ),
694	    "la_udante": (
695	        "The UDante treebank is based on the Latin texts of Dante Alighieri, "
696	        "taken from the DanteSearch corpus, originally created at the "
697	        "University of Pisa, Italy. It is a treebank of Latin language, more "
698	        "precisely of literary Medieval Latin (XIVth century)."
699	    ),
700	    "la_llct": (
701	        "This Universal Dependencies version of the LLCT (Late Latin Charter "
702	        "Treebank) consists of an automated conversion of the LLCT2 treebank "
703	        "from the Latin Dependency Treebank (LDT) format into the Universal "
704	        "Dependencies standard."
705	    ),
706	    "la_perseus": (
707	        "This Universal Dependencies Latin Treebank consists of an automatic "
708	        "conversion of a selection of passages from the Ancient Greek and "
709	        "Latin Dependency Treebank 2.1"
710	    ),
711	    "la_proiel": (
712	        "The Latin PROIEL treebank is based on the Latin data from the PROIEL "
713	        "treebank, and contains most of the Vulgate New Testament translations"
714	        " plus selections from Caesar's Gallic War, Cicero's Letters to "
715	        "Atticus, Palladius' Opus Agriculturae and the first book of Cicero's "
716	        "De officiis."
717	    ),
718	    "lv_lvtb": (
719	        "Latvian UD Treebank is based on Latvian Treebank (LVTB), being "
720	        "created at University of Latvia, Institute of Mathematics and "
721	        "Computer Science, Artificial Intelligence Laboratory."
722	    ),
723	    "lij_glt": (
724	        "The Genoese Ligurian Treebank is a small, manually annotated "
725	        "collection of contemporary Ligurian prose. The focus of the treebank "
726	        "is written Genoese, the koin variety of Ligurian which is associated"
727	        " with today's literary, journalistic and academic ligurophone sphere."
728	    ),
729	    "lt_alksnis": (
730	        "The Lithuanian dependency treebank ALKSNIS v3.0 (Vytautas Magnus "
731	        "University)."
732	    ),
733	    "lt_hse": (
734	        "Lithuanian treebank annotated manually (dependencies) using the "
735	        "Morphological Annotator by CCL, Vytautas Magnus University "
736	        "(http://tekstynas.vdu.lt/) and manual disambiguation. A pilot version"
737	        " which includes news and an essay by Tomas Venclova is available "
738	        "here."
739	    ),
740	    "olo_kkpp": (
741	        "UD Livvi-KKPP is a manually annotated new corpus of Livvi-Karelian "
742	        "made directly in the Universal dependencies annotation scheme. The "
743	        "data is collected from VepKar corpora and consists of mostly modern "
744	        "news texts but also some stories and educational texts."
745	    ),
746	    "nds_lsdc": (
747	        "The UD Low Saxon LSDC dataset consists of sentences in 18 Low Saxon "
748	        "dialects from both Germany and the Netherlands. These sentences are "
749	        "(or are to become) part of the LSDC dataset and represent the "
750	        "language from the 19th and early 20th century in genres such as short"
751	        " stories, novels, speeches, letters and fairytales."
752	    ),
753	    "mt_mudt": (
754	        "MUDT (Maltese Universal Dependencies Treebank) is a manually "
755	        "annotated treebank of Maltese, a Semitic language of Malta descended "
756	        "from North African Arabic with a significant amount of Italo-Romance "
757	        "influence. MUDT was designed as a balanced corpus with four major "
758	        "genres (see Splitting below) represented roughly equally."
759	    ),
760	    "gv_cadhan": (
761	        "This is the Cadhan Aonair UD treebank for Manx Gaelic, created by "
762	        "Kevin Scannell."
763	    ),
764	    "mr_ufal": (
765	        "UD Marathi is a manually annotated treebank consisting primarily of "
766	        "stories from Wikisource, and parts of an article on Wikipedia."
767	    ),
768	    "gun_dooley": (
769	        "UD Mbya_Guarani-Dooley is a corpus of narratives written in Mby "
770	        "Guaran (Tupian) in Brazil, and collected by Robert Dooley. Due to "
771	        "copyright restrictions, the corpus that is distributed as part of UD "
772	        "only contains the annotation (tags, features, relations) while the "
773	        "FORM and LEMMA columns are empty."
774	    ),
775	    "gun_thomas": (
776	        "UD Mbya_Guarani-Thomas is a corpus of Mby Guaran (Tupian) texts "
777	        "collected by Guillaume Thomas. The current version of the corpus "
778	        "consists of three speeches by Paulina Kerechu Nez Romero, a Mby "
779	        "Guaran speaker from Ytu, Caazap Department, Paraguay."
780	    ),
781	    "mdf_jr": (
782	        "Erme Universal Dependencies annotated texts Moksha are the origin of "
783	        "UD_Moksha-JR with annotation (CoNLL-U) for texts in the Moksha "
784	        "language, it originally consists of a sample from a number of fiction"
785	        " authors writing originals in Moksha."
786	    ),
787	    "myu_tudet": (
788	        "UD_Munduruku-TuDeT is a collection of annotated sentences in "
789	        "Munduruk. Together with UD_Akuntsu-TuDeT and UD_Tupinamba-TuDeT, "
790	        "UD_Munduruku-TuDeT is part of the TuLaR project."
791	    ),
792	    "pcm_nsc": (
793	        "A Universal Dependencies corpus for spoken Naija (Nigerian Pidgin)."
794	    ),
795	    "nyq_aha": (
796	        "The AHA Nayini Treebank is a small treebank for contemporary Nayini. "
797	        "Its corpus is collected and annotated manually. We have prepared this"
798	        " treebank based on interviews with Nayini speakers."
799	    ),
800	    "sme_giella": (
801	        "This is a North Smi treebank based on a manually disambiguated and "
802	        "function-labelled gold-standard corpus of North Smi produced by the "
803	        "Giellatekno team at UiT Norgga rktala universitehta."
804	    ),
805	    "no_bokmaal": (
806	        "The Norwegian UD treebank is based on the Bokml section of the "
807	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
808	        " Norwegian. NDT has been automatically converted to the UD scheme by "
809	        "Lilja vrelid at the University of Oslo."
810	    ),
811	    "no_nynorsk": (
812	        "The Norwegian UD treebank is based on the Nynorsk section of the "
813	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
814	        " Norwegian.  NDT has been automatically converted to the UD scheme by"
815	        " Lilja vrelid at the University of Oslo."
816	    ),
817	    "no_nynorsklia": (
818	        "This Norwegian treebank is based on the LIA treebank of transcribed "
819	        "spoken Norwegian dialects. The treebank has been automatically "
820	        "converted to the UD scheme by Lilja vrelid at the University of "
821	        "Oslo."
822	    ),
823	    "cu_proiel": (
824	        "The Old Church Slavonic (OCS) UD treebank is based on the Old Church "
825	        "Slavonic data from the PROIEL treebank and contains the text of the "
826	        "Codex Marianus New Testament translation."
827	    ),
828	    "fro_srcmf": (
829	        "UD_Old_French-SRCMF is a conversion of (part of) the SRCMF corpus "
830	        "(Syntactic Reference Corpus of Medieval French srcmf.org)."
831	    ),
832	    "orv_birchbark": (
833	        "UD Old_East_Slavic-Birchbark is based on the RNC Corpus of Birchbark "
834	        "Letters and includes documents written in 1025-1500 in an East Slavic"
835	        " vernacular (letters, household and business records, records for "
836	        "church services, spell against diseases, and other short "
837	        "inscriptions). The treebank is manually syntactically annotated in "
838	        "the UD 2.0 scheme, morphological and lexical annotation is a "
839	        "conversion of the original RNC annotation."
840	    ),
841	    "orv_rnc": (
842	        "`UD_Old_Russian-RNC` is a sample of the Middle Russian corpus "
843	        "(1300-1700), a part of the Russian National Corpus. The data were "
844	        "originally annotated according to the RNC and extended UD-Russian "
845	        "morphological schemas and UD 2.4 dependency schema."
846	    ),
847	    "orv_torot": (
848	        "UD_Old_Russian-TOROT is a conversion of a selection of the Old East "
849	        "Slavonic and Middle Russian data in the Troms Old Russian and OCS "
850	        "Treebank (TOROT), which was originally annotated in PROIEL dependency"
851	        " format."
852	    ),
853	    "otk_tonqq": (
854	        "`UD_Old_Turkish-Tonqq` is an Old Turkish treebank built upon Turkic "
855	        "script texts or sentences that are trivially convertible."
856	    ),
857	    "fa_perdt": (
858	        "The Persian Universal Dependency Treebank (PerUDT) is the result of "
859	        "automatic coversion of Persian Dependency Treebank (PerDT) with "
860	        "extensive manual corrections. Please refer to the follwoing work, if "
861	        "you use this data: Mohammad Sadegh Rasooli, Pegah Safari, Amirsaeid "
862	        "Moloodi, and Alireza Nourian. 'The Persian Dependency Treebank Made "
863	        "Universal'. 2020 (to appear)."
864	    ),
865	    "fa_seraji": (
866	        "The Persian Universal Dependency Treebank (Persian UD) is based on "
867	        "Uppsala Persian Dependency Treebank (UPDT). The conversion of the "
868	        "UPDT to the Universal Dependencies was performed semi-automatically "
869	        "with extensive manual checks and corrections."
870	    ),
871	    "pl_lfg": (
872	        "The LFG Enhanced UD treebank of Polish is based on a corpus of LFG "
873	        "(Lexical Functional Grammar) syntactic structures generated by an LFG"
874	        " grammar of Polish, POLFIE, and manually disambiguated by human "
875	        "annotators."
876	    ),
877	    "pl_pdb": (
878	        "The Polish PDB-UD treebank is based on the Polish Dependency Bank 2.0"
879	        " (PDB 2.0), created at the Institute of Computer Science, Polish "
880	        "Academy of Sciences in Warsaw. The PDB-UD treebank is an extended and"
881	        " corrected version of the Polish SZ-UD treebank (the release 1.2 to "
882	        "2.3)."
883	    ),
884	    "pl_pud": (
885	        "This is the Polish portion of the Parallel Universal Dependencies "
886	        "(PUD) treebanks, created at the Institute of Computer Science, Polish"
887	        " Academy of Sciences in Warsaw.Re"
888	    ),
889	    "pt_bosque": (
890	        "This Universal Dependencies (UD) Portuguese treebank is based on the "
891	        "Constraint Grammar converted version of the Bosque, which is part of "
892	        "the Floresta Sint(c)tica treebank. It contains both European "
893	        "(CETEMPblico) and Brazilian (CETENFolha) variants."
894	    ),
895	    "pt_gsd": (
896	        "The Brazilian Portuguese UD is converted from the Google Universal "
897	        "Dependency Treebank v2.0 (legacy)."
898	    ),
899	    "pt_pud": (
900	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
901	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
902	        "Raw Text to Universal Dependencies."
903	    ),
904	    "ro_art": (
905	        "The UD treebank ArT is a treebank of the Aromanian dialect of the "
906	        "Romanian language in UD format."
907	    ),
908	    "ro_nonstandard": (
909	        "The Romanian Non-standard UD treebank (called UAIC-RoDia) is based on"
910	        " UAIC-RoDia Treebank. UAIC-RoDia = ISLRN 156-635-615-024-0"
911	    ),
912	    "ro_rrt": (
913	        "The Romanian UD treebank (called RoRefTrees) (Barbu Mititelu et al., "
914	        "2016) is the reference treebank in UD format for standard Romanian."
915	    ),
916	    "ro_simonero": "SiMoNERo is a medical corpus of contemporary Romanian.",
917	    "ru_gsd": (
918	        "Russian Universal Dependencies Treebank annotated and converted by "
919	        "Google."
920	    ),
921	    "ru_pud": (
922	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
923	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
924	        "Raw Text to Universal Dependencies."
925	    ),
926	    "ru_syntagrus": "Russian data from the SynTagRus corpus.",
927	    "ru_taiga": (
928	        "Universal Dependencies treebank is based on data samples extracted "
929	        "from Taiga Corpus and MorphoRuEval-2017 and GramEval-2020 shared "
930	        "tasks collections."
931	    ),
932	    "sa_ufal": (
933	        "A small Sanskrit treebank of sentences from Pacatantra, an ancient "
934	        "Indian collection of interrelated fables by Vishnu Sharma."
935	    ),
936	    "sa_vedic": (
937	        "The Treebank of Vedic Sanskrit contains 4,000 sentences with 27,000 "
938	        "words chosen from metrical and prose passages of the gveda (RV), the"
939	        " aunaka recension of the Atharvaveda (S), the Maitryasahit "
940	        "(MS), and the Aitareya- (AB) and atapatha-Brhmaas (B). Lexical "
941	        "and morpho-syntactic information has been generated using a tagging "
942	        "software and manually validated. POS tags have been induced "
943	        "automatically from the morpho-sytactic information of each word."
944	    ),
945	    "gd_arcosg": (
946	        "A treebank of Scottish Gaelic based on the Annotated Reference Corpus"
947	        " Of Scottish Gaelic (ARCOSG)."
948	    ),
949	    "sr_set": (
950	        "The Serbian UD treebank is based on the "
951	        "[SETimes-SR](http://hdl.handle.net/11356/1200) corpus and additional "
952	        "news documents from the Serbian web."
953	    ),
954	    "sms_giellagas": (
955	        "The UD Skolt Sami Giellagas treebank is based almost entirely on "
956	        "spoken Skolt Sami corpora."
957	    ),
958	    "sk_snk": (
959	        "The Slovak UD treebank is based on data originally annotated as part "
960	        "of the Slovak National Corpus, following the annotation style of the "
961	        "Prague Dependency Treebank."
962	    ),
963	    "sl_ssj": (
964	        "The Slovenian UD Treebank is a rule-based conversion of the ssj500k "
965	        "treebank, the largest collection of manually syntactically annotated "
966	        "data in Slovenian, originally annotated in the JOS annotation scheme."
967	    ),
968	    "sl_sst": (
969	        "The Spoken Slovenian UD Treebank (SST) is the first syntactically "
970	        "annotated corpus of spoken Slovenian, based on a sample of the "
971	        "reference GOS corpus, a collection of transcribed audio recordings of"
972	        " monologic, dialogic and multi-party spontaneous speech in different "
973	        "everyday situations."
974	    ),
975	    "soj_aha": (
976	        "The AHA Soi Treebank is a small treebank for contemporary Soi. Its "
977	        "corpus is collected and annotated manually. We have prepared this "
978	        "treebank based on interviews with Soi speakers."
979	    ),
980	    "ajp_madar": (
981	        "The South_Levantine_Arabic-MADAR treebank consists of 100 "
982	        "manually-annotated sentences taken from the "
983	        "[MADAR](https://camel.abudhabi.nyu.edu/madar/) (Multi-Arabic Dialect "
984	        "Applications and Resources) project. "
985	    ),
986	    "es_ancora": "Spanish data from the AnCora corpus.",
987	    "es_gsd": (
988	        "The Spanish UD is converted from the content head version of the "
989	        "universal dependency treebank v2.0 (legacy)."
990	    ),
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {
1217	        "test": "UD_Arabic-PUD/r2.10/ar_pud-ud-test.conllu",
1218	    },
1219	    "hy_armtdp": {
1220	        "train": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-train.conllu",
1221	        "dev": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-dev.conllu",
1222	        "test": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-test.conllu",
1223	    },
1224	    "aii_as": {
1225	        "test": "UD_Assyrian-AS/r2.10/aii_as-ud-test.conllu",
1226	    },
1227	    "bm_crb": {
1228	        "test": "UD_Bambara-CRB/r2.10/bm_crb-ud-test.conllu",
1229	    },
1230	    "eu_bdt": {
1231	        "train": "UD_Basque-BDT/r2.10/eu_bdt-ud-train.conllu",
1232	        "dev": "UD_Basque-BDT/r2.10/eu_bdt-ud-dev.conllu",
1233	        "test": "UD_Basque-BDT/r2.10/eu_bdt-ud-test.conllu",
1234	    },
1235	    "bej_nsc": {
1236	        "test": "UD_Beja-NSC/r2.10/bej_nsc-ud-test.conllu",
1237	    },
1238	    "be_hse": {
1239	        "train": "UD_Belarusian-HSE/r2.10/be_hse-ud-train.conllu",
1240	        "dev": "UD_Belarusian-HSE/r2.10/be_hse-ud-dev.conllu",
1241	        "test": "UD_Belarusian-HSE/r2.10/be_hse-ud-test.conllu",
1242	    },
1243	    "bn_bru": {
1244	        "test": "UD_Bengali-BRU/r2.10/bn_bru-ud-test.conllu",
1245	    },
1246	    "bho_bhtb": {
1247	        "test": "UD_Bhojpuri-BHTB/r2.10/bho_bhtb-ud-test.conllu",
1248	    },
1249	    "br_keb": {
1250	        "test": "UD_Breton-KEB/r2.10/br_keb-ud-test.conllu",
1251	    },
1252	    "bg_btb": {
1253	        "train": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-train.conllu",
1254	        "dev": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-dev.conllu",
1255	        "test": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-test.conllu",
1256	    },
1257	    "bxr_bdt": {
1258	        "train": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-train.conllu",
1259	        "test": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-test.conllu",
1260	    },
1261	    "yue_hk": {
1262	        "test": "UD_Cantonese-HK/r2.10/yue_hk-ud-test.conllu",
1263	    },
1264	    "ca_ancora": {
1265	        "train": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-train.conllu",
1266	        "dev": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-dev.conllu",
1267	        "test": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-test.conllu",
1268	    },
1269	    "ceb_gja": {
1270	        "test": "UD_Cebuano-GJA/r2.10/ceb_gja-ud-test.conllu",
1271	    },
1272	    "zh_cfl": {
1273	        "test": "UD_Chinese-CFL/r2.10/zh_cfl-ud-test.conllu",
1274	    },
1275	    "zh_gsd": {
1276	        "train": "UD_Chinese-GSD/r2.10/zh_gsd-ud-train.conllu",
1277	        "dev": "UD_Chinese-GSD/r2.10/zh_gsd-ud-dev.conllu",
1278	        "test": "UD_Chinese-GSD/r2.10/zh_gsd-ud-test.conllu",
1279	    },
1280	    "zh_gsdsimp": {
1281	        "train": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-train.conllu",
1282	        "dev": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-dev.conllu",
1283	        "test": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-test.conllu",
1284	    },
1285	    "zh_hk": {
1286	        "test": "UD_Chinese-HK/r2.10/zh_hk-ud-test.conllu",
1287	    },
1288	    "zh_pud": {
1289	        "test": "UD_Chinese-PUD/r2.10/zh_pud-ud-test.conllu",
1290	    },
1291	    "ckt_hse": {
1292	        "test": "UD_Chukchi-HSE/r2.10/ckt_hse-ud-test.conllu",
1293	    },
1294	    "lzh_kyoto": {
1295	        "train": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-train.conllu",
1296	        "dev": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-dev.conllu",
1297	        "test": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-test.conllu",
1298	    },
1299	    "cop_scriptorium": {
1300	        "train": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-train.conllu",
1301	        "dev": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-dev.conllu",
1302	        "test": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-test.conllu",
1303	    },
1304	    "hr_set": {
1305	        "train": "UD_Croatian-SET/r2.10/hr_set-ud-train.conllu",
1306	        "dev": "UD_Croatian-SET/r2.10/hr_set-ud-dev.conllu",
1307	        "test": "UD_Croatian-SET/r2.10/hr_set-ud-test.conllu",
1308	    },
1309	    "cs_cac": {
1310	        "train": "UD_Czech-CAC/r2.10/cs_cac-ud-train.conllu",
1311	        "dev": "UD_Czech-CAC/r2.10/cs_cac-ud-dev.conllu",
1312	        "test": "UD_Czech-CAC/r2.10/cs_cac-ud-test.conllu",
1313	    },
1314	    "cs_cltt": {
1315	        "train": "UD_Czech-CLTT/r2.10/cs_cltt-ud-train.conllu",
1316	        "dev": "UD_Czech-CLTT/r2.10/cs_cltt-ud-dev.conllu",
1317	        "test": "UD_Czech-CLTT/r2.10/cs_cltt-ud-test.conllu",
1318	    },
1319	    "cs_fictree": {
1320	        "train": "UD_Czech-FicTree/r2.10/cs_fictree-ud-train.conllu",
1321	        "dev": "UD_Czech-FicTree/r2.10/cs_fictree-ud-dev.conllu",
1322	        "test": "UD_Czech-FicTree/r2.10/cs_fictree-ud-test.conllu",
1323	    },
1324	    "cs_pdt": {
1325	        "train": [
1326	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-l.conllu",
1327	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-m.conllu",
1328	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-c.conllu",
1329	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-v.conllu",
1330	        ],
1331	        "dev": "UD_Czech-PDT/r2.10/cs_pdt-ud-dev.conllu",
1332	        "test": "UD_Czech-PDT/r2.10/cs_pdt-ud-test.conllu",
1333	    },
1334	    "cs_pud": {
1335	        "test": "UD_Czech-PUD/r2.10/cs_pud-ud-test.conllu",
1336	    },
1337	    "da_ddt": {
1338	        "train": "UD_Danish-DDT/r2.10/da_ddt-ud-train.conllu",
1339	        "dev": "UD_Danish-DDT/r2.10/da_ddt-ud-dev.conllu",
1340	        "test": "UD_Danish-DDT/r2.10/da_ddt-ud-test.conllu",
1341	    },
1342	    "nl_alpino": {
1343	        "train": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-train.conllu",
1344	        "dev": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-dev.conllu",
1345	        "test": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-test.conllu",
1346	    },
1347	    "nl_lassysmall": {
1348	        "train": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-train.conllu",
1349	        "dev": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-dev.conllu",
1350	        "test": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-test.conllu",
1351	    },
1352	    "en_atis": {
1353	        "train": "UD_English-Atis/r2.10/en_atis-ud-train.conllu",
1354	        "dev": "UD_English-Atis/r2.10/en_atis-ud-dev.conllu",
1355	        "test": "UD_English-Atis/r2.10/en_atis-ud-test.conllu",
1356	    },
1357	    "en_esl": {
1358	        "train": "UD_English-ESL/r2.10/en_esl-ud-train.conllu",
1359	        "dev": "UD_English-ESL/r2.10/en_esl-ud-dev.conllu",
1360	        "test": "UD_English-ESL/r2.10/en_esl-ud-test.conllu",
1361	    },
1362	    "en_ewt": {
1363	        "train": "UD_English-EWT/r2.10/en_ewt-ud-train.conllu",
1364	        "dev": "UD_English-EWT/r2.10/en_ewt-ud-dev.conllu",
1365	        "test": "UD_English-EWT/r2.10/en_ewt-ud-test.conllu",
1366	    },
1367	    "en_gum": {
1368	        "train": "UD_English-GUM/r2.10/en_gum-ud-train.conllu",
1369	        "dev": "UD_English-GUM/r2.10/en_gum-ud-dev.conllu",
1370	        "test": "UD_English-GUM/r2.10/en_gum-ud-test.conllu",
1371	    },
1372	    "en_gumreddit": {
1373	        "train": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-train.conllu",
1374	        "dev": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-dev.conllu",
1375	        "test": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-test.conllu",
1376	    },
1377	    "en_lines": {
1378	        "train": "UD_English-LinES/r2.10/en_lines-ud-train.conllu",
1379	        "dev": "UD_English-LinES/r2.10/en_lines-ud-dev.conllu",
1380	        "test": "UD_English-LinES/r2.10/en_lines-ud-test.conllu",
1381	    },
1382	    "en_partut": {
1383	        "train": "UD_English-ParTUT/r2.10/en_partut-ud-train.conllu",
1384	        "dev": "UD_English-ParTUT/r2.10/en_partut-ud-dev.conllu",
1385	        "test": "UD_English-ParTUT/r2.10/en_partut-ud-test.conllu",
1386	    },
1387	    "en_pronouns": {
1388	        "test": "UD_English-Pronouns/r2.10/en_pronouns-ud-test.conllu",
1389	    },
1390	    "en_pud": {
1391	        "test": "UD_English-PUD/r2.10/en_pud-ud-test.conllu",
1392	    },
1393	    "myv_jr": {
1394	        "test": "UD_Erzya-JR/r2.10/myv_jr-ud-test.conllu",
1395	    },
1396	    "et_edt": {
1397	        "train": "UD_Estonian-EDT/r2.10/et_edt-ud-train.conllu",
1398	        "dev": "UD_Estonian-EDT/r2.10/et_edt-ud-dev.conllu",
1399	        "test": "UD_Estonian-EDT/r2.10/et_edt-ud-test.conllu",
1400	    },
1401	    "et_ewt": {
1402	        "train": "UD_Estonian-EWT/r2.10/et_ewt-ud-train.conllu",
1403	        "dev": "UD_Estonian-EWT/r2.10/et_ewt-ud-dev.conllu",
1404	        "test": "UD_Estonian-EWT/r2.10/et_ewt-ud-test.conllu",
1405	    },
1406	    "fo_farpahc": {
1407	        "train": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-train.conllu",
1408	        "dev": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-dev.conllu",
1409	        "test": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-test.conllu",
1410	    },
1411	    "fo_oft": {
1412	        "test": "UD_Faroese-OFT/r2.10/fo_oft-ud-test.conllu",
1413	    },
1414	    "fi_ftb": {
1415	        "train": "UD_Finnish-FTB/r2.10/fi_ftb-ud-train.conllu",
1416	        "dev": "UD_Finnish-FTB/r2.10/fi_ftb-ud-dev.conllu",
1417	        "test": "UD_Finnish-FTB/r2.10/fi_ftb-ud-test.conllu",
1418	    },
1419	    "fi_ood": {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:316
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
315	    "en_pud": (
316	        "This is the English portion of the Parallel Universal Dependencies "
317	        "(PUD) treebanks created for the CoNLL 2017 shared task on "
318	        "Multilingual Parsing from Raw Text to Universal Dependencies "
319	        "(http://universaldependencies.org/conll17/)."
320	    ),
321	    "myv_jr": (
322	        "UD Erzya is the original annotation (CoNLL-U) for texts in the Erzya "
323	        "language, it originally consists of a sample from a number of fiction"
324	        " authors writing originals in Erzya."
325	    ),
326	    "et_edt": (
327	        "UD Estonian is a converted version of the Estonian Dependency "
328	        "Treebank (EDT), originally annotated in the Constraint Grammar (CG) "
329	        "annotation scheme, and consisting of genres of fiction, newspaper "
330	        "texts and scientific texts. The treebank contains 30,972 trees, "
331	        "437,769 tokens."
332	    ),
333	    "et_ewt": (
334	        "UD EWT treebank consists of different genres of new media. The "
335	        "treebank contains 4,493 trees, 56,399 tokens."
336	    ),
337	    "fo_farpahc": (
338	        "UD_Icelandic-FarPaHC is a conversion of the Faroese Parsed Historical"
339	        " Corpus (FarPaHC) to the Universal Dependencies scheme. The "
340	        "conversion was done using UDConverter."
341	    ),
342	    "fo_oft": "This is a treebank of Faroese based on the Faroese Wikipedia.",
343	    "fi_ftb": (
344	        "FinnTreeBank 1 consists of manually annotated grammatical examples "
345	        "from VISK. The UD version of FinnTreeBank 1 was converted from a "
346	        "native annotation model with a script and later manually revised."
347	    ),
348	    "fi_ood": (
349	        "Finnish-OOD is an external out-of-domain test set for Finnish-TDT "
350	        "annotated natively into UD scheme."
351	    ),
352	    "fi_pud": (
353	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
354	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
355	        "Raw Text to Universal Dependencies."
356	    ),
357	    "fi_tdt": (
358	        "UD_Finnish-TDT is based on the Turku Dependency Treebank (TDT), a "
359	        "broad-coverage dependency treebank of general Finnish covering "
360	        "numerous genres. The conversion to UD was followed by extensive "
361	        "manual checks and corrections, and the treebank closely adheres to "
362	        "the UD guidelines."
363	    ),
364	    "fr_fqb": (
365	        "The corpus **UD_French-FQB** is an automatic conversion of the French"
366	        " QuestionBank v1, a corpus entirely made of questions."
367	    ),
368	    "fr_ftb": (
369	        "The Universal Dependency version of the French Treebank (Abeill et "
370	        "al., 2003), hereafter UD_French-FTB, is a treebank of sentences from "
371	        "the newspaper Le Monde, initially manually annotated with "
372	        "morphological information and phrase-structure and then converted to "
373	        "the Universal Dependencies annotation scheme."
374	    ),
375	    "fr_gsd": (
376	        "The **UD_French-GSD** was converted in 2015 from the content head "
377	        "version of the universal dependency treebank v2.0 "
378	        "(https://github.com/ryanmcd/uni-dep-tb). It is updated since 2015 "
379	        "independently from the previous source."
380	    ),
381	    "fr_partut": (
382	        "UD_French-ParTUT is a conversion of a multilingual parallel treebank "
383	        "developed at the University of Turin, and consisting of a variety of "
384	        "text genres, including talks, legal texts and Wikipedia articles, "
385	        "among others."
386	    ),
387	    "fr_rhapsodie": "A Universal Dependencies corpus for spoken French.",
388	    "fr_parisstories": (
389	        "Paris Stories is a corpus of oral French collected and transcribed by"
390	        " Linguistics students from Sorbonne Nouvelle and corrected by "
391	        "students from the Plurital Master's Degree of Computational "
392	        "Linguistics ( Inalco, Paris Nanterre, Sorbonne Nouvelle) between 2017"
393	        " and 2021. It contains monologues and dialogues from speakers living "
394	        "in the Parisian region."
395	    ),
396	    "fr_pud": (
397	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
398	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
399	        "Raw Text to Universal Dependencies."
400	    ),
401	    "fr_sequoia": (
402	        "UD_French-Sequoia is an automatic conversion of the Sequoia Treebank "
403	        "corpus French Sequoia corpus."
404	    ),
405	    "gl_ctg": (
406	        "The Galician UD treebank is based on the automatic parsing of the "
407	        "Galician Technical Corpus (http://sli.uvigo.gal/CTG) created at the "
408	        "University of Vigo by the the TALG NLP research group."
409	    ),
410	    "gl_treegal": (
411	        "The Galician-TreeGal is a treebank for Galician developed at LyS "
412	        "Group (Universidade da Corua)."
413	    ),
414	    "de_gsd": (
415	        "The German UD is converted from the content head version of the "
416	        "universal dependency treebank v2.0 (legacy)."
417	    ),
418	    "de_hdt": (
419	        "UD German-HDT is a conversion of the Hamburg Dependency Treebank, "
420	        "created at the University of Hamburg through manual annotation in "
421	        "conjunction with a standard for morphologically and syntactically "
422	        "annotating sentences as well as a constraint-based parser."
423	    ),
424	    "de_lit": (
425	        "This treebank aims at gathering texts of the German literary history."
426	        " Currently, it hosts Fragments of the early Romanticism, i.e. "
427	        "aphorism-like texts mainly dealing with philosophical issues "
428	        "concerning art, beauty and related topics."
429	    ),
430	    "de_pud": (
431	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
432	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
433	        "Raw Text to Universal Dependencies."
434	    ),
435	    "got_proiel": (
436	        "The UD Gothic treebank is based on the Gothic data from the PROIEL "
437	        "treebank, and consists of Wulfila's Bible translation."
438	    ),
439	    "el_gdt": (
440	        "The Greek UD treebank (UD_Greek-GDT) is derived from the Greek "
441	        "Dependency Treebank (http://gdt.ilsp.gr), a resource developed and "
442	        "maintained by researchers at the Institute for Language and Speech "
443	        "Processing/Athena R.C. (http://www.ilsp.gr)."
444	    ),
445	    "gub_tudet": (
446	        "UD_Guajajara-TuDeT is a collection of annotated sentences in "
447	        "Guajajara. Sentences stem from multiple sources such as descriptions "
448	        "of the language, short stories, dictionaries and translations from "
449	        "the New Testament. Sentence annotation and documentation by Lorena "
450	        "Martn Rodrguez and Fabrcio Ferraz Gerardi."
451	    ),
452	    "gn_oldtudet": (
453	        "UD_Guarani-OldTuDeT is a collection of annotated texts in Old "
454	        "Guaran. All known sources in this language are being annotated: "
455	        "cathesisms, grammars (seventeenth and eighteenth century), sentences "
456	        "from dictionaries, and other texts. Sentence annotation and "
457	        "documentation by Fabrcio Ferraz Gerardi and Lorena Martn Rodrguez."
458	    ),
459	    "he_htb": "A Universal Dependencies Corpus for Hebrew.",
460	    "he_iahltwiki": (
461	        "Publicly available subset of the IAHLT UD Hebrew Treebank's Wikipedia"
462	        " section (https://www.iahlt.org/)"
463	    ),
464	    "qfn_fame": (
465	        "UD_Frisian_Dutch-Fame is a selection of 400 sentences from the FAME! "
466	        "speech corpus by Yilmaz et al. (2016a, 2016b). The treebank is "
467	        "manually annotated using the UD scheme."
468	    ),
469	    "qhe_hiencs": (
470	        "The Hindi-English Code-switching treebank is based on code-switching "
471	        "tweets of Hindi and English multilingual speakers (mostly Indian) on "
472	        "Twitter. The treebank is manually annotated using UD sceheme. The "
473	        "training and evaluations sets were seperately annotated by different "
474	        "annotators using UD v2 and v1 guidelines respectively. The evaluation"
475	        " sets are automatically converted from UD v1 to v2."
476	    ),
477	    "hi_hdtb": (
478	        "The Hindi UD treebank is based on the Hindi Dependency Treebank "
479	        "(HDTB), created at IIIT Hyderabad, India."
480	    ),
481	    "hi_pud": (
482	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
483	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
484	        "Raw Text to Universal Dependencies."
485	    ),
486	    "hu_szeged": (
487	        "The Hungarian UD treebank is derived from the Szeged Dependency "
488	        "Treebank (Vincze et al. 2010)."
489	    ),
490	    "is_modern": (
491	        "UD_Icelandic-Modern is a conversion of the modern additions to the "
492	        "Icelandic Parsed Historical Corpus (IcePaHC) to the Universal "
493	        "Dependencies scheme."
494	    ),
495	    "is_icepahc": (
496	        "UD_Icelandic-IcePaHC is a conversion of the Icelandic Parsed "
497	        "Historical Corpus (IcePaHC) to the Universal Dependencies scheme. The"
498	        " conversion was done using UDConverter."
499	    ),
500	    "is_pud": (
501	        "Icelandic-PUD is the Icelandic part of the Parallel Universal "
502	        "Dependencies (PUD) treebanks."
503	    ),
504	    "id_csui": (
505	        "UD Indonesian-CSUI is a conversion from an Indonesian constituency "
506	        "treebank in the Penn Treebank format named Kethu that was also a "
507	        "conversion from a constituency treebank built by Dinakaramani et al. "
508	        "(2015). We named this treebank Indonesian-CSUI, since all the three "
509	        "versions of the treebanks were built at Faculty of Computer Science, "
510	        "Universitas Indonesia."
511	    ),
512	    "id_gsd": (
513	        "The Indonesian UD is converted from the content head version of the "
514	        "universal dependency treebank v2.0 (legacy)."
515	    ),
516	    "id_pud": (
517	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
518	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
519	        "Raw Text to Universal Dependencies."
520	    ),
521	    "ga_idt": (
522	        "A Universal Dependencies 4910-sentence treebank for modern Irish."
523	    ),
524	    "ga_twittirish": (
525	        "A Universal Dependencies treebank of 866 tweets in modern Irish."
526	    ),
527	    "it_isdt": (
528	        "The Italian corpus annotated according to the UD annotation scheme "
529	        "was obtained by conversion from ISDT (Italian Stanford Dependency "
530	        "Treebank), released for the dependency parsing shared task of "
531	        "Evalita-2014 (Bosco et al. 2014)."
532	    ),
533	    "it_partut": (
534	        "UD_Italian-ParTUT is a conversion of a multilingual parallel treebank"
535	        " developed at the University of Turin, and consisting of a variety of"
536	        " text genres, including talks, legal texts and Wikipedia articles, "
537	        "among others."
538	    ),
539	    "it_postwita": (
540	        "PoSTWITA-UD is a collection of Italian tweets annotated in Universal "
541	        "Dependencies that can be exploited for the training of NLP systems to"
542	        " enhance their performance on social media texts."
543	    ),
544	    "it_markit": (
545	        "It is MarkIT That is New: An Italian Treebank of Marked "
546	        "Constructions. Teresa Paccosi, Alessio Palmero Aprosio and Sara "
547	        "Tonelli, To appear in Proceedings of the Eighth Italian Conference on"
548	        " Computational Linguistics 2022 (CLIC-it 2021)"
549	    ),
550	    "it_valico": (
551	        "Manually corrected Treebank of Learner Italian drawn from the Valico "
552	        "corpus and correspondent corrected sentences."
553	    ),
554	    "it_pud": (
555	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
556	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
557	        "Raw Text to Universal Dependencies."
558	    ),
559	    "it_twittiro": (
560	        "TWITTIR-UD is a collection of ironic Italian tweets annotated in "
561	        "Universal Dependencies. The treebank can be exploited for the "
562	        "training of NLP systems to enhance their performance on social media "
563	        "texts, and in particular, for irony detection purposes."
564	    ),
565	    "it_vit": (
566	        "The UD_Italian-VIT corpus was obtained by conversion from VIT (Venice"
567	        " Italian Treebank), developed at the Laboratory of Computational "
568	        "Linguistics of the Universit Ca' Foscari in Venice (Delmonte et al. "
569	        "2007; Delmonte 2009; "
570	        "http://rondelmo.it/resource/VIT/Browser-VIT/index.htm)."
571	    ),
572	    "ja_pudluw": (
573	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
574	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
575	        "Raw Text to Universal Dependencies."
576	    ),
577	    "ja_bccwjluw": (
578	        "This Universal Dependencies (UD) Japanese treebank is based on the "
579	        "definition of UD Japanese convention described in the UD "
580	        "documentation. The original sentences are from `Balanced Corpus of "
581	        "Contemporary Written Japanese'(BCCWJ)."
582	    ),
583	    "ja_gsdluw": (
584	        "This Universal Dependencies (UD) Japanese treebank is based on the "
585	        "definition of UD Japanese convention described in the UD "
586	        "documentation. The original sentences are from Google UDT 2.0."
587	    ),
588	    "ja_bccwj": (
589	        "This Universal Dependencies (UD) Japanese treebank is based on the "
590	        "definition of UD Japanese convention described in the UD "
591	        "documentation. The original sentences are from `Balanced Corpus of "
592	        "Contemporary Written Japanese'(BCCWJ)."
593	    ),
594	    "ja_gsd": (
595	        "This Universal Dependencies (UD) Japanese treebank is based on the "
596	        "definition of UD Japanese convention described in the UD "
597	        "documentation.  The original sentences are from Google UDT 2.0."
598	    ),
599	    "ja_modern": (
600	        "This Universal Dependencies (UD) Japanese treebank is based on the "
601	        "definition of UD Japanese convention described in the UD "
602	        "documentation. The original sentences are from `Corpus of Historical "
603	        "Japanese' (CHJ)."
604	    ),
605	    "ja_pud": (
606	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
607	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
608	        " Raw Text to Universal "
609	        "Dependencies](http://universaldependencies.org/conll17/)."
610	    ),
611	    "jv_csui": (
612	        "UD Javanese-CSUI is a dependency treebank in Javanese, a regional "
613	        "language in Indonesia with more than 60 million users. The original "
614	        "sentences were taken from OPUS, especially from the WikiMatrix v1 "
615	        "corpus. We revised the sentences that contained more Indonesian words"
616	        " than Javanese words and manually annotated them."
617	    ),
618	    "urb_tudet": (
619	        "UD_Kaapor-TuDeT is a collection of annotated sentences in Ka'apor. "
620	        "The project is a work in progress and the treebank is being updated "
621	        "on a regular basis."
622	    ),
623	    "xnr_kdtb": (
624	        "The Kangri UD Treebank (KDTB) is a part of the Universal Dependency "
625	        "treebank project."
626	    ),
627	    "krl_kkpp": (
628	        "UD Karelian-KKPP is a manually annotated new corpus of Karelian made "
629	        "in Universal dependencies annotation scheme. The data is collected "
630	        "from VepKar corpora and consists of mostly modern news texts but also"
631	        " some stories and educational texts."
632	    ),
633	    "kk_ktb": (
634	        "The UD Kazakh treebank is a combination of text from various sources "
635	        "including Wikipedia, some folk tales, sentences from the UDHR, news "
636	        "and phrasebook sentences. Sentences IDs include partial document "
637	        "identifiers."
638	    ),
639	    "arr_tudet": (
640	        "UD_Karo-TuDeT is a collection of annotated sentences in Karo. The "
641	        "sentences stem from the only grammatical description of the language "
642	        "(Gabas, 1999) and from the sentences in the dictionary by the same "
643	        "author (Gabas, 2007). Sentence annotation and documentation by "
644	        "Fabrcio Ferraz Gerardi."
645	    ),
646	    "kfm_aha": (
647	        "The AHA Khunsari Treebank is a small treebank for contemporary "
648	        "Khunsari. Its corpus is collected and annotated manually. We have "
649	        "prepared this treebank based on interviews with Khunsari speakers."
650	    ),
651	    "quc_iu": (
652	        "UD Kiche-IU is a treebank consisting of sentences from a variety of"
653	        " text domains but principally dictionary example sentences and "
654	        "linguistic examples."
655	    ),
656	    "koi_uh": (
657	        "This is a Komi-Permyak literary language treebank consisting of "
658	        "original and translated texts."
659	    ),
660	    "kpv_ikdp": (
661	        "This treebank consists of dialectal transcriptions of spoken "
662	        "Komi-Zyrian. The current texts are short recorded segments from "
663	        "different areas where the Iva dialect of Komi language is spoken."
664	    ),
665	    "kpv_lattice": (
666	        "UD Komi-Zyrian Lattice is a treebank of written standard Komi-Zyrian."
667	    ),
668	    "ko_gsd": (
669	        "The Google Korean Universal Dependency Treebank is first converted "
670	        "from the Universal Dependency Treebank v2.0 (legacy), and then "
671	        "enhanced by Chun et al., 2018."
672	    ),
673	    "ko_kaist": (
674	        "The KAIST Korean Universal Dependency Treebank is generated by Chun "
675	        "et al., 2018 from the constituency trees in the KAIST Tree-Tagging "
676	        "Corpus."
677	    ),
678	    "ko_pud": (
679	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
680	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
681	        "Raw Text to Universal Dependencies."
682	    ),
683	    "kmr_mg": (
684	        "The UD Kurmanji corpus is a corpus of Kurmanji Kurdish. It contains "
685	        "fiction and encyclopaedic texts in roughly equal measure. It has been"
686	        " annotated natively in accordance with the UD annotation scheme."
687	    ),
688	    "la_ittb": (
689	        "Latin data from the _Index Thomisticus_ Treebank. Data are taken from"
690	        " the _Index Thomisticus_ corpus by Roberto Busa SJ, which contains "
691	        "the complete work by Thomas Aquinas (12251274; Medieval Latin) and "
692	        "by 61 other authors related to Thomas."
693	    ),
694	    "la_udante": (
695	        "The UDante treebank is based on the Latin texts of Dante Alighieri, "
696	        "taken from the DanteSearch corpus, originally created at the "
697	        "University of Pisa, Italy. It is a treebank of Latin language, more "
698	        "precisely of literary Medieval Latin (XIVth century)."
699	    ),
700	    "la_llct": (
701	        "This Universal Dependencies version of the LLCT (Late Latin Charter "
702	        "Treebank) consists of an automated conversion of the LLCT2 treebank "
703	        "from the Latin Dependency Treebank (LDT) format into the Universal "
704	        "Dependencies standard."
705	    ),
706	    "la_perseus": (
707	        "This Universal Dependencies Latin Treebank consists of an automatic "
708	        "conversion of a selection of passages from the Ancient Greek and "
709	        "Latin Dependency Treebank 2.1"
710	    ),
711	    "la_proiel": (
712	        "The Latin PROIEL treebank is based on the Latin data from the PROIEL "
713	        "treebank, and contains most of the Vulgate New Testament translations"
714	        " plus selections from Caesar's Gallic War, Cicero's Letters to "
715	        "Atticus, Palladius' Opus Agriculturae and the first book of Cicero's "
716	        "De officiis."
717	    ),
718	    "lv_lvtb": (
719	        "Latvian UD Treebank is based on Latvian Treebank (LVTB), being "
720	        "created at University of Latvia, Institute of Mathematics and "
721	        "Computer Science, Artificial Intelligence Laboratory."
722	    ),
723	    "lij_glt": (
724	        "The Genoese Ligurian Treebank is a small, manually annotated "
725	        "collection of contemporary Ligurian prose. The focus of the treebank "
726	        "is written Genoese, the koin variety of Ligurian which is associated"
727	        " with today's literary, journalistic and academic ligurophone sphere."
728	    ),
729	    "lt_alksnis": (
730	        "The Lithuanian dependency treebank ALKSNIS v3.0 (Vytautas Magnus "
731	        "University)."
732	    ),
733	    "lt_hse": (
734	        "Lithuanian treebank annotated manually (dependencies) using the "
735	        "Morphological Annotator by CCL, Vytautas Magnus University "
736	        "(http://tekstynas.vdu.lt/) and manual disambiguation. A pilot version"
737	        " which includes news and an essay by Tomas Venclova is available "
738	        "here."
739	    ),
740	    "olo_kkpp": (
741	        "UD Livvi-KKPP is a manually annotated new corpus of Livvi-Karelian "
742	        "made directly in the Universal dependencies annotation scheme. The "
743	        "data is collected from VepKar corpora and consists of mostly modern "
744	        "news texts but also some stories and educational texts."
745	    ),
746	    "nds_lsdc": (
747	        "The UD Low Saxon LSDC dataset consists of sentences in 18 Low Saxon "
748	        "dialects from both Germany and the Netherlands. These sentences are "
749	        "(or are to become) part of the LSDC dataset and represent the "
750	        "language from the 19th and early 20th century in genres such as short"
751	        " stories, novels, speeches, letters and fairytales."
752	    ),
753	    "mt_mudt": (
754	        "MUDT (Maltese Universal Dependencies Treebank) is a manually "
755	        "annotated treebank of Maltese, a Semitic language of Malta descended "
756	        "from North African Arabic with a significant amount of Italo-Romance "
757	        "influence. MUDT was designed as a balanced corpus with four major "
758	        "genres (see Splitting below) represented roughly equally."
759	    ),
760	    "gv_cadhan": (
761	        "This is the Cadhan Aonair UD treebank for Manx Gaelic, created by "
762	        "Kevin Scannell."
763	    ),
764	    "mr_ufal": (
765	        "UD Marathi is a manually annotated treebank consisting primarily of "
766	        "stories from Wikisource, and parts of an article on Wikipedia."
767	    ),
768	    "gun_dooley": (
769	        "UD Mbya_Guarani-Dooley is a corpus of narratives written in Mby "
770	        "Guaran (Tupian) in Brazil, and collected by Robert Dooley. Due to "
771	        "copyright restrictions, the corpus that is distributed as part of UD "
772	        "only contains the annotation (tags, features, relations) while the "
773	        "FORM and LEMMA columns are empty."
774	    ),
775	    "gun_thomas": (
776	        "UD Mbya_Guarani-Thomas is a corpus of Mby Guaran (Tupian) texts "
777	        "collected by Guillaume Thomas. The current version of the corpus "
778	        "consists of three speeches by Paulina Kerechu Nez Romero, a Mby "
779	        "Guaran speaker from Ytu, Caazap Department, Paraguay."
780	    ),
781	    "mdf_jr": (
782	        "Erme Universal Dependencies annotated texts Moksha are the origin of "
783	        "UD_Moksha-JR with annotation (CoNLL-U) for texts in the Moksha "
784	        "language, it originally consists of a sample from a number of fiction"
785	        " authors writing originals in Moksha."
786	    ),
787	    "myu_tudet": (
788	        "UD_Munduruku-TuDeT is a collection of annotated sentences in "
789	        "Munduruk. Together with UD_Akuntsu-TuDeT and UD_Tupinamba-TuDeT, "
790	        "UD_Munduruku-TuDeT is part of the TuLaR project."
791	    ),
792	    "pcm_nsc": (
793	        "A Universal Dependencies corpus for spoken Naija (Nigerian Pidgin)."
794	    ),
795	    "nyq_aha": (
796	        "The AHA Nayini Treebank is a small treebank for contemporary Nayini. "
797	        "Its corpus is collected and annotated manually. We have prepared this"
798	        " treebank based on interviews with Nayini speakers."
799	    ),
800	    "sme_giella": (
801	        "This is a North Smi treebank based on a manually disambiguated and "
802	        "function-labelled gold-standard corpus of North Smi produced by the "
803	        "Giellatekno team at UiT Norgga rktala universitehta."
804	    ),
805	    "no_bokmaal": (
806	        "The Norwegian UD treebank is based on the Bokml section of the "
807	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
808	        " Norwegian. NDT has been automatically converted to the UD scheme by "
809	        "Lilja vrelid at the University of Oslo."
810	    ),
811	    "no_nynorsk": (
812	        "The Norwegian UD treebank is based on the Nynorsk section of the "
813	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
814	        " Norwegian.  NDT has been automatically converted to the UD scheme by"
815	        " Lilja vrelid at the University of Oslo."
816	    ),
817	    "no_nynorsklia": (
818	        "This Norwegian treebank is based on the LIA treebank of transcribed "
819	        "spoken Norwegian dialects. The treebank has been automatically "
820	        "converted to the UD scheme by Lilja vrelid at the University of "
821	        "Oslo."
822	    ),
823	    "cu_proiel": (
824	        "The Old Church Slavonic (OCS) UD treebank is based on the Old Church "
825	        "Slavonic data from the PROIEL treebank and contains the text of the "
826	        "Codex Marianus New Testament translation."
827	    ),
828	    "fro_srcmf": (
829	        "UD_Old_French-SRCMF is a conversion of (part of) the SRCMF corpus "
830	        "(Syntactic Reference Corpus of Medieval French srcmf.org)."
831	    ),
832	    "orv_birchbark": (
833	        "UD Old_East_Slavic-Birchbark is based on the RNC Corpus of Birchbark "
834	        "Letters and includes documents written in 1025-1500 in an East Slavic"
835	        " vernacular (letters, household and business records, records for "
836	        "church services, spell against diseases, and other short "
837	        "inscriptions). The treebank is manually syntactically annotated in "
838	        "the UD 2.0 scheme, morphological and lexical annotation is a "
839	        "conversion of the original RNC annotation."
840	    ),
841	    "orv_rnc": (
842	        "`UD_Old_Russian-RNC` is a sample of the Middle Russian corpus "
843	        "(1300-1700), a part of the Russian National Corpus. The data were "
844	        "originally annotated according to the RNC and extended UD-Russian "
845	        "morphological schemas and UD 2.4 dependency schema."
846	    ),
847	    "orv_torot": (
848	        "UD_Old_Russian-TOROT is a conversion of a selection of the Old East "
849	        "Slavonic and Middle Russian data in the Troms Old Russian and OCS "
850	        "Treebank (TOROT), which was originally annotated in PROIEL dependency"
851	        " format."
852	    ),
853	    "otk_tonqq": (
854	        "`UD_Old_Turkish-Tonqq` is an Old Turkish treebank built upon Turkic "
855	        "script texts or sentences that are trivially convertible."
856	    ),
857	    "fa_perdt": (
858	        "The Persian Universal Dependency Treebank (PerUDT) is the result of "
859	        "automatic coversion of Persian Dependency Treebank (PerDT) with "
860	        "extensive manual corrections. Please refer to the follwoing work, if "
861	        "you use this data: Mohammad Sadegh Rasooli, Pegah Safari, Amirsaeid "
862	        "Moloodi, and Alireza Nourian. 'The Persian Dependency Treebank Made "
863	        "Universal'. 2020 (to appear)."
864	    ),
865	    "fa_seraji": (
866	        "The Persian Universal Dependency Treebank (Persian UD) is based on "
867	        "Uppsala Persian Dependency Treebank (UPDT). The conversion of the "
868	        "UPDT to the Universal Dependencies was performed semi-automatically "
869	        "with extensive manual checks and corrections."
870	    ),
871	    "pl_lfg": (
872	        "The LFG Enhanced UD treebank of Polish is based on a corpus of LFG "
873	        "(Lexical Functional Grammar) syntactic structures generated by an LFG"
874	        " grammar of Polish, POLFIE, and manually disambiguated by human "
875	        "annotators."
876	    ),
877	    "pl_pdb": (
878	        "The Polish PDB-UD treebank is based on the Polish Dependency Bank 2.0"
879	        " (PDB 2.0), created at the Institute of Computer Science, Polish "
880	        "Academy of Sciences in Warsaw. The PDB-UD treebank is an extended and"
881	        " corrected version of the Polish SZ-UD treebank (the release 1.2 to "
882	        "2.3)."
883	    ),
884	    "pl_pud": (
885	        "This is the Polish portion of the Parallel Universal Dependencies "
886	        "(PUD) treebanks, created at the Institute of Computer Science, Polish"
887	        " Academy of Sciences in Warsaw.Re"
888	    ),
889	    "pt_bosque": (
890	        "This Universal Dependencies (UD) Portuguese treebank is based on the "
891	        "Constraint Grammar converted version of the Bosque, which is part of "
892	        "the Floresta Sint(c)tica treebank. It contains both European "
893	        "(CETEMPblico) and Brazilian (CETENFolha) variants."
894	    ),
895	    "pt_gsd": (
896	        "The Brazilian Portuguese UD is converted from the Google Universal "
897	        "Dependency Treebank v2.0 (legacy)."
898	    ),
899	    "pt_pud": (
900	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
901	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
902	        "Raw Text to Universal Dependencies."
903	    ),
904	    "ro_art": (
905	        "The UD treebank ArT is a treebank of the Aromanian dialect of the "
906	        "Romanian language in UD format."
907	    ),
908	    "ro_nonstandard": (
909	        "The Romanian Non-standard UD treebank (called UAIC-RoDia) is based on"
910	        " UAIC-RoDia Treebank. UAIC-RoDia = ISLRN 156-635-615-024-0"
911	    ),
912	    "ro_rrt": (
913	        "The Romanian UD treebank (called RoRefTrees) (Barbu Mititelu et al., "
914	        "2016) is the reference treebank in UD format for standard Romanian."
915	    ),
916	    "ro_simonero": "SiMoNERo is a medical corpus of contemporary Romanian.",
917	    "ru_gsd": (
918	        "Russian Universal Dependencies Treebank annotated and converted by "
919	        "Google."
920	    ),
921	    "ru_pud": (
922	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
923	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
924	        "Raw Text to Universal Dependencies."
925	    ),
926	    "ru_syntagrus": "Russian data from the SynTagRus corpus.",
927	    "ru_taiga": (
928	        "Universal Dependencies treebank is based on data samples extracted "
929	        "from Taiga Corpus and MorphoRuEval-2017 and GramEval-2020 shared "
930	        "tasks collections."
931	    ),
932	    "sa_ufal": (
933	        "A small Sanskrit treebank of sentences from Pacatantra, an ancient "
934	        "Indian collection of interrelated fables by Vishnu Sharma."
935	    ),
936	    "sa_vedic": (
937	        "The Treebank of Vedic Sanskrit contains 4,000 sentences with 27,000 "
938	        "words chosen from metrical and prose passages of the gveda (RV), the"
939	        " aunaka recension of the Atharvaveda (S), the Maitryasahit "
940	        "(MS), and the Aitareya- (AB) and atapatha-Brhmaas (B). Lexical "
941	        "and morpho-syntactic information has been generated using a tagging "
942	        "software and manually validated. POS tags have been induced "
943	        "automatically from the morpho-sytactic information of each word."
944	    ),
945	    "gd_arcosg": (
946	        "A treebank of Scottish Gaelic based on the Annotated Reference Corpus"
947	        " Of Scottish Gaelic (ARCOSG)."
948	    ),
949	    "sr_set": (
950	        "The Serbian UD treebank is based on the "
951	        "[SETimes-SR](http://hdl.handle.net/11356/1200) corpus and additional "
952	        "news documents from the Serbian web."
953	    ),
954	    "sms_giellagas": (
955	        "The UD Skolt Sami Giellagas treebank is based almost entirely on "
956	        "spoken Skolt Sami corpora."
957	    ),
958	    "sk_snk": (
959	        "The Slovak UD treebank is based on data originally annotated as part "
960	        "of the Slovak National Corpus, following the annotation style of the "
961	        "Prague Dependency Treebank."
962	    ),
963	    "sl_ssj": (
964	        "The Slovenian UD Treebank is a rule-based conversion of the ssj500k "
965	        "treebank, the largest collection of manually syntactically annotated "
966	        "data in Slovenian, originally annotated in the JOS annotation scheme."
967	    ),
968	    "sl_sst": (
969	        "The Spoken Slovenian UD Treebank (SST) is the first syntactically "
970	        "annotated corpus of spoken Slovenian, based on a sample of the "
971	        "reference GOS corpus, a collection of transcribed audio recordings of"
972	        " monologic, dialogic and multi-party spontaneous speech in different "
973	        "everyday situations."
974	    ),
975	    "soj_aha": (
976	        "The AHA Soi Treebank is a small treebank for contemporary Soi. Its "
977	        "corpus is collected and annotated manually. We have prepared this "
978	        "treebank based on interviews with Soi speakers."
979	    ),
980	    "ajp_madar": (
981	        "The South_Levantine_Arabic-MADAR treebank consists of 100 "
982	        "manually-annotated sentences taken from the "
983	        "[MADAR](https://camel.abudhabi.nyu.edu/madar/) (Multi-Arabic Dialect "
984	        "Applications and Resources) project. "
985	    ),
986	    "es_ancora": "Spanish data from the AnCora corpus.",
987	    "es_gsd": (
988	        "The Spanish UD is converted from the content head version of the "
989	        "universal dependency treebank v2.0 (legacy)."
990	    ),
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {
1217	        "test": "UD_Arabic-PUD/r2.10/ar_pud-ud-test.conllu",
1218	    },
1219	    "hy_armtdp": {
1220	        "train": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-train.conllu",
1221	        "dev": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-dev.conllu",
1222	        "test": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-test.conllu",
1223	    },
1224	    "aii_as": {
1225	        "test": "UD_Assyrian-AS/r2.10/aii_as-ud-test.conllu",
1226	    },
1227	    "bm_crb": {
1228	        "test": "UD_Bambara-CRB/r2.10/bm_crb-ud-test.conllu",
1229	    },
1230	    "eu_bdt": {
1231	        "train": "UD_Basque-BDT/r2.10/eu_bdt-ud-train.conllu",
1232	        "dev": "UD_Basque-BDT/r2.10/eu_bdt-ud-dev.conllu",
1233	        "test": "UD_Basque-BDT/r2.10/eu_bdt-ud-test.conllu",
1234	    },
1235	    "bej_nsc": {
1236	        "test": "UD_Beja-NSC/r2.10/bej_nsc-ud-test.conllu",
1237	    },
1238	    "be_hse": {
1239	        "train": "UD_Belarusian-HSE/r2.10/be_hse-ud-train.conllu",
1240	        "dev": "UD_Belarusian-HSE/r2.10/be_hse-ud-dev.conllu",
1241	        "test": "UD_Belarusian-HSE/r2.10/be_hse-ud-test.conllu",
1242	    },
1243	    "bn_bru": {
1244	        "test": "UD_Bengali-BRU/r2.10/bn_bru-ud-test.conllu",
1245	    },
1246	    "bho_bhtb": {
1247	        "test": "UD_Bhojpuri-BHTB/r2.10/bho_bhtb-ud-test.conllu",
1248	    },
1249	    "br_keb": {
1250	        "test": "UD_Breton-KEB/r2.10/br_keb-ud-test.conllu",
1251	    },
1252	    "bg_btb": {
1253	        "train": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-train.conllu",
1254	        "dev": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-dev.conllu",
1255	        "test": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-test.conllu",
1256	    },
1257	    "bxr_bdt": {
1258	        "train": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-train.conllu",
1259	        "test": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-test.conllu",
1260	    },
1261	    "yue_hk": {
1262	        "test": "UD_Cantonese-HK/r2.10/yue_hk-ud-test.conllu",
1263	    },
1264	    "ca_ancora": {
1265	        "train": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-train.conllu",
1266	        "dev": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-dev.conllu",
1267	        "test": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-test.conllu",
1268	    },
1269	    "ceb_gja": {
1270	        "test": "UD_Cebuano-GJA/r2.10/ceb_gja-ud-test.conllu",
1271	    },
1272	    "zh_cfl": {
1273	        "test": "UD_Chinese-CFL/r2.10/zh_cfl-ud-test.conllu",
1274	    },
1275	    "zh_gsd": {
1276	        "train": "UD_Chinese-GSD/r2.10/zh_gsd-ud-train.conllu",
1277	        "dev": "UD_Chinese-GSD/r2.10/zh_gsd-ud-dev.conllu",
1278	        "test": "UD_Chinese-GSD/r2.10/zh_gsd-ud-test.conllu",
1279	    },
1280	    "zh_gsdsimp": {
1281	        "train": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-train.conllu",
1282	        "dev": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-dev.conllu",
1283	        "test": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-test.conllu",
1284	    },
1285	    "zh_hk": {
1286	        "test": "UD_Chinese-HK/r2.10/zh_hk-ud-test.conllu",
1287	    },
1288	    "zh_pud": {
1289	        "test": "UD_Chinese-PUD/r2.10/zh_pud-ud-test.conllu",
1290	    },
1291	    "ckt_hse": {
1292	        "test": "UD_Chukchi-HSE/r2.10/ckt_hse-ud-test.conllu",
1293	    },
1294	    "lzh_kyoto": {
1295	        "train": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-train.conllu",
1296	        "dev": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-dev.conllu",
1297	        "test": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-test.conllu",
1298	    },
1299	    "cop_scriptorium": {
1300	        "train": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-train.conllu",
1301	        "dev": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-dev.conllu",
1302	        "test": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-test.conllu",
1303	    },
1304	    "hr_set": {
1305	        "train": "UD_Croatian-SET/r2.10/hr_set-ud-train.conllu",
1306	        "dev": "UD_Croatian-SET/r2.10/hr_set-ud-dev.conllu",
1307	        "test": "UD_Croatian-SET/r2.10/hr_set-ud-test.conllu",
1308	    },
1309	    "cs_cac": {
1310	        "train": "UD_Czech-CAC/r2.10/cs_cac-ud-train.conllu",
1311	        "dev": "UD_Czech-CAC/r2.10/cs_cac-ud-dev.conllu",
1312	        "test": "UD_Czech-CAC/r2.10/cs_cac-ud-test.conllu",
1313	    },
1314	    "cs_cltt": {
1315	        "train": "UD_Czech-CLTT/r2.10/cs_cltt-ud-train.conllu",
1316	        "dev": "UD_Czech-CLTT/r2.10/cs_cltt-ud-dev.conllu",
1317	        "test": "UD_Czech-CLTT/r2.10/cs_cltt-ud-test.conllu",
1318	    },
1319	    "cs_fictree": {
1320	        "train": "UD_Czech-FicTree/r2.10/cs_fictree-ud-train.conllu",
1321	        "dev": "UD_Czech-FicTree/r2.10/cs_fictree-ud-dev.conllu",
1322	        "test": "UD_Czech-FicTree/r2.10/cs_fictree-ud-test.conllu",
1323	    },
1324	    "cs_pdt": {
1325	        "train": [
1326	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-l.conllu",
1327	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-m.conllu",
1328	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-c.conllu",
1329	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-v.conllu",
1330	        ],
1331	        "dev": "UD_Czech-PDT/r2.10/cs_pdt-ud-dev.conllu",
1332	        "test": "UD_Czech-PDT/r2.10/cs_pdt-ud-test.conllu",
1333	    },
1334	    "cs_pud": {
1335	        "test": "UD_Czech-PUD/r2.10/cs_pud-ud-test.conllu",
1336	    },
1337	    "da_ddt": {
1338	        "train": "UD_Danish-DDT/r2.10/da_ddt-ud-train.conllu",
1339	        "dev": "UD_Danish-DDT/r2.10/da_ddt-ud-dev.conllu",
1340	        "test": "UD_Danish-DDT/r2.10/da_ddt-ud-test.conllu",
1341	    },
1342	    "nl_alpino": {
1343	        "train": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-train.conllu",
1344	        "dev": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-dev.conllu",
1345	        "test": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-test.conllu",
1346	    },
1347	    "nl_lassysmall": {
1348	        "train": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-train.conllu",
1349	        "dev": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-dev.conllu",
1350	        "test": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-test.conllu",
1351	    },
1352	    "en_atis": {
1353	        "train": "UD_English-Atis/r2.10/en_atis-ud-train.conllu",
1354	        "dev": "UD_English-Atis/r2.10/en_atis-ud-dev.conllu",
1355	        "test": "UD_English-Atis/r2.10/en_atis-ud-test.conllu",
1356	    },
1357	    "en_esl": {
1358	        "train": "UD_English-ESL/r2.10/en_esl-ud-train.conllu",
1359	        "dev": "UD_English-ESL/r2.10/en_esl-ud-dev.conllu",
1360	        "test": "UD_English-ESL/r2.10/en_esl-ud-test.conllu",
1361	    },
1362	    "en_ewt": {
1363	        "train": "UD_English-EWT/r2.10/en_ewt-ud-train.conllu",
1364	        "dev": "UD_English-EWT/r2.10/en_ewt-ud-dev.conllu",
1365	        "test": "UD_English-EWT/r2.10/en_ewt-ud-test.conllu",
1366	    },
1367	    "en_gum": {
1368	        "train": "UD_English-GUM/r2.10/en_gum-ud-train.conllu",
1369	        "dev": "UD_English-GUM/r2.10/en_gum-ud-dev.conllu",
1370	        "test": "UD_English-GUM/r2.10/en_gum-ud-test.conllu",
1371	    },
1372	    "en_gumreddit": {
1373	        "train": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-train.conllu",
1374	        "dev": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-dev.conllu",
1375	        "test": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-test.conllu",
1376	    },
1377	    "en_lines": {
1378	        "train": "UD_English-LinES/r2.10/en_lines-ud-train.conllu",
1379	        "dev": "UD_English-LinES/r2.10/en_lines-ud-dev.conllu",
1380	        "test": "UD_English-LinES/r2.10/en_lines-ud-test.conllu",
1381	    },
1382	    "en_partut": {
1383	        "train": "UD_English-ParTUT/r2.10/en_partut-ud-train.conllu",
1384	        "dev": "UD_English-ParTUT/r2.10/en_partut-ud-dev.conllu",
1385	        "test": "UD_English-ParTUT/r2.10/en_partut-ud-test.conllu",
1386	    },
1387	    "en_pronouns": {
1388	        "test": "UD_English-Pronouns/r2.10/en_pronouns-ud-test.conllu",
1389	    },
1390	    "en_pud": {
1391	        "test": "UD_English-PUD/r2.10/en_pud-ud-test.conllu",
1392	    },
1393	    "myv_jr": {
1394	        "test": "UD_Erzya-JR/r2.10/myv_jr-ud-test.conllu",
1395	    },
1396	    "et_edt": {
1397	        "train": "UD_Estonian-EDT/r2.10/et_edt-ud-train.conllu",
1398	        "dev": "UD_Estonian-EDT/r2.10/et_edt-ud-dev.conllu",
1399	        "test": "UD_Estonian-EDT/r2.10/et_edt-ud-test.conllu",
1400	    },
1401	    "et_ewt": {
1402	        "train": "UD_Estonian-EWT/r2.10/et_ewt-ud-train.conllu",
1403	        "dev": "UD_Estonian-EWT/r2.10/et_ewt-ud-dev.conllu",
1404	        "test": "UD_Estonian-EWT/r2.10/et_ewt-ud-test.conllu",
1405	    },
1406	    "fo_farpahc": {
1407	        "train": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-train.conllu",
1408	        "dev": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-dev.conllu",
1409	        "test": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-test.conllu",
1410	    },
1411	    "fo_oft": {
1412	        "test": "UD_Faroese-OFT/r2.10/fo_oft-ud-test.conllu",
1413	    },
1414	    "fi_ftb": {
1415	        "train": "UD_Finnish-FTB/r2.10/fi_ftb-ud-train.conllu",
1416	        "dev": "UD_Finnish-FTB/r2.10/fi_ftb-ud-dev.conllu",
1417	        "test": "UD_Finnish-FTB/r2.10/fi_ftb-ud-test.conllu",
1418	    },
1419	    "fi_ood": {
1420	        "test": "UD_Finnish-OOD/r2.10/fi_ood-ud-test.conllu",
1421	    },
1422	    "fi_pud": {
1423	        "test": "UD_Finnish-PUD/r2.10/fi_pud-ud-test.conllu",
1424	    },
1425	    "fi_tdt": {
1426	        "train": "UD_Finnish-TDT/r2.10/fi_tdt-ud-train.conllu",
1427	        "dev": "UD_Finnish-TDT/r2.10/fi_tdt-ud-dev.conllu",
1428	        "test": "UD_Finnish-TDT/r2.10/fi_tdt-ud-test.conllu",
1429	    },
1430	    "fr_parisstories": {
1431	        "train": "UD_French-ParisStories/r2.10/fr_parisstories-ud-train.conllu",
1432	        "test": "UD_French-ParisStories/r2.10/fr_parisstories-ud-test.conllu",
1433	    },
1434	    "fr_fqb": {
1435	        "test": "UD_French-FQB/r2.10/fr_fqb-ud-test.conllu",
1436	    },
1437	    "fr_rhapsodie": {
1438	        "train": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-train.conllu",
1439	        "dev": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-dev.conllu",
1440	        "test": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-test.conllu",
1441	    },
1442	    "fr_ftb": {
1443	        "train": "UD_French-FTB/r2.10/fr_ftb-ud-train.conllu",
1444	        "dev": "UD_French-FTB/r2.10/fr_ftb-ud-dev.conllu",
1445	        "test": "UD_French-FTB/r2.10/fr_ftb-ud-test.conllu",
1446	    },
1447	    "fr_gsd": {
1448	        "train": "UD_French-GSD/r2.10/fr_gsd-ud-train.conllu",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:376
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
375	    "fr_gsd": (
376	        "The **UD_French-GSD** was converted in 2015 from the content head "
377	        "version of the universal dependency treebank v2.0 "
378	        "(https://github.com/ryanmcd/uni-dep-tb). It is updated since 2015 "
379	        "independently from the previous source."
380	    ),
381	    "fr_partut": (
382	        "UD_French-ParTUT is a conversion of a multilingual parallel treebank "
383	        "developed at the University of Turin, and consisting of a variety of "
384	        "text genres, including talks, legal texts and Wikipedia articles, "
385	        "among others."
386	    ),
387	    "fr_rhapsodie": "A Universal Dependencies corpus for spoken French.",
388	    "fr_parisstories": (
389	        "Paris Stories is a corpus of oral French collected and transcribed by"
390	        " Linguistics students from Sorbonne Nouvelle and corrected by "
391	        "students from the Plurital Master's Degree of Computational "
392	        "Linguistics ( Inalco, Paris Nanterre, Sorbonne Nouvelle) between 2017"
393	        " and 2021. It contains monologues and dialogues from speakers living "
394	        "in the Parisian region."
395	    ),
396	    "fr_pud": (
397	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
398	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
399	        "Raw Text to Universal Dependencies."
400	    ),
401	    "fr_sequoia": (
402	        "UD_French-Sequoia is an automatic conversion of the Sequoia Treebank "
403	        "corpus French Sequoia corpus."
404	    ),
405	    "gl_ctg": (
406	        "The Galician UD treebank is based on the automatic parsing of the "
407	        "Galician Technical Corpus (http://sli.uvigo.gal/CTG) created at the "
408	        "University of Vigo by the the TALG NLP research group."
409	    ),
410	    "gl_treegal": (
411	        "The Galician-TreeGal is a treebank for Galician developed at LyS "
412	        "Group (Universidade da Corua)."
413	    ),
414	    "de_gsd": (
415	        "The German UD is converted from the content head version of the "
416	        "universal dependency treebank v2.0 (legacy)."
417	    ),
418	    "de_hdt": (
419	        "UD German-HDT is a conversion of the Hamburg Dependency Treebank, "
420	        "created at the University of Hamburg through manual annotation in "
421	        "conjunction with a standard for morphologically and syntactically "
422	        "annotating sentences as well as a constraint-based parser."
423	    ),
424	    "de_lit": (
425	        "This treebank aims at gathering texts of the German literary history."
426	        " Currently, it hosts Fragments of the early Romanticism, i.e. "
427	        "aphorism-like texts mainly dealing with philosophical issues "
428	        "concerning art, beauty and related topics."
429	    ),
430	    "de_pud": (
431	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
432	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
433	        "Raw Text to Universal Dependencies."
434	    ),
435	    "got_proiel": (
436	        "The UD Gothic treebank is based on the Gothic data from the PROIEL "
437	        "treebank, and consists of Wulfila's Bible translation."
438	    ),
439	    "el_gdt": (
440	        "The Greek UD treebank (UD_Greek-GDT) is derived from the Greek "
441	        "Dependency Treebank (http://gdt.ilsp.gr), a resource developed and "
442	        "maintained by researchers at the Institute for Language and Speech "
443	        "Processing/Athena R.C. (http://www.ilsp.gr)."
444	    ),
445	    "gub_tudet": (
446	        "UD_Guajajara-TuDeT is a collection of annotated sentences in "
447	        "Guajajara. Sentences stem from multiple sources such as descriptions "
448	        "of the language, short stories, dictionaries and translations from "
449	        "the New Testament. Sentence annotation and documentation by Lorena "
450	        "Martn Rodrguez and Fabrcio Ferraz Gerardi."
451	    ),
452	    "gn_oldtudet": (
453	        "UD_Guarani-OldTuDeT is a collection of annotated texts in Old "
454	        "Guaran. All known sources in this language are being annotated: "
455	        "cathesisms, grammars (seventeenth and eighteenth century), sentences "
456	        "from dictionaries, and other texts. Sentence annotation and "
457	        "documentation by Fabrcio Ferraz Gerardi and Lorena Martn Rodrguez."
458	    ),
459	    "he_htb": "A Universal Dependencies Corpus for Hebrew.",
460	    "he_iahltwiki": (
461	        "Publicly available subset of the IAHLT UD Hebrew Treebank's Wikipedia"
462	        " section (https://www.iahlt.org/)"
463	    ),
464	    "qfn_fame": (
465	        "UD_Frisian_Dutch-Fame is a selection of 400 sentences from the FAME! "
466	        "speech corpus by Yilmaz et al. (2016a, 2016b). The treebank is "
467	        "manually annotated using the UD scheme."
468	    ),
469	    "qhe_hiencs": (
470	        "The Hindi-English Code-switching treebank is based on code-switching "
471	        "tweets of Hindi and English multilingual speakers (mostly Indian) on "
472	        "Twitter. The treebank is manually annotated using UD sceheme. The "
473	        "training and evaluations sets were seperately annotated by different "
474	        "annotators using UD v2 and v1 guidelines respectively. The evaluation"
475	        " sets are automatically converted from UD v1 to v2."
476	    ),
477	    "hi_hdtb": (
478	        "The Hindi UD treebank is based on the Hindi Dependency Treebank "
479	        "(HDTB), created at IIIT Hyderabad, India."
480	    ),
481	    "hi_pud": (
482	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
483	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
484	        "Raw Text to Universal Dependencies."
485	    ),
486	    "hu_szeged": (
487	        "The Hungarian UD treebank is derived from the Szeged Dependency "
488	        "Treebank (Vincze et al. 2010)."
489	    ),
490	    "is_modern": (
491	        "UD_Icelandic-Modern is a conversion of the modern additions to the "
492	        "Icelandic Parsed Historical Corpus (IcePaHC) to the Universal "
493	        "Dependencies scheme."
494	    ),
495	    "is_icepahc": (
496	        "UD_Icelandic-IcePaHC is a conversion of the Icelandic Parsed "
497	        "Historical Corpus (IcePaHC) to the Universal Dependencies scheme. The"
498	        " conversion was done using UDConverter."
499	    ),
500	    "is_pud": (
501	        "Icelandic-PUD is the Icelandic part of the Parallel Universal "
502	        "Dependencies (PUD) treebanks."
503	    ),
504	    "id_csui": (
505	        "UD Indonesian-CSUI is a conversion from an Indonesian constituency "
506	        "treebank in the Penn Treebank format named Kethu that was also a "
507	        "conversion from a constituency treebank built by Dinakaramani et al. "
508	        "(2015). We named this treebank Indonesian-CSUI, since all the three "
509	        "versions of the treebanks were built at Faculty of Computer Science, "
510	        "Universitas Indonesia."
511	    ),
512	    "id_gsd": (
513	        "The Indonesian UD is converted from the content head version of the "
514	        "universal dependency treebank v2.0 (legacy)."
515	    ),
516	    "id_pud": (
517	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
518	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
519	        "Raw Text to Universal Dependencies."
520	    ),
521	    "ga_idt": (
522	        "A Universal Dependencies 4910-sentence treebank for modern Irish."
523	    ),
524	    "ga_twittirish": (
525	        "A Universal Dependencies treebank of 866 tweets in modern Irish."
526	    ),
527	    "it_isdt": (
528	        "The Italian corpus annotated according to the UD annotation scheme "
529	        "was obtained by conversion from ISDT (Italian Stanford Dependency "
530	        "Treebank), released for the dependency parsing shared task of "
531	        "Evalita-2014 (Bosco et al. 2014)."
532	    ),
533	    "it_partut": (
534	        "UD_Italian-ParTUT is a conversion of a multilingual parallel treebank"
535	        " developed at the University of Turin, and consisting of a variety of"
536	        " text genres, including talks, legal texts and Wikipedia articles, "
537	        "among others."
538	    ),
539	    "it_postwita": (
540	        "PoSTWITA-UD is a collection of Italian tweets annotated in Universal "
541	        "Dependencies that can be exploited for the training of NLP systems to"
542	        " enhance their performance on social media texts."
543	    ),
544	    "it_markit": (
545	        "It is MarkIT That is New: An Italian Treebank of Marked "
546	        "Constructions. Teresa Paccosi, Alessio Palmero Aprosio and Sara "
547	        "Tonelli, To appear in Proceedings of the Eighth Italian Conference on"
548	        " Computational Linguistics 2022 (CLIC-it 2021)"
549	    ),
550	    "it_valico": (
551	        "Manually corrected Treebank of Learner Italian drawn from the Valico "
552	        "corpus and correspondent corrected sentences."
553	    ),
554	    "it_pud": (
555	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
556	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
557	        "Raw Text to Universal Dependencies."
558	    ),
559	    "it_twittiro": (
560	        "TWITTIR-UD is a collection of ironic Italian tweets annotated in "
561	        "Universal Dependencies. The treebank can be exploited for the "
562	        "training of NLP systems to enhance their performance on social media "
563	        "texts, and in particular, for irony detection purposes."
564	    ),
565	    "it_vit": (
566	        "The UD_Italian-VIT corpus was obtained by conversion from VIT (Venice"
567	        " Italian Treebank), developed at the Laboratory of Computational "
568	        "Linguistics of the Universit Ca' Foscari in Venice (Delmonte et al. "
569	        "2007; Delmonte 2009; "
570	        "http://rondelmo.it/resource/VIT/Browser-VIT/index.htm)."
571	    ),
572	    "ja_pudluw": (
573	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
574	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
575	        "Raw Text to Universal Dependencies."
576	    ),
577	    "ja_bccwjluw": (
578	        "This Universal Dependencies (UD) Japanese treebank is based on the "
579	        "definition of UD Japanese convention described in the UD "
580	        "documentation. The original sentences are from `Balanced Corpus of "
581	        "Contemporary Written Japanese'(BCCWJ)."
582	    ),
583	    "ja_gsdluw": (
584	        "This Universal Dependencies (UD) Japanese treebank is based on the "
585	        "definition of UD Japanese convention described in the UD "
586	        "documentation. The original sentences are from Google UDT 2.0."
587	    ),
588	    "ja_bccwj": (
589	        "This Universal Dependencies (UD) Japanese treebank is based on the "
590	        "definition of UD Japanese convention described in the UD "
591	        "documentation. The original sentences are from `Balanced Corpus of "
592	        "Contemporary Written Japanese'(BCCWJ)."
593	    ),
594	    "ja_gsd": (
595	        "This Universal Dependencies (UD) Japanese treebank is based on the "
596	        "definition of UD Japanese convention described in the UD "
597	        "documentation.  The original sentences are from Google UDT 2.0."
598	    ),
599	    "ja_modern": (
600	        "This Universal Dependencies (UD) Japanese treebank is based on the "
601	        "definition of UD Japanese convention described in the UD "
602	        "documentation. The original sentences are from `Corpus of Historical "
603	        "Japanese' (CHJ)."
604	    ),
605	    "ja_pud": (
606	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
607	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
608	        " Raw Text to Universal "
609	        "Dependencies](http://universaldependencies.org/conll17/)."
610	    ),
611	    "jv_csui": (
612	        "UD Javanese-CSUI is a dependency treebank in Javanese, a regional "
613	        "language in Indonesia with more than 60 million users. The original "
614	        "sentences were taken from OPUS, especially from the WikiMatrix v1 "
615	        "corpus. We revised the sentences that contained more Indonesian words"
616	        " than Javanese words and manually annotated them."
617	    ),
618	    "urb_tudet": (
619	        "UD_Kaapor-TuDeT is a collection of annotated sentences in Ka'apor. "
620	        "The project is a work in progress and the treebank is being updated "
621	        "on a regular basis."
622	    ),
623	    "xnr_kdtb": (
624	        "The Kangri UD Treebank (KDTB) is a part of the Universal Dependency "
625	        "treebank project."
626	    ),
627	    "krl_kkpp": (
628	        "UD Karelian-KKPP is a manually annotated new corpus of Karelian made "
629	        "in Universal dependencies annotation scheme. The data is collected "
630	        "from VepKar corpora and consists of mostly modern news texts but also"
631	        " some stories and educational texts."
632	    ),
633	    "kk_ktb": (
634	        "The UD Kazakh treebank is a combination of text from various sources "
635	        "including Wikipedia, some folk tales, sentences from the UDHR, news "
636	        "and phrasebook sentences. Sentences IDs include partial document "
637	        "identifiers."
638	    ),
639	    "arr_tudet": (
640	        "UD_Karo-TuDeT is a collection of annotated sentences in Karo. The "
641	        "sentences stem from the only grammatical description of the language "
642	        "(Gabas, 1999) and from the sentences in the dictionary by the same "
643	        "author (Gabas, 2007). Sentence annotation and documentation by "
644	        "Fabrcio Ferraz Gerardi."
645	    ),
646	    "kfm_aha": (
647	        "The AHA Khunsari Treebank is a small treebank for contemporary "
648	        "Khunsari. Its corpus is collected and annotated manually. We have "
649	        "prepared this treebank based on interviews with Khunsari speakers."
650	    ),
651	    "quc_iu": (
652	        "UD Kiche-IU is a treebank consisting of sentences from a variety of"
653	        " text domains but principally dictionary example sentences and "
654	        "linguistic examples."
655	    ),
656	    "koi_uh": (
657	        "This is a Komi-Permyak literary language treebank consisting of "
658	        "original and translated texts."
659	    ),
660	    "kpv_ikdp": (
661	        "This treebank consists of dialectal transcriptions of spoken "
662	        "Komi-Zyrian. The current texts are short recorded segments from "
663	        "different areas where the Iva dialect of Komi language is spoken."
664	    ),
665	    "kpv_lattice": (
666	        "UD Komi-Zyrian Lattice is a treebank of written standard Komi-Zyrian."
667	    ),
668	    "ko_gsd": (
669	        "The Google Korean Universal Dependency Treebank is first converted "
670	        "from the Universal Dependency Treebank v2.0 (legacy), and then "
671	        "enhanced by Chun et al., 2018."
672	    ),
673	    "ko_kaist": (
674	        "The KAIST Korean Universal Dependency Treebank is generated by Chun "
675	        "et al., 2018 from the constituency trees in the KAIST Tree-Tagging "
676	        "Corpus."
677	    ),
678	    "ko_pud": (
679	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
680	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
681	        "Raw Text to Universal Dependencies."
682	    ),
683	    "kmr_mg": (
684	        "The UD Kurmanji corpus is a corpus of Kurmanji Kurdish. It contains "
685	        "fiction and encyclopaedic texts in roughly equal measure. It has been"
686	        " annotated natively in accordance with the UD annotation scheme."
687	    ),
688	    "la_ittb": (
689	        "Latin data from the _Index Thomisticus_ Treebank. Data are taken from"
690	        " the _Index Thomisticus_ corpus by Roberto Busa SJ, which contains "
691	        "the complete work by Thomas Aquinas (12251274; Medieval Latin) and "
692	        "by 61 other authors related to Thomas."
693	    ),
694	    "la_udante": (
695	        "The UDante treebank is based on the Latin texts of Dante Alighieri, "
696	        "taken from the DanteSearch corpus, originally created at the "
697	        "University of Pisa, Italy. It is a treebank of Latin language, more "
698	        "precisely of literary Medieval Latin (XIVth century)."
699	    ),
700	    "la_llct": (
701	        "This Universal Dependencies version of the LLCT (Late Latin Charter "
702	        "Treebank) consists of an automated conversion of the LLCT2 treebank "
703	        "from the Latin Dependency Treebank (LDT) format into the Universal "
704	        "Dependencies standard."
705	    ),
706	    "la_perseus": (
707	        "This Universal Dependencies Latin Treebank consists of an automatic "
708	        "conversion of a selection of passages from the Ancient Greek and "
709	        "Latin Dependency Treebank 2.1"
710	    ),
711	    "la_proiel": (
712	        "The Latin PROIEL treebank is based on the Latin data from the PROIEL "
713	        "treebank, and contains most of the Vulgate New Testament translations"
714	        " plus selections from Caesar's Gallic War, Cicero's Letters to "
715	        "Atticus, Palladius' Opus Agriculturae and the first book of Cicero's "
716	        "De officiis."
717	    ),
718	    "lv_lvtb": (
719	        "Latvian UD Treebank is based on Latvian Treebank (LVTB), being "
720	        "created at University of Latvia, Institute of Mathematics and "
721	        "Computer Science, Artificial Intelligence Laboratory."
722	    ),
723	    "lij_glt": (
724	        "The Genoese Ligurian Treebank is a small, manually annotated "
725	        "collection of contemporary Ligurian prose. The focus of the treebank "
726	        "is written Genoese, the koin variety of Ligurian which is associated"
727	        " with today's literary, journalistic and academic ligurophone sphere."
728	    ),
729	    "lt_alksnis": (
730	        "The Lithuanian dependency treebank ALKSNIS v3.0 (Vytautas Magnus "
731	        "University)."
732	    ),
733	    "lt_hse": (
734	        "Lithuanian treebank annotated manually (dependencies) using the "
735	        "Morphological Annotator by CCL, Vytautas Magnus University "
736	        "(http://tekstynas.vdu.lt/) and manual disambiguation. A pilot version"
737	        " which includes news and an essay by Tomas Venclova is available "
738	        "here."
739	    ),
740	    "olo_kkpp": (
741	        "UD Livvi-KKPP is a manually annotated new corpus of Livvi-Karelian "
742	        "made directly in the Universal dependencies annotation scheme. The "
743	        "data is collected from VepKar corpora and consists of mostly modern "
744	        "news texts but also some stories and educational texts."
745	    ),
746	    "nds_lsdc": (
747	        "The UD Low Saxon LSDC dataset consists of sentences in 18 Low Saxon "
748	        "dialects from both Germany and the Netherlands. These sentences are "
749	        "(or are to become) part of the LSDC dataset and represent the "
750	        "language from the 19th and early 20th century in genres such as short"
751	        " stories, novels, speeches, letters and fairytales."
752	    ),
753	    "mt_mudt": (
754	        "MUDT (Maltese Universal Dependencies Treebank) is a manually "
755	        "annotated treebank of Maltese, a Semitic language of Malta descended "
756	        "from North African Arabic with a significant amount of Italo-Romance "
757	        "influence. MUDT was designed as a balanced corpus with four major "
758	        "genres (see Splitting below) represented roughly equally."
759	    ),
760	    "gv_cadhan": (
761	        "This is the Cadhan Aonair UD treebank for Manx Gaelic, created by "
762	        "Kevin Scannell."
763	    ),
764	    "mr_ufal": (
765	        "UD Marathi is a manually annotated treebank consisting primarily of "
766	        "stories from Wikisource, and parts of an article on Wikipedia."
767	    ),
768	    "gun_dooley": (
769	        "UD Mbya_Guarani-Dooley is a corpus of narratives written in Mby "
770	        "Guaran (Tupian) in Brazil, and collected by Robert Dooley. Due to "
771	        "copyright restrictions, the corpus that is distributed as part of UD "
772	        "only contains the annotation (tags, features, relations) while the "
773	        "FORM and LEMMA columns are empty."
774	    ),
775	    "gun_thomas": (
776	        "UD Mbya_Guarani-Thomas is a corpus of Mby Guaran (Tupian) texts "
777	        "collected by Guillaume Thomas. The current version of the corpus "
778	        "consists of three speeches by Paulina Kerechu Nez Romero, a Mby "
779	        "Guaran speaker from Ytu, Caazap Department, Paraguay."
780	    ),
781	    "mdf_jr": (
782	        "Erme Universal Dependencies annotated texts Moksha are the origin of "
783	        "UD_Moksha-JR with annotation (CoNLL-U) for texts in the Moksha "
784	        "language, it originally consists of a sample from a number of fiction"
785	        " authors writing originals in Moksha."
786	    ),
787	    "myu_tudet": (
788	        "UD_Munduruku-TuDeT is a collection of annotated sentences in "
789	        "Munduruk. Together with UD_Akuntsu-TuDeT and UD_Tupinamba-TuDeT, "
790	        "UD_Munduruku-TuDeT is part of the TuLaR project."
791	    ),
792	    "pcm_nsc": (
793	        "A Universal Dependencies corpus for spoken Naija (Nigerian Pidgin)."
794	    ),
795	    "nyq_aha": (
796	        "The AHA Nayini Treebank is a small treebank for contemporary Nayini. "
797	        "Its corpus is collected and annotated manually. We have prepared this"
798	        " treebank based on interviews with Nayini speakers."
799	    ),
800	    "sme_giella": (
801	        "This is a North Smi treebank based on a manually disambiguated and "
802	        "function-labelled gold-standard corpus of North Smi produced by the "
803	        "Giellatekno team at UiT Norgga rktala universitehta."
804	    ),
805	    "no_bokmaal": (
806	        "The Norwegian UD treebank is based on the Bokml section of the "
807	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
808	        " Norwegian. NDT has been automatically converted to the UD scheme by "
809	        "Lilja vrelid at the University of Oslo."
810	    ),
811	    "no_nynorsk": (
812	        "The Norwegian UD treebank is based on the Nynorsk section of the "
813	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
814	        " Norwegian.  NDT has been automatically converted to the UD scheme by"
815	        " Lilja vrelid at the University of Oslo."
816	    ),
817	    "no_nynorsklia": (
818	        "This Norwegian treebank is based on the LIA treebank of transcribed "
819	        "spoken Norwegian dialects. The treebank has been automatically "
820	        "converted to the UD scheme by Lilja vrelid at the University of "
821	        "Oslo."
822	    ),
823	    "cu_proiel": (
824	        "The Old Church Slavonic (OCS) UD treebank is based on the Old Church "
825	        "Slavonic data from the PROIEL treebank and contains the text of the "
826	        "Codex Marianus New Testament translation."
827	    ),
828	    "fro_srcmf": (
829	        "UD_Old_French-SRCMF is a conversion of (part of) the SRCMF corpus "
830	        "(Syntactic Reference Corpus of Medieval French srcmf.org)."
831	    ),
832	    "orv_birchbark": (
833	        "UD Old_East_Slavic-Birchbark is based on the RNC Corpus of Birchbark "
834	        "Letters and includes documents written in 1025-1500 in an East Slavic"
835	        " vernacular (letters, household and business records, records for "
836	        "church services, spell against diseases, and other short "
837	        "inscriptions). The treebank is manually syntactically annotated in "
838	        "the UD 2.0 scheme, morphological and lexical annotation is a "
839	        "conversion of the original RNC annotation."
840	    ),
841	    "orv_rnc": (
842	        "`UD_Old_Russian-RNC` is a sample of the Middle Russian corpus "
843	        "(1300-1700), a part of the Russian National Corpus. The data were "
844	        "originally annotated according to the RNC and extended UD-Russian "
845	        "morphological schemas and UD 2.4 dependency schema."
846	    ),
847	    "orv_torot": (
848	        "UD_Old_Russian-TOROT is a conversion of a selection of the Old East "
849	        "Slavonic and Middle Russian data in the Troms Old Russian and OCS "
850	        "Treebank (TOROT), which was originally annotated in PROIEL dependency"
851	        " format."
852	    ),
853	    "otk_tonqq": (
854	        "`UD_Old_Turkish-Tonqq` is an Old Turkish treebank built upon Turkic "
855	        "script texts or sentences that are trivially convertible."
856	    ),
857	    "fa_perdt": (
858	        "The Persian Universal Dependency Treebank (PerUDT) is the result of "
859	        "automatic coversion of Persian Dependency Treebank (PerDT) with "
860	        "extensive manual corrections. Please refer to the follwoing work, if "
861	        "you use this data: Mohammad Sadegh Rasooli, Pegah Safari, Amirsaeid "
862	        "Moloodi, and Alireza Nourian. 'The Persian Dependency Treebank Made "
863	        "Universal'. 2020 (to appear)."
864	    ),
865	    "fa_seraji": (
866	        "The Persian Universal Dependency Treebank (Persian UD) is based on "
867	        "Uppsala Persian Dependency Treebank (UPDT). The conversion of the "
868	        "UPDT to the Universal Dependencies was performed semi-automatically "
869	        "with extensive manual checks and corrections."
870	    ),
871	    "pl_lfg": (
872	        "The LFG Enhanced UD treebank of Polish is based on a corpus of LFG "
873	        "(Lexical Functional Grammar) syntactic structures generated by an LFG"
874	        " grammar of Polish, POLFIE, and manually disambiguated by human "
875	        "annotators."
876	    ),
877	    "pl_pdb": (
878	        "The Polish PDB-UD treebank is based on the Polish Dependency Bank 2.0"
879	        " (PDB 2.0), created at the Institute of Computer Science, Polish "
880	        "Academy of Sciences in Warsaw. The PDB-UD treebank is an extended and"
881	        " corrected version of the Polish SZ-UD treebank (the release 1.2 to "
882	        "2.3)."
883	    ),
884	    "pl_pud": (
885	        "This is the Polish portion of the Parallel Universal Dependencies "
886	        "(PUD) treebanks, created at the Institute of Computer Science, Polish"
887	        " Academy of Sciences in Warsaw.Re"
888	    ),
889	    "pt_bosque": (
890	        "This Universal Dependencies (UD) Portuguese treebank is based on the "
891	        "Constraint Grammar converted version of the Bosque, which is part of "
892	        "the Floresta Sint(c)tica treebank. It contains both European "
893	        "(CETEMPblico) and Brazilian (CETENFolha) variants."
894	    ),
895	    "pt_gsd": (
896	        "The Brazilian Portuguese UD is converted from the Google Universal "
897	        "Dependency Treebank v2.0 (legacy)."
898	    ),
899	    "pt_pud": (
900	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
901	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
902	        "Raw Text to Universal Dependencies."
903	    ),
904	    "ro_art": (
905	        "The UD treebank ArT is a treebank of the Aromanian dialect of the "
906	        "Romanian language in UD format."
907	    ),
908	    "ro_nonstandard": (
909	        "The Romanian Non-standard UD treebank (called UAIC-RoDia) is based on"
910	        " UAIC-RoDia Treebank. UAIC-RoDia = ISLRN 156-635-615-024-0"
911	    ),
912	    "ro_rrt": (
913	        "The Romanian UD treebank (called RoRefTrees) (Barbu Mititelu et al., "
914	        "2016) is the reference treebank in UD format for standard Romanian."
915	    ),
916	    "ro_simonero": "SiMoNERo is a medical corpus of contemporary Romanian.",
917	    "ru_gsd": (
918	        "Russian Universal Dependencies Treebank annotated and converted by "
919	        "Google."
920	    ),
921	    "ru_pud": (
922	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
923	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
924	        "Raw Text to Universal Dependencies."
925	    ),
926	    "ru_syntagrus": "Russian data from the SynTagRus corpus.",
927	    "ru_taiga": (
928	        "Universal Dependencies treebank is based on data samples extracted "
929	        "from Taiga Corpus and MorphoRuEval-2017 and GramEval-2020 shared "
930	        "tasks collections."
931	    ),
932	    "sa_ufal": (
933	        "A small Sanskrit treebank of sentences from Pacatantra, an ancient "
934	        "Indian collection of interrelated fables by Vishnu Sharma."
935	    ),
936	    "sa_vedic": (
937	        "The Treebank of Vedic Sanskrit contains 4,000 sentences with 27,000 "
938	        "words chosen from metrical and prose passages of the gveda (RV), the"
939	        " aunaka recension of the Atharvaveda (S), the Maitryasahit "
940	        "(MS), and the Aitareya- (AB) and atapatha-Brhmaas (B). Lexical "
941	        "and morpho-syntactic information has been generated using a tagging "
942	        "software and manually validated. POS tags have been induced "
943	        "automatically from the morpho-sytactic information of each word."
944	    ),
945	    "gd_arcosg": (
946	        "A treebank of Scottish Gaelic based on the Annotated Reference Corpus"
947	        " Of Scottish Gaelic (ARCOSG)."
948	    ),
949	    "sr_set": (
950	        "The Serbian UD treebank is based on the "
951	        "[SETimes-SR](http://hdl.handle.net/11356/1200) corpus and additional "
952	        "news documents from the Serbian web."
953	    ),
954	    "sms_giellagas": (
955	        "The UD Skolt Sami Giellagas treebank is based almost entirely on "
956	        "spoken Skolt Sami corpora."
957	    ),
958	    "sk_snk": (
959	        "The Slovak UD treebank is based on data originally annotated as part "
960	        "of the Slovak National Corpus, following the annotation style of the "
961	        "Prague Dependency Treebank."
962	    ),
963	    "sl_ssj": (
964	        "The Slovenian UD Treebank is a rule-based conversion of the ssj500k "
965	        "treebank, the largest collection of manually syntactically annotated "
966	        "data in Slovenian, originally annotated in the JOS annotation scheme."
967	    ),
968	    "sl_sst": (
969	        "The Spoken Slovenian UD Treebank (SST) is the first syntactically "
970	        "annotated corpus of spoken Slovenian, based on a sample of the "
971	        "reference GOS corpus, a collection of transcribed audio recordings of"
972	        " monologic, dialogic and multi-party spontaneous speech in different "
973	        "everyday situations."
974	    ),
975	    "soj_aha": (
976	        "The AHA Soi Treebank is a small treebank for contemporary Soi. Its "
977	        "corpus is collected and annotated manually. We have prepared this "
978	        "treebank based on interviews with Soi speakers."
979	    ),
980	    "ajp_madar": (
981	        "The South_Levantine_Arabic-MADAR treebank consists of 100 "
982	        "manually-annotated sentences taken from the "
983	        "[MADAR](https://camel.abudhabi.nyu.edu/madar/) (Multi-Arabic Dialect "
984	        "Applications and Resources) project. "
985	    ),
986	    "es_ancora": "Spanish data from the AnCora corpus.",
987	    "es_gsd": (
988	        "The Spanish UD is converted from the content head version of the "
989	        "universal dependency treebank v2.0 (legacy)."
990	    ),
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {
1217	        "test": "UD_Arabic-PUD/r2.10/ar_pud-ud-test.conllu",
1218	    },
1219	    "hy_armtdp": {
1220	        "train": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-train.conllu",
1221	        "dev": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-dev.conllu",
1222	        "test": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-test.conllu",
1223	    },
1224	    "aii_as": {
1225	        "test": "UD_Assyrian-AS/r2.10/aii_as-ud-test.conllu",
1226	    },
1227	    "bm_crb": {
1228	        "test": "UD_Bambara-CRB/r2.10/bm_crb-ud-test.conllu",
1229	    },
1230	    "eu_bdt": {
1231	        "train": "UD_Basque-BDT/r2.10/eu_bdt-ud-train.conllu",
1232	        "dev": "UD_Basque-BDT/r2.10/eu_bdt-ud-dev.conllu",
1233	        "test": "UD_Basque-BDT/r2.10/eu_bdt-ud-test.conllu",
1234	    },
1235	    "bej_nsc": {
1236	        "test": "UD_Beja-NSC/r2.10/bej_nsc-ud-test.conllu",
1237	    },
1238	    "be_hse": {
1239	        "train": "UD_Belarusian-HSE/r2.10/be_hse-ud-train.conllu",
1240	        "dev": "UD_Belarusian-HSE/r2.10/be_hse-ud-dev.conllu",
1241	        "test": "UD_Belarusian-HSE/r2.10/be_hse-ud-test.conllu",
1242	    },
1243	    "bn_bru": {
1244	        "test": "UD_Bengali-BRU/r2.10/bn_bru-ud-test.conllu",
1245	    },
1246	    "bho_bhtb": {
1247	        "test": "UD_Bhojpuri-BHTB/r2.10/bho_bhtb-ud-test.conllu",
1248	    },
1249	    "br_keb": {
1250	        "test": "UD_Breton-KEB/r2.10/br_keb-ud-test.conllu",
1251	    },
1252	    "bg_btb": {
1253	        "train": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-train.conllu",
1254	        "dev": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-dev.conllu",
1255	        "test": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-test.conllu",
1256	    },
1257	    "bxr_bdt": {
1258	        "train": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-train.conllu",
1259	        "test": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-test.conllu",
1260	    },
1261	    "yue_hk": {
1262	        "test": "UD_Cantonese-HK/r2.10/yue_hk-ud-test.conllu",
1263	    },
1264	    "ca_ancora": {
1265	        "train": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-train.conllu",
1266	        "dev": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-dev.conllu",
1267	        "test": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-test.conllu",
1268	    },
1269	    "ceb_gja": {
1270	        "test": "UD_Cebuano-GJA/r2.10/ceb_gja-ud-test.conllu",
1271	    },
1272	    "zh_cfl": {
1273	        "test": "UD_Chinese-CFL/r2.10/zh_cfl-ud-test.conllu",
1274	    },
1275	    "zh_gsd": {
1276	        "train": "UD_Chinese-GSD/r2.10/zh_gsd-ud-train.conllu",
1277	        "dev": "UD_Chinese-GSD/r2.10/zh_gsd-ud-dev.conllu",
1278	        "test": "UD_Chinese-GSD/r2.10/zh_gsd-ud-test.conllu",
1279	    },
1280	    "zh_gsdsimp": {
1281	        "train": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-train.conllu",
1282	        "dev": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-dev.conllu",
1283	        "test": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-test.conllu",
1284	    },
1285	    "zh_hk": {
1286	        "test": "UD_Chinese-HK/r2.10/zh_hk-ud-test.conllu",
1287	    },
1288	    "zh_pud": {
1289	        "test": "UD_Chinese-PUD/r2.10/zh_pud-ud-test.conllu",
1290	    },
1291	    "ckt_hse": {
1292	        "test": "UD_Chukchi-HSE/r2.10/ckt_hse-ud-test.conllu",
1293	    },
1294	    "lzh_kyoto": {
1295	        "train": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-train.conllu",
1296	        "dev": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-dev.conllu",
1297	        "test": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-test.conllu",
1298	    },
1299	    "cop_scriptorium": {
1300	        "train": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-train.conllu",
1301	        "dev": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-dev.conllu",
1302	        "test": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-test.conllu",
1303	    },
1304	    "hr_set": {
1305	        "train": "UD_Croatian-SET/r2.10/hr_set-ud-train.conllu",
1306	        "dev": "UD_Croatian-SET/r2.10/hr_set-ud-dev.conllu",
1307	        "test": "UD_Croatian-SET/r2.10/hr_set-ud-test.conllu",
1308	    },
1309	    "cs_cac": {
1310	        "train": "UD_Czech-CAC/r2.10/cs_cac-ud-train.conllu",
1311	        "dev": "UD_Czech-CAC/r2.10/cs_cac-ud-dev.conllu",
1312	        "test": "UD_Czech-CAC/r2.10/cs_cac-ud-test.conllu",
1313	    },
1314	    "cs_cltt": {
1315	        "train": "UD_Czech-CLTT/r2.10/cs_cltt-ud-train.conllu",
1316	        "dev": "UD_Czech-CLTT/r2.10/cs_cltt-ud-dev.conllu",
1317	        "test": "UD_Czech-CLTT/r2.10/cs_cltt-ud-test.conllu",
1318	    },
1319	    "cs_fictree": {
1320	        "train": "UD_Czech-FicTree/r2.10/cs_fictree-ud-train.conllu",
1321	        "dev": "UD_Czech-FicTree/r2.10/cs_fictree-ud-dev.conllu",
1322	        "test": "UD_Czech-FicTree/r2.10/cs_fictree-ud-test.conllu",
1323	    },
1324	    "cs_pdt": {
1325	        "train": [
1326	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-l.conllu",
1327	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-m.conllu",
1328	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-c.conllu",
1329	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-v.conllu",
1330	        ],
1331	        "dev": "UD_Czech-PDT/r2.10/cs_pdt-ud-dev.conllu",
1332	        "test": "UD_Czech-PDT/r2.10/cs_pdt-ud-test.conllu",
1333	    },
1334	    "cs_pud": {
1335	        "test": "UD_Czech-PUD/r2.10/cs_pud-ud-test.conllu",
1336	    },
1337	    "da_ddt": {
1338	        "train": "UD_Danish-DDT/r2.10/da_ddt-ud-train.conllu",
1339	        "dev": "UD_Danish-DDT/r2.10/da_ddt-ud-dev.conllu",
1340	        "test": "UD_Danish-DDT/r2.10/da_ddt-ud-test.conllu",
1341	    },
1342	    "nl_alpino": {
1343	        "train": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-train.conllu",
1344	        "dev": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-dev.conllu",
1345	        "test": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-test.conllu",
1346	    },
1347	    "nl_lassysmall": {
1348	        "train": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-train.conllu",
1349	        "dev": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-dev.conllu",
1350	        "test": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-test.conllu",
1351	    },
1352	    "en_atis": {
1353	        "train": "UD_English-Atis/r2.10/en_atis-ud-train.conllu",
1354	        "dev": "UD_English-Atis/r2.10/en_atis-ud-dev.conllu",
1355	        "test": "UD_English-Atis/r2.10/en_atis-ud-test.conllu",
1356	    },
1357	    "en_esl": {
1358	        "train": "UD_English-ESL/r2.10/en_esl-ud-train.conllu",
1359	        "dev": "UD_English-ESL/r2.10/en_esl-ud-dev.conllu",
1360	        "test": "UD_English-ESL/r2.10/en_esl-ud-test.conllu",
1361	    },
1362	    "en_ewt": {
1363	        "train": "UD_English-EWT/r2.10/en_ewt-ud-train.conllu",
1364	        "dev": "UD_English-EWT/r2.10/en_ewt-ud-dev.conllu",
1365	        "test": "UD_English-EWT/r2.10/en_ewt-ud-test.conllu",
1366	    },
1367	    "en_gum": {
1368	        "train": "UD_English-GUM/r2.10/en_gum-ud-train.conllu",
1369	        "dev": "UD_English-GUM/r2.10/en_gum-ud-dev.conllu",
1370	        "test": "UD_English-GUM/r2.10/en_gum-ud-test.conllu",
1371	    },
1372	    "en_gumreddit": {
1373	        "train": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-train.conllu",
1374	        "dev": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-dev.conllu",
1375	        "test": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-test.conllu",
1376	    },
1377	    "en_lines": {
1378	        "train": "UD_English-LinES/r2.10/en_lines-ud-train.conllu",
1379	        "dev": "UD_English-LinES/r2.10/en_lines-ud-dev.conllu",
1380	        "test": "UD_English-LinES/r2.10/en_lines-ud-test.conllu",
1381	    },
1382	    "en_partut": {
1383	        "train": "UD_English-ParTUT/r2.10/en_partut-ud-train.conllu",
1384	        "dev": "UD_English-ParTUT/r2.10/en_partut-ud-dev.conllu",
1385	        "test": "UD_English-ParTUT/r2.10/en_partut-ud-test.conllu",
1386	    },
1387	    "en_pronouns": {
1388	        "test": "UD_English-Pronouns/r2.10/en_pronouns-ud-test.conllu",
1389	    },
1390	    "en_pud": {
1391	        "test": "UD_English-PUD/r2.10/en_pud-ud-test.conllu",
1392	    },
1393	    "myv_jr": {
1394	        "test": "UD_Erzya-JR/r2.10/myv_jr-ud-test.conllu",
1395	    },
1396	    "et_edt": {
1397	        "train": "UD_Estonian-EDT/r2.10/et_edt-ud-train.conllu",
1398	        "dev": "UD_Estonian-EDT/r2.10/et_edt-ud-dev.conllu",
1399	        "test": "UD_Estonian-EDT/r2.10/et_edt-ud-test.conllu",
1400	    },
1401	    "et_ewt": {
1402	        "train": "UD_Estonian-EWT/r2.10/et_ewt-ud-train.conllu",
1403	        "dev": "UD_Estonian-EWT/r2.10/et_ewt-ud-dev.conllu",
1404	        "test": "UD_Estonian-EWT/r2.10/et_ewt-ud-test.conllu",
1405	    },
1406	    "fo_farpahc": {
1407	        "train": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-train.conllu",
1408	        "dev": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-dev.conllu",
1409	        "test": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-test.conllu",
1410	    },
1411	    "fo_oft": {
1412	        "test": "UD_Faroese-OFT/r2.10/fo_oft-ud-test.conllu",
1413	    },
1414	    "fi_ftb": {
1415	        "train": "UD_Finnish-FTB/r2.10/fi_ftb-ud-train.conllu",
1416	        "dev": "UD_Finnish-FTB/r2.10/fi_ftb-ud-dev.conllu",
1417	        "test": "UD_Finnish-FTB/r2.10/fi_ftb-ud-test.conllu",
1418	    },
1419	    "fi_ood": {
1420	        "test": "UD_Finnish-OOD/r2.10/fi_ood-ud-test.conllu",
1421	    },
1422	    "fi_pud": {
1423	        "test": "UD_Finnish-PUD/r2.10/fi_pud-ud-test.conllu",
1424	    },
1425	    "fi_tdt": {
1426	        "train": "UD_Finnish-TDT/r2.10/fi_tdt-ud-train.conllu",
1427	        "dev": "UD_Finnish-TDT/r2.10/fi_tdt-ud-dev.conllu",
1428	        "test": "UD_Finnish-TDT/r2.10/fi_tdt-ud-test.conllu",
1429	    },
1430	    "fr_parisstories": {
1431	        "train": "UD_French-ParisStories/r2.10/fr_parisstories-ud-train.conllu",
1432	        "test": "UD_French-ParisStories/r2.10/fr_parisstories-ud-test.conllu",
1433	    },
1434	    "fr_fqb": {
1435	        "test": "UD_French-FQB/r2.10/fr_fqb-ud-test.conllu",
1436	    },
1437	    "fr_rhapsodie": {
1438	        "train": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-train.conllu",
1439	        "dev": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-dev.conllu",
1440	        "test": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-test.conllu",
1441	    },
1442	    "fr_ftb": {
1443	        "train": "UD_French-FTB/r2.10/fr_ftb-ud-train.conllu",
1444	        "dev": "UD_French-FTB/r2.10/fr_ftb-ud-dev.conllu",
1445	        "test": "UD_French-FTB/r2.10/fr_ftb-ud-test.conllu",
1446	    },
1447	    "fr_gsd": {
1448	        "train": "UD_French-GSD/r2.10/fr_gsd-ud-train.conllu",
1449	        "dev": "UD_French-GSD/r2.10/fr_gsd-ud-dev.conllu",
1450	        "test": "UD_French-GSD/r2.10/fr_gsd-ud-test.conllu",
1451	    },
1452	    "fr_partut": {
1453	        "train": "UD_French-ParTUT/r2.10/fr_partut-ud-train.conllu",
1454	        "dev": "UD_French-ParTUT/r2.10/fr_partut-ud-dev.conllu",
1455	        "test": "UD_French-ParTUT/r2.10/fr_partut-ud-test.conllu",
1456	    },
1457	    "fr_pud": {
1458	        "test": "UD_French-PUD/r2.10/fr_pud-ud-test.conllu",
1459	    },
1460	    "fr_sequoia": {
1461	        "train": "UD_French-Sequoia/r2.10/fr_sequoia-ud-train.conllu",
1462	        "dev": "UD_French-Sequoia/r2.10/fr_sequoia-ud-dev.conllu",
1463	        "test": "UD_French-Sequoia/r2.10/fr_sequoia-ud-test.conllu",
1464	    },
1465	    "qfn_fame": {
1466	        "test": "UD_Frisian_Dutch-Fame/r2.10/qfn_fame-ud-test.conllu",
1467	    },
1468	    "gl_ctg": {
1469	        "train": "UD_Galician-CTG/r2.10/gl_ctg-ud-train.conllu",
1470	        "dev": "UD_Galician-CTG/r2.10/gl_ctg-ud-dev.conllu",
1471	        "test": "UD_Galician-CTG/r2.10/gl_ctg-ud-test.conllu",
1472	    },
1473	    "gl_treegal": {
1474	        "train": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-train.conllu",
1475	        "test": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-test.conllu",
1476	    },
1477	    "de_gsd": {
1478	        "train": "UD_German-GSD/r2.10/de_gsd-ud-train.conllu",
1479	        "dev": "UD_German-GSD/r2.10/de_gsd-ud-dev.conllu",
1480	        "test": "UD_German-GSD/r2.10/de_gsd-ud-test.conllu",
1481	    },
1482	    "de_hdt": {
1483	        "train": [
1484	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-1.conllu",
1485	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-2.conllu",
1486	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-1.conllu",
1487	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-2.conllu",
1488	        ],
1489	        "dev": "UD_German-HDT/r2.10/de_hdt-ud-dev.conllu",
1490	        "test": "UD_German-HDT/r2.10/de_hdt-ud-test.conllu",
1491	    },
1492	    "de_lit": {
1493	        "test": "UD_German-LIT/r2.10/de_lit-ud-test.conllu",
1494	    },
1495	    "de_pud": {
1496	        "test": "UD_German-PUD/r2.10/de_pud-ud-test.conllu",
1497	    },
1498	    "got_proiel": {
1499	        "train": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-train.conllu",
1500	        "dev": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-dev.conllu",
1501	        "test": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-test.conllu",
1502	    },
1503	    "el_gdt": {
1504	        "train": "UD_Greek-GDT/r2.10/el_gdt-ud-train.conllu",
1505	        "dev": "UD_Greek-GDT/r2.10/el_gdt-ud-dev.conllu",
1506	        "test": "UD_Greek-GDT/r2.10/el_gdt-ud-test.conllu",
1507	    },
1508	    "gub_tudet": {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:406
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
405	    "gl_ctg": (
406	        "The Galician UD treebank is based on the automatic parsing of the "
407	        "Galician Technical Corpus (http://sli.uvigo.gal/CTG) created at the "
408	        "University of Vigo by the the TALG NLP research group."
409	    ),
410	    "gl_treegal": (
411	        "The Galician-TreeGal is a treebank for Galician developed at LyS "
412	        "Group (Universidade da Corua)."
413	    ),
414	    "de_gsd": (
415	        "The German UD is converted from the content head version of the "
416	        "universal dependency treebank v2.0 (legacy)."
417	    ),
418	    "de_hdt": (
419	        "UD German-HDT is a conversion of the Hamburg Dependency Treebank, "
420	        "created at the University of Hamburg through manual annotation in "
421	        "conjunction with a standard for morphologically and syntactically "
422	        "annotating sentences as well as a constraint-based parser."
423	    ),
424	    "de_lit": (
425	        "This treebank aims at gathering texts of the German literary history."
426	        " Currently, it hosts Fragments of the early Romanticism, i.e. "
427	        "aphorism-like texts mainly dealing with philosophical issues "
428	        "concerning art, beauty and related topics."
429	    ),
430	    "de_pud": (
431	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
432	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
433	        "Raw Text to Universal Dependencies."
434	    ),
435	    "got_proiel": (
436	        "The UD Gothic treebank is based on the Gothic data from the PROIEL "
437	        "treebank, and consists of Wulfila's Bible translation."
438	    ),
439	    "el_gdt": (
440	        "The Greek UD treebank (UD_Greek-GDT) is derived from the Greek "
441	        "Dependency Treebank (http://gdt.ilsp.gr), a resource developed and "
442	        "maintained by researchers at the Institute for Language and Speech "
443	        "Processing/Athena R.C. (http://www.ilsp.gr)."
444	    ),
445	    "gub_tudet": (
446	        "UD_Guajajara-TuDeT is a collection of annotated sentences in "
447	        "Guajajara. Sentences stem from multiple sources such as descriptions "
448	        "of the language, short stories, dictionaries and translations from "
449	        "the New Testament. Sentence annotation and documentation by Lorena "
450	        "Martn Rodrguez and Fabrcio Ferraz Gerardi."
451	    ),
452	    "gn_oldtudet": (
453	        "UD_Guarani-OldTuDeT is a collection of annotated texts in Old "
454	        "Guaran. All known sources in this language are being annotated: "
455	        "cathesisms, grammars (seventeenth and eighteenth century), sentences "
456	        "from dictionaries, and other texts. Sentence annotation and "
457	        "documentation by Fabrcio Ferraz Gerardi and Lorena Martn Rodrguez."
458	    ),
459	    "he_htb": "A Universal Dependencies Corpus for Hebrew.",
460	    "he_iahltwiki": (
461	        "Publicly available subset of the IAHLT UD Hebrew Treebank's Wikipedia"
462	        " section (https://www.iahlt.org/)"
463	    ),
464	    "qfn_fame": (
465	        "UD_Frisian_Dutch-Fame is a selection of 400 sentences from the FAME! "
466	        "speech corpus by Yilmaz et al. (2016a, 2016b). The treebank is "
467	        "manually annotated using the UD scheme."
468	    ),
469	    "qhe_hiencs": (
470	        "The Hindi-English Code-switching treebank is based on code-switching "
471	        "tweets of Hindi and English multilingual speakers (mostly Indian) on "
472	        "Twitter. The treebank is manually annotated using UD sceheme. The "
473	        "training and evaluations sets were seperately annotated by different "
474	        "annotators using UD v2 and v1 guidelines respectively. The evaluation"
475	        " sets are automatically converted from UD v1 to v2."
476	    ),
477	    "hi_hdtb": (
478	        "The Hindi UD treebank is based on the Hindi Dependency Treebank "
479	        "(HDTB), created at IIIT Hyderabad, India."
480	    ),
481	    "hi_pud": (
482	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
483	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
484	        "Raw Text to Universal Dependencies."
485	    ),
486	    "hu_szeged": (
487	        "The Hungarian UD treebank is derived from the Szeged Dependency "
488	        "Treebank (Vincze et al. 2010)."
489	    ),
490	    "is_modern": (
491	        "UD_Icelandic-Modern is a conversion of the modern additions to the "
492	        "Icelandic Parsed Historical Corpus (IcePaHC) to the Universal "
493	        "Dependencies scheme."
494	    ),
495	    "is_icepahc": (
496	        "UD_Icelandic-IcePaHC is a conversion of the Icelandic Parsed "
497	        "Historical Corpus (IcePaHC) to the Universal Dependencies scheme. The"
498	        " conversion was done using UDConverter."
499	    ),
500	    "is_pud": (
501	        "Icelandic-PUD is the Icelandic part of the Parallel Universal "
502	        "Dependencies (PUD) treebanks."
503	    ),
504	    "id_csui": (
505	        "UD Indonesian-CSUI is a conversion from an Indonesian constituency "
506	        "treebank in the Penn Treebank format named Kethu that was also a "
507	        "conversion from a constituency treebank built by Dinakaramani et al. "
508	        "(2015). We named this treebank Indonesian-CSUI, since all the three "
509	        "versions of the treebanks were built at Faculty of Computer Science, "
510	        "Universitas Indonesia."
511	    ),
512	    "id_gsd": (
513	        "The Indonesian UD is converted from the content head version of the "
514	        "universal dependency treebank v2.0 (legacy)."
515	    ),
516	    "id_pud": (
517	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
518	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
519	        "Raw Text to Universal Dependencies."
520	    ),
521	    "ga_idt": (
522	        "A Universal Dependencies 4910-sentence treebank for modern Irish."
523	    ),
524	    "ga_twittirish": (
525	        "A Universal Dependencies treebank of 866 tweets in modern Irish."
526	    ),
527	    "it_isdt": (
528	        "The Italian corpus annotated according to the UD annotation scheme "
529	        "was obtained by conversion from ISDT (Italian Stanford Dependency "
530	        "Treebank), released for the dependency parsing shared task of "
531	        "Evalita-2014 (Bosco et al. 2014)."
532	    ),
533	    "it_partut": (
534	        "UD_Italian-ParTUT is a conversion of a multilingual parallel treebank"
535	        " developed at the University of Turin, and consisting of a variety of"
536	        " text genres, including talks, legal texts and Wikipedia articles, "
537	        "among others."
538	    ),
539	    "it_postwita": (
540	        "PoSTWITA-UD is a collection of Italian tweets annotated in Universal "
541	        "Dependencies that can be exploited for the training of NLP systems to"
542	        " enhance their performance on social media texts."
543	    ),
544	    "it_markit": (
545	        "It is MarkIT That is New: An Italian Treebank of Marked "
546	        "Constructions. Teresa Paccosi, Alessio Palmero Aprosio and Sara "
547	        "Tonelli, To appear in Proceedings of the Eighth Italian Conference on"
548	        " Computational Linguistics 2022 (CLIC-it 2021)"
549	    ),
550	    "it_valico": (
551	        "Manually corrected Treebank of Learner Italian drawn from the Valico "
552	        "corpus and correspondent corrected sentences."
553	    ),
554	    "it_pud": (
555	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
556	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
557	        "Raw Text to Universal Dependencies."
558	    ),
559	    "it_twittiro": (
560	        "TWITTIR-UD is a collection of ironic Italian tweets annotated in "
561	        "Universal Dependencies. The treebank can be exploited for the "
562	        "training of NLP systems to enhance their performance on social media "
563	        "texts, and in particular, for irony detection purposes."
564	    ),
565	    "it_vit": (
566	        "The UD_Italian-VIT corpus was obtained by conversion from VIT (Venice"
567	        " Italian Treebank), developed at the Laboratory of Computational "
568	        "Linguistics of the Universit Ca' Foscari in Venice (Delmonte et al. "
569	        "2007; Delmonte 2009; "
570	        "http://rondelmo.it/resource/VIT/Browser-VIT/index.htm)."
571	    ),
572	    "ja_pudluw": (
573	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
574	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
575	        "Raw Text to Universal Dependencies."
576	    ),
577	    "ja_bccwjluw": (
578	        "This Universal Dependencies (UD) Japanese treebank is based on the "
579	        "definition of UD Japanese convention described in the UD "
580	        "documentation. The original sentences are from `Balanced Corpus of "
581	        "Contemporary Written Japanese'(BCCWJ)."
582	    ),
583	    "ja_gsdluw": (
584	        "This Universal Dependencies (UD) Japanese treebank is based on the "
585	        "definition of UD Japanese convention described in the UD "
586	        "documentation. The original sentences are from Google UDT 2.0."
587	    ),
588	    "ja_bccwj": (
589	        "This Universal Dependencies (UD) Japanese treebank is based on the "
590	        "definition of UD Japanese convention described in the UD "
591	        "documentation. The original sentences are from `Balanced Corpus of "
592	        "Contemporary Written Japanese'(BCCWJ)."
593	    ),
594	    "ja_gsd": (
595	        "This Universal Dependencies (UD) Japanese treebank is based on the "
596	        "definition of UD Japanese convention described in the UD "
597	        "documentation.  The original sentences are from Google UDT 2.0."
598	    ),
599	    "ja_modern": (
600	        "This Universal Dependencies (UD) Japanese treebank is based on the "
601	        "definition of UD Japanese convention described in the UD "
602	        "documentation. The original sentences are from `Corpus of Historical "
603	        "Japanese' (CHJ)."
604	    ),
605	    "ja_pud": (
606	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
607	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
608	        " Raw Text to Universal "
609	        "Dependencies](http://universaldependencies.org/conll17/)."
610	    ),
611	    "jv_csui": (
612	        "UD Javanese-CSUI is a dependency treebank in Javanese, a regional "
613	        "language in Indonesia with more than 60 million users. The original "
614	        "sentences were taken from OPUS, especially from the WikiMatrix v1 "
615	        "corpus. We revised the sentences that contained more Indonesian words"
616	        " than Javanese words and manually annotated them."
617	    ),
618	    "urb_tudet": (
619	        "UD_Kaapor-TuDeT is a collection of annotated sentences in Ka'apor. "
620	        "The project is a work in progress and the treebank is being updated "
621	        "on a regular basis."
622	    ),
623	    "xnr_kdtb": (
624	        "The Kangri UD Treebank (KDTB) is a part of the Universal Dependency "
625	        "treebank project."
626	    ),
627	    "krl_kkpp": (
628	        "UD Karelian-KKPP is a manually annotated new corpus of Karelian made "
629	        "in Universal dependencies annotation scheme. The data is collected "
630	        "from VepKar corpora and consists of mostly modern news texts but also"
631	        " some stories and educational texts."
632	    ),
633	    "kk_ktb": (
634	        "The UD Kazakh treebank is a combination of text from various sources "
635	        "including Wikipedia, some folk tales, sentences from the UDHR, news "
636	        "and phrasebook sentences. Sentences IDs include partial document "
637	        "identifiers."
638	    ),
639	    "arr_tudet": (
640	        "UD_Karo-TuDeT is a collection of annotated sentences in Karo. The "
641	        "sentences stem from the only grammatical description of the language "
642	        "(Gabas, 1999) and from the sentences in the dictionary by the same "
643	        "author (Gabas, 2007). Sentence annotation and documentation by "
644	        "Fabrcio Ferraz Gerardi."
645	    ),
646	    "kfm_aha": (
647	        "The AHA Khunsari Treebank is a small treebank for contemporary "
648	        "Khunsari. Its corpus is collected and annotated manually. We have "
649	        "prepared this treebank based on interviews with Khunsari speakers."
650	    ),
651	    "quc_iu": (
652	        "UD Kiche-IU is a treebank consisting of sentences from a variety of"
653	        " text domains but principally dictionary example sentences and "
654	        "linguistic examples."
655	    ),
656	    "koi_uh": (
657	        "This is a Komi-Permyak literary language treebank consisting of "
658	        "original and translated texts."
659	    ),
660	    "kpv_ikdp": (
661	        "This treebank consists of dialectal transcriptions of spoken "
662	        "Komi-Zyrian. The current texts are short recorded segments from "
663	        "different areas where the Iva dialect of Komi language is spoken."
664	    ),
665	    "kpv_lattice": (
666	        "UD Komi-Zyrian Lattice is a treebank of written standard Komi-Zyrian."
667	    ),
668	    "ko_gsd": (
669	        "The Google Korean Universal Dependency Treebank is first converted "
670	        "from the Universal Dependency Treebank v2.0 (legacy), and then "
671	        "enhanced by Chun et al., 2018."
672	    ),
673	    "ko_kaist": (
674	        "The KAIST Korean Universal Dependency Treebank is generated by Chun "
675	        "et al., 2018 from the constituency trees in the KAIST Tree-Tagging "
676	        "Corpus."
677	    ),
678	    "ko_pud": (
679	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
680	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
681	        "Raw Text to Universal Dependencies."
682	    ),
683	    "kmr_mg": (
684	        "The UD Kurmanji corpus is a corpus of Kurmanji Kurdish. It contains "
685	        "fiction and encyclopaedic texts in roughly equal measure. It has been"
686	        " annotated natively in accordance with the UD annotation scheme."
687	    ),
688	    "la_ittb": (
689	        "Latin data from the _Index Thomisticus_ Treebank. Data are taken from"
690	        " the _Index Thomisticus_ corpus by Roberto Busa SJ, which contains "
691	        "the complete work by Thomas Aquinas (12251274; Medieval Latin) and "
692	        "by 61 other authors related to Thomas."
693	    ),
694	    "la_udante": (
695	        "The UDante treebank is based on the Latin texts of Dante Alighieri, "
696	        "taken from the DanteSearch corpus, originally created at the "
697	        "University of Pisa, Italy. It is a treebank of Latin language, more "
698	        "precisely of literary Medieval Latin (XIVth century)."
699	    ),
700	    "la_llct": (
701	        "This Universal Dependencies version of the LLCT (Late Latin Charter "
702	        "Treebank) consists of an automated conversion of the LLCT2 treebank "
703	        "from the Latin Dependency Treebank (LDT) format into the Universal "
704	        "Dependencies standard."
705	    ),
706	    "la_perseus": (
707	        "This Universal Dependencies Latin Treebank consists of an automatic "
708	        "conversion of a selection of passages from the Ancient Greek and "
709	        "Latin Dependency Treebank 2.1"
710	    ),
711	    "la_proiel": (
712	        "The Latin PROIEL treebank is based on the Latin data from the PROIEL "
713	        "treebank, and contains most of the Vulgate New Testament translations"
714	        " plus selections from Caesar's Gallic War, Cicero's Letters to "
715	        "Atticus, Palladius' Opus Agriculturae and the first book of Cicero's "
716	        "De officiis."
717	    ),
718	    "lv_lvtb": (
719	        "Latvian UD Treebank is based on Latvian Treebank (LVTB), being "
720	        "created at University of Latvia, Institute of Mathematics and "
721	        "Computer Science, Artificial Intelligence Laboratory."
722	    ),
723	    "lij_glt": (
724	        "The Genoese Ligurian Treebank is a small, manually annotated "
725	        "collection of contemporary Ligurian prose. The focus of the treebank "
726	        "is written Genoese, the koin variety of Ligurian which is associated"
727	        " with today's literary, journalistic and academic ligurophone sphere."
728	    ),
729	    "lt_alksnis": (
730	        "The Lithuanian dependency treebank ALKSNIS v3.0 (Vytautas Magnus "
731	        "University)."
732	    ),
733	    "lt_hse": (
734	        "Lithuanian treebank annotated manually (dependencies) using the "
735	        "Morphological Annotator by CCL, Vytautas Magnus University "
736	        "(http://tekstynas.vdu.lt/) and manual disambiguation. A pilot version"
737	        " which includes news and an essay by Tomas Venclova is available "
738	        "here."
739	    ),
740	    "olo_kkpp": (
741	        "UD Livvi-KKPP is a manually annotated new corpus of Livvi-Karelian "
742	        "made directly in the Universal dependencies annotation scheme. The "
743	        "data is collected from VepKar corpora and consists of mostly modern "
744	        "news texts but also some stories and educational texts."
745	    ),
746	    "nds_lsdc": (
747	        "The UD Low Saxon LSDC dataset consists of sentences in 18 Low Saxon "
748	        "dialects from both Germany and the Netherlands. These sentences are "
749	        "(or are to become) part of the LSDC dataset and represent the "
750	        "language from the 19th and early 20th century in genres such as short"
751	        " stories, novels, speeches, letters and fairytales."
752	    ),
753	    "mt_mudt": (
754	        "MUDT (Maltese Universal Dependencies Treebank) is a manually "
755	        "annotated treebank of Maltese, a Semitic language of Malta descended "
756	        "from North African Arabic with a significant amount of Italo-Romance "
757	        "influence. MUDT was designed as a balanced corpus with four major "
758	        "genres (see Splitting below) represented roughly equally."
759	    ),
760	    "gv_cadhan": (
761	        "This is the Cadhan Aonair UD treebank for Manx Gaelic, created by "
762	        "Kevin Scannell."
763	    ),
764	    "mr_ufal": (
765	        "UD Marathi is a manually annotated treebank consisting primarily of "
766	        "stories from Wikisource, and parts of an article on Wikipedia."
767	    ),
768	    "gun_dooley": (
769	        "UD Mbya_Guarani-Dooley is a corpus of narratives written in Mby "
770	        "Guaran (Tupian) in Brazil, and collected by Robert Dooley. Due to "
771	        "copyright restrictions, the corpus that is distributed as part of UD "
772	        "only contains the annotation (tags, features, relations) while the "
773	        "FORM and LEMMA columns are empty."
774	    ),
775	    "gun_thomas": (
776	        "UD Mbya_Guarani-Thomas is a corpus of Mby Guaran (Tupian) texts "
777	        "collected by Guillaume Thomas. The current version of the corpus "
778	        "consists of three speeches by Paulina Kerechu Nez Romero, a Mby "
779	        "Guaran speaker from Ytu, Caazap Department, Paraguay."
780	    ),
781	    "mdf_jr": (
782	        "Erme Universal Dependencies annotated texts Moksha are the origin of "
783	        "UD_Moksha-JR with annotation (CoNLL-U) for texts in the Moksha "
784	        "language, it originally consists of a sample from a number of fiction"
785	        " authors writing originals in Moksha."
786	    ),
787	    "myu_tudet": (
788	        "UD_Munduruku-TuDeT is a collection of annotated sentences in "
789	        "Munduruk. Together with UD_Akuntsu-TuDeT and UD_Tupinamba-TuDeT, "
790	        "UD_Munduruku-TuDeT is part of the TuLaR project."
791	    ),
792	    "pcm_nsc": (
793	        "A Universal Dependencies corpus for spoken Naija (Nigerian Pidgin)."
794	    ),
795	    "nyq_aha": (
796	        "The AHA Nayini Treebank is a small treebank for contemporary Nayini. "
797	        "Its corpus is collected and annotated manually. We have prepared this"
798	        " treebank based on interviews with Nayini speakers."
799	    ),
800	    "sme_giella": (
801	        "This is a North Smi treebank based on a manually disambiguated and "
802	        "function-labelled gold-standard corpus of North Smi produced by the "
803	        "Giellatekno team at UiT Norgga rktala universitehta."
804	    ),
805	    "no_bokmaal": (
806	        "The Norwegian UD treebank is based on the Bokml section of the "
807	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
808	        " Norwegian. NDT has been automatically converted to the UD scheme by "
809	        "Lilja vrelid at the University of Oslo."
810	    ),
811	    "no_nynorsk": (
812	        "The Norwegian UD treebank is based on the Nynorsk section of the "
813	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
814	        " Norwegian.  NDT has been automatically converted to the UD scheme by"
815	        " Lilja vrelid at the University of Oslo."
816	    ),
817	    "no_nynorsklia": (
818	        "This Norwegian treebank is based on the LIA treebank of transcribed "
819	        "spoken Norwegian dialects. The treebank has been automatically "
820	        "converted to the UD scheme by Lilja vrelid at the University of "
821	        "Oslo."
822	    ),
823	    "cu_proiel": (
824	        "The Old Church Slavonic (OCS) UD treebank is based on the Old Church "
825	        "Slavonic data from the PROIEL treebank and contains the text of the "
826	        "Codex Marianus New Testament translation."
827	    ),
828	    "fro_srcmf": (
829	        "UD_Old_French-SRCMF is a conversion of (part of) the SRCMF corpus "
830	        "(Syntactic Reference Corpus of Medieval French srcmf.org)."
831	    ),
832	    "orv_birchbark": (
833	        "UD Old_East_Slavic-Birchbark is based on the RNC Corpus of Birchbark "
834	        "Letters and includes documents written in 1025-1500 in an East Slavic"
835	        " vernacular (letters, household and business records, records for "
836	        "church services, spell against diseases, and other short "
837	        "inscriptions). The treebank is manually syntactically annotated in "
838	        "the UD 2.0 scheme, morphological and lexical annotation is a "
839	        "conversion of the original RNC annotation."
840	    ),
841	    "orv_rnc": (
842	        "`UD_Old_Russian-RNC` is a sample of the Middle Russian corpus "
843	        "(1300-1700), a part of the Russian National Corpus. The data were "
844	        "originally annotated according to the RNC and extended UD-Russian "
845	        "morphological schemas and UD 2.4 dependency schema."
846	    ),
847	    "orv_torot": (
848	        "UD_Old_Russian-TOROT is a conversion of a selection of the Old East "
849	        "Slavonic and Middle Russian data in the Troms Old Russian and OCS "
850	        "Treebank (TOROT), which was originally annotated in PROIEL dependency"
851	        " format."
852	    ),
853	    "otk_tonqq": (
854	        "`UD_Old_Turkish-Tonqq` is an Old Turkish treebank built upon Turkic "
855	        "script texts or sentences that are trivially convertible."
856	    ),
857	    "fa_perdt": (
858	        "The Persian Universal Dependency Treebank (PerUDT) is the result of "
859	        "automatic coversion of Persian Dependency Treebank (PerDT) with "
860	        "extensive manual corrections. Please refer to the follwoing work, if "
861	        "you use this data: Mohammad Sadegh Rasooli, Pegah Safari, Amirsaeid "
862	        "Moloodi, and Alireza Nourian. 'The Persian Dependency Treebank Made "
863	        "Universal'. 2020 (to appear)."
864	    ),
865	    "fa_seraji": (
866	        "The Persian Universal Dependency Treebank (Persian UD) is based on "
867	        "Uppsala Persian Dependency Treebank (UPDT). The conversion of the "
868	        "UPDT to the Universal Dependencies was performed semi-automatically "
869	        "with extensive manual checks and corrections."
870	    ),
871	    "pl_lfg": (
872	        "The LFG Enhanced UD treebank of Polish is based on a corpus of LFG "
873	        "(Lexical Functional Grammar) syntactic structures generated by an LFG"
874	        " grammar of Polish, POLFIE, and manually disambiguated by human "
875	        "annotators."
876	    ),
877	    "pl_pdb": (
878	        "The Polish PDB-UD treebank is based on the Polish Dependency Bank 2.0"
879	        " (PDB 2.0), created at the Institute of Computer Science, Polish "
880	        "Academy of Sciences in Warsaw. The PDB-UD treebank is an extended and"
881	        " corrected version of the Polish SZ-UD treebank (the release 1.2 to "
882	        "2.3)."
883	    ),
884	    "pl_pud": (
885	        "This is the Polish portion of the Parallel Universal Dependencies "
886	        "(PUD) treebanks, created at the Institute of Computer Science, Polish"
887	        " Academy of Sciences in Warsaw.Re"
888	    ),
889	    "pt_bosque": (
890	        "This Universal Dependencies (UD) Portuguese treebank is based on the "
891	        "Constraint Grammar converted version of the Bosque, which is part of "
892	        "the Floresta Sint(c)tica treebank. It contains both European "
893	        "(CETEMPblico) and Brazilian (CETENFolha) variants."
894	    ),
895	    "pt_gsd": (
896	        "The Brazilian Portuguese UD is converted from the Google Universal "
897	        "Dependency Treebank v2.0 (legacy)."
898	    ),
899	    "pt_pud": (
900	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
901	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
902	        "Raw Text to Universal Dependencies."
903	    ),
904	    "ro_art": (
905	        "The UD treebank ArT is a treebank of the Aromanian dialect of the "
906	        "Romanian language in UD format."
907	    ),
908	    "ro_nonstandard": (
909	        "The Romanian Non-standard UD treebank (called UAIC-RoDia) is based on"
910	        " UAIC-RoDia Treebank. UAIC-RoDia = ISLRN 156-635-615-024-0"
911	    ),
912	    "ro_rrt": (
913	        "The Romanian UD treebank (called RoRefTrees) (Barbu Mititelu et al., "
914	        "2016) is the reference treebank in UD format for standard Romanian."
915	    ),
916	    "ro_simonero": "SiMoNERo is a medical corpus of contemporary Romanian.",
917	    "ru_gsd": (
918	        "Russian Universal Dependencies Treebank annotated and converted by "
919	        "Google."
920	    ),
921	    "ru_pud": (
922	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
923	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
924	        "Raw Text to Universal Dependencies."
925	    ),
926	    "ru_syntagrus": "Russian data from the SynTagRus corpus.",
927	    "ru_taiga": (
928	        "Universal Dependencies treebank is based on data samples extracted "
929	        "from Taiga Corpus and MorphoRuEval-2017 and GramEval-2020 shared "
930	        "tasks collections."
931	    ),
932	    "sa_ufal": (
933	        "A small Sanskrit treebank of sentences from Pacatantra, an ancient "
934	        "Indian collection of interrelated fables by Vishnu Sharma."
935	    ),
936	    "sa_vedic": (
937	        "The Treebank of Vedic Sanskrit contains 4,000 sentences with 27,000 "
938	        "words chosen from metrical and prose passages of the gveda (RV), the"
939	        " aunaka recension of the Atharvaveda (S), the Maitryasahit "
940	        "(MS), and the Aitareya- (AB) and atapatha-Brhmaas (B). Lexical "
941	        "and morpho-syntactic information has been generated using a tagging "
942	        "software and manually validated. POS tags have been induced "
943	        "automatically from the morpho-sytactic information of each word."
944	    ),
945	    "gd_arcosg": (
946	        "A treebank of Scottish Gaelic based on the Annotated Reference Corpus"
947	        " Of Scottish Gaelic (ARCOSG)."
948	    ),
949	    "sr_set": (
950	        "The Serbian UD treebank is based on the "
951	        "[SETimes-SR](http://hdl.handle.net/11356/1200) corpus and additional "
952	        "news documents from the Serbian web."
953	    ),
954	    "sms_giellagas": (
955	        "The UD Skolt Sami Giellagas treebank is based almost entirely on "
956	        "spoken Skolt Sami corpora."
957	    ),
958	    "sk_snk": (
959	        "The Slovak UD treebank is based on data originally annotated as part "
960	        "of the Slovak National Corpus, following the annotation style of the "
961	        "Prague Dependency Treebank."
962	    ),
963	    "sl_ssj": (
964	        "The Slovenian UD Treebank is a rule-based conversion of the ssj500k "
965	        "treebank, the largest collection of manually syntactically annotated "
966	        "data in Slovenian, originally annotated in the JOS annotation scheme."
967	    ),
968	    "sl_sst": (
969	        "The Spoken Slovenian UD Treebank (SST) is the first syntactically "
970	        "annotated corpus of spoken Slovenian, based on a sample of the "
971	        "reference GOS corpus, a collection of transcribed audio recordings of"
972	        " monologic, dialogic and multi-party spontaneous speech in different "
973	        "everyday situations."
974	    ),
975	    "soj_aha": (
976	        "The AHA Soi Treebank is a small treebank for contemporary Soi. Its "
977	        "corpus is collected and annotated manually. We have prepared this "
978	        "treebank based on interviews with Soi speakers."
979	    ),
980	    "ajp_madar": (
981	        "The South_Levantine_Arabic-MADAR treebank consists of 100 "
982	        "manually-annotated sentences taken from the "
983	        "[MADAR](https://camel.abudhabi.nyu.edu/madar/) (Multi-Arabic Dialect "
984	        "Applications and Resources) project. "
985	    ),
986	    "es_ancora": "Spanish data from the AnCora corpus.",
987	    "es_gsd": (
988	        "The Spanish UD is converted from the content head version of the "
989	        "universal dependency treebank v2.0 (legacy)."
990	    ),
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {
1217	        "test": "UD_Arabic-PUD/r2.10/ar_pud-ud-test.conllu",
1218	    },
1219	    "hy_armtdp": {
1220	        "train": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-train.conllu",
1221	        "dev": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-dev.conllu",
1222	        "test": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-test.conllu",
1223	    },
1224	    "aii_as": {
1225	        "test": "UD_Assyrian-AS/r2.10/aii_as-ud-test.conllu",
1226	    },
1227	    "bm_crb": {
1228	        "test": "UD_Bambara-CRB/r2.10/bm_crb-ud-test.conllu",
1229	    },
1230	    "eu_bdt": {
1231	        "train": "UD_Basque-BDT/r2.10/eu_bdt-ud-train.conllu",
1232	        "dev": "UD_Basque-BDT/r2.10/eu_bdt-ud-dev.conllu",
1233	        "test": "UD_Basque-BDT/r2.10/eu_bdt-ud-test.conllu",
1234	    },
1235	    "bej_nsc": {
1236	        "test": "UD_Beja-NSC/r2.10/bej_nsc-ud-test.conllu",
1237	    },
1238	    "be_hse": {
1239	        "train": "UD_Belarusian-HSE/r2.10/be_hse-ud-train.conllu",
1240	        "dev": "UD_Belarusian-HSE/r2.10/be_hse-ud-dev.conllu",
1241	        "test": "UD_Belarusian-HSE/r2.10/be_hse-ud-test.conllu",
1242	    },
1243	    "bn_bru": {
1244	        "test": "UD_Bengali-BRU/r2.10/bn_bru-ud-test.conllu",
1245	    },
1246	    "bho_bhtb": {
1247	        "test": "UD_Bhojpuri-BHTB/r2.10/bho_bhtb-ud-test.conllu",
1248	    },
1249	    "br_keb": {
1250	        "test": "UD_Breton-KEB/r2.10/br_keb-ud-test.conllu",
1251	    },
1252	    "bg_btb": {
1253	        "train": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-train.conllu",
1254	        "dev": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-dev.conllu",
1255	        "test": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-test.conllu",
1256	    },
1257	    "bxr_bdt": {
1258	        "train": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-train.conllu",
1259	        "test": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-test.conllu",
1260	    },
1261	    "yue_hk": {
1262	        "test": "UD_Cantonese-HK/r2.10/yue_hk-ud-test.conllu",
1263	    },
1264	    "ca_ancora": {
1265	        "train": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-train.conllu",
1266	        "dev": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-dev.conllu",
1267	        "test": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-test.conllu",
1268	    },
1269	    "ceb_gja": {
1270	        "test": "UD_Cebuano-GJA/r2.10/ceb_gja-ud-test.conllu",
1271	    },
1272	    "zh_cfl": {
1273	        "test": "UD_Chinese-CFL/r2.10/zh_cfl-ud-test.conllu",
1274	    },
1275	    "zh_gsd": {
1276	        "train": "UD_Chinese-GSD/r2.10/zh_gsd-ud-train.conllu",
1277	        "dev": "UD_Chinese-GSD/r2.10/zh_gsd-ud-dev.conllu",
1278	        "test": "UD_Chinese-GSD/r2.10/zh_gsd-ud-test.conllu",
1279	    },
1280	    "zh_gsdsimp": {
1281	        "train": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-train.conllu",
1282	        "dev": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-dev.conllu",
1283	        "test": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-test.conllu",
1284	    },
1285	    "zh_hk": {
1286	        "test": "UD_Chinese-HK/r2.10/zh_hk-ud-test.conllu",
1287	    },
1288	    "zh_pud": {
1289	        "test": "UD_Chinese-PUD/r2.10/zh_pud-ud-test.conllu",
1290	    },
1291	    "ckt_hse": {
1292	        "test": "UD_Chukchi-HSE/r2.10/ckt_hse-ud-test.conllu",
1293	    },
1294	    "lzh_kyoto": {
1295	        "train": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-train.conllu",
1296	        "dev": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-dev.conllu",
1297	        "test": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-test.conllu",
1298	    },
1299	    "cop_scriptorium": {
1300	        "train": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-train.conllu",
1301	        "dev": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-dev.conllu",
1302	        "test": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-test.conllu",
1303	    },
1304	    "hr_set": {
1305	        "train": "UD_Croatian-SET/r2.10/hr_set-ud-train.conllu",
1306	        "dev": "UD_Croatian-SET/r2.10/hr_set-ud-dev.conllu",
1307	        "test": "UD_Croatian-SET/r2.10/hr_set-ud-test.conllu",
1308	    },
1309	    "cs_cac": {
1310	        "train": "UD_Czech-CAC/r2.10/cs_cac-ud-train.conllu",
1311	        "dev": "UD_Czech-CAC/r2.10/cs_cac-ud-dev.conllu",
1312	        "test": "UD_Czech-CAC/r2.10/cs_cac-ud-test.conllu",
1313	    },
1314	    "cs_cltt": {
1315	        "train": "UD_Czech-CLTT/r2.10/cs_cltt-ud-train.conllu",
1316	        "dev": "UD_Czech-CLTT/r2.10/cs_cltt-ud-dev.conllu",
1317	        "test": "UD_Czech-CLTT/r2.10/cs_cltt-ud-test.conllu",
1318	    },
1319	    "cs_fictree": {
1320	        "train": "UD_Czech-FicTree/r2.10/cs_fictree-ud-train.conllu",
1321	        "dev": "UD_Czech-FicTree/r2.10/cs_fictree-ud-dev.conllu",
1322	        "test": "UD_Czech-FicTree/r2.10/cs_fictree-ud-test.conllu",
1323	    },
1324	    "cs_pdt": {
1325	        "train": [
1326	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-l.conllu",
1327	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-m.conllu",
1328	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-c.conllu",
1329	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-v.conllu",
1330	        ],
1331	        "dev": "UD_Czech-PDT/r2.10/cs_pdt-ud-dev.conllu",
1332	        "test": "UD_Czech-PDT/r2.10/cs_pdt-ud-test.conllu",
1333	    },
1334	    "cs_pud": {
1335	        "test": "UD_Czech-PUD/r2.10/cs_pud-ud-test.conllu",
1336	    },
1337	    "da_ddt": {
1338	        "train": "UD_Danish-DDT/r2.10/da_ddt-ud-train.conllu",
1339	        "dev": "UD_Danish-DDT/r2.10/da_ddt-ud-dev.conllu",
1340	        "test": "UD_Danish-DDT/r2.10/da_ddt-ud-test.conllu",
1341	    },
1342	    "nl_alpino": {
1343	        "train": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-train.conllu",
1344	        "dev": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-dev.conllu",
1345	        "test": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-test.conllu",
1346	    },
1347	    "nl_lassysmall": {
1348	        "train": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-train.conllu",
1349	        "dev": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-dev.conllu",
1350	        "test": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-test.conllu",
1351	    },
1352	    "en_atis": {
1353	        "train": "UD_English-Atis/r2.10/en_atis-ud-train.conllu",
1354	        "dev": "UD_English-Atis/r2.10/en_atis-ud-dev.conllu",
1355	        "test": "UD_English-Atis/r2.10/en_atis-ud-test.conllu",
1356	    },
1357	    "en_esl": {
1358	        "train": "UD_English-ESL/r2.10/en_esl-ud-train.conllu",
1359	        "dev": "UD_English-ESL/r2.10/en_esl-ud-dev.conllu",
1360	        "test": "UD_English-ESL/r2.10/en_esl-ud-test.conllu",
1361	    },
1362	    "en_ewt": {
1363	        "train": "UD_English-EWT/r2.10/en_ewt-ud-train.conllu",
1364	        "dev": "UD_English-EWT/r2.10/en_ewt-ud-dev.conllu",
1365	        "test": "UD_English-EWT/r2.10/en_ewt-ud-test.conllu",
1366	    },
1367	    "en_gum": {
1368	        "train": "UD_English-GUM/r2.10/en_gum-ud-train.conllu",
1369	        "dev": "UD_English-GUM/r2.10/en_gum-ud-dev.conllu",
1370	        "test": "UD_English-GUM/r2.10/en_gum-ud-test.conllu",
1371	    },
1372	    "en_gumreddit": {
1373	        "train": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-train.conllu",
1374	        "dev": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-dev.conllu",
1375	        "test": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-test.conllu",
1376	    },
1377	    "en_lines": {
1378	        "train": "UD_English-LinES/r2.10/en_lines-ud-train.conllu",
1379	        "dev": "UD_English-LinES/r2.10/en_lines-ud-dev.conllu",
1380	        "test": "UD_English-LinES/r2.10/en_lines-ud-test.conllu",
1381	    },
1382	    "en_partut": {
1383	        "train": "UD_English-ParTUT/r2.10/en_partut-ud-train.conllu",
1384	        "dev": "UD_English-ParTUT/r2.10/en_partut-ud-dev.conllu",
1385	        "test": "UD_English-ParTUT/r2.10/en_partut-ud-test.conllu",
1386	    },
1387	    "en_pronouns": {
1388	        "test": "UD_English-Pronouns/r2.10/en_pronouns-ud-test.conllu",
1389	    },
1390	    "en_pud": {
1391	        "test": "UD_English-PUD/r2.10/en_pud-ud-test.conllu",
1392	    },
1393	    "myv_jr": {
1394	        "test": "UD_Erzya-JR/r2.10/myv_jr-ud-test.conllu",
1395	    },
1396	    "et_edt": {
1397	        "train": "UD_Estonian-EDT/r2.10/et_edt-ud-train.conllu",
1398	        "dev": "UD_Estonian-EDT/r2.10/et_edt-ud-dev.conllu",
1399	        "test": "UD_Estonian-EDT/r2.10/et_edt-ud-test.conllu",
1400	    },
1401	    "et_ewt": {
1402	        "train": "UD_Estonian-EWT/r2.10/et_ewt-ud-train.conllu",
1403	        "dev": "UD_Estonian-EWT/r2.10/et_ewt-ud-dev.conllu",
1404	        "test": "UD_Estonian-EWT/r2.10/et_ewt-ud-test.conllu",
1405	    },
1406	    "fo_farpahc": {
1407	        "train": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-train.conllu",
1408	        "dev": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-dev.conllu",
1409	        "test": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-test.conllu",
1410	    },
1411	    "fo_oft": {
1412	        "test": "UD_Faroese-OFT/r2.10/fo_oft-ud-test.conllu",
1413	    },
1414	    "fi_ftb": {
1415	        "train": "UD_Finnish-FTB/r2.10/fi_ftb-ud-train.conllu",
1416	        "dev": "UD_Finnish-FTB/r2.10/fi_ftb-ud-dev.conllu",
1417	        "test": "UD_Finnish-FTB/r2.10/fi_ftb-ud-test.conllu",
1418	    },
1419	    "fi_ood": {
1420	        "test": "UD_Finnish-OOD/r2.10/fi_ood-ud-test.conllu",
1421	    },
1422	    "fi_pud": {
1423	        "test": "UD_Finnish-PUD/r2.10/fi_pud-ud-test.conllu",
1424	    },
1425	    "fi_tdt": {
1426	        "train": "UD_Finnish-TDT/r2.10/fi_tdt-ud-train.conllu",
1427	        "dev": "UD_Finnish-TDT/r2.10/fi_tdt-ud-dev.conllu",
1428	        "test": "UD_Finnish-TDT/r2.10/fi_tdt-ud-test.conllu",
1429	    },
1430	    "fr_parisstories": {
1431	        "train": "UD_French-ParisStories/r2.10/fr_parisstories-ud-train.conllu",
1432	        "test": "UD_French-ParisStories/r2.10/fr_parisstories-ud-test.conllu",
1433	    },
1434	    "fr_fqb": {
1435	        "test": "UD_French-FQB/r2.10/fr_fqb-ud-test.conllu",
1436	    },
1437	    "fr_rhapsodie": {
1438	        "train": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-train.conllu",
1439	        "dev": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-dev.conllu",
1440	        "test": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-test.conllu",
1441	    },
1442	    "fr_ftb": {
1443	        "train": "UD_French-FTB/r2.10/fr_ftb-ud-train.conllu",
1444	        "dev": "UD_French-FTB/r2.10/fr_ftb-ud-dev.conllu",
1445	        "test": "UD_French-FTB/r2.10/fr_ftb-ud-test.conllu",
1446	    },
1447	    "fr_gsd": {
1448	        "train": "UD_French-GSD/r2.10/fr_gsd-ud-train.conllu",
1449	        "dev": "UD_French-GSD/r2.10/fr_gsd-ud-dev.conllu",
1450	        "test": "UD_French-GSD/r2.10/fr_gsd-ud-test.conllu",
1451	    },
1452	    "fr_partut": {
1453	        "train": "UD_French-ParTUT/r2.10/fr_partut-ud-train.conllu",
1454	        "dev": "UD_French-ParTUT/r2.10/fr_partut-ud-dev.conllu",
1455	        "test": "UD_French-ParTUT/r2.10/fr_partut-ud-test.conllu",
1456	    },
1457	    "fr_pud": {
1458	        "test": "UD_French-PUD/r2.10/fr_pud-ud-test.conllu",
1459	    },
1460	    "fr_sequoia": {
1461	        "train": "UD_French-Sequoia/r2.10/fr_sequoia-ud-train.conllu",
1462	        "dev": "UD_French-Sequoia/r2.10/fr_sequoia-ud-dev.conllu",
1463	        "test": "UD_French-Sequoia/r2.10/fr_sequoia-ud-test.conllu",
1464	    },
1465	    "qfn_fame": {
1466	        "test": "UD_Frisian_Dutch-Fame/r2.10/qfn_fame-ud-test.conllu",
1467	    },
1468	    "gl_ctg": {
1469	        "train": "UD_Galician-CTG/r2.10/gl_ctg-ud-train.conllu",
1470	        "dev": "UD_Galician-CTG/r2.10/gl_ctg-ud-dev.conllu",
1471	        "test": "UD_Galician-CTG/r2.10/gl_ctg-ud-test.conllu",
1472	    },
1473	    "gl_treegal": {
1474	        "train": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-train.conllu",
1475	        "test": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-test.conllu",
1476	    },
1477	    "de_gsd": {
1478	        "train": "UD_German-GSD/r2.10/de_gsd-ud-train.conllu",
1479	        "dev": "UD_German-GSD/r2.10/de_gsd-ud-dev.conllu",
1480	        "test": "UD_German-GSD/r2.10/de_gsd-ud-test.conllu",
1481	    },
1482	    "de_hdt": {
1483	        "train": [
1484	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-1.conllu",
1485	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-2.conllu",
1486	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-1.conllu",
1487	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-2.conllu",
1488	        ],
1489	        "dev": "UD_German-HDT/r2.10/de_hdt-ud-dev.conllu",
1490	        "test": "UD_German-HDT/r2.10/de_hdt-ud-test.conllu",
1491	    },
1492	    "de_lit": {
1493	        "test": "UD_German-LIT/r2.10/de_lit-ud-test.conllu",
1494	    },
1495	    "de_pud": {
1496	        "test": "UD_German-PUD/r2.10/de_pud-ud-test.conllu",
1497	    },
1498	    "got_proiel": {
1499	        "train": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-train.conllu",
1500	        "dev": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-dev.conllu",
1501	        "test": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-test.conllu",
1502	    },
1503	    "el_gdt": {
1504	        "train": "UD_Greek-GDT/r2.10/el_gdt-ud-train.conllu",
1505	        "dev": "UD_Greek-GDT/r2.10/el_gdt-ud-dev.conllu",
1506	        "test": "UD_Greek-GDT/r2.10/el_gdt-ud-test.conllu",
1507	    },
1508	    "gub_tudet": {
1509	        "test": "UD_Guajajara-TuDeT/r2.10/gub_tudet-ud-test.conllu",
1510	    },
1511	    "gn_oldtudet": {
1512	        "test": "UD_Guarani-OldTuDeT/r2.10/gn_oldtudet-ud-test.conllu",
1513	    },
1514	    "he_iahltwiki": {
1515	        "train": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-train.conllu",
1516	        "dev": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-dev.conllu",
1517	        "test": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-test.conllu",
1518	    },
1519	    "he_htb": {
1520	        "train": "UD_Hebrew-HTB/r2.10/he_htb-ud-train.conllu",
1521	        "dev": "UD_Hebrew-HTB/r2.10/he_htb-ud-dev.conllu",
1522	        "test": "UD_Hebrew-HTB/r2.10/he_htb-ud-test.conllu",
1523	    },
1524	    "qhe_hiencs": {
1525	        "train": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-train.conllu",
1526	        "dev": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-dev.conllu",
1527	        "test": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-test.conllu",
1528	    },
1529	    "hi_hdtb": {
1530	        "train": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-train.conllu",
1531	        "dev": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-dev.conllu",
1532	        "test": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-test.conllu",
1533	    },
1534	    "hi_pud": {
1535	        "test": "UD_Hindi-PUD/r2.10/hi_pud-ud-test.conllu",
1536	    },
1537	    "hu_szeged": {
1538	        "train": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-train.conllu",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:440
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
439	    "el_gdt": (
440	        "The Greek UD treebank (UD_Greek-GDT) is derived from the Greek "
441	        "Dependency Treebank (http://gdt.ilsp.gr), a resource developed and "
442	        "maintained by researchers at the Institute for Language and Speech "
443	        "Processing/Athena R.C. (http://www.ilsp.gr)."
444	    ),
445	    "gub_tudet": (
446	        "UD_Guajajara-TuDeT is a collection of annotated sentences in "
447	        "Guajajara. Sentences stem from multiple sources such as descriptions "
448	        "of the language, short stories, dictionaries and translations from "
449	        "the New Testament. Sentence annotation and documentation by Lorena "
450	        "Martn Rodrguez and Fabrcio Ferraz Gerardi."
451	    ),
452	    "gn_oldtudet": (
453	        "UD_Guarani-OldTuDeT is a collection of annotated texts in Old "
454	        "Guaran. All known sources in this language are being annotated: "
455	        "cathesisms, grammars (seventeenth and eighteenth century), sentences "
456	        "from dictionaries, and other texts. Sentence annotation and "
457	        "documentation by Fabrcio Ferraz Gerardi and Lorena Martn Rodrguez."
458	    ),
459	    "he_htb": "A Universal Dependencies Corpus for Hebrew.",
460	    "he_iahltwiki": (
461	        "Publicly available subset of the IAHLT UD Hebrew Treebank's Wikipedia"
462	        " section (https://www.iahlt.org/)"
463	    ),
464	    "qfn_fame": (
465	        "UD_Frisian_Dutch-Fame is a selection of 400 sentences from the FAME! "
466	        "speech corpus by Yilmaz et al. (2016a, 2016b). The treebank is "
467	        "manually annotated using the UD scheme."
468	    ),
469	    "qhe_hiencs": (
470	        "The Hindi-English Code-switching treebank is based on code-switching "
471	        "tweets of Hindi and English multilingual speakers (mostly Indian) on "
472	        "Twitter. The treebank is manually annotated using UD sceheme. The "
473	        "training and evaluations sets were seperately annotated by different "
474	        "annotators using UD v2 and v1 guidelines respectively. The evaluation"
475	        " sets are automatically converted from UD v1 to v2."
476	    ),
477	    "hi_hdtb": (
478	        "The Hindi UD treebank is based on the Hindi Dependency Treebank "
479	        "(HDTB), created at IIIT Hyderabad, India."
480	    ),
481	    "hi_pud": (
482	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
483	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
484	        "Raw Text to Universal Dependencies."
485	    ),
486	    "hu_szeged": (
487	        "The Hungarian UD treebank is derived from the Szeged Dependency "
488	        "Treebank (Vincze et al. 2010)."
489	    ),
490	    "is_modern": (
491	        "UD_Icelandic-Modern is a conversion of the modern additions to the "
492	        "Icelandic Parsed Historical Corpus (IcePaHC) to the Universal "
493	        "Dependencies scheme."
494	    ),
495	    "is_icepahc": (
496	        "UD_Icelandic-IcePaHC is a conversion of the Icelandic Parsed "
497	        "Historical Corpus (IcePaHC) to the Universal Dependencies scheme. The"
498	        " conversion was done using UDConverter."
499	    ),
500	    "is_pud": (
501	        "Icelandic-PUD is the Icelandic part of the Parallel Universal "
502	        "Dependencies (PUD) treebanks."
503	    ),
504	    "id_csui": (
505	        "UD Indonesian-CSUI is a conversion from an Indonesian constituency "
506	        "treebank in the Penn Treebank format named Kethu that was also a "
507	        "conversion from a constituency treebank built by Dinakaramani et al. "
508	        "(2015). We named this treebank Indonesian-CSUI, since all the three "
509	        "versions of the treebanks were built at Faculty of Computer Science, "
510	        "Universitas Indonesia."
511	    ),
512	    "id_gsd": (
513	        "The Indonesian UD is converted from the content head version of the "
514	        "universal dependency treebank v2.0 (legacy)."
515	    ),
516	    "id_pud": (
517	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
518	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
519	        "Raw Text to Universal Dependencies."
520	    ),
521	    "ga_idt": (
522	        "A Universal Dependencies 4910-sentence treebank for modern Irish."
523	    ),
524	    "ga_twittirish": (
525	        "A Universal Dependencies treebank of 866 tweets in modern Irish."
526	    ),
527	    "it_isdt": (
528	        "The Italian corpus annotated according to the UD annotation scheme "
529	        "was obtained by conversion from ISDT (Italian Stanford Dependency "
530	        "Treebank), released for the dependency parsing shared task of "
531	        "Evalita-2014 (Bosco et al. 2014)."
532	    ),
533	    "it_partut": (
534	        "UD_Italian-ParTUT is a conversion of a multilingual parallel treebank"
535	        " developed at the University of Turin, and consisting of a variety of"
536	        " text genres, including talks, legal texts and Wikipedia articles, "
537	        "among others."
538	    ),
539	    "it_postwita": (
540	        "PoSTWITA-UD is a collection of Italian tweets annotated in Universal "
541	        "Dependencies that can be exploited for the training of NLP systems to"
542	        " enhance their performance on social media texts."
543	    ),
544	    "it_markit": (
545	        "It is MarkIT That is New: An Italian Treebank of Marked "
546	        "Constructions. Teresa Paccosi, Alessio Palmero Aprosio and Sara "
547	        "Tonelli, To appear in Proceedings of the Eighth Italian Conference on"
548	        " Computational Linguistics 2022 (CLIC-it 2021)"
549	    ),
550	    "it_valico": (
551	        "Manually corrected Treebank of Learner Italian drawn from the Valico "
552	        "corpus and correspondent corrected sentences."
553	    ),
554	    "it_pud": (
555	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
556	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
557	        "Raw Text to Universal Dependencies."
558	    ),
559	    "it_twittiro": (
560	        "TWITTIR-UD is a collection of ironic Italian tweets annotated in "
561	        "Universal Dependencies. The treebank can be exploited for the "
562	        "training of NLP systems to enhance their performance on social media "
563	        "texts, and in particular, for irony detection purposes."
564	    ),
565	    "it_vit": (
566	        "The UD_Italian-VIT corpus was obtained by conversion from VIT (Venice"
567	        " Italian Treebank), developed at the Laboratory of Computational "
568	        "Linguistics of the Universit Ca' Foscari in Venice (Delmonte et al. "
569	        "2007; Delmonte 2009; "
570	        "http://rondelmo.it/resource/VIT/Browser-VIT/index.htm)."
571	    ),
572	    "ja_pudluw": (
573	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
574	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
575	        "Raw Text to Universal Dependencies."
576	    ),
577	    "ja_bccwjluw": (
578	        "This Universal Dependencies (UD) Japanese treebank is based on the "
579	        "definition of UD Japanese convention described in the UD "
580	        "documentation. The original sentences are from `Balanced Corpus of "
581	        "Contemporary Written Japanese'(BCCWJ)."
582	    ),
583	    "ja_gsdluw": (
584	        "This Universal Dependencies (UD) Japanese treebank is based on the "
585	        "definition of UD Japanese convention described in the UD "
586	        "documentation. The original sentences are from Google UDT 2.0."
587	    ),
588	    "ja_bccwj": (
589	        "This Universal Dependencies (UD) Japanese treebank is based on the "
590	        "definition of UD Japanese convention described in the UD "
591	        "documentation. The original sentences are from `Balanced Corpus of "
592	        "Contemporary Written Japanese'(BCCWJ)."
593	    ),
594	    "ja_gsd": (
595	        "This Universal Dependencies (UD) Japanese treebank is based on the "
596	        "definition of UD Japanese convention described in the UD "
597	        "documentation.  The original sentences are from Google UDT 2.0."
598	    ),
599	    "ja_modern": (
600	        "This Universal Dependencies (UD) Japanese treebank is based on the "
601	        "definition of UD Japanese convention described in the UD "
602	        "documentation. The original sentences are from `Corpus of Historical "
603	        "Japanese' (CHJ)."
604	    ),
605	    "ja_pud": (
606	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
607	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
608	        " Raw Text to Universal "
609	        "Dependencies](http://universaldependencies.org/conll17/)."
610	    ),
611	    "jv_csui": (
612	        "UD Javanese-CSUI is a dependency treebank in Javanese, a regional "
613	        "language in Indonesia with more than 60 million users. The original "
614	        "sentences were taken from OPUS, especially from the WikiMatrix v1 "
615	        "corpus. We revised the sentences that contained more Indonesian words"
616	        " than Javanese words and manually annotated them."
617	    ),
618	    "urb_tudet": (
619	        "UD_Kaapor-TuDeT is a collection of annotated sentences in Ka'apor. "
620	        "The project is a work in progress and the treebank is being updated "
621	        "on a regular basis."
622	    ),
623	    "xnr_kdtb": (
624	        "The Kangri UD Treebank (KDTB) is a part of the Universal Dependency "
625	        "treebank project."
626	    ),
627	    "krl_kkpp": (
628	        "UD Karelian-KKPP is a manually annotated new corpus of Karelian made "
629	        "in Universal dependencies annotation scheme. The data is collected "
630	        "from VepKar corpora and consists of mostly modern news texts but also"
631	        " some stories and educational texts."
632	    ),
633	    "kk_ktb": (
634	        "The UD Kazakh treebank is a combination of text from various sources "
635	        "including Wikipedia, some folk tales, sentences from the UDHR, news "
636	        "and phrasebook sentences. Sentences IDs include partial document "
637	        "identifiers."
638	    ),
639	    "arr_tudet": (
640	        "UD_Karo-TuDeT is a collection of annotated sentences in Karo. The "
641	        "sentences stem from the only grammatical description of the language "
642	        "(Gabas, 1999) and from the sentences in the dictionary by the same "
643	        "author (Gabas, 2007). Sentence annotation and documentation by "
644	        "Fabrcio Ferraz Gerardi."
645	    ),
646	    "kfm_aha": (
647	        "The AHA Khunsari Treebank is a small treebank for contemporary "
648	        "Khunsari. Its corpus is collected and annotated manually. We have "
649	        "prepared this treebank based on interviews with Khunsari speakers."
650	    ),
651	    "quc_iu": (
652	        "UD Kiche-IU is a treebank consisting of sentences from a variety of"
653	        " text domains but principally dictionary example sentences and "
654	        "linguistic examples."
655	    ),
656	    "koi_uh": (
657	        "This is a Komi-Permyak literary language treebank consisting of "
658	        "original and translated texts."
659	    ),
660	    "kpv_ikdp": (
661	        "This treebank consists of dialectal transcriptions of spoken "
662	        "Komi-Zyrian. The current texts are short recorded segments from "
663	        "different areas where the Iva dialect of Komi language is spoken."
664	    ),
665	    "kpv_lattice": (
666	        "UD Komi-Zyrian Lattice is a treebank of written standard Komi-Zyrian."
667	    ),
668	    "ko_gsd": (
669	        "The Google Korean Universal Dependency Treebank is first converted "
670	        "from the Universal Dependency Treebank v2.0 (legacy), and then "
671	        "enhanced by Chun et al., 2018."
672	    ),
673	    "ko_kaist": (
674	        "The KAIST Korean Universal Dependency Treebank is generated by Chun "
675	        "et al., 2018 from the constituency trees in the KAIST Tree-Tagging "
676	        "Corpus."
677	    ),
678	    "ko_pud": (
679	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
680	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
681	        "Raw Text to Universal Dependencies."
682	    ),
683	    "kmr_mg": (
684	        "The UD Kurmanji corpus is a corpus of Kurmanji Kurdish. It contains "
685	        "fiction and encyclopaedic texts in roughly equal measure. It has been"
686	        " annotated natively in accordance with the UD annotation scheme."
687	    ),
688	    "la_ittb": (
689	        "Latin data from the _Index Thomisticus_ Treebank. Data are taken from"
690	        " the _Index Thomisticus_ corpus by Roberto Busa SJ, which contains "
691	        "the complete work by Thomas Aquinas (12251274; Medieval Latin) and "
692	        "by 61 other authors related to Thomas."
693	    ),
694	    "la_udante": (
695	        "The UDante treebank is based on the Latin texts of Dante Alighieri, "
696	        "taken from the DanteSearch corpus, originally created at the "
697	        "University of Pisa, Italy. It is a treebank of Latin language, more "
698	        "precisely of literary Medieval Latin (XIVth century)."
699	    ),
700	    "la_llct": (
701	        "This Universal Dependencies version of the LLCT (Late Latin Charter "
702	        "Treebank) consists of an automated conversion of the LLCT2 treebank "
703	        "from the Latin Dependency Treebank (LDT) format into the Universal "
704	        "Dependencies standard."
705	    ),
706	    "la_perseus": (
707	        "This Universal Dependencies Latin Treebank consists of an automatic "
708	        "conversion of a selection of passages from the Ancient Greek and "
709	        "Latin Dependency Treebank 2.1"
710	    ),
711	    "la_proiel": (
712	        "The Latin PROIEL treebank is based on the Latin data from the PROIEL "
713	        "treebank, and contains most of the Vulgate New Testament translations"
714	        " plus selections from Caesar's Gallic War, Cicero's Letters to "
715	        "Atticus, Palladius' Opus Agriculturae and the first book of Cicero's "
716	        "De officiis."
717	    ),
718	    "lv_lvtb": (
719	        "Latvian UD Treebank is based on Latvian Treebank (LVTB), being "
720	        "created at University of Latvia, Institute of Mathematics and "
721	        "Computer Science, Artificial Intelligence Laboratory."
722	    ),
723	    "lij_glt": (
724	        "The Genoese Ligurian Treebank is a small, manually annotated "
725	        "collection of contemporary Ligurian prose. The focus of the treebank "
726	        "is written Genoese, the koin variety of Ligurian which is associated"
727	        " with today's literary, journalistic and academic ligurophone sphere."
728	    ),
729	    "lt_alksnis": (
730	        "The Lithuanian dependency treebank ALKSNIS v3.0 (Vytautas Magnus "
731	        "University)."
732	    ),
733	    "lt_hse": (
734	        "Lithuanian treebank annotated manually (dependencies) using the "
735	        "Morphological Annotator by CCL, Vytautas Magnus University "
736	        "(http://tekstynas.vdu.lt/) and manual disambiguation. A pilot version"
737	        " which includes news and an essay by Tomas Venclova is available "
738	        "here."
739	    ),
740	    "olo_kkpp": (
741	        "UD Livvi-KKPP is a manually annotated new corpus of Livvi-Karelian "
742	        "made directly in the Universal dependencies annotation scheme. The "
743	        "data is collected from VepKar corpora and consists of mostly modern "
744	        "news texts but also some stories and educational texts."
745	    ),
746	    "nds_lsdc": (
747	        "The UD Low Saxon LSDC dataset consists of sentences in 18 Low Saxon "
748	        "dialects from both Germany and the Netherlands. These sentences are "
749	        "(or are to become) part of the LSDC dataset and represent the "
750	        "language from the 19th and early 20th century in genres such as short"
751	        " stories, novels, speeches, letters and fairytales."
752	    ),
753	    "mt_mudt": (
754	        "MUDT (Maltese Universal Dependencies Treebank) is a manually "
755	        "annotated treebank of Maltese, a Semitic language of Malta descended "
756	        "from North African Arabic with a significant amount of Italo-Romance "
757	        "influence. MUDT was designed as a balanced corpus with four major "
758	        "genres (see Splitting below) represented roughly equally."
759	    ),
760	    "gv_cadhan": (
761	        "This is the Cadhan Aonair UD treebank for Manx Gaelic, created by "
762	        "Kevin Scannell."
763	    ),
764	    "mr_ufal": (
765	        "UD Marathi is a manually annotated treebank consisting primarily of "
766	        "stories from Wikisource, and parts of an article on Wikipedia."
767	    ),
768	    "gun_dooley": (
769	        "UD Mbya_Guarani-Dooley is a corpus of narratives written in Mby "
770	        "Guaran (Tupian) in Brazil, and collected by Robert Dooley. Due to "
771	        "copyright restrictions, the corpus that is distributed as part of UD "
772	        "only contains the annotation (tags, features, relations) while the "
773	        "FORM and LEMMA columns are empty."
774	    ),
775	    "gun_thomas": (
776	        "UD Mbya_Guarani-Thomas is a corpus of Mby Guaran (Tupian) texts "
777	        "collected by Guillaume Thomas. The current version of the corpus "
778	        "consists of three speeches by Paulina Kerechu Nez Romero, a Mby "
779	        "Guaran speaker from Ytu, Caazap Department, Paraguay."
780	    ),
781	    "mdf_jr": (
782	        "Erme Universal Dependencies annotated texts Moksha are the origin of "
783	        "UD_Moksha-JR with annotation (CoNLL-U) for texts in the Moksha "
784	        "language, it originally consists of a sample from a number of fiction"
785	        " authors writing originals in Moksha."
786	    ),
787	    "myu_tudet": (
788	        "UD_Munduruku-TuDeT is a collection of annotated sentences in "
789	        "Munduruk. Together with UD_Akuntsu-TuDeT and UD_Tupinamba-TuDeT, "
790	        "UD_Munduruku-TuDeT is part of the TuLaR project."
791	    ),
792	    "pcm_nsc": (
793	        "A Universal Dependencies corpus for spoken Naija (Nigerian Pidgin)."
794	    ),
795	    "nyq_aha": (
796	        "The AHA Nayini Treebank is a small treebank for contemporary Nayini. "
797	        "Its corpus is collected and annotated manually. We have prepared this"
798	        " treebank based on interviews with Nayini speakers."
799	    ),
800	    "sme_giella": (
801	        "This is a North Smi treebank based on a manually disambiguated and "
802	        "function-labelled gold-standard corpus of North Smi produced by the "
803	        "Giellatekno team at UiT Norgga rktala universitehta."
804	    ),
805	    "no_bokmaal": (
806	        "The Norwegian UD treebank is based on the Bokml section of the "
807	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
808	        " Norwegian. NDT has been automatically converted to the UD scheme by "
809	        "Lilja vrelid at the University of Oslo."
810	    ),
811	    "no_nynorsk": (
812	        "The Norwegian UD treebank is based on the Nynorsk section of the "
813	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
814	        " Norwegian.  NDT has been automatically converted to the UD scheme by"
815	        " Lilja vrelid at the University of Oslo."
816	    ),
817	    "no_nynorsklia": (
818	        "This Norwegian treebank is based on the LIA treebank of transcribed "
819	        "spoken Norwegian dialects. The treebank has been automatically "
820	        "converted to the UD scheme by Lilja vrelid at the University of "
821	        "Oslo."
822	    ),
823	    "cu_proiel": (
824	        "The Old Church Slavonic (OCS) UD treebank is based on the Old Church "
825	        "Slavonic data from the PROIEL treebank and contains the text of the "
826	        "Codex Marianus New Testament translation."
827	    ),
828	    "fro_srcmf": (
829	        "UD_Old_French-SRCMF is a conversion of (part of) the SRCMF corpus "
830	        "(Syntactic Reference Corpus of Medieval French srcmf.org)."
831	    ),
832	    "orv_birchbark": (
833	        "UD Old_East_Slavic-Birchbark is based on the RNC Corpus of Birchbark "
834	        "Letters and includes documents written in 1025-1500 in an East Slavic"
835	        " vernacular (letters, household and business records, records for "
836	        "church services, spell against diseases, and other short "
837	        "inscriptions). The treebank is manually syntactically annotated in "
838	        "the UD 2.0 scheme, morphological and lexical annotation is a "
839	        "conversion of the original RNC annotation."
840	    ),
841	    "orv_rnc": (
842	        "`UD_Old_Russian-RNC` is a sample of the Middle Russian corpus "
843	        "(1300-1700), a part of the Russian National Corpus. The data were "
844	        "originally annotated according to the RNC and extended UD-Russian "
845	        "morphological schemas and UD 2.4 dependency schema."
846	    ),
847	    "orv_torot": (
848	        "UD_Old_Russian-TOROT is a conversion of a selection of the Old East "
849	        "Slavonic and Middle Russian data in the Troms Old Russian and OCS "
850	        "Treebank (TOROT), which was originally annotated in PROIEL dependency"
851	        " format."
852	    ),
853	    "otk_tonqq": (
854	        "`UD_Old_Turkish-Tonqq` is an Old Turkish treebank built upon Turkic "
855	        "script texts or sentences that are trivially convertible."
856	    ),
857	    "fa_perdt": (
858	        "The Persian Universal Dependency Treebank (PerUDT) is the result of "
859	        "automatic coversion of Persian Dependency Treebank (PerDT) with "
860	        "extensive manual corrections. Please refer to the follwoing work, if "
861	        "you use this data: Mohammad Sadegh Rasooli, Pegah Safari, Amirsaeid "
862	        "Moloodi, and Alireza Nourian. 'The Persian Dependency Treebank Made "
863	        "Universal'. 2020 (to appear)."
864	    ),
865	    "fa_seraji": (
866	        "The Persian Universal Dependency Treebank (Persian UD) is based on "
867	        "Uppsala Persian Dependency Treebank (UPDT). The conversion of the "
868	        "UPDT to the Universal Dependencies was performed semi-automatically "
869	        "with extensive manual checks and corrections."
870	    ),
871	    "pl_lfg": (
872	        "The LFG Enhanced UD treebank of Polish is based on a corpus of LFG "
873	        "(Lexical Functional Grammar) syntactic structures generated by an LFG"
874	        " grammar of Polish, POLFIE, and manually disambiguated by human "
875	        "annotators."
876	    ),
877	    "pl_pdb": (
878	        "The Polish PDB-UD treebank is based on the Polish Dependency Bank 2.0"
879	        " (PDB 2.0), created at the Institute of Computer Science, Polish "
880	        "Academy of Sciences in Warsaw. The PDB-UD treebank is an extended and"
881	        " corrected version of the Polish SZ-UD treebank (the release 1.2 to "
882	        "2.3)."
883	    ),
884	    "pl_pud": (
885	        "This is the Polish portion of the Parallel Universal Dependencies "
886	        "(PUD) treebanks, created at the Institute of Computer Science, Polish"
887	        " Academy of Sciences in Warsaw.Re"
888	    ),
889	    "pt_bosque": (
890	        "This Universal Dependencies (UD) Portuguese treebank is based on the "
891	        "Constraint Grammar converted version of the Bosque, which is part of "
892	        "the Floresta Sint(c)tica treebank. It contains both European "
893	        "(CETEMPblico) and Brazilian (CETENFolha) variants."
894	    ),
895	    "pt_gsd": (
896	        "The Brazilian Portuguese UD is converted from the Google Universal "
897	        "Dependency Treebank v2.0 (legacy)."
898	    ),
899	    "pt_pud": (
900	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
901	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
902	        "Raw Text to Universal Dependencies."
903	    ),
904	    "ro_art": (
905	        "The UD treebank ArT is a treebank of the Aromanian dialect of the "
906	        "Romanian language in UD format."
907	    ),
908	    "ro_nonstandard": (
909	        "The Romanian Non-standard UD treebank (called UAIC-RoDia) is based on"
910	        " UAIC-RoDia Treebank. UAIC-RoDia = ISLRN 156-635-615-024-0"
911	    ),
912	    "ro_rrt": (
913	        "The Romanian UD treebank (called RoRefTrees) (Barbu Mititelu et al., "
914	        "2016) is the reference treebank in UD format for standard Romanian."
915	    ),
916	    "ro_simonero": "SiMoNERo is a medical corpus of contemporary Romanian.",
917	    "ru_gsd": (
918	        "Russian Universal Dependencies Treebank annotated and converted by "
919	        "Google."
920	    ),
921	    "ru_pud": (
922	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
923	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
924	        "Raw Text to Universal Dependencies."
925	    ),
926	    "ru_syntagrus": "Russian data from the SynTagRus corpus.",
927	    "ru_taiga": (
928	        "Universal Dependencies treebank is based on data samples extracted "
929	        "from Taiga Corpus and MorphoRuEval-2017 and GramEval-2020 shared "
930	        "tasks collections."
931	    ),
932	    "sa_ufal": (
933	        "A small Sanskrit treebank of sentences from Pacatantra, an ancient "
934	        "Indian collection of interrelated fables by Vishnu Sharma."
935	    ),
936	    "sa_vedic": (
937	        "The Treebank of Vedic Sanskrit contains 4,000 sentences with 27,000 "
938	        "words chosen from metrical and prose passages of the gveda (RV), the"
939	        " aunaka recension of the Atharvaveda (S), the Maitryasahit "
940	        "(MS), and the Aitareya- (AB) and atapatha-Brhmaas (B). Lexical "
941	        "and morpho-syntactic information has been generated using a tagging "
942	        "software and manually validated. POS tags have been induced "
943	        "automatically from the morpho-sytactic information of each word."
944	    ),
945	    "gd_arcosg": (
946	        "A treebank of Scottish Gaelic based on the Annotated Reference Corpus"
947	        " Of Scottish Gaelic (ARCOSG)."
948	    ),
949	    "sr_set": (
950	        "The Serbian UD treebank is based on the "
951	        "[SETimes-SR](http://hdl.handle.net/11356/1200) corpus and additional "
952	        "news documents from the Serbian web."
953	    ),
954	    "sms_giellagas": (
955	        "The UD Skolt Sami Giellagas treebank is based almost entirely on "
956	        "spoken Skolt Sami corpora."
957	    ),
958	    "sk_snk": (
959	        "The Slovak UD treebank is based on data originally annotated as part "
960	        "of the Slovak National Corpus, following the annotation style of the "
961	        "Prague Dependency Treebank."
962	    ),
963	    "sl_ssj": (
964	        "The Slovenian UD Treebank is a rule-based conversion of the ssj500k "
965	        "treebank, the largest collection of manually syntactically annotated "
966	        "data in Slovenian, originally annotated in the JOS annotation scheme."
967	    ),
968	    "sl_sst": (
969	        "The Spoken Slovenian UD Treebank (SST) is the first syntactically "
970	        "annotated corpus of spoken Slovenian, based on a sample of the "
971	        "reference GOS corpus, a collection of transcribed audio recordings of"
972	        " monologic, dialogic and multi-party spontaneous speech in different "
973	        "everyday situations."
974	    ),
975	    "soj_aha": (
976	        "The AHA Soi Treebank is a small treebank for contemporary Soi. Its "
977	        "corpus is collected and annotated manually. We have prepared this "
978	        "treebank based on interviews with Soi speakers."
979	    ),
980	    "ajp_madar": (
981	        "The South_Levantine_Arabic-MADAR treebank consists of 100 "
982	        "manually-annotated sentences taken from the "
983	        "[MADAR](https://camel.abudhabi.nyu.edu/madar/) (Multi-Arabic Dialect "
984	        "Applications and Resources) project. "
985	    ),
986	    "es_ancora": "Spanish data from the AnCora corpus.",
987	    "es_gsd": (
988	        "The Spanish UD is converted from the content head version of the "
989	        "universal dependency treebank v2.0 (legacy)."
990	    ),
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {
1217	        "test": "UD_Arabic-PUD/r2.10/ar_pud-ud-test.conllu",
1218	    },
1219	    "hy_armtdp": {
1220	        "train": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-train.conllu",
1221	        "dev": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-dev.conllu",
1222	        "test": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-test.conllu",
1223	    },
1224	    "aii_as": {
1225	        "test": "UD_Assyrian-AS/r2.10/aii_as-ud-test.conllu",
1226	    },
1227	    "bm_crb": {
1228	        "test": "UD_Bambara-CRB/r2.10/bm_crb-ud-test.conllu",
1229	    },
1230	    "eu_bdt": {
1231	        "train": "UD_Basque-BDT/r2.10/eu_bdt-ud-train.conllu",
1232	        "dev": "UD_Basque-BDT/r2.10/eu_bdt-ud-dev.conllu",
1233	        "test": "UD_Basque-BDT/r2.10/eu_bdt-ud-test.conllu",
1234	    },
1235	    "bej_nsc": {
1236	        "test": "UD_Beja-NSC/r2.10/bej_nsc-ud-test.conllu",
1237	    },
1238	    "be_hse": {
1239	        "train": "UD_Belarusian-HSE/r2.10/be_hse-ud-train.conllu",
1240	        "dev": "UD_Belarusian-HSE/r2.10/be_hse-ud-dev.conllu",
1241	        "test": "UD_Belarusian-HSE/r2.10/be_hse-ud-test.conllu",
1242	    },
1243	    "bn_bru": {
1244	        "test": "UD_Bengali-BRU/r2.10/bn_bru-ud-test.conllu",
1245	    },
1246	    "bho_bhtb": {
1247	        "test": "UD_Bhojpuri-BHTB/r2.10/bho_bhtb-ud-test.conllu",
1248	    },
1249	    "br_keb": {
1250	        "test": "UD_Breton-KEB/r2.10/br_keb-ud-test.conllu",
1251	    },
1252	    "bg_btb": {
1253	        "train": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-train.conllu",
1254	        "dev": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-dev.conllu",
1255	        "test": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-test.conllu",
1256	    },
1257	    "bxr_bdt": {
1258	        "train": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-train.conllu",
1259	        "test": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-test.conllu",
1260	    },
1261	    "yue_hk": {
1262	        "test": "UD_Cantonese-HK/r2.10/yue_hk-ud-test.conllu",
1263	    },
1264	    "ca_ancora": {
1265	        "train": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-train.conllu",
1266	        "dev": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-dev.conllu",
1267	        "test": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-test.conllu",
1268	    },
1269	    "ceb_gja": {
1270	        "test": "UD_Cebuano-GJA/r2.10/ceb_gja-ud-test.conllu",
1271	    },
1272	    "zh_cfl": {
1273	        "test": "UD_Chinese-CFL/r2.10/zh_cfl-ud-test.conllu",
1274	    },
1275	    "zh_gsd": {
1276	        "train": "UD_Chinese-GSD/r2.10/zh_gsd-ud-train.conllu",
1277	        "dev": "UD_Chinese-GSD/r2.10/zh_gsd-ud-dev.conllu",
1278	        "test": "UD_Chinese-GSD/r2.10/zh_gsd-ud-test.conllu",
1279	    },
1280	    "zh_gsdsimp": {
1281	        "train": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-train.conllu",
1282	        "dev": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-dev.conllu",
1283	        "test": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-test.conllu",
1284	    },
1285	    "zh_hk": {
1286	        "test": "UD_Chinese-HK/r2.10/zh_hk-ud-test.conllu",
1287	    },
1288	    "zh_pud": {
1289	        "test": "UD_Chinese-PUD/r2.10/zh_pud-ud-test.conllu",
1290	    },
1291	    "ckt_hse": {
1292	        "test": "UD_Chukchi-HSE/r2.10/ckt_hse-ud-test.conllu",
1293	    },
1294	    "lzh_kyoto": {
1295	        "train": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-train.conllu",
1296	        "dev": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-dev.conllu",
1297	        "test": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-test.conllu",
1298	    },
1299	    "cop_scriptorium": {
1300	        "train": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-train.conllu",
1301	        "dev": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-dev.conllu",
1302	        "test": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-test.conllu",
1303	    },
1304	    "hr_set": {
1305	        "train": "UD_Croatian-SET/r2.10/hr_set-ud-train.conllu",
1306	        "dev": "UD_Croatian-SET/r2.10/hr_set-ud-dev.conllu",
1307	        "test": "UD_Croatian-SET/r2.10/hr_set-ud-test.conllu",
1308	    },
1309	    "cs_cac": {
1310	        "train": "UD_Czech-CAC/r2.10/cs_cac-ud-train.conllu",
1311	        "dev": "UD_Czech-CAC/r2.10/cs_cac-ud-dev.conllu",
1312	        "test": "UD_Czech-CAC/r2.10/cs_cac-ud-test.conllu",
1313	    },
1314	    "cs_cltt": {
1315	        "train": "UD_Czech-CLTT/r2.10/cs_cltt-ud-train.conllu",
1316	        "dev": "UD_Czech-CLTT/r2.10/cs_cltt-ud-dev.conllu",
1317	        "test": "UD_Czech-CLTT/r2.10/cs_cltt-ud-test.conllu",
1318	    },
1319	    "cs_fictree": {
1320	        "train": "UD_Czech-FicTree/r2.10/cs_fictree-ud-train.conllu",
1321	        "dev": "UD_Czech-FicTree/r2.10/cs_fictree-ud-dev.conllu",
1322	        "test": "UD_Czech-FicTree/r2.10/cs_fictree-ud-test.conllu",
1323	    },
1324	    "cs_pdt": {
1325	        "train": [
1326	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-l.conllu",
1327	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-m.conllu",
1328	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-c.conllu",
1329	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-v.conllu",
1330	        ],
1331	        "dev": "UD_Czech-PDT/r2.10/cs_pdt-ud-dev.conllu",
1332	        "test": "UD_Czech-PDT/r2.10/cs_pdt-ud-test.conllu",
1333	    },
1334	    "cs_pud": {
1335	        "test": "UD_Czech-PUD/r2.10/cs_pud-ud-test.conllu",
1336	    },
1337	    "da_ddt": {
1338	        "train": "UD_Danish-DDT/r2.10/da_ddt-ud-train.conllu",
1339	        "dev": "UD_Danish-DDT/r2.10/da_ddt-ud-dev.conllu",
1340	        "test": "UD_Danish-DDT/r2.10/da_ddt-ud-test.conllu",
1341	    },
1342	    "nl_alpino": {
1343	        "train": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-train.conllu",
1344	        "dev": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-dev.conllu",
1345	        "test": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-test.conllu",
1346	    },
1347	    "nl_lassysmall": {
1348	        "train": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-train.conllu",
1349	        "dev": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-dev.conllu",
1350	        "test": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-test.conllu",
1351	    },
1352	    "en_atis": {
1353	        "train": "UD_English-Atis/r2.10/en_atis-ud-train.conllu",
1354	        "dev": "UD_English-Atis/r2.10/en_atis-ud-dev.conllu",
1355	        "test": "UD_English-Atis/r2.10/en_atis-ud-test.conllu",
1356	    },
1357	    "en_esl": {
1358	        "train": "UD_English-ESL/r2.10/en_esl-ud-train.conllu",
1359	        "dev": "UD_English-ESL/r2.10/en_esl-ud-dev.conllu",
1360	        "test": "UD_English-ESL/r2.10/en_esl-ud-test.conllu",
1361	    },
1362	    "en_ewt": {
1363	        "train": "UD_English-EWT/r2.10/en_ewt-ud-train.conllu",
1364	        "dev": "UD_English-EWT/r2.10/en_ewt-ud-dev.conllu",
1365	        "test": "UD_English-EWT/r2.10/en_ewt-ud-test.conllu",
1366	    },
1367	    "en_gum": {
1368	        "train": "UD_English-GUM/r2.10/en_gum-ud-train.conllu",
1369	        "dev": "UD_English-GUM/r2.10/en_gum-ud-dev.conllu",
1370	        "test": "UD_English-GUM/r2.10/en_gum-ud-test.conllu",
1371	    },
1372	    "en_gumreddit": {
1373	        "train": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-train.conllu",
1374	        "dev": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-dev.conllu",
1375	        "test": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-test.conllu",
1376	    },
1377	    "en_lines": {
1378	        "train": "UD_English-LinES/r2.10/en_lines-ud-train.conllu",
1379	        "dev": "UD_English-LinES/r2.10/en_lines-ud-dev.conllu",
1380	        "test": "UD_English-LinES/r2.10/en_lines-ud-test.conllu",
1381	    },
1382	    "en_partut": {
1383	        "train": "UD_English-ParTUT/r2.10/en_partut-ud-train.conllu",
1384	        "dev": "UD_English-ParTUT/r2.10/en_partut-ud-dev.conllu",
1385	        "test": "UD_English-ParTUT/r2.10/en_partut-ud-test.conllu",
1386	    },
1387	    "en_pronouns": {
1388	        "test": "UD_English-Pronouns/r2.10/en_pronouns-ud-test.conllu",
1389	    },
1390	    "en_pud": {
1391	        "test": "UD_English-PUD/r2.10/en_pud-ud-test.conllu",
1392	    },
1393	    "myv_jr": {
1394	        "test": "UD_Erzya-JR/r2.10/myv_jr-ud-test.conllu",
1395	    },
1396	    "et_edt": {
1397	        "train": "UD_Estonian-EDT/r2.10/et_edt-ud-train.conllu",
1398	        "dev": "UD_Estonian-EDT/r2.10/et_edt-ud-dev.conllu",
1399	        "test": "UD_Estonian-EDT/r2.10/et_edt-ud-test.conllu",
1400	    },
1401	    "et_ewt": {
1402	        "train": "UD_Estonian-EWT/r2.10/et_ewt-ud-train.conllu",
1403	        "dev": "UD_Estonian-EWT/r2.10/et_ewt-ud-dev.conllu",
1404	        "test": "UD_Estonian-EWT/r2.10/et_ewt-ud-test.conllu",
1405	    },
1406	    "fo_farpahc": {
1407	        "train": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-train.conllu",
1408	        "dev": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-dev.conllu",
1409	        "test": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-test.conllu",
1410	    },
1411	    "fo_oft": {
1412	        "test": "UD_Faroese-OFT/r2.10/fo_oft-ud-test.conllu",
1413	    },
1414	    "fi_ftb": {
1415	        "train": "UD_Finnish-FTB/r2.10/fi_ftb-ud-train.conllu",
1416	        "dev": "UD_Finnish-FTB/r2.10/fi_ftb-ud-dev.conllu",
1417	        "test": "UD_Finnish-FTB/r2.10/fi_ftb-ud-test.conllu",
1418	    },
1419	    "fi_ood": {
1420	        "test": "UD_Finnish-OOD/r2.10/fi_ood-ud-test.conllu",
1421	    },
1422	    "fi_pud": {
1423	        "test": "UD_Finnish-PUD/r2.10/fi_pud-ud-test.conllu",
1424	    },
1425	    "fi_tdt": {
1426	        "train": "UD_Finnish-TDT/r2.10/fi_tdt-ud-train.conllu",
1427	        "dev": "UD_Finnish-TDT/r2.10/fi_tdt-ud-dev.conllu",
1428	        "test": "UD_Finnish-TDT/r2.10/fi_tdt-ud-test.conllu",
1429	    },
1430	    "fr_parisstories": {
1431	        "train": "UD_French-ParisStories/r2.10/fr_parisstories-ud-train.conllu",
1432	        "test": "UD_French-ParisStories/r2.10/fr_parisstories-ud-test.conllu",
1433	    },
1434	    "fr_fqb": {
1435	        "test": "UD_French-FQB/r2.10/fr_fqb-ud-test.conllu",
1436	    },
1437	    "fr_rhapsodie": {
1438	        "train": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-train.conllu",
1439	        "dev": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-dev.conllu",
1440	        "test": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-test.conllu",
1441	    },
1442	    "fr_ftb": {
1443	        "train": "UD_French-FTB/r2.10/fr_ftb-ud-train.conllu",
1444	        "dev": "UD_French-FTB/r2.10/fr_ftb-ud-dev.conllu",
1445	        "test": "UD_French-FTB/r2.10/fr_ftb-ud-test.conllu",
1446	    },
1447	    "fr_gsd": {
1448	        "train": "UD_French-GSD/r2.10/fr_gsd-ud-train.conllu",
1449	        "dev": "UD_French-GSD/r2.10/fr_gsd-ud-dev.conllu",
1450	        "test": "UD_French-GSD/r2.10/fr_gsd-ud-test.conllu",
1451	    },
1452	    "fr_partut": {
1453	        "train": "UD_French-ParTUT/r2.10/fr_partut-ud-train.conllu",
1454	        "dev": "UD_French-ParTUT/r2.10/fr_partut-ud-dev.conllu",
1455	        "test": "UD_French-ParTUT/r2.10/fr_partut-ud-test.conllu",
1456	    },
1457	    "fr_pud": {
1458	        "test": "UD_French-PUD/r2.10/fr_pud-ud-test.conllu",
1459	    },
1460	    "fr_sequoia": {
1461	        "train": "UD_French-Sequoia/r2.10/fr_sequoia-ud-train.conllu",
1462	        "dev": "UD_French-Sequoia/r2.10/fr_sequoia-ud-dev.conllu",
1463	        "test": "UD_French-Sequoia/r2.10/fr_sequoia-ud-test.conllu",
1464	    },
1465	    "qfn_fame": {
1466	        "test": "UD_Frisian_Dutch-Fame/r2.10/qfn_fame-ud-test.conllu",
1467	    },
1468	    "gl_ctg": {
1469	        "train": "UD_Galician-CTG/r2.10/gl_ctg-ud-train.conllu",
1470	        "dev": "UD_Galician-CTG/r2.10/gl_ctg-ud-dev.conllu",
1471	        "test": "UD_Galician-CTG/r2.10/gl_ctg-ud-test.conllu",
1472	    },
1473	    "gl_treegal": {
1474	        "train": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-train.conllu",
1475	        "test": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-test.conllu",
1476	    },
1477	    "de_gsd": {
1478	        "train": "UD_German-GSD/r2.10/de_gsd-ud-train.conllu",
1479	        "dev": "UD_German-GSD/r2.10/de_gsd-ud-dev.conllu",
1480	        "test": "UD_German-GSD/r2.10/de_gsd-ud-test.conllu",
1481	    },
1482	    "de_hdt": {
1483	        "train": [
1484	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-1.conllu",
1485	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-2.conllu",
1486	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-1.conllu",
1487	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-2.conllu",
1488	        ],
1489	        "dev": "UD_German-HDT/r2.10/de_hdt-ud-dev.conllu",
1490	        "test": "UD_German-HDT/r2.10/de_hdt-ud-test.conllu",
1491	    },
1492	    "de_lit": {
1493	        "test": "UD_German-LIT/r2.10/de_lit-ud-test.conllu",
1494	    },
1495	    "de_pud": {
1496	        "test": "UD_German-PUD/r2.10/de_pud-ud-test.conllu",
1497	    },
1498	    "got_proiel": {
1499	        "train": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-train.conllu",
1500	        "dev": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-dev.conllu",
1501	        "test": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-test.conllu",
1502	    },
1503	    "el_gdt": {
1504	        "train": "UD_Greek-GDT/r2.10/el_gdt-ud-train.conllu",
1505	        "dev": "UD_Greek-GDT/r2.10/el_gdt-ud-dev.conllu",
1506	        "test": "UD_Greek-GDT/r2.10/el_gdt-ud-test.conllu",
1507	    },
1508	    "gub_tudet": {
1509	        "test": "UD_Guajajara-TuDeT/r2.10/gub_tudet-ud-test.conllu",
1510	    },
1511	    "gn_oldtudet": {
1512	        "test": "UD_Guarani-OldTuDeT/r2.10/gn_oldtudet-ud-test.conllu",
1513	    },
1514	    "he_iahltwiki": {
1515	        "train": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-train.conllu",
1516	        "dev": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-dev.conllu",
1517	        "test": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-test.conllu",
1518	    },
1519	    "he_htb": {
1520	        "train": "UD_Hebrew-HTB/r2.10/he_htb-ud-train.conllu",
1521	        "dev": "UD_Hebrew-HTB/r2.10/he_htb-ud-dev.conllu",
1522	        "test": "UD_Hebrew-HTB/r2.10/he_htb-ud-test.conllu",
1523	    },
1524	    "qhe_hiencs": {
1525	        "train": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-train.conllu",
1526	        "dev": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-dev.conllu",
1527	        "test": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-test.conllu",
1528	    },
1529	    "hi_hdtb": {
1530	        "train": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-train.conllu",
1531	        "dev": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-dev.conllu",
1532	        "test": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-test.conllu",
1533	    },
1534	    "hi_pud": {
1535	        "test": "UD_Hindi-PUD/r2.10/hi_pud-ud-test.conllu",
1536	    },
1537	    "hu_szeged": {
1538	        "train": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-train.conllu",
1539	        "dev": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-dev.conllu",
1540	        "test": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-test.conllu",
1541	    },
1542	    "is_modern": {
1543	        "train": "UD_Icelandic-Modern/r2.10/is_modern-ud-train.conllu",
1544	        "dev": "UD_Icelandic-Modern/r2.10/is_modern-ud-dev.conllu",
1545	        "test": "UD_Icelandic-Modern/r2.10/is_modern-ud-test.conllu",
1546	    },
1547	    "is_icepahc": {
1548	        "train": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-train.conllu",
1549	        "dev": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-dev.conllu",
1550	        "test": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-test.conllu",
1551	    },
1552	    "is_pud": {
1553	        "test": "UD_Icelandic-PUD/r2.10/is_pud-ud-test.conllu",
1554	    },
1555	    "id_csui": {
1556	        "train": "UD_Indonesian-CSUI/r2.10/id_csui-ud-train.conllu",
1557	        "test": "UD_Indonesian-CSUI/r2.10/id_csui-ud-test.conllu",
1558	    },
1559	    "id_gsd": {
1560	        "train": "UD_Indonesian-GSD/r2.10/id_gsd-ud-train.conllu",
1561	        "dev": "UD_Indonesian-GSD/r2.10/id_gsd-ud-dev.conllu",
1562	        "test": "UD_Indonesian-GSD/r2.10/id_gsd-ud-test.conllu",
1563	    },
1564	    "id_pud": {
1565	        "test": "UD_Indonesian-PUD/r2.10/id_pud-ud-test.conllu",
1566	    },
1567	    "ga_twittirish": {
1568	        "test": "UD_Irish-TwittIrish/r2.10/ga_twittirish-ud-test.conllu",
1569	    },
1570	    "ga_idt": {
1571	        "train": "UD_Irish-IDT/r2.10/ga_idt-ud-train.conllu",
1572	        "dev": "UD_Irish-IDT/r2.10/ga_idt-ud-dev.conllu",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:461
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
460	    "he_iahltwiki": (
461	        "Publicly available subset of the IAHLT UD Hebrew Treebank's Wikipedia"
462	        " section (https://www.iahlt.org/)"
463	    ),
464	    "qfn_fame": (
465	        "UD_Frisian_Dutch-Fame is a selection of 400 sentences from the FAME! "
466	        "speech corpus by Yilmaz et al. (2016a, 2016b). The treebank is "
467	        "manually annotated using the UD scheme."
468	    ),
469	    "qhe_hiencs": (
470	        "The Hindi-English Code-switching treebank is based on code-switching "
471	        "tweets of Hindi and English multilingual speakers (mostly Indian) on "
472	        "Twitter. The treebank is manually annotated using UD sceheme. The "
473	        "training and evaluations sets were seperately annotated by different "
474	        "annotators using UD v2 and v1 guidelines respectively. The evaluation"
475	        " sets are automatically converted from UD v1 to v2."
476	    ),
477	    "hi_hdtb": (
478	        "The Hindi UD treebank is based on the Hindi Dependency Treebank "
479	        "(HDTB), created at IIIT Hyderabad, India."
480	    ),
481	    "hi_pud": (
482	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
483	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
484	        "Raw Text to Universal Dependencies."
485	    ),
486	    "hu_szeged": (
487	        "The Hungarian UD treebank is derived from the Szeged Dependency "
488	        "Treebank (Vincze et al. 2010)."
489	    ),
490	    "is_modern": (
491	        "UD_Icelandic-Modern is a conversion of the modern additions to the "
492	        "Icelandic Parsed Historical Corpus (IcePaHC) to the Universal "
493	        "Dependencies scheme."
494	    ),
495	    "is_icepahc": (
496	        "UD_Icelandic-IcePaHC is a conversion of the Icelandic Parsed "
497	        "Historical Corpus (IcePaHC) to the Universal Dependencies scheme. The"
498	        " conversion was done using UDConverter."
499	    ),
500	    "is_pud": (
501	        "Icelandic-PUD is the Icelandic part of the Parallel Universal "
502	        "Dependencies (PUD) treebanks."
503	    ),
504	    "id_csui": (
505	        "UD Indonesian-CSUI is a conversion from an Indonesian constituency "
506	        "treebank in the Penn Treebank format named Kethu that was also a "
507	        "conversion from a constituency treebank built by Dinakaramani et al. "
508	        "(2015). We named this treebank Indonesian-CSUI, since all the three "
509	        "versions of the treebanks were built at Faculty of Computer Science, "
510	        "Universitas Indonesia."
511	    ),
512	    "id_gsd": (
513	        "The Indonesian UD is converted from the content head version of the "
514	        "universal dependency treebank v2.0 (legacy)."
515	    ),
516	    "id_pud": (
517	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
518	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
519	        "Raw Text to Universal Dependencies."
520	    ),
521	    "ga_idt": (
522	        "A Universal Dependencies 4910-sentence treebank for modern Irish."
523	    ),
524	    "ga_twittirish": (
525	        "A Universal Dependencies treebank of 866 tweets in modern Irish."
526	    ),
527	    "it_isdt": (
528	        "The Italian corpus annotated according to the UD annotation scheme "
529	        "was obtained by conversion from ISDT (Italian Stanford Dependency "
530	        "Treebank), released for the dependency parsing shared task of "
531	        "Evalita-2014 (Bosco et al. 2014)."
532	    ),
533	    "it_partut": (
534	        "UD_Italian-ParTUT is a conversion of a multilingual parallel treebank"
535	        " developed at the University of Turin, and consisting of a variety of"
536	        " text genres, including talks, legal texts and Wikipedia articles, "
537	        "among others."
538	    ),
539	    "it_postwita": (
540	        "PoSTWITA-UD is a collection of Italian tweets annotated in Universal "
541	        "Dependencies that can be exploited for the training of NLP systems to"
542	        " enhance their performance on social media texts."
543	    ),
544	    "it_markit": (
545	        "It is MarkIT That is New: An Italian Treebank of Marked "
546	        "Constructions. Teresa Paccosi, Alessio Palmero Aprosio and Sara "
547	        "Tonelli, To appear in Proceedings of the Eighth Italian Conference on"
548	        " Computational Linguistics 2022 (CLIC-it 2021)"
549	    ),
550	    "it_valico": (
551	        "Manually corrected Treebank of Learner Italian drawn from the Valico "
552	        "corpus and correspondent corrected sentences."
553	    ),
554	    "it_pud": (
555	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
556	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
557	        "Raw Text to Universal Dependencies."
558	    ),
559	    "it_twittiro": (
560	        "TWITTIR-UD is a collection of ironic Italian tweets annotated in "
561	        "Universal Dependencies. The treebank can be exploited for the "
562	        "training of NLP systems to enhance their performance on social media "
563	        "texts, and in particular, for irony detection purposes."
564	    ),
565	    "it_vit": (
566	        "The UD_Italian-VIT corpus was obtained by conversion from VIT (Venice"
567	        " Italian Treebank), developed at the Laboratory of Computational "
568	        "Linguistics of the Universit Ca' Foscari in Venice (Delmonte et al. "
569	        "2007; Delmonte 2009; "
570	        "http://rondelmo.it/resource/VIT/Browser-VIT/index.htm)."
571	    ),
572	    "ja_pudluw": (
573	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
574	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
575	        "Raw Text to Universal Dependencies."
576	    ),
577	    "ja_bccwjluw": (
578	        "This Universal Dependencies (UD) Japanese treebank is based on the "
579	        "definition of UD Japanese convention described in the UD "
580	        "documentation. The original sentences are from `Balanced Corpus of "
581	        "Contemporary Written Japanese'(BCCWJ)."
582	    ),
583	    "ja_gsdluw": (
584	        "This Universal Dependencies (UD) Japanese treebank is based on the "
585	        "definition of UD Japanese convention described in the UD "
586	        "documentation. The original sentences are from Google UDT 2.0."
587	    ),
588	    "ja_bccwj": (
589	        "This Universal Dependencies (UD) Japanese treebank is based on the "
590	        "definition of UD Japanese convention described in the UD "
591	        "documentation. The original sentences are from `Balanced Corpus of "
592	        "Contemporary Written Japanese'(BCCWJ)."
593	    ),
594	    "ja_gsd": (
595	        "This Universal Dependencies (UD) Japanese treebank is based on the "
596	        "definition of UD Japanese convention described in the UD "
597	        "documentation.  The original sentences are from Google UDT 2.0."
598	    ),
599	    "ja_modern": (
600	        "This Universal Dependencies (UD) Japanese treebank is based on the "
601	        "definition of UD Japanese convention described in the UD "
602	        "documentation. The original sentences are from `Corpus of Historical "
603	        "Japanese' (CHJ)."
604	    ),
605	    "ja_pud": (
606	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
607	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
608	        " Raw Text to Universal "
609	        "Dependencies](http://universaldependencies.org/conll17/)."
610	    ),
611	    "jv_csui": (
612	        "UD Javanese-CSUI is a dependency treebank in Javanese, a regional "
613	        "language in Indonesia with more than 60 million users. The original "
614	        "sentences were taken from OPUS, especially from the WikiMatrix v1 "
615	        "corpus. We revised the sentences that contained more Indonesian words"
616	        " than Javanese words and manually annotated them."
617	    ),
618	    "urb_tudet": (
619	        "UD_Kaapor-TuDeT is a collection of annotated sentences in Ka'apor. "
620	        "The project is a work in progress and the treebank is being updated "
621	        "on a regular basis."
622	    ),
623	    "xnr_kdtb": (
624	        "The Kangri UD Treebank (KDTB) is a part of the Universal Dependency "
625	        "treebank project."
626	    ),
627	    "krl_kkpp": (
628	        "UD Karelian-KKPP is a manually annotated new corpus of Karelian made "
629	        "in Universal dependencies annotation scheme. The data is collected "
630	        "from VepKar corpora and consists of mostly modern news texts but also"
631	        " some stories and educational texts."
632	    ),
633	    "kk_ktb": (
634	        "The UD Kazakh treebank is a combination of text from various sources "
635	        "including Wikipedia, some folk tales, sentences from the UDHR, news "
636	        "and phrasebook sentences. Sentences IDs include partial document "
637	        "identifiers."
638	    ),
639	    "arr_tudet": (
640	        "UD_Karo-TuDeT is a collection of annotated sentences in Karo. The "
641	        "sentences stem from the only grammatical description of the language "
642	        "(Gabas, 1999) and from the sentences in the dictionary by the same "
643	        "author (Gabas, 2007). Sentence annotation and documentation by "
644	        "Fabrcio Ferraz Gerardi."
645	    ),
646	    "kfm_aha": (
647	        "The AHA Khunsari Treebank is a small treebank for contemporary "
648	        "Khunsari. Its corpus is collected and annotated manually. We have "
649	        "prepared this treebank based on interviews with Khunsari speakers."
650	    ),
651	    "quc_iu": (
652	        "UD Kiche-IU is a treebank consisting of sentences from a variety of"
653	        " text domains but principally dictionary example sentences and "
654	        "linguistic examples."
655	    ),
656	    "koi_uh": (
657	        "This is a Komi-Permyak literary language treebank consisting of "
658	        "original and translated texts."
659	    ),
660	    "kpv_ikdp": (
661	        "This treebank consists of dialectal transcriptions of spoken "
662	        "Komi-Zyrian. The current texts are short recorded segments from "
663	        "different areas where the Iva dialect of Komi language is spoken."
664	    ),
665	    "kpv_lattice": (
666	        "UD Komi-Zyrian Lattice is a treebank of written standard Komi-Zyrian."
667	    ),
668	    "ko_gsd": (
669	        "The Google Korean Universal Dependency Treebank is first converted "
670	        "from the Universal Dependency Treebank v2.0 (legacy), and then "
671	        "enhanced by Chun et al., 2018."
672	    ),
673	    "ko_kaist": (
674	        "The KAIST Korean Universal Dependency Treebank is generated by Chun "
675	        "et al., 2018 from the constituency trees in the KAIST Tree-Tagging "
676	        "Corpus."
677	    ),
678	    "ko_pud": (
679	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
680	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
681	        "Raw Text to Universal Dependencies."
682	    ),
683	    "kmr_mg": (
684	        "The UD Kurmanji corpus is a corpus of Kurmanji Kurdish. It contains "
685	        "fiction and encyclopaedic texts in roughly equal measure. It has been"
686	        " annotated natively in accordance with the UD annotation scheme."
687	    ),
688	    "la_ittb": (
689	        "Latin data from the _Index Thomisticus_ Treebank. Data are taken from"
690	        " the _Index Thomisticus_ corpus by Roberto Busa SJ, which contains "
691	        "the complete work by Thomas Aquinas (12251274; Medieval Latin) and "
692	        "by 61 other authors related to Thomas."
693	    ),
694	    "la_udante": (
695	        "The UDante treebank is based on the Latin texts of Dante Alighieri, "
696	        "taken from the DanteSearch corpus, originally created at the "
697	        "University of Pisa, Italy. It is a treebank of Latin language, more "
698	        "precisely of literary Medieval Latin (XIVth century)."
699	    ),
700	    "la_llct": (
701	        "This Universal Dependencies version of the LLCT (Late Latin Charter "
702	        "Treebank) consists of an automated conversion of the LLCT2 treebank "
703	        "from the Latin Dependency Treebank (LDT) format into the Universal "
704	        "Dependencies standard."
705	    ),
706	    "la_perseus": (
707	        "This Universal Dependencies Latin Treebank consists of an automatic "
708	        "conversion of a selection of passages from the Ancient Greek and "
709	        "Latin Dependency Treebank 2.1"
710	    ),
711	    "la_proiel": (
712	        "The Latin PROIEL treebank is based on the Latin data from the PROIEL "
713	        "treebank, and contains most of the Vulgate New Testament translations"
714	        " plus selections from Caesar's Gallic War, Cicero's Letters to "
715	        "Atticus, Palladius' Opus Agriculturae and the first book of Cicero's "
716	        "De officiis."
717	    ),
718	    "lv_lvtb": (
719	        "Latvian UD Treebank is based on Latvian Treebank (LVTB), being "
720	        "created at University of Latvia, Institute of Mathematics and "
721	        "Computer Science, Artificial Intelligence Laboratory."
722	    ),
723	    "lij_glt": (
724	        "The Genoese Ligurian Treebank is a small, manually annotated "
725	        "collection of contemporary Ligurian prose. The focus of the treebank "
726	        "is written Genoese, the koin variety of Ligurian which is associated"
727	        " with today's literary, journalistic and academic ligurophone sphere."
728	    ),
729	    "lt_alksnis": (
730	        "The Lithuanian dependency treebank ALKSNIS v3.0 (Vytautas Magnus "
731	        "University)."
732	    ),
733	    "lt_hse": (
734	        "Lithuanian treebank annotated manually (dependencies) using the "
735	        "Morphological Annotator by CCL, Vytautas Magnus University "
736	        "(http://tekstynas.vdu.lt/) and manual disambiguation. A pilot version"
737	        " which includes news and an essay by Tomas Venclova is available "
738	        "here."
739	    ),
740	    "olo_kkpp": (
741	        "UD Livvi-KKPP is a manually annotated new corpus of Livvi-Karelian "
742	        "made directly in the Universal dependencies annotation scheme. The "
743	        "data is collected from VepKar corpora and consists of mostly modern "
744	        "news texts but also some stories and educational texts."
745	    ),
746	    "nds_lsdc": (
747	        "The UD Low Saxon LSDC dataset consists of sentences in 18 Low Saxon "
748	        "dialects from both Germany and the Netherlands. These sentences are "
749	        "(or are to become) part of the LSDC dataset and represent the "
750	        "language from the 19th and early 20th century in genres such as short"
751	        " stories, novels, speeches, letters and fairytales."
752	    ),
753	    "mt_mudt": (
754	        "MUDT (Maltese Universal Dependencies Treebank) is a manually "
755	        "annotated treebank of Maltese, a Semitic language of Malta descended "
756	        "from North African Arabic with a significant amount of Italo-Romance "
757	        "influence. MUDT was designed as a balanced corpus with four major "
758	        "genres (see Splitting below) represented roughly equally."
759	    ),
760	    "gv_cadhan": (
761	        "This is the Cadhan Aonair UD treebank for Manx Gaelic, created by "
762	        "Kevin Scannell."
763	    ),
764	    "mr_ufal": (
765	        "UD Marathi is a manually annotated treebank consisting primarily of "
766	        "stories from Wikisource, and parts of an article on Wikipedia."
767	    ),
768	    "gun_dooley": (
769	        "UD Mbya_Guarani-Dooley is a corpus of narratives written in Mby "
770	        "Guaran (Tupian) in Brazil, and collected by Robert Dooley. Due to "
771	        "copyright restrictions, the corpus that is distributed as part of UD "
772	        "only contains the annotation (tags, features, relations) while the "
773	        "FORM and LEMMA columns are empty."
774	    ),
775	    "gun_thomas": (
776	        "UD Mbya_Guarani-Thomas is a corpus of Mby Guaran (Tupian) texts "
777	        "collected by Guillaume Thomas. The current version of the corpus "
778	        "consists of three speeches by Paulina Kerechu Nez Romero, a Mby "
779	        "Guaran speaker from Ytu, Caazap Department, Paraguay."
780	    ),
781	    "mdf_jr": (
782	        "Erme Universal Dependencies annotated texts Moksha are the origin of "
783	        "UD_Moksha-JR with annotation (CoNLL-U) for texts in the Moksha "
784	        "language, it originally consists of a sample from a number of fiction"
785	        " authors writing originals in Moksha."
786	    ),
787	    "myu_tudet": (
788	        "UD_Munduruku-TuDeT is a collection of annotated sentences in "
789	        "Munduruk. Together with UD_Akuntsu-TuDeT and UD_Tupinamba-TuDeT, "
790	        "UD_Munduruku-TuDeT is part of the TuLaR project."
791	    ),
792	    "pcm_nsc": (
793	        "A Universal Dependencies corpus for spoken Naija (Nigerian Pidgin)."
794	    ),
795	    "nyq_aha": (
796	        "The AHA Nayini Treebank is a small treebank for contemporary Nayini. "
797	        "Its corpus is collected and annotated manually. We have prepared this"
798	        " treebank based on interviews with Nayini speakers."
799	    ),
800	    "sme_giella": (
801	        "This is a North Smi treebank based on a manually disambiguated and "
802	        "function-labelled gold-standard corpus of North Smi produced by the "
803	        "Giellatekno team at UiT Norgga rktala universitehta."
804	    ),
805	    "no_bokmaal": (
806	        "The Norwegian UD treebank is based on the Bokml section of the "
807	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
808	        " Norwegian. NDT has been automatically converted to the UD scheme by "
809	        "Lilja vrelid at the University of Oslo."
810	    ),
811	    "no_nynorsk": (
812	        "The Norwegian UD treebank is based on the Nynorsk section of the "
813	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
814	        " Norwegian.  NDT has been automatically converted to the UD scheme by"
815	        " Lilja vrelid at the University of Oslo."
816	    ),
817	    "no_nynorsklia": (
818	        "This Norwegian treebank is based on the LIA treebank of transcribed "
819	        "spoken Norwegian dialects. The treebank has been automatically "
820	        "converted to the UD scheme by Lilja vrelid at the University of "
821	        "Oslo."
822	    ),
823	    "cu_proiel": (
824	        "The Old Church Slavonic (OCS) UD treebank is based on the Old Church "
825	        "Slavonic data from the PROIEL treebank and contains the text of the "
826	        "Codex Marianus New Testament translation."
827	    ),
828	    "fro_srcmf": (
829	        "UD_Old_French-SRCMF is a conversion of (part of) the SRCMF corpus "
830	        "(Syntactic Reference Corpus of Medieval French srcmf.org)."
831	    ),
832	    "orv_birchbark": (
833	        "UD Old_East_Slavic-Birchbark is based on the RNC Corpus of Birchbark "
834	        "Letters and includes documents written in 1025-1500 in an East Slavic"
835	        " vernacular (letters, household and business records, records for "
836	        "church services, spell against diseases, and other short "
837	        "inscriptions). The treebank is manually syntactically annotated in "
838	        "the UD 2.0 scheme, morphological and lexical annotation is a "
839	        "conversion of the original RNC annotation."
840	    ),
841	    "orv_rnc": (
842	        "`UD_Old_Russian-RNC` is a sample of the Middle Russian corpus "
843	        "(1300-1700), a part of the Russian National Corpus. The data were "
844	        "originally annotated according to the RNC and extended UD-Russian "
845	        "morphological schemas and UD 2.4 dependency schema."
846	    ),
847	    "orv_torot": (
848	        "UD_Old_Russian-TOROT is a conversion of a selection of the Old East "
849	        "Slavonic and Middle Russian data in the Troms Old Russian and OCS "
850	        "Treebank (TOROT), which was originally annotated in PROIEL dependency"
851	        " format."
852	    ),
853	    "otk_tonqq": (
854	        "`UD_Old_Turkish-Tonqq` is an Old Turkish treebank built upon Turkic "
855	        "script texts or sentences that are trivially convertible."
856	    ),
857	    "fa_perdt": (
858	        "The Persian Universal Dependency Treebank (PerUDT) is the result of "
859	        "automatic coversion of Persian Dependency Treebank (PerDT) with "
860	        "extensive manual corrections. Please refer to the follwoing work, if "
861	        "you use this data: Mohammad Sadegh Rasooli, Pegah Safari, Amirsaeid "
862	        "Moloodi, and Alireza Nourian. 'The Persian Dependency Treebank Made "
863	        "Universal'. 2020 (to appear)."
864	    ),
865	    "fa_seraji": (
866	        "The Persian Universal Dependency Treebank (Persian UD) is based on "
867	        "Uppsala Persian Dependency Treebank (UPDT). The conversion of the "
868	        "UPDT to the Universal Dependencies was performed semi-automatically "
869	        "with extensive manual checks and corrections."
870	    ),
871	    "pl_lfg": (
872	        "The LFG Enhanced UD treebank of Polish is based on a corpus of LFG "
873	        "(Lexical Functional Grammar) syntactic structures generated by an LFG"
874	        " grammar of Polish, POLFIE, and manually disambiguated by human "
875	        "annotators."
876	    ),
877	    "pl_pdb": (
878	        "The Polish PDB-UD treebank is based on the Polish Dependency Bank 2.0"
879	        " (PDB 2.0), created at the Institute of Computer Science, Polish "
880	        "Academy of Sciences in Warsaw. The PDB-UD treebank is an extended and"
881	        " corrected version of the Polish SZ-UD treebank (the release 1.2 to "
882	        "2.3)."
883	    ),
884	    "pl_pud": (
885	        "This is the Polish portion of the Parallel Universal Dependencies "
886	        "(PUD) treebanks, created at the Institute of Computer Science, Polish"
887	        " Academy of Sciences in Warsaw.Re"
888	    ),
889	    "pt_bosque": (
890	        "This Universal Dependencies (UD) Portuguese treebank is based on the "
891	        "Constraint Grammar converted version of the Bosque, which is part of "
892	        "the Floresta Sint(c)tica treebank. It contains both European "
893	        "(CETEMPblico) and Brazilian (CETENFolha) variants."
894	    ),
895	    "pt_gsd": (
896	        "The Brazilian Portuguese UD is converted from the Google Universal "
897	        "Dependency Treebank v2.0 (legacy)."
898	    ),
899	    "pt_pud": (
900	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
901	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
902	        "Raw Text to Universal Dependencies."
903	    ),
904	    "ro_art": (
905	        "The UD treebank ArT is a treebank of the Aromanian dialect of the "
906	        "Romanian language in UD format."
907	    ),
908	    "ro_nonstandard": (
909	        "The Romanian Non-standard UD treebank (called UAIC-RoDia) is based on"
910	        " UAIC-RoDia Treebank. UAIC-RoDia = ISLRN 156-635-615-024-0"
911	    ),
912	    "ro_rrt": (
913	        "The Romanian UD treebank (called RoRefTrees) (Barbu Mititelu et al., "
914	        "2016) is the reference treebank in UD format for standard Romanian."
915	    ),
916	    "ro_simonero": "SiMoNERo is a medical corpus of contemporary Romanian.",
917	    "ru_gsd": (
918	        "Russian Universal Dependencies Treebank annotated and converted by "
919	        "Google."
920	    ),
921	    "ru_pud": (
922	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
923	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
924	        "Raw Text to Universal Dependencies."
925	    ),
926	    "ru_syntagrus": "Russian data from the SynTagRus corpus.",
927	    "ru_taiga": (
928	        "Universal Dependencies treebank is based on data samples extracted "
929	        "from Taiga Corpus and MorphoRuEval-2017 and GramEval-2020 shared "
930	        "tasks collections."
931	    ),
932	    "sa_ufal": (
933	        "A small Sanskrit treebank of sentences from Pacatantra, an ancient "
934	        "Indian collection of interrelated fables by Vishnu Sharma."
935	    ),
936	    "sa_vedic": (
937	        "The Treebank of Vedic Sanskrit contains 4,000 sentences with 27,000 "
938	        "words chosen from metrical and prose passages of the gveda (RV), the"
939	        " aunaka recension of the Atharvaveda (S), the Maitryasahit "
940	        "(MS), and the Aitareya- (AB) and atapatha-Brhmaas (B). Lexical "
941	        "and morpho-syntactic information has been generated using a tagging "
942	        "software and manually validated. POS tags have been induced "
943	        "automatically from the morpho-sytactic information of each word."
944	    ),
945	    "gd_arcosg": (
946	        "A treebank of Scottish Gaelic based on the Annotated Reference Corpus"
947	        " Of Scottish Gaelic (ARCOSG)."
948	    ),
949	    "sr_set": (
950	        "The Serbian UD treebank is based on the "
951	        "[SETimes-SR](http://hdl.handle.net/11356/1200) corpus and additional "
952	        "news documents from the Serbian web."
953	    ),
954	    "sms_giellagas": (
955	        "The UD Skolt Sami Giellagas treebank is based almost entirely on "
956	        "spoken Skolt Sami corpora."
957	    ),
958	    "sk_snk": (
959	        "The Slovak UD treebank is based on data originally annotated as part "
960	        "of the Slovak National Corpus, following the annotation style of the "
961	        "Prague Dependency Treebank."
962	    ),
963	    "sl_ssj": (
964	        "The Slovenian UD Treebank is a rule-based conversion of the ssj500k "
965	        "treebank, the largest collection of manually syntactically annotated "
966	        "data in Slovenian, originally annotated in the JOS annotation scheme."
967	    ),
968	    "sl_sst": (
969	        "The Spoken Slovenian UD Treebank (SST) is the first syntactically "
970	        "annotated corpus of spoken Slovenian, based on a sample of the "
971	        "reference GOS corpus, a collection of transcribed audio recordings of"
972	        " monologic, dialogic and multi-party spontaneous speech in different "
973	        "everyday situations."
974	    ),
975	    "soj_aha": (
976	        "The AHA Soi Treebank is a small treebank for contemporary Soi. Its "
977	        "corpus is collected and annotated manually. We have prepared this "
978	        "treebank based on interviews with Soi speakers."
979	    ),
980	    "ajp_madar": (
981	        "The South_Levantine_Arabic-MADAR treebank consists of 100 "
982	        "manually-annotated sentences taken from the "
983	        "[MADAR](https://camel.abudhabi.nyu.edu/madar/) (Multi-Arabic Dialect "
984	        "Applications and Resources) project. "
985	    ),
986	    "es_ancora": "Spanish data from the AnCora corpus.",
987	    "es_gsd": (
988	        "The Spanish UD is converted from the content head version of the "
989	        "universal dependency treebank v2.0 (legacy)."
990	    ),
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {
1217	        "test": "UD_Arabic-PUD/r2.10/ar_pud-ud-test.conllu",
1218	    },
1219	    "hy_armtdp": {
1220	        "train": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-train.conllu",
1221	        "dev": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-dev.conllu",
1222	        "test": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-test.conllu",
1223	    },
1224	    "aii_as": {
1225	        "test": "UD_Assyrian-AS/r2.10/aii_as-ud-test.conllu",
1226	    },
1227	    "bm_crb": {
1228	        "test": "UD_Bambara-CRB/r2.10/bm_crb-ud-test.conllu",
1229	    },
1230	    "eu_bdt": {
1231	        "train": "UD_Basque-BDT/r2.10/eu_bdt-ud-train.conllu",
1232	        "dev": "UD_Basque-BDT/r2.10/eu_bdt-ud-dev.conllu",
1233	        "test": "UD_Basque-BDT/r2.10/eu_bdt-ud-test.conllu",
1234	    },
1235	    "bej_nsc": {
1236	        "test": "UD_Beja-NSC/r2.10/bej_nsc-ud-test.conllu",
1237	    },
1238	    "be_hse": {
1239	        "train": "UD_Belarusian-HSE/r2.10/be_hse-ud-train.conllu",
1240	        "dev": "UD_Belarusian-HSE/r2.10/be_hse-ud-dev.conllu",
1241	        "test": "UD_Belarusian-HSE/r2.10/be_hse-ud-test.conllu",
1242	    },
1243	    "bn_bru": {
1244	        "test": "UD_Bengali-BRU/r2.10/bn_bru-ud-test.conllu",
1245	    },
1246	    "bho_bhtb": {
1247	        "test": "UD_Bhojpuri-BHTB/r2.10/bho_bhtb-ud-test.conllu",
1248	    },
1249	    "br_keb": {
1250	        "test": "UD_Breton-KEB/r2.10/br_keb-ud-test.conllu",
1251	    },
1252	    "bg_btb": {
1253	        "train": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-train.conllu",
1254	        "dev": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-dev.conllu",
1255	        "test": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-test.conllu",
1256	    },
1257	    "bxr_bdt": {
1258	        "train": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-train.conllu",
1259	        "test": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-test.conllu",
1260	    },
1261	    "yue_hk": {
1262	        "test": "UD_Cantonese-HK/r2.10/yue_hk-ud-test.conllu",
1263	    },
1264	    "ca_ancora": {
1265	        "train": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-train.conllu",
1266	        "dev": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-dev.conllu",
1267	        "test": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-test.conllu",
1268	    },
1269	    "ceb_gja": {
1270	        "test": "UD_Cebuano-GJA/r2.10/ceb_gja-ud-test.conllu",
1271	    },
1272	    "zh_cfl": {
1273	        "test": "UD_Chinese-CFL/r2.10/zh_cfl-ud-test.conllu",
1274	    },
1275	    "zh_gsd": {
1276	        "train": "UD_Chinese-GSD/r2.10/zh_gsd-ud-train.conllu",
1277	        "dev": "UD_Chinese-GSD/r2.10/zh_gsd-ud-dev.conllu",
1278	        "test": "UD_Chinese-GSD/r2.10/zh_gsd-ud-test.conllu",
1279	    },
1280	    "zh_gsdsimp": {
1281	        "train": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-train.conllu",
1282	        "dev": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-dev.conllu",
1283	        "test": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-test.conllu",
1284	    },
1285	    "zh_hk": {
1286	        "test": "UD_Chinese-HK/r2.10/zh_hk-ud-test.conllu",
1287	    },
1288	    "zh_pud": {
1289	        "test": "UD_Chinese-PUD/r2.10/zh_pud-ud-test.conllu",
1290	    },
1291	    "ckt_hse": {
1292	        "test": "UD_Chukchi-HSE/r2.10/ckt_hse-ud-test.conllu",
1293	    },
1294	    "lzh_kyoto": {
1295	        "train": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-train.conllu",
1296	        "dev": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-dev.conllu",
1297	        "test": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-test.conllu",
1298	    },
1299	    "cop_scriptorium": {
1300	        "train": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-train.conllu",
1301	        "dev": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-dev.conllu",
1302	        "test": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-test.conllu",
1303	    },
1304	    "hr_set": {
1305	        "train": "UD_Croatian-SET/r2.10/hr_set-ud-train.conllu",
1306	        "dev": "UD_Croatian-SET/r2.10/hr_set-ud-dev.conllu",
1307	        "test": "UD_Croatian-SET/r2.10/hr_set-ud-test.conllu",
1308	    },
1309	    "cs_cac": {
1310	        "train": "UD_Czech-CAC/r2.10/cs_cac-ud-train.conllu",
1311	        "dev": "UD_Czech-CAC/r2.10/cs_cac-ud-dev.conllu",
1312	        "test": "UD_Czech-CAC/r2.10/cs_cac-ud-test.conllu",
1313	    },
1314	    "cs_cltt": {
1315	        "train": "UD_Czech-CLTT/r2.10/cs_cltt-ud-train.conllu",
1316	        "dev": "UD_Czech-CLTT/r2.10/cs_cltt-ud-dev.conllu",
1317	        "test": "UD_Czech-CLTT/r2.10/cs_cltt-ud-test.conllu",
1318	    },
1319	    "cs_fictree": {
1320	        "train": "UD_Czech-FicTree/r2.10/cs_fictree-ud-train.conllu",
1321	        "dev": "UD_Czech-FicTree/r2.10/cs_fictree-ud-dev.conllu",
1322	        "test": "UD_Czech-FicTree/r2.10/cs_fictree-ud-test.conllu",
1323	    },
1324	    "cs_pdt": {
1325	        "train": [
1326	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-l.conllu",
1327	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-m.conllu",
1328	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-c.conllu",
1329	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-v.conllu",
1330	        ],
1331	        "dev": "UD_Czech-PDT/r2.10/cs_pdt-ud-dev.conllu",
1332	        "test": "UD_Czech-PDT/r2.10/cs_pdt-ud-test.conllu",
1333	    },
1334	    "cs_pud": {
1335	        "test": "UD_Czech-PUD/r2.10/cs_pud-ud-test.conllu",
1336	    },
1337	    "da_ddt": {
1338	        "train": "UD_Danish-DDT/r2.10/da_ddt-ud-train.conllu",
1339	        "dev": "UD_Danish-DDT/r2.10/da_ddt-ud-dev.conllu",
1340	        "test": "UD_Danish-DDT/r2.10/da_ddt-ud-test.conllu",
1341	    },
1342	    "nl_alpino": {
1343	        "train": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-train.conllu",
1344	        "dev": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-dev.conllu",
1345	        "test": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-test.conllu",
1346	    },
1347	    "nl_lassysmall": {
1348	        "train": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-train.conllu",
1349	        "dev": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-dev.conllu",
1350	        "test": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-test.conllu",
1351	    },
1352	    "en_atis": {
1353	        "train": "UD_English-Atis/r2.10/en_atis-ud-train.conllu",
1354	        "dev": "UD_English-Atis/r2.10/en_atis-ud-dev.conllu",
1355	        "test": "UD_English-Atis/r2.10/en_atis-ud-test.conllu",
1356	    },
1357	    "en_esl": {
1358	        "train": "UD_English-ESL/r2.10/en_esl-ud-train.conllu",
1359	        "dev": "UD_English-ESL/r2.10/en_esl-ud-dev.conllu",
1360	        "test": "UD_English-ESL/r2.10/en_esl-ud-test.conllu",
1361	    },
1362	    "en_ewt": {
1363	        "train": "UD_English-EWT/r2.10/en_ewt-ud-train.conllu",
1364	        "dev": "UD_English-EWT/r2.10/en_ewt-ud-dev.conllu",
1365	        "test": "UD_English-EWT/r2.10/en_ewt-ud-test.conllu",
1366	    },
1367	    "en_gum": {
1368	        "train": "UD_English-GUM/r2.10/en_gum-ud-train.conllu",
1369	        "dev": "UD_English-GUM/r2.10/en_gum-ud-dev.conllu",
1370	        "test": "UD_English-GUM/r2.10/en_gum-ud-test.conllu",
1371	    },
1372	    "en_gumreddit": {
1373	        "train": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-train.conllu",
1374	        "dev": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-dev.conllu",
1375	        "test": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-test.conllu",
1376	    },
1377	    "en_lines": {
1378	        "train": "UD_English-LinES/r2.10/en_lines-ud-train.conllu",
1379	        "dev": "UD_English-LinES/r2.10/en_lines-ud-dev.conllu",
1380	        "test": "UD_English-LinES/r2.10/en_lines-ud-test.conllu",
1381	    },
1382	    "en_partut": {
1383	        "train": "UD_English-ParTUT/r2.10/en_partut-ud-train.conllu",
1384	        "dev": "UD_English-ParTUT/r2.10/en_partut-ud-dev.conllu",
1385	        "test": "UD_English-ParTUT/r2.10/en_partut-ud-test.conllu",
1386	    },
1387	    "en_pronouns": {
1388	        "test": "UD_English-Pronouns/r2.10/en_pronouns-ud-test.conllu",
1389	    },
1390	    "en_pud": {
1391	        "test": "UD_English-PUD/r2.10/en_pud-ud-test.conllu",
1392	    },
1393	    "myv_jr": {
1394	        "test": "UD_Erzya-JR/r2.10/myv_jr-ud-test.conllu",
1395	    },
1396	    "et_edt": {
1397	        "train": "UD_Estonian-EDT/r2.10/et_edt-ud-train.conllu",
1398	        "dev": "UD_Estonian-EDT/r2.10/et_edt-ud-dev.conllu",
1399	        "test": "UD_Estonian-EDT/r2.10/et_edt-ud-test.conllu",
1400	    },
1401	    "et_ewt": {
1402	        "train": "UD_Estonian-EWT/r2.10/et_ewt-ud-train.conllu",
1403	        "dev": "UD_Estonian-EWT/r2.10/et_ewt-ud-dev.conllu",
1404	        "test": "UD_Estonian-EWT/r2.10/et_ewt-ud-test.conllu",
1405	    },
1406	    "fo_farpahc": {
1407	        "train": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-train.conllu",
1408	        "dev": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-dev.conllu",
1409	        "test": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-test.conllu",
1410	    },
1411	    "fo_oft": {
1412	        "test": "UD_Faroese-OFT/r2.10/fo_oft-ud-test.conllu",
1413	    },
1414	    "fi_ftb": {
1415	        "train": "UD_Finnish-FTB/r2.10/fi_ftb-ud-train.conllu",
1416	        "dev": "UD_Finnish-FTB/r2.10/fi_ftb-ud-dev.conllu",
1417	        "test": "UD_Finnish-FTB/r2.10/fi_ftb-ud-test.conllu",
1418	    },
1419	    "fi_ood": {
1420	        "test": "UD_Finnish-OOD/r2.10/fi_ood-ud-test.conllu",
1421	    },
1422	    "fi_pud": {
1423	        "test": "UD_Finnish-PUD/r2.10/fi_pud-ud-test.conllu",
1424	    },
1425	    "fi_tdt": {
1426	        "train": "UD_Finnish-TDT/r2.10/fi_tdt-ud-train.conllu",
1427	        "dev": "UD_Finnish-TDT/r2.10/fi_tdt-ud-dev.conllu",
1428	        "test": "UD_Finnish-TDT/r2.10/fi_tdt-ud-test.conllu",
1429	    },
1430	    "fr_parisstories": {
1431	        "train": "UD_French-ParisStories/r2.10/fr_parisstories-ud-train.conllu",
1432	        "test": "UD_French-ParisStories/r2.10/fr_parisstories-ud-test.conllu",
1433	    },
1434	    "fr_fqb": {
1435	        "test": "UD_French-FQB/r2.10/fr_fqb-ud-test.conllu",
1436	    },
1437	    "fr_rhapsodie": {
1438	        "train": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-train.conllu",
1439	        "dev": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-dev.conllu",
1440	        "test": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-test.conllu",
1441	    },
1442	    "fr_ftb": {
1443	        "train": "UD_French-FTB/r2.10/fr_ftb-ud-train.conllu",
1444	        "dev": "UD_French-FTB/r2.10/fr_ftb-ud-dev.conllu",
1445	        "test": "UD_French-FTB/r2.10/fr_ftb-ud-test.conllu",
1446	    },
1447	    "fr_gsd": {
1448	        "train": "UD_French-GSD/r2.10/fr_gsd-ud-train.conllu",
1449	        "dev": "UD_French-GSD/r2.10/fr_gsd-ud-dev.conllu",
1450	        "test": "UD_French-GSD/r2.10/fr_gsd-ud-test.conllu",
1451	    },
1452	    "fr_partut": {
1453	        "train": "UD_French-ParTUT/r2.10/fr_partut-ud-train.conllu",
1454	        "dev": "UD_French-ParTUT/r2.10/fr_partut-ud-dev.conllu",
1455	        "test": "UD_French-ParTUT/r2.10/fr_partut-ud-test.conllu",
1456	    },
1457	    "fr_pud": {
1458	        "test": "UD_French-PUD/r2.10/fr_pud-ud-test.conllu",
1459	    },
1460	    "fr_sequoia": {
1461	        "train": "UD_French-Sequoia/r2.10/fr_sequoia-ud-train.conllu",
1462	        "dev": "UD_French-Sequoia/r2.10/fr_sequoia-ud-dev.conllu",
1463	        "test": "UD_French-Sequoia/r2.10/fr_sequoia-ud-test.conllu",
1464	    },
1465	    "qfn_fame": {
1466	        "test": "UD_Frisian_Dutch-Fame/r2.10/qfn_fame-ud-test.conllu",
1467	    },
1468	    "gl_ctg": {
1469	        "train": "UD_Galician-CTG/r2.10/gl_ctg-ud-train.conllu",
1470	        "dev": "UD_Galician-CTG/r2.10/gl_ctg-ud-dev.conllu",
1471	        "test": "UD_Galician-CTG/r2.10/gl_ctg-ud-test.conllu",
1472	    },
1473	    "gl_treegal": {
1474	        "train": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-train.conllu",
1475	        "test": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-test.conllu",
1476	    },
1477	    "de_gsd": {
1478	        "train": "UD_German-GSD/r2.10/de_gsd-ud-train.conllu",
1479	        "dev": "UD_German-GSD/r2.10/de_gsd-ud-dev.conllu",
1480	        "test": "UD_German-GSD/r2.10/de_gsd-ud-test.conllu",
1481	    },
1482	    "de_hdt": {
1483	        "train": [
1484	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-1.conllu",
1485	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-2.conllu",
1486	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-1.conllu",
1487	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-2.conllu",
1488	        ],
1489	        "dev": "UD_German-HDT/r2.10/de_hdt-ud-dev.conllu",
1490	        "test": "UD_German-HDT/r2.10/de_hdt-ud-test.conllu",
1491	    },
1492	    "de_lit": {
1493	        "test": "UD_German-LIT/r2.10/de_lit-ud-test.conllu",
1494	    },
1495	    "de_pud": {
1496	        "test": "UD_German-PUD/r2.10/de_pud-ud-test.conllu",
1497	    },
1498	    "got_proiel": {
1499	        "train": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-train.conllu",
1500	        "dev": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-dev.conllu",
1501	        "test": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-test.conllu",
1502	    },
1503	    "el_gdt": {
1504	        "train": "UD_Greek-GDT/r2.10/el_gdt-ud-train.conllu",
1505	        "dev": "UD_Greek-GDT/r2.10/el_gdt-ud-dev.conllu",
1506	        "test": "UD_Greek-GDT/r2.10/el_gdt-ud-test.conllu",
1507	    },
1508	    "gub_tudet": {
1509	        "test": "UD_Guajajara-TuDeT/r2.10/gub_tudet-ud-test.conllu",
1510	    },
1511	    "gn_oldtudet": {
1512	        "test": "UD_Guarani-OldTuDeT/r2.10/gn_oldtudet-ud-test.conllu",
1513	    },
1514	    "he_iahltwiki": {
1515	        "train": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-train.conllu",
1516	        "dev": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-dev.conllu",
1517	        "test": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-test.conllu",
1518	    },
1519	    "he_htb": {
1520	        "train": "UD_Hebrew-HTB/r2.10/he_htb-ud-train.conllu",
1521	        "dev": "UD_Hebrew-HTB/r2.10/he_htb-ud-dev.conllu",
1522	        "test": "UD_Hebrew-HTB/r2.10/he_htb-ud-test.conllu",
1523	    },
1524	    "qhe_hiencs": {
1525	        "train": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-train.conllu",
1526	        "dev": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-dev.conllu",
1527	        "test": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-test.conllu",
1528	    },
1529	    "hi_hdtb": {
1530	        "train": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-train.conllu",
1531	        "dev": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-dev.conllu",
1532	        "test": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-test.conllu",
1533	    },
1534	    "hi_pud": {
1535	        "test": "UD_Hindi-PUD/r2.10/hi_pud-ud-test.conllu",
1536	    },
1537	    "hu_szeged": {
1538	        "train": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-train.conllu",
1539	        "dev": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-dev.conllu",
1540	        "test": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-test.conllu",
1541	    },
1542	    "is_modern": {
1543	        "train": "UD_Icelandic-Modern/r2.10/is_modern-ud-train.conllu",
1544	        "dev": "UD_Icelandic-Modern/r2.10/is_modern-ud-dev.conllu",
1545	        "test": "UD_Icelandic-Modern/r2.10/is_modern-ud-test.conllu",
1546	    },
1547	    "is_icepahc": {
1548	        "train": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-train.conllu",
1549	        "dev": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-dev.conllu",
1550	        "test": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-test.conllu",
1551	    },
1552	    "is_pud": {
1553	        "test": "UD_Icelandic-PUD/r2.10/is_pud-ud-test.conllu",
1554	    },
1555	    "id_csui": {
1556	        "train": "UD_Indonesian-CSUI/r2.10/id_csui-ud-train.conllu",
1557	        "test": "UD_Indonesian-CSUI/r2.10/id_csui-ud-test.conllu",
1558	    },
1559	    "id_gsd": {
1560	        "train": "UD_Indonesian-GSD/r2.10/id_gsd-ud-train.conllu",
1561	        "dev": "UD_Indonesian-GSD/r2.10/id_gsd-ud-dev.conllu",
1562	        "test": "UD_Indonesian-GSD/r2.10/id_gsd-ud-test.conllu",
1563	    },
1564	    "id_pud": {
1565	        "test": "UD_Indonesian-PUD/r2.10/id_pud-ud-test.conllu",
1566	    },
1567	    "ga_twittirish": {
1568	        "test": "UD_Irish-TwittIrish/r2.10/ga_twittirish-ud-test.conllu",
1569	    },
1570	    "ga_idt": {
1571	        "train": "UD_Irish-IDT/r2.10/ga_idt-ud-train.conllu",
1572	        "dev": "UD_Irish-IDT/r2.10/ga_idt-ud-dev.conllu",
1573	        "test": "UD_Irish-IDT/r2.10/ga_idt-ud-test.conllu",
1574	    },
1575	    "it_valico": {
1576	        "test": "UD_Italian-Valico/r2.10/it_valico-ud-test.conllu",
1577	    },
1578	    "it_markit": {
1579	        "train": "UD_Italian-MarkIT/r2.10/it_markit-ud-train.conllu",
1580	        "dev": "UD_Italian-MarkIT/r2.10/it_markit-ud-dev.conllu",
1581	        "test": "UD_Italian-MarkIT/r2.10/it_markit-ud-test.conllu",
1582	    },
1583	    "it_isdt": {
1584	        "train": "UD_Italian-ISDT/r2.10/it_isdt-ud-train.conllu",
1585	        "dev": "UD_Italian-ISDT/r2.10/it_isdt-ud-dev.conllu",
1586	        "test": "UD_Italian-ISDT/r2.10/it_isdt-ud-test.conllu",
1587	    },
1588	    "it_partut": {
1589	        "train": "UD_Italian-ParTUT/r2.10/it_partut-ud-train.conllu",
1590	        "dev": "UD_Italian-ParTUT/r2.10/it_partut-ud-dev.conllu",
1591	        "test": "UD_Italian-ParTUT/r2.10/it_partut-ud-test.conllu",
1592	    },
1593	    "it_postwita": {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:566
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
565	    "it_vit": (
566	        "The UD_Italian-VIT corpus was obtained by conversion from VIT (Venice"
567	        " Italian Treebank), developed at the Laboratory of Computational "
568	        "Linguistics of the Universit Ca' Foscari in Venice (Delmonte et al. "
569	        "2007; Delmonte 2009; "
570	        "http://rondelmo.it/resource/VIT/Browser-VIT/index.htm)."
571	    ),
572	    "ja_pudluw": (
573	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
574	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
575	        "Raw Text to Universal Dependencies."
576	    ),
577	    "ja_bccwjluw": (
578	        "This Universal Dependencies (UD) Japanese treebank is based on the "
579	        "definition of UD Japanese convention described in the UD "
580	        "documentation. The original sentences are from `Balanced Corpus of "
581	        "Contemporary Written Japanese'(BCCWJ)."
582	    ),
583	    "ja_gsdluw": (
584	        "This Universal Dependencies (UD) Japanese treebank is based on the "
585	        "definition of UD Japanese convention described in the UD "
586	        "documentation. The original sentences are from Google UDT 2.0."
587	    ),
588	    "ja_bccwj": (
589	        "This Universal Dependencies (UD) Japanese treebank is based on the "
590	        "definition of UD Japanese convention described in the UD "
591	        "documentation. The original sentences are from `Balanced Corpus of "
592	        "Contemporary Written Japanese'(BCCWJ)."
593	    ),
594	    "ja_gsd": (
595	        "This Universal Dependencies (UD) Japanese treebank is based on the "
596	        "definition of UD Japanese convention described in the UD "
597	        "documentation.  The original sentences are from Google UDT 2.0."
598	    ),
599	    "ja_modern": (
600	        "This Universal Dependencies (UD) Japanese treebank is based on the "
601	        "definition of UD Japanese convention described in the UD "
602	        "documentation. The original sentences are from `Corpus of Historical "
603	        "Japanese' (CHJ)."
604	    ),
605	    "ja_pud": (
606	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
607	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
608	        " Raw Text to Universal "
609	        "Dependencies](http://universaldependencies.org/conll17/)."
610	    ),
611	    "jv_csui": (
612	        "UD Javanese-CSUI is a dependency treebank in Javanese, a regional "
613	        "language in Indonesia with more than 60 million users. The original "
614	        "sentences were taken from OPUS, especially from the WikiMatrix v1 "
615	        "corpus. We revised the sentences that contained more Indonesian words"
616	        " than Javanese words and manually annotated them."
617	    ),
618	    "urb_tudet": (
619	        "UD_Kaapor-TuDeT is a collection of annotated sentences in Ka'apor. "
620	        "The project is a work in progress and the treebank is being updated "
621	        "on a regular basis."
622	    ),
623	    "xnr_kdtb": (
624	        "The Kangri UD Treebank (KDTB) is a part of the Universal Dependency "
625	        "treebank project."
626	    ),
627	    "krl_kkpp": (
628	        "UD Karelian-KKPP is a manually annotated new corpus of Karelian made "
629	        "in Universal dependencies annotation scheme. The data is collected "
630	        "from VepKar corpora and consists of mostly modern news texts but also"
631	        " some stories and educational texts."
632	    ),
633	    "kk_ktb": (
634	        "The UD Kazakh treebank is a combination of text from various sources "
635	        "including Wikipedia, some folk tales, sentences from the UDHR, news "
636	        "and phrasebook sentences. Sentences IDs include partial document "
637	        "identifiers."
638	    ),
639	    "arr_tudet": (
640	        "UD_Karo-TuDeT is a collection of annotated sentences in Karo. The "
641	        "sentences stem from the only grammatical description of the language "
642	        "(Gabas, 1999) and from the sentences in the dictionary by the same "
643	        "author (Gabas, 2007). Sentence annotation and documentation by "
644	        "Fabrcio Ferraz Gerardi."
645	    ),
646	    "kfm_aha": (
647	        "The AHA Khunsari Treebank is a small treebank for contemporary "
648	        "Khunsari. Its corpus is collected and annotated manually. We have "
649	        "prepared this treebank based on interviews with Khunsari speakers."
650	    ),
651	    "quc_iu": (
652	        "UD Kiche-IU is a treebank consisting of sentences from a variety of"
653	        " text domains but principally dictionary example sentences and "
654	        "linguistic examples."
655	    ),
656	    "koi_uh": (
657	        "This is a Komi-Permyak literary language treebank consisting of "
658	        "original and translated texts."
659	    ),
660	    "kpv_ikdp": (
661	        "This treebank consists of dialectal transcriptions of spoken "
662	        "Komi-Zyrian. The current texts are short recorded segments from "
663	        "different areas where the Iva dialect of Komi language is spoken."
664	    ),
665	    "kpv_lattice": (
666	        "UD Komi-Zyrian Lattice is a treebank of written standard Komi-Zyrian."
667	    ),
668	    "ko_gsd": (
669	        "The Google Korean Universal Dependency Treebank is first converted "
670	        "from the Universal Dependency Treebank v2.0 (legacy), and then "
671	        "enhanced by Chun et al., 2018."
672	    ),
673	    "ko_kaist": (
674	        "The KAIST Korean Universal Dependency Treebank is generated by Chun "
675	        "et al., 2018 from the constituency trees in the KAIST Tree-Tagging "
676	        "Corpus."
677	    ),
678	    "ko_pud": (
679	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
680	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
681	        "Raw Text to Universal Dependencies."
682	    ),
683	    "kmr_mg": (
684	        "The UD Kurmanji corpus is a corpus of Kurmanji Kurdish. It contains "
685	        "fiction and encyclopaedic texts in roughly equal measure. It has been"
686	        " annotated natively in accordance with the UD annotation scheme."
687	    ),
688	    "la_ittb": (
689	        "Latin data from the _Index Thomisticus_ Treebank. Data are taken from"
690	        " the _Index Thomisticus_ corpus by Roberto Busa SJ, which contains "
691	        "the complete work by Thomas Aquinas (12251274; Medieval Latin) and "
692	        "by 61 other authors related to Thomas."
693	    ),
694	    "la_udante": (
695	        "The UDante treebank is based on the Latin texts of Dante Alighieri, "
696	        "taken from the DanteSearch corpus, originally created at the "
697	        "University of Pisa, Italy. It is a treebank of Latin language, more "
698	        "precisely of literary Medieval Latin (XIVth century)."
699	    ),
700	    "la_llct": (
701	        "This Universal Dependencies version of the LLCT (Late Latin Charter "
702	        "Treebank) consists of an automated conversion of the LLCT2 treebank "
703	        "from the Latin Dependency Treebank (LDT) format into the Universal "
704	        "Dependencies standard."
705	    ),
706	    "la_perseus": (
707	        "This Universal Dependencies Latin Treebank consists of an automatic "
708	        "conversion of a selection of passages from the Ancient Greek and "
709	        "Latin Dependency Treebank 2.1"
710	    ),
711	    "la_proiel": (
712	        "The Latin PROIEL treebank is based on the Latin data from the PROIEL "
713	        "treebank, and contains most of the Vulgate New Testament translations"
714	        " plus selections from Caesar's Gallic War, Cicero's Letters to "
715	        "Atticus, Palladius' Opus Agriculturae and the first book of Cicero's "
716	        "De officiis."
717	    ),
718	    "lv_lvtb": (
719	        "Latvian UD Treebank is based on Latvian Treebank (LVTB), being "
720	        "created at University of Latvia, Institute of Mathematics and "
721	        "Computer Science, Artificial Intelligence Laboratory."
722	    ),
723	    "lij_glt": (
724	        "The Genoese Ligurian Treebank is a small, manually annotated "
725	        "collection of contemporary Ligurian prose. The focus of the treebank "
726	        "is written Genoese, the koin variety of Ligurian which is associated"
727	        " with today's literary, journalistic and academic ligurophone sphere."
728	    ),
729	    "lt_alksnis": (
730	        "The Lithuanian dependency treebank ALKSNIS v3.0 (Vytautas Magnus "
731	        "University)."
732	    ),
733	    "lt_hse": (
734	        "Lithuanian treebank annotated manually (dependencies) using the "
735	        "Morphological Annotator by CCL, Vytautas Magnus University "
736	        "(http://tekstynas.vdu.lt/) and manual disambiguation. A pilot version"
737	        " which includes news and an essay by Tomas Venclova is available "
738	        "here."
739	    ),
740	    "olo_kkpp": (
741	        "UD Livvi-KKPP is a manually annotated new corpus of Livvi-Karelian "
742	        "made directly in the Universal dependencies annotation scheme. The "
743	        "data is collected from VepKar corpora and consists of mostly modern "
744	        "news texts but also some stories and educational texts."
745	    ),
746	    "nds_lsdc": (
747	        "The UD Low Saxon LSDC dataset consists of sentences in 18 Low Saxon "
748	        "dialects from both Germany and the Netherlands. These sentences are "
749	        "(or are to become) part of the LSDC dataset and represent the "
750	        "language from the 19th and early 20th century in genres such as short"
751	        " stories, novels, speeches, letters and fairytales."
752	    ),
753	    "mt_mudt": (
754	        "MUDT (Maltese Universal Dependencies Treebank) is a manually "
755	        "annotated treebank of Maltese, a Semitic language of Malta descended "
756	        "from North African Arabic with a significant amount of Italo-Romance "
757	        "influence. MUDT was designed as a balanced corpus with four major "
758	        "genres (see Splitting below) represented roughly equally."
759	    ),
760	    "gv_cadhan": (
761	        "This is the Cadhan Aonair UD treebank for Manx Gaelic, created by "
762	        "Kevin Scannell."
763	    ),
764	    "mr_ufal": (
765	        "UD Marathi is a manually annotated treebank consisting primarily of "
766	        "stories from Wikisource, and parts of an article on Wikipedia."
767	    ),
768	    "gun_dooley": (
769	        "UD Mbya_Guarani-Dooley is a corpus of narratives written in Mby "
770	        "Guaran (Tupian) in Brazil, and collected by Robert Dooley. Due to "
771	        "copyright restrictions, the corpus that is distributed as part of UD "
772	        "only contains the annotation (tags, features, relations) while the "
773	        "FORM and LEMMA columns are empty."
774	    ),
775	    "gun_thomas": (
776	        "UD Mbya_Guarani-Thomas is a corpus of Mby Guaran (Tupian) texts "
777	        "collected by Guillaume Thomas. The current version of the corpus "
778	        "consists of three speeches by Paulina Kerechu Nez Romero, a Mby "
779	        "Guaran speaker from Ytu, Caazap Department, Paraguay."
780	    ),
781	    "mdf_jr": (
782	        "Erme Universal Dependencies annotated texts Moksha are the origin of "
783	        "UD_Moksha-JR with annotation (CoNLL-U) for texts in the Moksha "
784	        "language, it originally consists of a sample from a number of fiction"
785	        " authors writing originals in Moksha."
786	    ),
787	    "myu_tudet": (
788	        "UD_Munduruku-TuDeT is a collection of annotated sentences in "
789	        "Munduruk. Together with UD_Akuntsu-TuDeT and UD_Tupinamba-TuDeT, "
790	        "UD_Munduruku-TuDeT is part of the TuLaR project."
791	    ),
792	    "pcm_nsc": (
793	        "A Universal Dependencies corpus for spoken Naija (Nigerian Pidgin)."
794	    ),
795	    "nyq_aha": (
796	        "The AHA Nayini Treebank is a small treebank for contemporary Nayini. "
797	        "Its corpus is collected and annotated manually. We have prepared this"
798	        " treebank based on interviews with Nayini speakers."
799	    ),
800	    "sme_giella": (
801	        "This is a North Smi treebank based on a manually disambiguated and "
802	        "function-labelled gold-standard corpus of North Smi produced by the "
803	        "Giellatekno team at UiT Norgga rktala universitehta."
804	    ),
805	    "no_bokmaal": (
806	        "The Norwegian UD treebank is based on the Bokml section of the "
807	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
808	        " Norwegian. NDT has been automatically converted to the UD scheme by "
809	        "Lilja vrelid at the University of Oslo."
810	    ),
811	    "no_nynorsk": (
812	        "The Norwegian UD treebank is based on the Nynorsk section of the "
813	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
814	        " Norwegian.  NDT has been automatically converted to the UD scheme by"
815	        " Lilja vrelid at the University of Oslo."
816	    ),
817	    "no_nynorsklia": (
818	        "This Norwegian treebank is based on the LIA treebank of transcribed "
819	        "spoken Norwegian dialects. The treebank has been automatically "
820	        "converted to the UD scheme by Lilja vrelid at the University of "
821	        "Oslo."
822	    ),
823	    "cu_proiel": (
824	        "The Old Church Slavonic (OCS) UD treebank is based on the Old Church "
825	        "Slavonic data from the PROIEL treebank and contains the text of the "
826	        "Codex Marianus New Testament translation."
827	    ),
828	    "fro_srcmf": (
829	        "UD_Old_French-SRCMF is a conversion of (part of) the SRCMF corpus "
830	        "(Syntactic Reference Corpus of Medieval French srcmf.org)."
831	    ),
832	    "orv_birchbark": (
833	        "UD Old_East_Slavic-Birchbark is based on the RNC Corpus of Birchbark "
834	        "Letters and includes documents written in 1025-1500 in an East Slavic"
835	        " vernacular (letters, household and business records, records for "
836	        "church services, spell against diseases, and other short "
837	        "inscriptions). The treebank is manually syntactically annotated in "
838	        "the UD 2.0 scheme, morphological and lexical annotation is a "
839	        "conversion of the original RNC annotation."
840	    ),
841	    "orv_rnc": (
842	        "`UD_Old_Russian-RNC` is a sample of the Middle Russian corpus "
843	        "(1300-1700), a part of the Russian National Corpus. The data were "
844	        "originally annotated according to the RNC and extended UD-Russian "
845	        "morphological schemas and UD 2.4 dependency schema."
846	    ),
847	    "orv_torot": (
848	        "UD_Old_Russian-TOROT is a conversion of a selection of the Old East "
849	        "Slavonic and Middle Russian data in the Troms Old Russian and OCS "
850	        "Treebank (TOROT), which was originally annotated in PROIEL dependency"
851	        " format."
852	    ),
853	    "otk_tonqq": (
854	        "`UD_Old_Turkish-Tonqq` is an Old Turkish treebank built upon Turkic "
855	        "script texts or sentences that are trivially convertible."
856	    ),
857	    "fa_perdt": (
858	        "The Persian Universal Dependency Treebank (PerUDT) is the result of "
859	        "automatic coversion of Persian Dependency Treebank (PerDT) with "
860	        "extensive manual corrections. Please refer to the follwoing work, if "
861	        "you use this data: Mohammad Sadegh Rasooli, Pegah Safari, Amirsaeid "
862	        "Moloodi, and Alireza Nourian. 'The Persian Dependency Treebank Made "
863	        "Universal'. 2020 (to appear)."
864	    ),
865	    "fa_seraji": (
866	        "The Persian Universal Dependency Treebank (Persian UD) is based on "
867	        "Uppsala Persian Dependency Treebank (UPDT). The conversion of the "
868	        "UPDT to the Universal Dependencies was performed semi-automatically "
869	        "with extensive manual checks and corrections."
870	    ),
871	    "pl_lfg": (
872	        "The LFG Enhanced UD treebank of Polish is based on a corpus of LFG "
873	        "(Lexical Functional Grammar) syntactic structures generated by an LFG"
874	        " grammar of Polish, POLFIE, and manually disambiguated by human "
875	        "annotators."
876	    ),
877	    "pl_pdb": (
878	        "The Polish PDB-UD treebank is based on the Polish Dependency Bank 2.0"
879	        " (PDB 2.0), created at the Institute of Computer Science, Polish "
880	        "Academy of Sciences in Warsaw. The PDB-UD treebank is an extended and"
881	        " corrected version of the Polish SZ-UD treebank (the release 1.2 to "
882	        "2.3)."
883	    ),
884	    "pl_pud": (
885	        "This is the Polish portion of the Parallel Universal Dependencies "
886	        "(PUD) treebanks, created at the Institute of Computer Science, Polish"
887	        " Academy of Sciences in Warsaw.Re"
888	    ),
889	    "pt_bosque": (
890	        "This Universal Dependencies (UD) Portuguese treebank is based on the "
891	        "Constraint Grammar converted version of the Bosque, which is part of "
892	        "the Floresta Sint(c)tica treebank. It contains both European "
893	        "(CETEMPblico) and Brazilian (CETENFolha) variants."
894	    ),
895	    "pt_gsd": (
896	        "The Brazilian Portuguese UD is converted from the Google Universal "
897	        "Dependency Treebank v2.0 (legacy)."
898	    ),
899	    "pt_pud": (
900	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
901	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
902	        "Raw Text to Universal Dependencies."
903	    ),
904	    "ro_art": (
905	        "The UD treebank ArT is a treebank of the Aromanian dialect of the "
906	        "Romanian language in UD format."
907	    ),
908	    "ro_nonstandard": (
909	        "The Romanian Non-standard UD treebank (called UAIC-RoDia) is based on"
910	        " UAIC-RoDia Treebank. UAIC-RoDia = ISLRN 156-635-615-024-0"
911	    ),
912	    "ro_rrt": (
913	        "The Romanian UD treebank (called RoRefTrees) (Barbu Mititelu et al., "
914	        "2016) is the reference treebank in UD format for standard Romanian."
915	    ),
916	    "ro_simonero": "SiMoNERo is a medical corpus of contemporary Romanian.",
917	    "ru_gsd": (
918	        "Russian Universal Dependencies Treebank annotated and converted by "
919	        "Google."
920	    ),
921	    "ru_pud": (
922	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
923	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
924	        "Raw Text to Universal Dependencies."
925	    ),
926	    "ru_syntagrus": "Russian data from the SynTagRus corpus.",
927	    "ru_taiga": (
928	        "Universal Dependencies treebank is based on data samples extracted "
929	        "from Taiga Corpus and MorphoRuEval-2017 and GramEval-2020 shared "
930	        "tasks collections."
931	    ),
932	    "sa_ufal": (
933	        "A small Sanskrit treebank of sentences from Pacatantra, an ancient "
934	        "Indian collection of interrelated fables by Vishnu Sharma."
935	    ),
936	    "sa_vedic": (
937	        "The Treebank of Vedic Sanskrit contains 4,000 sentences with 27,000 "
938	        "words chosen from metrical and prose passages of the gveda (RV), the"
939	        " aunaka recension of the Atharvaveda (S), the Maitryasahit "
940	        "(MS), and the Aitareya- (AB) and atapatha-Brhmaas (B). Lexical "
941	        "and morpho-syntactic information has been generated using a tagging "
942	        "software and manually validated. POS tags have been induced "
943	        "automatically from the morpho-sytactic information of each word."
944	    ),
945	    "gd_arcosg": (
946	        "A treebank of Scottish Gaelic based on the Annotated Reference Corpus"
947	        " Of Scottish Gaelic (ARCOSG)."
948	    ),
949	    "sr_set": (
950	        "The Serbian UD treebank is based on the "
951	        "[SETimes-SR](http://hdl.handle.net/11356/1200) corpus and additional "
952	        "news documents from the Serbian web."
953	    ),
954	    "sms_giellagas": (
955	        "The UD Skolt Sami Giellagas treebank is based almost entirely on "
956	        "spoken Skolt Sami corpora."
957	    ),
958	    "sk_snk": (
959	        "The Slovak UD treebank is based on data originally annotated as part "
960	        "of the Slovak National Corpus, following the annotation style of the "
961	        "Prague Dependency Treebank."
962	    ),
963	    "sl_ssj": (
964	        "The Slovenian UD Treebank is a rule-based conversion of the ssj500k "
965	        "treebank, the largest collection of manually syntactically annotated "
966	        "data in Slovenian, originally annotated in the JOS annotation scheme."
967	    ),
968	    "sl_sst": (
969	        "The Spoken Slovenian UD Treebank (SST) is the first syntactically "
970	        "annotated corpus of spoken Slovenian, based on a sample of the "
971	        "reference GOS corpus, a collection of transcribed audio recordings of"
972	        " monologic, dialogic and multi-party spontaneous speech in different "
973	        "everyday situations."
974	    ),
975	    "soj_aha": (
976	        "The AHA Soi Treebank is a small treebank for contemporary Soi. Its "
977	        "corpus is collected and annotated manually. We have prepared this "
978	        "treebank based on interviews with Soi speakers."
979	    ),
980	    "ajp_madar": (
981	        "The South_Levantine_Arabic-MADAR treebank consists of 100 "
982	        "manually-annotated sentences taken from the "
983	        "[MADAR](https://camel.abudhabi.nyu.edu/madar/) (Multi-Arabic Dialect "
984	        "Applications and Resources) project. "
985	    ),
986	    "es_ancora": "Spanish data from the AnCora corpus.",
987	    "es_gsd": (
988	        "The Spanish UD is converted from the content head version of the "
989	        "universal dependency treebank v2.0 (legacy)."
990	    ),
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {
1217	        "test": "UD_Arabic-PUD/r2.10/ar_pud-ud-test.conllu",
1218	    },
1219	    "hy_armtdp": {
1220	        "train": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-train.conllu",
1221	        "dev": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-dev.conllu",
1222	        "test": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-test.conllu",
1223	    },
1224	    "aii_as": {
1225	        "test": "UD_Assyrian-AS/r2.10/aii_as-ud-test.conllu",
1226	    },
1227	    "bm_crb": {
1228	        "test": "UD_Bambara-CRB/r2.10/bm_crb-ud-test.conllu",
1229	    },
1230	    "eu_bdt": {
1231	        "train": "UD_Basque-BDT/r2.10/eu_bdt-ud-train.conllu",
1232	        "dev": "UD_Basque-BDT/r2.10/eu_bdt-ud-dev.conllu",
1233	        "test": "UD_Basque-BDT/r2.10/eu_bdt-ud-test.conllu",
1234	    },
1235	    "bej_nsc": {
1236	        "test": "UD_Beja-NSC/r2.10/bej_nsc-ud-test.conllu",
1237	    },
1238	    "be_hse": {
1239	        "train": "UD_Belarusian-HSE/r2.10/be_hse-ud-train.conllu",
1240	        "dev": "UD_Belarusian-HSE/r2.10/be_hse-ud-dev.conllu",
1241	        "test": "UD_Belarusian-HSE/r2.10/be_hse-ud-test.conllu",
1242	    },
1243	    "bn_bru": {
1244	        "test": "UD_Bengali-BRU/r2.10/bn_bru-ud-test.conllu",
1245	    },
1246	    "bho_bhtb": {
1247	        "test": "UD_Bhojpuri-BHTB/r2.10/bho_bhtb-ud-test.conllu",
1248	    },
1249	    "br_keb": {
1250	        "test": "UD_Breton-KEB/r2.10/br_keb-ud-test.conllu",
1251	    },
1252	    "bg_btb": {
1253	        "train": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-train.conllu",
1254	        "dev": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-dev.conllu",
1255	        "test": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-test.conllu",
1256	    },
1257	    "bxr_bdt": {
1258	        "train": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-train.conllu",
1259	        "test": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-test.conllu",
1260	    },
1261	    "yue_hk": {
1262	        "test": "UD_Cantonese-HK/r2.10/yue_hk-ud-test.conllu",
1263	    },
1264	    "ca_ancora": {
1265	        "train": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-train.conllu",
1266	        "dev": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-dev.conllu",
1267	        "test": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-test.conllu",
1268	    },
1269	    "ceb_gja": {
1270	        "test": "UD_Cebuano-GJA/r2.10/ceb_gja-ud-test.conllu",
1271	    },
1272	    "zh_cfl": {
1273	        "test": "UD_Chinese-CFL/r2.10/zh_cfl-ud-test.conllu",
1274	    },
1275	    "zh_gsd": {
1276	        "train": "UD_Chinese-GSD/r2.10/zh_gsd-ud-train.conllu",
1277	        "dev": "UD_Chinese-GSD/r2.10/zh_gsd-ud-dev.conllu",
1278	        "test": "UD_Chinese-GSD/r2.10/zh_gsd-ud-test.conllu",
1279	    },
1280	    "zh_gsdsimp": {
1281	        "train": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-train.conllu",
1282	        "dev": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-dev.conllu",
1283	        "test": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-test.conllu",
1284	    },
1285	    "zh_hk": {
1286	        "test": "UD_Chinese-HK/r2.10/zh_hk-ud-test.conllu",
1287	    },
1288	    "zh_pud": {
1289	        "test": "UD_Chinese-PUD/r2.10/zh_pud-ud-test.conllu",
1290	    },
1291	    "ckt_hse": {
1292	        "test": "UD_Chukchi-HSE/r2.10/ckt_hse-ud-test.conllu",
1293	    },
1294	    "lzh_kyoto": {
1295	        "train": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-train.conllu",
1296	        "dev": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-dev.conllu",
1297	        "test": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-test.conllu",
1298	    },
1299	    "cop_scriptorium": {
1300	        "train": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-train.conllu",
1301	        "dev": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-dev.conllu",
1302	        "test": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-test.conllu",
1303	    },
1304	    "hr_set": {
1305	        "train": "UD_Croatian-SET/r2.10/hr_set-ud-train.conllu",
1306	        "dev": "UD_Croatian-SET/r2.10/hr_set-ud-dev.conllu",
1307	        "test": "UD_Croatian-SET/r2.10/hr_set-ud-test.conllu",
1308	    },
1309	    "cs_cac": {
1310	        "train": "UD_Czech-CAC/r2.10/cs_cac-ud-train.conllu",
1311	        "dev": "UD_Czech-CAC/r2.10/cs_cac-ud-dev.conllu",
1312	        "test": "UD_Czech-CAC/r2.10/cs_cac-ud-test.conllu",
1313	    },
1314	    "cs_cltt": {
1315	        "train": "UD_Czech-CLTT/r2.10/cs_cltt-ud-train.conllu",
1316	        "dev": "UD_Czech-CLTT/r2.10/cs_cltt-ud-dev.conllu",
1317	        "test": "UD_Czech-CLTT/r2.10/cs_cltt-ud-test.conllu",
1318	    },
1319	    "cs_fictree": {
1320	        "train": "UD_Czech-FicTree/r2.10/cs_fictree-ud-train.conllu",
1321	        "dev": "UD_Czech-FicTree/r2.10/cs_fictree-ud-dev.conllu",
1322	        "test": "UD_Czech-FicTree/r2.10/cs_fictree-ud-test.conllu",
1323	    },
1324	    "cs_pdt": {
1325	        "train": [
1326	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-l.conllu",
1327	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-m.conllu",
1328	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-c.conllu",
1329	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-v.conllu",
1330	        ],
1331	        "dev": "UD_Czech-PDT/r2.10/cs_pdt-ud-dev.conllu",
1332	        "test": "UD_Czech-PDT/r2.10/cs_pdt-ud-test.conllu",
1333	    },
1334	    "cs_pud": {
1335	        "test": "UD_Czech-PUD/r2.10/cs_pud-ud-test.conllu",
1336	    },
1337	    "da_ddt": {
1338	        "train": "UD_Danish-DDT/r2.10/da_ddt-ud-train.conllu",
1339	        "dev": "UD_Danish-DDT/r2.10/da_ddt-ud-dev.conllu",
1340	        "test": "UD_Danish-DDT/r2.10/da_ddt-ud-test.conllu",
1341	    },
1342	    "nl_alpino": {
1343	        "train": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-train.conllu",
1344	        "dev": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-dev.conllu",
1345	        "test": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-test.conllu",
1346	    },
1347	    "nl_lassysmall": {
1348	        "train": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-train.conllu",
1349	        "dev": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-dev.conllu",
1350	        "test": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-test.conllu",
1351	    },
1352	    "en_atis": {
1353	        "train": "UD_English-Atis/r2.10/en_atis-ud-train.conllu",
1354	        "dev": "UD_English-Atis/r2.10/en_atis-ud-dev.conllu",
1355	        "test": "UD_English-Atis/r2.10/en_atis-ud-test.conllu",
1356	    },
1357	    "en_esl": {
1358	        "train": "UD_English-ESL/r2.10/en_esl-ud-train.conllu",
1359	        "dev": "UD_English-ESL/r2.10/en_esl-ud-dev.conllu",
1360	        "test": "UD_English-ESL/r2.10/en_esl-ud-test.conllu",
1361	    },
1362	    "en_ewt": {
1363	        "train": "UD_English-EWT/r2.10/en_ewt-ud-train.conllu",
1364	        "dev": "UD_English-EWT/r2.10/en_ewt-ud-dev.conllu",
1365	        "test": "UD_English-EWT/r2.10/en_ewt-ud-test.conllu",
1366	    },
1367	    "en_gum": {
1368	        "train": "UD_English-GUM/r2.10/en_gum-ud-train.conllu",
1369	        "dev": "UD_English-GUM/r2.10/en_gum-ud-dev.conllu",
1370	        "test": "UD_English-GUM/r2.10/en_gum-ud-test.conllu",
1371	    },
1372	    "en_gumreddit": {
1373	        "train": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-train.conllu",
1374	        "dev": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-dev.conllu",
1375	        "test": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-test.conllu",
1376	    },
1377	    "en_lines": {
1378	        "train": "UD_English-LinES/r2.10/en_lines-ud-train.conllu",
1379	        "dev": "UD_English-LinES/r2.10/en_lines-ud-dev.conllu",
1380	        "test": "UD_English-LinES/r2.10/en_lines-ud-test.conllu",
1381	    },
1382	    "en_partut": {
1383	        "train": "UD_English-ParTUT/r2.10/en_partut-ud-train.conllu",
1384	        "dev": "UD_English-ParTUT/r2.10/en_partut-ud-dev.conllu",
1385	        "test": "UD_English-ParTUT/r2.10/en_partut-ud-test.conllu",
1386	    },
1387	    "en_pronouns": {
1388	        "test": "UD_English-Pronouns/r2.10/en_pronouns-ud-test.conllu",
1389	    },
1390	    "en_pud": {
1391	        "test": "UD_English-PUD/r2.10/en_pud-ud-test.conllu",
1392	    },
1393	    "myv_jr": {
1394	        "test": "UD_Erzya-JR/r2.10/myv_jr-ud-test.conllu",
1395	    },
1396	    "et_edt": {
1397	        "train": "UD_Estonian-EDT/r2.10/et_edt-ud-train.conllu",
1398	        "dev": "UD_Estonian-EDT/r2.10/et_edt-ud-dev.conllu",
1399	        "test": "UD_Estonian-EDT/r2.10/et_edt-ud-test.conllu",
1400	    },
1401	    "et_ewt": {
1402	        "train": "UD_Estonian-EWT/r2.10/et_ewt-ud-train.conllu",
1403	        "dev": "UD_Estonian-EWT/r2.10/et_ewt-ud-dev.conllu",
1404	        "test": "UD_Estonian-EWT/r2.10/et_ewt-ud-test.conllu",
1405	    },
1406	    "fo_farpahc": {
1407	        "train": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-train.conllu",
1408	        "dev": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-dev.conllu",
1409	        "test": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-test.conllu",
1410	    },
1411	    "fo_oft": {
1412	        "test": "UD_Faroese-OFT/r2.10/fo_oft-ud-test.conllu",
1413	    },
1414	    "fi_ftb": {
1415	        "train": "UD_Finnish-FTB/r2.10/fi_ftb-ud-train.conllu",
1416	        "dev": "UD_Finnish-FTB/r2.10/fi_ftb-ud-dev.conllu",
1417	        "test": "UD_Finnish-FTB/r2.10/fi_ftb-ud-test.conllu",
1418	    },
1419	    "fi_ood": {
1420	        "test": "UD_Finnish-OOD/r2.10/fi_ood-ud-test.conllu",
1421	    },
1422	    "fi_pud": {
1423	        "test": "UD_Finnish-PUD/r2.10/fi_pud-ud-test.conllu",
1424	    },
1425	    "fi_tdt": {
1426	        "train": "UD_Finnish-TDT/r2.10/fi_tdt-ud-train.conllu",
1427	        "dev": "UD_Finnish-TDT/r2.10/fi_tdt-ud-dev.conllu",
1428	        "test": "UD_Finnish-TDT/r2.10/fi_tdt-ud-test.conllu",
1429	    },
1430	    "fr_parisstories": {
1431	        "train": "UD_French-ParisStories/r2.10/fr_parisstories-ud-train.conllu",
1432	        "test": "UD_French-ParisStories/r2.10/fr_parisstories-ud-test.conllu",
1433	    },
1434	    "fr_fqb": {
1435	        "test": "UD_French-FQB/r2.10/fr_fqb-ud-test.conllu",
1436	    },
1437	    "fr_rhapsodie": {
1438	        "train": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-train.conllu",
1439	        "dev": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-dev.conllu",
1440	        "test": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-test.conllu",
1441	    },
1442	    "fr_ftb": {
1443	        "train": "UD_French-FTB/r2.10/fr_ftb-ud-train.conllu",
1444	        "dev": "UD_French-FTB/r2.10/fr_ftb-ud-dev.conllu",
1445	        "test": "UD_French-FTB/r2.10/fr_ftb-ud-test.conllu",
1446	    },
1447	    "fr_gsd": {
1448	        "train": "UD_French-GSD/r2.10/fr_gsd-ud-train.conllu",
1449	        "dev": "UD_French-GSD/r2.10/fr_gsd-ud-dev.conllu",
1450	        "test": "UD_French-GSD/r2.10/fr_gsd-ud-test.conllu",
1451	    },
1452	    "fr_partut": {
1453	        "train": "UD_French-ParTUT/r2.10/fr_partut-ud-train.conllu",
1454	        "dev": "UD_French-ParTUT/r2.10/fr_partut-ud-dev.conllu",
1455	        "test": "UD_French-ParTUT/r2.10/fr_partut-ud-test.conllu",
1456	    },
1457	    "fr_pud": {
1458	        "test": "UD_French-PUD/r2.10/fr_pud-ud-test.conllu",
1459	    },
1460	    "fr_sequoia": {
1461	        "train": "UD_French-Sequoia/r2.10/fr_sequoia-ud-train.conllu",
1462	        "dev": "UD_French-Sequoia/r2.10/fr_sequoia-ud-dev.conllu",
1463	        "test": "UD_French-Sequoia/r2.10/fr_sequoia-ud-test.conllu",
1464	    },
1465	    "qfn_fame": {
1466	        "test": "UD_Frisian_Dutch-Fame/r2.10/qfn_fame-ud-test.conllu",
1467	    },
1468	    "gl_ctg": {
1469	        "train": "UD_Galician-CTG/r2.10/gl_ctg-ud-train.conllu",
1470	        "dev": "UD_Galician-CTG/r2.10/gl_ctg-ud-dev.conllu",
1471	        "test": "UD_Galician-CTG/r2.10/gl_ctg-ud-test.conllu",
1472	    },
1473	    "gl_treegal": {
1474	        "train": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-train.conllu",
1475	        "test": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-test.conllu",
1476	    },
1477	    "de_gsd": {
1478	        "train": "UD_German-GSD/r2.10/de_gsd-ud-train.conllu",
1479	        "dev": "UD_German-GSD/r2.10/de_gsd-ud-dev.conllu",
1480	        "test": "UD_German-GSD/r2.10/de_gsd-ud-test.conllu",
1481	    },
1482	    "de_hdt": {
1483	        "train": [
1484	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-1.conllu",
1485	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-2.conllu",
1486	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-1.conllu",
1487	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-2.conllu",
1488	        ],
1489	        "dev": "UD_German-HDT/r2.10/de_hdt-ud-dev.conllu",
1490	        "test": "UD_German-HDT/r2.10/de_hdt-ud-test.conllu",
1491	    },
1492	    "de_lit": {
1493	        "test": "UD_German-LIT/r2.10/de_lit-ud-test.conllu",
1494	    },
1495	    "de_pud": {
1496	        "test": "UD_German-PUD/r2.10/de_pud-ud-test.conllu",
1497	    },
1498	    "got_proiel": {
1499	        "train": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-train.conllu",
1500	        "dev": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-dev.conllu",
1501	        "test": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-test.conllu",
1502	    },
1503	    "el_gdt": {
1504	        "train": "UD_Greek-GDT/r2.10/el_gdt-ud-train.conllu",
1505	        "dev": "UD_Greek-GDT/r2.10/el_gdt-ud-dev.conllu",
1506	        "test": "UD_Greek-GDT/r2.10/el_gdt-ud-test.conllu",
1507	    },
1508	    "gub_tudet": {
1509	        "test": "UD_Guajajara-TuDeT/r2.10/gub_tudet-ud-test.conllu",
1510	    },
1511	    "gn_oldtudet": {
1512	        "test": "UD_Guarani-OldTuDeT/r2.10/gn_oldtudet-ud-test.conllu",
1513	    },
1514	    "he_iahltwiki": {
1515	        "train": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-train.conllu",
1516	        "dev": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-dev.conllu",
1517	        "test": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-test.conllu",
1518	    },
1519	    "he_htb": {
1520	        "train": "UD_Hebrew-HTB/r2.10/he_htb-ud-train.conllu",
1521	        "dev": "UD_Hebrew-HTB/r2.10/he_htb-ud-dev.conllu",
1522	        "test": "UD_Hebrew-HTB/r2.10/he_htb-ud-test.conllu",
1523	    },
1524	    "qhe_hiencs": {
1525	        "train": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-train.conllu",
1526	        "dev": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-dev.conllu",
1527	        "test": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-test.conllu",
1528	    },
1529	    "hi_hdtb": {
1530	        "train": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-train.conllu",
1531	        "dev": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-dev.conllu",
1532	        "test": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-test.conllu",
1533	    },
1534	    "hi_pud": {
1535	        "test": "UD_Hindi-PUD/r2.10/hi_pud-ud-test.conllu",
1536	    },
1537	    "hu_szeged": {
1538	        "train": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-train.conllu",
1539	        "dev": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-dev.conllu",
1540	        "test": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-test.conllu",
1541	    },
1542	    "is_modern": {
1543	        "train": "UD_Icelandic-Modern/r2.10/is_modern-ud-train.conllu",
1544	        "dev": "UD_Icelandic-Modern/r2.10/is_modern-ud-dev.conllu",
1545	        "test": "UD_Icelandic-Modern/r2.10/is_modern-ud-test.conllu",
1546	    },
1547	    "is_icepahc": {
1548	        "train": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-train.conllu",
1549	        "dev": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-dev.conllu",
1550	        "test": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-test.conllu",
1551	    },
1552	    "is_pud": {
1553	        "test": "UD_Icelandic-PUD/r2.10/is_pud-ud-test.conllu",
1554	    },
1555	    "id_csui": {
1556	        "train": "UD_Indonesian-CSUI/r2.10/id_csui-ud-train.conllu",
1557	        "test": "UD_Indonesian-CSUI/r2.10/id_csui-ud-test.conllu",
1558	    },
1559	    "id_gsd": {
1560	        "train": "UD_Indonesian-GSD/r2.10/id_gsd-ud-train.conllu",
1561	        "dev": "UD_Indonesian-GSD/r2.10/id_gsd-ud-dev.conllu",
1562	        "test": "UD_Indonesian-GSD/r2.10/id_gsd-ud-test.conllu",
1563	    },
1564	    "id_pud": {
1565	        "test": "UD_Indonesian-PUD/r2.10/id_pud-ud-test.conllu",
1566	    },
1567	    "ga_twittirish": {
1568	        "test": "UD_Irish-TwittIrish/r2.10/ga_twittirish-ud-test.conllu",
1569	    },
1570	    "ga_idt": {
1571	        "train": "UD_Irish-IDT/r2.10/ga_idt-ud-train.conllu",
1572	        "dev": "UD_Irish-IDT/r2.10/ga_idt-ud-dev.conllu",
1573	        "test": "UD_Irish-IDT/r2.10/ga_idt-ud-test.conllu",
1574	    },
1575	    "it_valico": {
1576	        "test": "UD_Italian-Valico/r2.10/it_valico-ud-test.conllu",
1577	    },
1578	    "it_markit": {
1579	        "train": "UD_Italian-MarkIT/r2.10/it_markit-ud-train.conllu",
1580	        "dev": "UD_Italian-MarkIT/r2.10/it_markit-ud-dev.conllu",
1581	        "test": "UD_Italian-MarkIT/r2.10/it_markit-ud-test.conllu",
1582	    },
1583	    "it_isdt": {
1584	        "train": "UD_Italian-ISDT/r2.10/it_isdt-ud-train.conllu",
1585	        "dev": "UD_Italian-ISDT/r2.10/it_isdt-ud-dev.conllu",
1586	        "test": "UD_Italian-ISDT/r2.10/it_isdt-ud-test.conllu",
1587	    },
1588	    "it_partut": {
1589	        "train": "UD_Italian-ParTUT/r2.10/it_partut-ud-train.conllu",
1590	        "dev": "UD_Italian-ParTUT/r2.10/it_partut-ud-dev.conllu",
1591	        "test": "UD_Italian-ParTUT/r2.10/it_partut-ud-test.conllu",
1592	    },
1593	    "it_postwita": {
1594	        "train": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-train.conllu",
1595	        "dev": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-dev.conllu",
1596	        "test": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-test.conllu",
1597	    },
1598	    "it_pud": {
1599	        "test": "UD_Italian-PUD/r2.10/it_pud-ud-test.conllu",
1600	    },
1601	    "it_twittiro": {
1602	        "train": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-train.conllu",
1603	        "dev": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-dev.conllu",
1604	        "test": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-test.conllu",
1605	    },
1606	    "it_vit": {
1607	        "train": "UD_Italian-VIT/r2.10/it_vit-ud-train.conllu",
1608	        "dev": "UD_Italian-VIT/r2.10/it_vit-ud-dev.conllu",
1609	        "test": "UD_Italian-VIT/r2.10/it_vit-ud-test.conllu",
1610	    },
1611	    "ja_gsdluw": {
1612	        "train": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-train.conllu",
1613	        "dev": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-dev.conllu",
1614	        "test": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-test.conllu",
1615	    },
1616	    "ja_pudluw": {
1617	        "test": "UD_Japanese-PUDLUW/r2.10/ja_pudluw-ud-test.conllu",
1618	    },
1619	    "ja_bccwjluw": {
1620	        "train": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-train.conllu",
1621	        "dev": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-dev.conllu",
1622	        "test": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-test.conllu",
1623	    },
1624	    "ja_bccwj": {
1625	        "train": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-train.conllu",
1626	        "dev": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-dev.conllu",
1627	        "test": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-test.conllu",
1628	    },
1629	    "ja_gsd": {
1630	        "train": "UD_Japanese-GSD/r2.10/ja_gsd-ud-train.conllu",
1631	        "dev": "UD_Japanese-GSD/r2.10/ja_gsd-ud-dev.conllu",
1632	        "test": "UD_Japanese-GSD/r2.10/ja_gsd-ud-test.conllu",
1633	    },
1634	    "ja_modern": {
1635	        "test": "UD_Japanese-Modern/r2.10/ja_modern-ud-test.conllu",
1636	    },
1637	    "ja_pud": {
1638	        "test": "UD_Japanese-PUD/r2.10/ja_pud-ud-test.conllu",
1639	    },
1640	    "jv_csui": {
1641	        "test": "UD_Javanese-CSUI/r2.10/jv_csui-ud-test.conllu",
1642	    },
1643	    "urb_tudet": {
1644	        "test": "UD_Kaapor-TuDeT/r2.10/urb_tudet-ud-test.conllu",
1645	    },
1646	    "xnr_kdtb": {
1647	        "test": "UD_Kangri-KDTB/r2.10/xnr_kdtb-ud-test.conllu",
1648	    },
1649	    "krl_kkpp": {
1650	        "test": "UD_Karelian-KKPP/r2.10/krl_kkpp-ud-test.conllu",
1651	    },
1652	    "arr_tudet": {
1653	        "test": "UD_Karo-TuDeT/r2.10/arr_tudet-ud-test.conllu",
1654	    },
1655	    "kk_ktb": {
1656	        "train": "UD_Kazakh-KTB/r2.10/kk_ktb-ud-train.conllu",
1657	        "test": "UD_Kazakh-KTB/r2.10/kk_ktb-ud-test.conllu",
1658	    },
1659	    "kfm_aha": {
1660	        "test": "UD_Khunsari-AHA/r2.10/kfm_aha-ud-test.conllu",
1661	    },
1662	    "quc_iu": {
1663	        "test": "UD_Kiche-IU/r2.10/quc_iu-ud-test.conllu",
1664	    },
1665	    "koi_uh": {
1666	        "test": "UD_Komi_Permyak-UH/r2.10/koi_uh-ud-test.conllu",
1667	    },
1668	    "kpv_ikdp": {
1669	        "test": "UD_Komi_Zyrian-IKDP/r2.10/kpv_ikdp-ud-test.conllu",
1670	    },
1671	    "kpv_lattice": {
1672	        "test": "UD_Komi_Zyrian-Lattice/r2.10/kpv_lattice-ud-test.conllu",
1673	    },
1674	    "ko_gsd": {
1675	        "train": "UD_Korean-GSD/r2.10/ko_gsd-ud-train.conllu",
1676	        "dev": "UD_Korean-GSD/r2.10/ko_gsd-ud-dev.conllu",
1677	        "test": "UD_Korean-GSD/r2.10/ko_gsd-ud-test.conllu",
1678	    },
1679	    "ko_kaist": {
1680	        "train": "UD_Korean-Kaist/r2.10/ko_kaist-ud-train.conllu",
1681	        "dev": "UD_Korean-Kaist/r2.10/ko_kaist-ud-dev.conllu",
1682	        "test": "UD_Korean-Kaist/r2.10/ko_kaist-ud-test.conllu",
1683	    },
1684	    "ko_pud": {
1685	        "test": "UD_Korean-PUD/r2.10/ko_pud-ud-test.conllu",
1686	    },
1687	    "kmr_mg": {
1688	        "train": "UD_Kurmanji-MG/r2.10/kmr_mg-ud-train.conllu",
1689	        "test": "UD_Kurmanji-MG/r2.10/kmr_mg-ud-test.conllu",
1690	    },
1691	    "la_udante": {
1692	        "train": "UD_Latin-UDante/r2.10/la_udante-ud-train.conllu",
1693	        "dev": "UD_Latin-UDante/r2.10/la_udante-ud-dev.conllu",
1694	        "test": "UD_Latin-UDante/r2.10/la_udante-ud-test.conllu",
1695	    },
1696	    "la_ittb": {
1697	        "train": "UD_Latin-ITTB/r2.10/la_ittb-ud-train.conllu",
1698	        "dev": "UD_Latin-ITTB/r2.10/la_ittb-ud-dev.conllu",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:606
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
605	    "ja_pud": (
606	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
607	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
608	        " Raw Text to Universal "
609	        "Dependencies](http://universaldependencies.org/conll17/)."
610	    ),
611	    "jv_csui": (
612	        "UD Javanese-CSUI is a dependency treebank in Javanese, a regional "
613	        "language in Indonesia with more than 60 million users. The original "
614	        "sentences were taken from OPUS, especially from the WikiMatrix v1 "
615	        "corpus. We revised the sentences that contained more Indonesian words"
616	        " than Javanese words and manually annotated them."
617	    ),
618	    "urb_tudet": (
619	        "UD_Kaapor-TuDeT is a collection of annotated sentences in Ka'apor. "
620	        "The project is a work in progress and the treebank is being updated "
621	        "on a regular basis."
622	    ),
623	    "xnr_kdtb": (
624	        "The Kangri UD Treebank (KDTB) is a part of the Universal Dependency "
625	        "treebank project."
626	    ),
627	    "krl_kkpp": (
628	        "UD Karelian-KKPP is a manually annotated new corpus of Karelian made "
629	        "in Universal dependencies annotation scheme. The data is collected "
630	        "from VepKar corpora and consists of mostly modern news texts but also"
631	        " some stories and educational texts."
632	    ),
633	    "kk_ktb": (
634	        "The UD Kazakh treebank is a combination of text from various sources "
635	        "including Wikipedia, some folk tales, sentences from the UDHR, news "
636	        "and phrasebook sentences. Sentences IDs include partial document "
637	        "identifiers."
638	    ),
639	    "arr_tudet": (
640	        "UD_Karo-TuDeT is a collection of annotated sentences in Karo. The "
641	        "sentences stem from the only grammatical description of the language "
642	        "(Gabas, 1999) and from the sentences in the dictionary by the same "
643	        "author (Gabas, 2007). Sentence annotation and documentation by "
644	        "Fabrcio Ferraz Gerardi."
645	    ),
646	    "kfm_aha": (
647	        "The AHA Khunsari Treebank is a small treebank for contemporary "
648	        "Khunsari. Its corpus is collected and annotated manually. We have "
649	        "prepared this treebank based on interviews with Khunsari speakers."
650	    ),
651	    "quc_iu": (
652	        "UD Kiche-IU is a treebank consisting of sentences from a variety of"
653	        " text domains but principally dictionary example sentences and "
654	        "linguistic examples."
655	    ),
656	    "koi_uh": (
657	        "This is a Komi-Permyak literary language treebank consisting of "
658	        "original and translated texts."
659	    ),
660	    "kpv_ikdp": (
661	        "This treebank consists of dialectal transcriptions of spoken "
662	        "Komi-Zyrian. The current texts are short recorded segments from "
663	        "different areas where the Iva dialect of Komi language is spoken."
664	    ),
665	    "kpv_lattice": (
666	        "UD Komi-Zyrian Lattice is a treebank of written standard Komi-Zyrian."
667	    ),
668	    "ko_gsd": (
669	        "The Google Korean Universal Dependency Treebank is first converted "
670	        "from the Universal Dependency Treebank v2.0 (legacy), and then "
671	        "enhanced by Chun et al., 2018."
672	    ),
673	    "ko_kaist": (
674	        "The KAIST Korean Universal Dependency Treebank is generated by Chun "
675	        "et al., 2018 from the constituency trees in the KAIST Tree-Tagging "
676	        "Corpus."
677	    ),
678	    "ko_pud": (
679	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
680	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
681	        "Raw Text to Universal Dependencies."
682	    ),
683	    "kmr_mg": (
684	        "The UD Kurmanji corpus is a corpus of Kurmanji Kurdish. It contains "
685	        "fiction and encyclopaedic texts in roughly equal measure. It has been"
686	        " annotated natively in accordance with the UD annotation scheme."
687	    ),
688	    "la_ittb": (
689	        "Latin data from the _Index Thomisticus_ Treebank. Data are taken from"
690	        " the _Index Thomisticus_ corpus by Roberto Busa SJ, which contains "
691	        "the complete work by Thomas Aquinas (12251274; Medieval Latin) and "
692	        "by 61 other authors related to Thomas."
693	    ),
694	    "la_udante": (
695	        "The UDante treebank is based on the Latin texts of Dante Alighieri, "
696	        "taken from the DanteSearch corpus, originally created at the "
697	        "University of Pisa, Italy. It is a treebank of Latin language, more "
698	        "precisely of literary Medieval Latin (XIVth century)."
699	    ),
700	    "la_llct": (
701	        "This Universal Dependencies version of the LLCT (Late Latin Charter "
702	        "Treebank) consists of an automated conversion of the LLCT2 treebank "
703	        "from the Latin Dependency Treebank (LDT) format into the Universal "
704	        "Dependencies standard."
705	    ),
706	    "la_perseus": (
707	        "This Universal Dependencies Latin Treebank consists of an automatic "
708	        "conversion of a selection of passages from the Ancient Greek and "
709	        "Latin Dependency Treebank 2.1"
710	    ),
711	    "la_proiel": (
712	        "The Latin PROIEL treebank is based on the Latin data from the PROIEL "
713	        "treebank, and contains most of the Vulgate New Testament translations"
714	        " plus selections from Caesar's Gallic War, Cicero's Letters to "
715	        "Atticus, Palladius' Opus Agriculturae and the first book of Cicero's "
716	        "De officiis."
717	    ),
718	    "lv_lvtb": (
719	        "Latvian UD Treebank is based on Latvian Treebank (LVTB), being "
720	        "created at University of Latvia, Institute of Mathematics and "
721	        "Computer Science, Artificial Intelligence Laboratory."
722	    ),
723	    "lij_glt": (
724	        "The Genoese Ligurian Treebank is a small, manually annotated "
725	        "collection of contemporary Ligurian prose. The focus of the treebank "
726	        "is written Genoese, the koin variety of Ligurian which is associated"
727	        " with today's literary, journalistic and academic ligurophone sphere."
728	    ),
729	    "lt_alksnis": (
730	        "The Lithuanian dependency treebank ALKSNIS v3.0 (Vytautas Magnus "
731	        "University)."
732	    ),
733	    "lt_hse": (
734	        "Lithuanian treebank annotated manually (dependencies) using the "
735	        "Morphological Annotator by CCL, Vytautas Magnus University "
736	        "(http://tekstynas.vdu.lt/) and manual disambiguation. A pilot version"
737	        " which includes news and an essay by Tomas Venclova is available "
738	        "here."
739	    ),
740	    "olo_kkpp": (
741	        "UD Livvi-KKPP is a manually annotated new corpus of Livvi-Karelian "
742	        "made directly in the Universal dependencies annotation scheme. The "
743	        "data is collected from VepKar corpora and consists of mostly modern "
744	        "news texts but also some stories and educational texts."
745	    ),
746	    "nds_lsdc": (
747	        "The UD Low Saxon LSDC dataset consists of sentences in 18 Low Saxon "
748	        "dialects from both Germany and the Netherlands. These sentences are "
749	        "(or are to become) part of the LSDC dataset and represent the "
750	        "language from the 19th and early 20th century in genres such as short"
751	        " stories, novels, speeches, letters and fairytales."
752	    ),
753	    "mt_mudt": (
754	        "MUDT (Maltese Universal Dependencies Treebank) is a manually "
755	        "annotated treebank of Maltese, a Semitic language of Malta descended "
756	        "from North African Arabic with a significant amount of Italo-Romance "
757	        "influence. MUDT was designed as a balanced corpus with four major "
758	        "genres (see Splitting below) represented roughly equally."
759	    ),
760	    "gv_cadhan": (
761	        "This is the Cadhan Aonair UD treebank for Manx Gaelic, created by "
762	        "Kevin Scannell."
763	    ),
764	    "mr_ufal": (
765	        "UD Marathi is a manually annotated treebank consisting primarily of "
766	        "stories from Wikisource, and parts of an article on Wikipedia."
767	    ),
768	    "gun_dooley": (
769	        "UD Mbya_Guarani-Dooley is a corpus of narratives written in Mby "
770	        "Guaran (Tupian) in Brazil, and collected by Robert Dooley. Due to "
771	        "copyright restrictions, the corpus that is distributed as part of UD "
772	        "only contains the annotation (tags, features, relations) while the "
773	        "FORM and LEMMA columns are empty."
774	    ),
775	    "gun_thomas": (
776	        "UD Mbya_Guarani-Thomas is a corpus of Mby Guaran (Tupian) texts "
777	        "collected by Guillaume Thomas. The current version of the corpus "
778	        "consists of three speeches by Paulina Kerechu Nez Romero, a Mby "
779	        "Guaran speaker from Ytu, Caazap Department, Paraguay."
780	    ),
781	    "mdf_jr": (
782	        "Erme Universal Dependencies annotated texts Moksha are the origin of "
783	        "UD_Moksha-JR with annotation (CoNLL-U) for texts in the Moksha "
784	        "language, it originally consists of a sample from a number of fiction"
785	        " authors writing originals in Moksha."
786	    ),
787	    "myu_tudet": (
788	        "UD_Munduruku-TuDeT is a collection of annotated sentences in "
789	        "Munduruk. Together with UD_Akuntsu-TuDeT and UD_Tupinamba-TuDeT, "
790	        "UD_Munduruku-TuDeT is part of the TuLaR project."
791	    ),
792	    "pcm_nsc": (
793	        "A Universal Dependencies corpus for spoken Naija (Nigerian Pidgin)."
794	    ),
795	    "nyq_aha": (
796	        "The AHA Nayini Treebank is a small treebank for contemporary Nayini. "
797	        "Its corpus is collected and annotated manually. We have prepared this"
798	        " treebank based on interviews with Nayini speakers."
799	    ),
800	    "sme_giella": (
801	        "This is a North Smi treebank based on a manually disambiguated and "
802	        "function-labelled gold-standard corpus of North Smi produced by the "
803	        "Giellatekno team at UiT Norgga rktala universitehta."
804	    ),
805	    "no_bokmaal": (
806	        "The Norwegian UD treebank is based on the Bokml section of the "
807	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
808	        " Norwegian. NDT has been automatically converted to the UD scheme by "
809	        "Lilja vrelid at the University of Oslo."
810	    ),
811	    "no_nynorsk": (
812	        "The Norwegian UD treebank is based on the Nynorsk section of the "
813	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
814	        " Norwegian.  NDT has been automatically converted to the UD scheme by"
815	        " Lilja vrelid at the University of Oslo."
816	    ),
817	    "no_nynorsklia": (
818	        "This Norwegian treebank is based on the LIA treebank of transcribed "
819	        "spoken Norwegian dialects. The treebank has been automatically "
820	        "converted to the UD scheme by Lilja vrelid at the University of "
821	        "Oslo."
822	    ),
823	    "cu_proiel": (
824	        "The Old Church Slavonic (OCS) UD treebank is based on the Old Church "
825	        "Slavonic data from the PROIEL treebank and contains the text of the "
826	        "Codex Marianus New Testament translation."
827	    ),
828	    "fro_srcmf": (
829	        "UD_Old_French-SRCMF is a conversion of (part of) the SRCMF corpus "
830	        "(Syntactic Reference Corpus of Medieval French srcmf.org)."
831	    ),
832	    "orv_birchbark": (
833	        "UD Old_East_Slavic-Birchbark is based on the RNC Corpus of Birchbark "
834	        "Letters and includes documents written in 1025-1500 in an East Slavic"
835	        " vernacular (letters, household and business records, records for "
836	        "church services, spell against diseases, and other short "
837	        "inscriptions). The treebank is manually syntactically annotated in "
838	        "the UD 2.0 scheme, morphological and lexical annotation is a "
839	        "conversion of the original RNC annotation."
840	    ),
841	    "orv_rnc": (
842	        "`UD_Old_Russian-RNC` is a sample of the Middle Russian corpus "
843	        "(1300-1700), a part of the Russian National Corpus. The data were "
844	        "originally annotated according to the RNC and extended UD-Russian "
845	        "morphological schemas and UD 2.4 dependency schema."
846	    ),
847	    "orv_torot": (
848	        "UD_Old_Russian-TOROT is a conversion of a selection of the Old East "
849	        "Slavonic and Middle Russian data in the Troms Old Russian and OCS "
850	        "Treebank (TOROT), which was originally annotated in PROIEL dependency"
851	        " format."
852	    ),
853	    "otk_tonqq": (
854	        "`UD_Old_Turkish-Tonqq` is an Old Turkish treebank built upon Turkic "
855	        "script texts or sentences that are trivially convertible."
856	    ),
857	    "fa_perdt": (
858	        "The Persian Universal Dependency Treebank (PerUDT) is the result of "
859	        "automatic coversion of Persian Dependency Treebank (PerDT) with "
860	        "extensive manual corrections. Please refer to the follwoing work, if "
861	        "you use this data: Mohammad Sadegh Rasooli, Pegah Safari, Amirsaeid "
862	        "Moloodi, and Alireza Nourian. 'The Persian Dependency Treebank Made "
863	        "Universal'. 2020 (to appear)."
864	    ),
865	    "fa_seraji": (
866	        "The Persian Universal Dependency Treebank (Persian UD) is based on "
867	        "Uppsala Persian Dependency Treebank (UPDT). The conversion of the "
868	        "UPDT to the Universal Dependencies was performed semi-automatically "
869	        "with extensive manual checks and corrections."
870	    ),
871	    "pl_lfg": (
872	        "The LFG Enhanced UD treebank of Polish is based on a corpus of LFG "
873	        "(Lexical Functional Grammar) syntactic structures generated by an LFG"
874	        " grammar of Polish, POLFIE, and manually disambiguated by human "
875	        "annotators."
876	    ),
877	    "pl_pdb": (
878	        "The Polish PDB-UD treebank is based on the Polish Dependency Bank 2.0"
879	        " (PDB 2.0), created at the Institute of Computer Science, Polish "
880	        "Academy of Sciences in Warsaw. The PDB-UD treebank is an extended and"
881	        " corrected version of the Polish SZ-UD treebank (the release 1.2 to "
882	        "2.3)."
883	    ),
884	    "pl_pud": (
885	        "This is the Polish portion of the Parallel Universal Dependencies "
886	        "(PUD) treebanks, created at the Institute of Computer Science, Polish"
887	        " Academy of Sciences in Warsaw.Re"
888	    ),
889	    "pt_bosque": (
890	        "This Universal Dependencies (UD) Portuguese treebank is based on the "
891	        "Constraint Grammar converted version of the Bosque, which is part of "
892	        "the Floresta Sint(c)tica treebank. It contains both European "
893	        "(CETEMPblico) and Brazilian (CETENFolha) variants."
894	    ),
895	    "pt_gsd": (
896	        "The Brazilian Portuguese UD is converted from the Google Universal "
897	        "Dependency Treebank v2.0 (legacy)."
898	    ),
899	    "pt_pud": (
900	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
901	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
902	        "Raw Text to Universal Dependencies."
903	    ),
904	    "ro_art": (
905	        "The UD treebank ArT is a treebank of the Aromanian dialect of the "
906	        "Romanian language in UD format."
907	    ),
908	    "ro_nonstandard": (
909	        "The Romanian Non-standard UD treebank (called UAIC-RoDia) is based on"
910	        " UAIC-RoDia Treebank. UAIC-RoDia = ISLRN 156-635-615-024-0"
911	    ),
912	    "ro_rrt": (
913	        "The Romanian UD treebank (called RoRefTrees) (Barbu Mititelu et al., "
914	        "2016) is the reference treebank in UD format for standard Romanian."
915	    ),
916	    "ro_simonero": "SiMoNERo is a medical corpus of contemporary Romanian.",
917	    "ru_gsd": (
918	        "Russian Universal Dependencies Treebank annotated and converted by "
919	        "Google."
920	    ),
921	    "ru_pud": (
922	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
923	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
924	        "Raw Text to Universal Dependencies."
925	    ),
926	    "ru_syntagrus": "Russian data from the SynTagRus corpus.",
927	    "ru_taiga": (
928	        "Universal Dependencies treebank is based on data samples extracted "
929	        "from Taiga Corpus and MorphoRuEval-2017 and GramEval-2020 shared "
930	        "tasks collections."
931	    ),
932	    "sa_ufal": (
933	        "A small Sanskrit treebank of sentences from Pacatantra, an ancient "
934	        "Indian collection of interrelated fables by Vishnu Sharma."
935	    ),
936	    "sa_vedic": (
937	        "The Treebank of Vedic Sanskrit contains 4,000 sentences with 27,000 "
938	        "words chosen from metrical and prose passages of the gveda (RV), the"
939	        " aunaka recension of the Atharvaveda (S), the Maitryasahit "
940	        "(MS), and the Aitareya- (AB) and atapatha-Brhmaas (B). Lexical "
941	        "and morpho-syntactic information has been generated using a tagging "
942	        "software and manually validated. POS tags have been induced "
943	        "automatically from the morpho-sytactic information of each word."
944	    ),
945	    "gd_arcosg": (
946	        "A treebank of Scottish Gaelic based on the Annotated Reference Corpus"
947	        " Of Scottish Gaelic (ARCOSG)."
948	    ),
949	    "sr_set": (
950	        "The Serbian UD treebank is based on the "
951	        "[SETimes-SR](http://hdl.handle.net/11356/1200) corpus and additional "
952	        "news documents from the Serbian web."
953	    ),
954	    "sms_giellagas": (
955	        "The UD Skolt Sami Giellagas treebank is based almost entirely on "
956	        "spoken Skolt Sami corpora."
957	    ),
958	    "sk_snk": (
959	        "The Slovak UD treebank is based on data originally annotated as part "
960	        "of the Slovak National Corpus, following the annotation style of the "
961	        "Prague Dependency Treebank."
962	    ),
963	    "sl_ssj": (
964	        "The Slovenian UD Treebank is a rule-based conversion of the ssj500k "
965	        "treebank, the largest collection of manually syntactically annotated "
966	        "data in Slovenian, originally annotated in the JOS annotation scheme."
967	    ),
968	    "sl_sst": (
969	        "The Spoken Slovenian UD Treebank (SST) is the first syntactically "
970	        "annotated corpus of spoken Slovenian, based on a sample of the "
971	        "reference GOS corpus, a collection of transcribed audio recordings of"
972	        " monologic, dialogic and multi-party spontaneous speech in different "
973	        "everyday situations."
974	    ),
975	    "soj_aha": (
976	        "The AHA Soi Treebank is a small treebank for contemporary Soi. Its "
977	        "corpus is collected and annotated manually. We have prepared this "
978	        "treebank based on interviews with Soi speakers."
979	    ),
980	    "ajp_madar": (
981	        "The South_Levantine_Arabic-MADAR treebank consists of 100 "
982	        "manually-annotated sentences taken from the "
983	        "[MADAR](https://camel.abudhabi.nyu.edu/madar/) (Multi-Arabic Dialect "
984	        "Applications and Resources) project. "
985	    ),
986	    "es_ancora": "Spanish data from the AnCora corpus.",
987	    "es_gsd": (
988	        "The Spanish UD is converted from the content head version of the "
989	        "universal dependency treebank v2.0 (legacy)."
990	    ),
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {
1217	        "test": "UD_Arabic-PUD/r2.10/ar_pud-ud-test.conllu",
1218	    },
1219	    "hy_armtdp": {
1220	        "train": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-train.conllu",
1221	        "dev": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-dev.conllu",
1222	        "test": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-test.conllu",
1223	    },
1224	    "aii_as": {
1225	        "test": "UD_Assyrian-AS/r2.10/aii_as-ud-test.conllu",
1226	    },
1227	    "bm_crb": {
1228	        "test": "UD_Bambara-CRB/r2.10/bm_crb-ud-test.conllu",
1229	    },
1230	    "eu_bdt": {
1231	        "train": "UD_Basque-BDT/r2.10/eu_bdt-ud-train.conllu",
1232	        "dev": "UD_Basque-BDT/r2.10/eu_bdt-ud-dev.conllu",
1233	        "test": "UD_Basque-BDT/r2.10/eu_bdt-ud-test.conllu",
1234	    },
1235	    "bej_nsc": {
1236	        "test": "UD_Beja-NSC/r2.10/bej_nsc-ud-test.conllu",
1237	    },
1238	    "be_hse": {
1239	        "train": "UD_Belarusian-HSE/r2.10/be_hse-ud-train.conllu",
1240	        "dev": "UD_Belarusian-HSE/r2.10/be_hse-ud-dev.conllu",
1241	        "test": "UD_Belarusian-HSE/r2.10/be_hse-ud-test.conllu",
1242	    },
1243	    "bn_bru": {
1244	        "test": "UD_Bengali-BRU/r2.10/bn_bru-ud-test.conllu",
1245	    },
1246	    "bho_bhtb": {
1247	        "test": "UD_Bhojpuri-BHTB/r2.10/bho_bhtb-ud-test.conllu",
1248	    },
1249	    "br_keb": {
1250	        "test": "UD_Breton-KEB/r2.10/br_keb-ud-test.conllu",
1251	    },
1252	    "bg_btb": {
1253	        "train": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-train.conllu",
1254	        "dev": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-dev.conllu",
1255	        "test": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-test.conllu",
1256	    },
1257	    "bxr_bdt": {
1258	        "train": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-train.conllu",
1259	        "test": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-test.conllu",
1260	    },
1261	    "yue_hk": {
1262	        "test": "UD_Cantonese-HK/r2.10/yue_hk-ud-test.conllu",
1263	    },
1264	    "ca_ancora": {
1265	        "train": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-train.conllu",
1266	        "dev": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-dev.conllu",
1267	        "test": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-test.conllu",
1268	    },
1269	    "ceb_gja": {
1270	        "test": "UD_Cebuano-GJA/r2.10/ceb_gja-ud-test.conllu",
1271	    },
1272	    "zh_cfl": {
1273	        "test": "UD_Chinese-CFL/r2.10/zh_cfl-ud-test.conllu",
1274	    },
1275	    "zh_gsd": {
1276	        "train": "UD_Chinese-GSD/r2.10/zh_gsd-ud-train.conllu",
1277	        "dev": "UD_Chinese-GSD/r2.10/zh_gsd-ud-dev.conllu",
1278	        "test": "UD_Chinese-GSD/r2.10/zh_gsd-ud-test.conllu",
1279	    },
1280	    "zh_gsdsimp": {
1281	        "train": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-train.conllu",
1282	        "dev": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-dev.conllu",
1283	        "test": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-test.conllu",
1284	    },
1285	    "zh_hk": {
1286	        "test": "UD_Chinese-HK/r2.10/zh_hk-ud-test.conllu",
1287	    },
1288	    "zh_pud": {
1289	        "test": "UD_Chinese-PUD/r2.10/zh_pud-ud-test.conllu",
1290	    },
1291	    "ckt_hse": {
1292	        "test": "UD_Chukchi-HSE/r2.10/ckt_hse-ud-test.conllu",
1293	    },
1294	    "lzh_kyoto": {
1295	        "train": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-train.conllu",
1296	        "dev": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-dev.conllu",
1297	        "test": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-test.conllu",
1298	    },
1299	    "cop_scriptorium": {
1300	        "train": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-train.conllu",
1301	        "dev": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-dev.conllu",
1302	        "test": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-test.conllu",
1303	    },
1304	    "hr_set": {
1305	        "train": "UD_Croatian-SET/r2.10/hr_set-ud-train.conllu",
1306	        "dev": "UD_Croatian-SET/r2.10/hr_set-ud-dev.conllu",
1307	        "test": "UD_Croatian-SET/r2.10/hr_set-ud-test.conllu",
1308	    },
1309	    "cs_cac": {
1310	        "train": "UD_Czech-CAC/r2.10/cs_cac-ud-train.conllu",
1311	        "dev": "UD_Czech-CAC/r2.10/cs_cac-ud-dev.conllu",
1312	        "test": "UD_Czech-CAC/r2.10/cs_cac-ud-test.conllu",
1313	    },
1314	    "cs_cltt": {
1315	        "train": "UD_Czech-CLTT/r2.10/cs_cltt-ud-train.conllu",
1316	        "dev": "UD_Czech-CLTT/r2.10/cs_cltt-ud-dev.conllu",
1317	        "test": "UD_Czech-CLTT/r2.10/cs_cltt-ud-test.conllu",
1318	    },
1319	    "cs_fictree": {
1320	        "train": "UD_Czech-FicTree/r2.10/cs_fictree-ud-train.conllu",
1321	        "dev": "UD_Czech-FicTree/r2.10/cs_fictree-ud-dev.conllu",
1322	        "test": "UD_Czech-FicTree/r2.10/cs_fictree-ud-test.conllu",
1323	    },
1324	    "cs_pdt": {
1325	        "train": [
1326	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-l.conllu",
1327	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-m.conllu",
1328	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-c.conllu",
1329	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-v.conllu",
1330	        ],
1331	        "dev": "UD_Czech-PDT/r2.10/cs_pdt-ud-dev.conllu",
1332	        "test": "UD_Czech-PDT/r2.10/cs_pdt-ud-test.conllu",
1333	    },
1334	    "cs_pud": {
1335	        "test": "UD_Czech-PUD/r2.10/cs_pud-ud-test.conllu",
1336	    },
1337	    "da_ddt": {
1338	        "train": "UD_Danish-DDT/r2.10/da_ddt-ud-train.conllu",
1339	        "dev": "UD_Danish-DDT/r2.10/da_ddt-ud-dev.conllu",
1340	        "test": "UD_Danish-DDT/r2.10/da_ddt-ud-test.conllu",
1341	    },
1342	    "nl_alpino": {
1343	        "train": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-train.conllu",
1344	        "dev": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-dev.conllu",
1345	        "test": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-test.conllu",
1346	    },
1347	    "nl_lassysmall": {
1348	        "train": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-train.conllu",
1349	        "dev": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-dev.conllu",
1350	        "test": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-test.conllu",
1351	    },
1352	    "en_atis": {
1353	        "train": "UD_English-Atis/r2.10/en_atis-ud-train.conllu",
1354	        "dev": "UD_English-Atis/r2.10/en_atis-ud-dev.conllu",
1355	        "test": "UD_English-Atis/r2.10/en_atis-ud-test.conllu",
1356	    },
1357	    "en_esl": {
1358	        "train": "UD_English-ESL/r2.10/en_esl-ud-train.conllu",
1359	        "dev": "UD_English-ESL/r2.10/en_esl-ud-dev.conllu",
1360	        "test": "UD_English-ESL/r2.10/en_esl-ud-test.conllu",
1361	    },
1362	    "en_ewt": {
1363	        "train": "UD_English-EWT/r2.10/en_ewt-ud-train.conllu",
1364	        "dev": "UD_English-EWT/r2.10/en_ewt-ud-dev.conllu",
1365	        "test": "UD_English-EWT/r2.10/en_ewt-ud-test.conllu",
1366	    },
1367	    "en_gum": {
1368	        "train": "UD_English-GUM/r2.10/en_gum-ud-train.conllu",
1369	        "dev": "UD_English-GUM/r2.10/en_gum-ud-dev.conllu",
1370	        "test": "UD_English-GUM/r2.10/en_gum-ud-test.conllu",
1371	    },
1372	    "en_gumreddit": {
1373	        "train": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-train.conllu",
1374	        "dev": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-dev.conllu",
1375	        "test": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-test.conllu",
1376	    },
1377	    "en_lines": {
1378	        "train": "UD_English-LinES/r2.10/en_lines-ud-train.conllu",
1379	        "dev": "UD_English-LinES/r2.10/en_lines-ud-dev.conllu",
1380	        "test": "UD_English-LinES/r2.10/en_lines-ud-test.conllu",
1381	    },
1382	    "en_partut": {
1383	        "train": "UD_English-ParTUT/r2.10/en_partut-ud-train.conllu",
1384	        "dev": "UD_English-ParTUT/r2.10/en_partut-ud-dev.conllu",
1385	        "test": "UD_English-ParTUT/r2.10/en_partut-ud-test.conllu",
1386	    },
1387	    "en_pronouns": {
1388	        "test": "UD_English-Pronouns/r2.10/en_pronouns-ud-test.conllu",
1389	    },
1390	    "en_pud": {
1391	        "test": "UD_English-PUD/r2.10/en_pud-ud-test.conllu",
1392	    },
1393	    "myv_jr": {
1394	        "test": "UD_Erzya-JR/r2.10/myv_jr-ud-test.conllu",
1395	    },
1396	    "et_edt": {
1397	        "train": "UD_Estonian-EDT/r2.10/et_edt-ud-train.conllu",
1398	        "dev": "UD_Estonian-EDT/r2.10/et_edt-ud-dev.conllu",
1399	        "test": "UD_Estonian-EDT/r2.10/et_edt-ud-test.conllu",
1400	    },
1401	    "et_ewt": {
1402	        "train": "UD_Estonian-EWT/r2.10/et_ewt-ud-train.conllu",
1403	        "dev": "UD_Estonian-EWT/r2.10/et_ewt-ud-dev.conllu",
1404	        "test": "UD_Estonian-EWT/r2.10/et_ewt-ud-test.conllu",
1405	    },
1406	    "fo_farpahc": {
1407	        "train": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-train.conllu",
1408	        "dev": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-dev.conllu",
1409	        "test": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-test.conllu",
1410	    },
1411	    "fo_oft": {
1412	        "test": "UD_Faroese-OFT/r2.10/fo_oft-ud-test.conllu",
1413	    },
1414	    "fi_ftb": {
1415	        "train": "UD_Finnish-FTB/r2.10/fi_ftb-ud-train.conllu",
1416	        "dev": "UD_Finnish-FTB/r2.10/fi_ftb-ud-dev.conllu",
1417	        "test": "UD_Finnish-FTB/r2.10/fi_ftb-ud-test.conllu",
1418	    },
1419	    "fi_ood": {
1420	        "test": "UD_Finnish-OOD/r2.10/fi_ood-ud-test.conllu",
1421	    },
1422	    "fi_pud": {
1423	        "test": "UD_Finnish-PUD/r2.10/fi_pud-ud-test.conllu",
1424	    },
1425	    "fi_tdt": {
1426	        "train": "UD_Finnish-TDT/r2.10/fi_tdt-ud-train.conllu",
1427	        "dev": "UD_Finnish-TDT/r2.10/fi_tdt-ud-dev.conllu",
1428	        "test": "UD_Finnish-TDT/r2.10/fi_tdt-ud-test.conllu",
1429	    },
1430	    "fr_parisstories": {
1431	        "train": "UD_French-ParisStories/r2.10/fr_parisstories-ud-train.conllu",
1432	        "test": "UD_French-ParisStories/r2.10/fr_parisstories-ud-test.conllu",
1433	    },
1434	    "fr_fqb": {
1435	        "test": "UD_French-FQB/r2.10/fr_fqb-ud-test.conllu",
1436	    },
1437	    "fr_rhapsodie": {
1438	        "train": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-train.conllu",
1439	        "dev": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-dev.conllu",
1440	        "test": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-test.conllu",
1441	    },
1442	    "fr_ftb": {
1443	        "train": "UD_French-FTB/r2.10/fr_ftb-ud-train.conllu",
1444	        "dev": "UD_French-FTB/r2.10/fr_ftb-ud-dev.conllu",
1445	        "test": "UD_French-FTB/r2.10/fr_ftb-ud-test.conllu",
1446	    },
1447	    "fr_gsd": {
1448	        "train": "UD_French-GSD/r2.10/fr_gsd-ud-train.conllu",
1449	        "dev": "UD_French-GSD/r2.10/fr_gsd-ud-dev.conllu",
1450	        "test": "UD_French-GSD/r2.10/fr_gsd-ud-test.conllu",
1451	    },
1452	    "fr_partut": {
1453	        "train": "UD_French-ParTUT/r2.10/fr_partut-ud-train.conllu",
1454	        "dev": "UD_French-ParTUT/r2.10/fr_partut-ud-dev.conllu",
1455	        "test": "UD_French-ParTUT/r2.10/fr_partut-ud-test.conllu",
1456	    },
1457	    "fr_pud": {
1458	        "test": "UD_French-PUD/r2.10/fr_pud-ud-test.conllu",
1459	    },
1460	    "fr_sequoia": {
1461	        "train": "UD_French-Sequoia/r2.10/fr_sequoia-ud-train.conllu",
1462	        "dev": "UD_French-Sequoia/r2.10/fr_sequoia-ud-dev.conllu",
1463	        "test": "UD_French-Sequoia/r2.10/fr_sequoia-ud-test.conllu",
1464	    },
1465	    "qfn_fame": {
1466	        "test": "UD_Frisian_Dutch-Fame/r2.10/qfn_fame-ud-test.conllu",
1467	    },
1468	    "gl_ctg": {
1469	        "train": "UD_Galician-CTG/r2.10/gl_ctg-ud-train.conllu",
1470	        "dev": "UD_Galician-CTG/r2.10/gl_ctg-ud-dev.conllu",
1471	        "test": "UD_Galician-CTG/r2.10/gl_ctg-ud-test.conllu",
1472	    },
1473	    "gl_treegal": {
1474	        "train": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-train.conllu",
1475	        "test": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-test.conllu",
1476	    },
1477	    "de_gsd": {
1478	        "train": "UD_German-GSD/r2.10/de_gsd-ud-train.conllu",
1479	        "dev": "UD_German-GSD/r2.10/de_gsd-ud-dev.conllu",
1480	        "test": "UD_German-GSD/r2.10/de_gsd-ud-test.conllu",
1481	    },
1482	    "de_hdt": {
1483	        "train": [
1484	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-1.conllu",
1485	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-2.conllu",
1486	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-1.conllu",
1487	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-2.conllu",
1488	        ],
1489	        "dev": "UD_German-HDT/r2.10/de_hdt-ud-dev.conllu",
1490	        "test": "UD_German-HDT/r2.10/de_hdt-ud-test.conllu",
1491	    },
1492	    "de_lit": {
1493	        "test": "UD_German-LIT/r2.10/de_lit-ud-test.conllu",
1494	    },
1495	    "de_pud": {
1496	        "test": "UD_German-PUD/r2.10/de_pud-ud-test.conllu",
1497	    },
1498	    "got_proiel": {
1499	        "train": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-train.conllu",
1500	        "dev": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-dev.conllu",
1501	        "test": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-test.conllu",
1502	    },
1503	    "el_gdt": {
1504	        "train": "UD_Greek-GDT/r2.10/el_gdt-ud-train.conllu",
1505	        "dev": "UD_Greek-GDT/r2.10/el_gdt-ud-dev.conllu",
1506	        "test": "UD_Greek-GDT/r2.10/el_gdt-ud-test.conllu",
1507	    },
1508	    "gub_tudet": {
1509	        "test": "UD_Guajajara-TuDeT/r2.10/gub_tudet-ud-test.conllu",
1510	    },
1511	    "gn_oldtudet": {
1512	        "test": "UD_Guarani-OldTuDeT/r2.10/gn_oldtudet-ud-test.conllu",
1513	    },
1514	    "he_iahltwiki": {
1515	        "train": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-train.conllu",
1516	        "dev": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-dev.conllu",
1517	        "test": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-test.conllu",
1518	    },
1519	    "he_htb": {
1520	        "train": "UD_Hebrew-HTB/r2.10/he_htb-ud-train.conllu",
1521	        "dev": "UD_Hebrew-HTB/r2.10/he_htb-ud-dev.conllu",
1522	        "test": "UD_Hebrew-HTB/r2.10/he_htb-ud-test.conllu",
1523	    },
1524	    "qhe_hiencs": {
1525	        "train": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-train.conllu",
1526	        "dev": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-dev.conllu",
1527	        "test": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-test.conllu",
1528	    },
1529	    "hi_hdtb": {
1530	        "train": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-train.conllu",
1531	        "dev": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-dev.conllu",
1532	        "test": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-test.conllu",
1533	    },
1534	    "hi_pud": {
1535	        "test": "UD_Hindi-PUD/r2.10/hi_pud-ud-test.conllu",
1536	    },
1537	    "hu_szeged": {
1538	        "train": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-train.conllu",
1539	        "dev": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-dev.conllu",
1540	        "test": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-test.conllu",
1541	    },
1542	    "is_modern": {
1543	        "train": "UD_Icelandic-Modern/r2.10/is_modern-ud-train.conllu",
1544	        "dev": "UD_Icelandic-Modern/r2.10/is_modern-ud-dev.conllu",
1545	        "test": "UD_Icelandic-Modern/r2.10/is_modern-ud-test.conllu",
1546	    },
1547	    "is_icepahc": {
1548	        "train": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-train.conllu",
1549	        "dev": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-dev.conllu",
1550	        "test": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-test.conllu",
1551	    },
1552	    "is_pud": {
1553	        "test": "UD_Icelandic-PUD/r2.10/is_pud-ud-test.conllu",
1554	    },
1555	    "id_csui": {
1556	        "train": "UD_Indonesian-CSUI/r2.10/id_csui-ud-train.conllu",
1557	        "test": "UD_Indonesian-CSUI/r2.10/id_csui-ud-test.conllu",
1558	    },
1559	    "id_gsd": {
1560	        "train": "UD_Indonesian-GSD/r2.10/id_gsd-ud-train.conllu",
1561	        "dev": "UD_Indonesian-GSD/r2.10/id_gsd-ud-dev.conllu",
1562	        "test": "UD_Indonesian-GSD/r2.10/id_gsd-ud-test.conllu",
1563	    },
1564	    "id_pud": {
1565	        "test": "UD_Indonesian-PUD/r2.10/id_pud-ud-test.conllu",
1566	    },
1567	    "ga_twittirish": {
1568	        "test": "UD_Irish-TwittIrish/r2.10/ga_twittirish-ud-test.conllu",
1569	    },
1570	    "ga_idt": {
1571	        "train": "UD_Irish-IDT/r2.10/ga_idt-ud-train.conllu",
1572	        "dev": "UD_Irish-IDT/r2.10/ga_idt-ud-dev.conllu",
1573	        "test": "UD_Irish-IDT/r2.10/ga_idt-ud-test.conllu",
1574	    },
1575	    "it_valico": {
1576	        "test": "UD_Italian-Valico/r2.10/it_valico-ud-test.conllu",
1577	    },
1578	    "it_markit": {
1579	        "train": "UD_Italian-MarkIT/r2.10/it_markit-ud-train.conllu",
1580	        "dev": "UD_Italian-MarkIT/r2.10/it_markit-ud-dev.conllu",
1581	        "test": "UD_Italian-MarkIT/r2.10/it_markit-ud-test.conllu",
1582	    },
1583	    "it_isdt": {
1584	        "train": "UD_Italian-ISDT/r2.10/it_isdt-ud-train.conllu",
1585	        "dev": "UD_Italian-ISDT/r2.10/it_isdt-ud-dev.conllu",
1586	        "test": "UD_Italian-ISDT/r2.10/it_isdt-ud-test.conllu",
1587	    },
1588	    "it_partut": {
1589	        "train": "UD_Italian-ParTUT/r2.10/it_partut-ud-train.conllu",
1590	        "dev": "UD_Italian-ParTUT/r2.10/it_partut-ud-dev.conllu",
1591	        "test": "UD_Italian-ParTUT/r2.10/it_partut-ud-test.conllu",
1592	    },
1593	    "it_postwita": {
1594	        "train": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-train.conllu",
1595	        "dev": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-dev.conllu",
1596	        "test": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-test.conllu",
1597	    },
1598	    "it_pud": {
1599	        "test": "UD_Italian-PUD/r2.10/it_pud-ud-test.conllu",
1600	    },
1601	    "it_twittiro": {
1602	        "train": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-train.conllu",
1603	        "dev": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-dev.conllu",
1604	        "test": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-test.conllu",
1605	    },
1606	    "it_vit": {
1607	        "train": "UD_Italian-VIT/r2.10/it_vit-ud-train.conllu",
1608	        "dev": "UD_Italian-VIT/r2.10/it_vit-ud-dev.conllu",
1609	        "test": "UD_Italian-VIT/r2.10/it_vit-ud-test.conllu",
1610	    },
1611	    "ja_gsdluw": {
1612	        "train": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-train.conllu",
1613	        "dev": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-dev.conllu",
1614	        "test": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-test.conllu",
1615	    },
1616	    "ja_pudluw": {
1617	        "test": "UD_Japanese-PUDLUW/r2.10/ja_pudluw-ud-test.conllu",
1618	    },
1619	    "ja_bccwjluw": {
1620	        "train": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-train.conllu",
1621	        "dev": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-dev.conllu",
1622	        "test": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-test.conllu",
1623	    },
1624	    "ja_bccwj": {
1625	        "train": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-train.conllu",
1626	        "dev": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-dev.conllu",
1627	        "test": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-test.conllu",
1628	    },
1629	    "ja_gsd": {
1630	        "train": "UD_Japanese-GSD/r2.10/ja_gsd-ud-train.conllu",
1631	        "dev": "UD_Japanese-GSD/r2.10/ja_gsd-ud-dev.conllu",
1632	        "test": "UD_Japanese-GSD/r2.10/ja_gsd-ud-test.conllu",
1633	    },
1634	    "ja_modern": {
1635	        "test": "UD_Japanese-Modern/r2.10/ja_modern-ud-test.conllu",
1636	    },
1637	    "ja_pud": {
1638	        "test": "UD_Japanese-PUD/r2.10/ja_pud-ud-test.conllu",
1639	    },
1640	    "jv_csui": {
1641	        "test": "UD_Javanese-CSUI/r2.10/jv_csui-ud-test.conllu",
1642	    },
1643	    "urb_tudet": {
1644	        "test": "UD_Kaapor-TuDeT/r2.10/urb_tudet-ud-test.conllu",
1645	    },
1646	    "xnr_kdtb": {
1647	        "test": "UD_Kangri-KDTB/r2.10/xnr_kdtb-ud-test.conllu",
1648	    },
1649	    "krl_kkpp": {
1650	        "test": "UD_Karelian-KKPP/r2.10/krl_kkpp-ud-test.conllu",
1651	    },
1652	    "arr_tudet": {
1653	        "test": "UD_Karo-TuDeT/r2.10/arr_tudet-ud-test.conllu",
1654	    },
1655	    "kk_ktb": {
1656	        "train": "UD_Kazakh-KTB/r2.10/kk_ktb-ud-train.conllu",
1657	        "test": "UD_Kazakh-KTB/r2.10/kk_ktb-ud-test.conllu",
1658	    },
1659	    "kfm_aha": {
1660	        "test": "UD_Khunsari-AHA/r2.10/kfm_aha-ud-test.conllu",
1661	    },
1662	    "quc_iu": {
1663	        "test": "UD_Kiche-IU/r2.10/quc_iu-ud-test.conllu",
1664	    },
1665	    "koi_uh": {
1666	        "test": "UD_Komi_Permyak-UH/r2.10/koi_uh-ud-test.conllu",
1667	    },
1668	    "kpv_ikdp": {
1669	        "test": "UD_Komi_Zyrian-IKDP/r2.10/kpv_ikdp-ud-test.conllu",
1670	    },
1671	    "kpv_lattice": {
1672	        "test": "UD_Komi_Zyrian-Lattice/r2.10/kpv_lattice-ud-test.conllu",
1673	    },
1674	    "ko_gsd": {
1675	        "train": "UD_Korean-GSD/r2.10/ko_gsd-ud-train.conllu",
1676	        "dev": "UD_Korean-GSD/r2.10/ko_gsd-ud-dev.conllu",
1677	        "test": "UD_Korean-GSD/r2.10/ko_gsd-ud-test.conllu",
1678	    },
1679	    "ko_kaist": {
1680	        "train": "UD_Korean-Kaist/r2.10/ko_kaist-ud-train.conllu",
1681	        "dev": "UD_Korean-Kaist/r2.10/ko_kaist-ud-dev.conllu",
1682	        "test": "UD_Korean-Kaist/r2.10/ko_kaist-ud-test.conllu",
1683	    },
1684	    "ko_pud": {
1685	        "test": "UD_Korean-PUD/r2.10/ko_pud-ud-test.conllu",
1686	    },
1687	    "kmr_mg": {
1688	        "train": "UD_Kurmanji-MG/r2.10/kmr_mg-ud-train.conllu",
1689	        "test": "UD_Kurmanji-MG/r2.10/kmr_mg-ud-test.conllu",
1690	    },
1691	    "la_udante": {
1692	        "train": "UD_Latin-UDante/r2.10/la_udante-ud-train.conllu",
1693	        "dev": "UD_Latin-UDante/r2.10/la_udante-ud-dev.conllu",
1694	        "test": "UD_Latin-UDante/r2.10/la_udante-ud-test.conllu",
1695	    },
1696	    "la_ittb": {
1697	        "train": "UD_Latin-ITTB/r2.10/la_ittb-ud-train.conllu",
1698	        "dev": "UD_Latin-ITTB/r2.10/la_ittb-ud-dev.conllu",
1699	        "test": "UD_Latin-ITTB/r2.10/la_ittb-ud-test.conllu",
1700	    },
1701	    "la_llct": {
1702	        "train": "UD_Latin-LLCT/r2.10/la_llct-ud-train.conllu",
1703	        "dev": "UD_Latin-LLCT/r2.10/la_llct-ud-dev.conllu",
1704	        "test": "UD_Latin-LLCT/r2.10/la_llct-ud-test.conllu",
1705	    },
1706	    "la_perseus": {
1707	        "train": "UD_Latin-Perseus/r2.10/la_perseus-ud-train.conllu",
1708	        "test": "UD_Latin-Perseus/r2.10/la_perseus-ud-test.conllu",
1709	    },
1710	    "la_proiel": {
1711	        "train": "UD_Latin-PROIEL/r2.10/la_proiel-ud-train.conllu",
1712	        "dev": "UD_Latin-PROIEL/r2.10/la_proiel-ud-dev.conllu",
1713	        "test": "UD_Latin-PROIEL/r2.10/la_proiel-ud-test.conllu",
1714	    },
1715	    "lv_lvtb": {
1716	        "train": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-train.conllu",
1717	        "dev": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-dev.conllu",
1718	        "test": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-test.conllu",
1719	    },
1720	    "lij_glt": {
1721	        "train": "UD_Ligurian-GLT/r2.10/lij_glt-ud-train.conllu",
1722	        "test": "UD_Ligurian-GLT/r2.10/lij_glt-ud-test.conllu",
1723	    },
1724	    "lt_alksnis": {
1725	        "train": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-train.conllu",
1726	        "dev": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-dev.conllu",
1727	        "test": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-test.conllu",
1728	    },
1729	    "lt_hse": {
1730	        "train": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1731	        "dev": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1732	        "test": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1733	    },
1734	    "olo_kkpp": {
1735	        "train": "UD_Livvi-KKPP/r2.10/olo_kkpp-ud-train.conllu",
1736	        "test": "UD_Livvi-KKPP/r2.10/olo_kkpp-ud-test.conllu",
1737	    },
1738	    "nds_lsdc": {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:734
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
733	    "lt_hse": (
734	        "Lithuanian treebank annotated manually (dependencies) using the "
735	        "Morphological Annotator by CCL, Vytautas Magnus University "
736	        "(http://tekstynas.vdu.lt/) and manual disambiguation. A pilot version"
737	        " which includes news and an essay by Tomas Venclova is available "
738	        "here."
739	    ),
740	    "olo_kkpp": (
741	        "UD Livvi-KKPP is a manually annotated new corpus of Livvi-Karelian "
742	        "made directly in the Universal dependencies annotation scheme. The "
743	        "data is collected from VepKar corpora and consists of mostly modern "
744	        "news texts but also some stories and educational texts."
745	    ),
746	    "nds_lsdc": (
747	        "The UD Low Saxon LSDC dataset consists of sentences in 18 Low Saxon "
748	        "dialects from both Germany and the Netherlands. These sentences are "
749	        "(or are to become) part of the LSDC dataset and represent the "
750	        "language from the 19th and early 20th century in genres such as short"
751	        " stories, novels, speeches, letters and fairytales."
752	    ),
753	    "mt_mudt": (
754	        "MUDT (Maltese Universal Dependencies Treebank) is a manually "
755	        "annotated treebank of Maltese, a Semitic language of Malta descended "
756	        "from North African Arabic with a significant amount of Italo-Romance "
757	        "influence. MUDT was designed as a balanced corpus with four major "
758	        "genres (see Splitting below) represented roughly equally."
759	    ),
760	    "gv_cadhan": (
761	        "This is the Cadhan Aonair UD treebank for Manx Gaelic, created by "
762	        "Kevin Scannell."
763	    ),
764	    "mr_ufal": (
765	        "UD Marathi is a manually annotated treebank consisting primarily of "
766	        "stories from Wikisource, and parts of an article on Wikipedia."
767	    ),
768	    "gun_dooley": (
769	        "UD Mbya_Guarani-Dooley is a corpus of narratives written in Mby "
770	        "Guaran (Tupian) in Brazil, and collected by Robert Dooley. Due to "
771	        "copyright restrictions, the corpus that is distributed as part of UD "
772	        "only contains the annotation (tags, features, relations) while the "
773	        "FORM and LEMMA columns are empty."
774	    ),
775	    "gun_thomas": (
776	        "UD Mbya_Guarani-Thomas is a corpus of Mby Guaran (Tupian) texts "
777	        "collected by Guillaume Thomas. The current version of the corpus "
778	        "consists of three speeches by Paulina Kerechu Nez Romero, a Mby "
779	        "Guaran speaker from Ytu, Caazap Department, Paraguay."
780	    ),
781	    "mdf_jr": (
782	        "Erme Universal Dependencies annotated texts Moksha are the origin of "
783	        "UD_Moksha-JR with annotation (CoNLL-U) for texts in the Moksha "
784	        "language, it originally consists of a sample from a number of fiction"
785	        " authors writing originals in Moksha."
786	    ),
787	    "myu_tudet": (
788	        "UD_Munduruku-TuDeT is a collection of annotated sentences in "
789	        "Munduruk. Together with UD_Akuntsu-TuDeT and UD_Tupinamba-TuDeT, "
790	        "UD_Munduruku-TuDeT is part of the TuLaR project."
791	    ),
792	    "pcm_nsc": (
793	        "A Universal Dependencies corpus for spoken Naija (Nigerian Pidgin)."
794	    ),
795	    "nyq_aha": (
796	        "The AHA Nayini Treebank is a small treebank for contemporary Nayini. "
797	        "Its corpus is collected and annotated manually. We have prepared this"
798	        " treebank based on interviews with Nayini speakers."
799	    ),
800	    "sme_giella": (
801	        "This is a North Smi treebank based on a manually disambiguated and "
802	        "function-labelled gold-standard corpus of North Smi produced by the "
803	        "Giellatekno team at UiT Norgga rktala universitehta."
804	    ),
805	    "no_bokmaal": (
806	        "The Norwegian UD treebank is based on the Bokml section of the "
807	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
808	        " Norwegian. NDT has been automatically converted to the UD scheme by "
809	        "Lilja vrelid at the University of Oslo."
810	    ),
811	    "no_nynorsk": (
812	        "The Norwegian UD treebank is based on the Nynorsk section of the "
813	        "Norwegian Dependency Treebank (NDT), which is a syntactic treebank of"
814	        " Norwegian.  NDT has been automatically converted to the UD scheme by"
815	        " Lilja vrelid at the University of Oslo."
816	    ),
817	    "no_nynorsklia": (
818	        "This Norwegian treebank is based on the LIA treebank of transcribed "
819	        "spoken Norwegian dialects. The treebank has been automatically "
820	        "converted to the UD scheme by Lilja vrelid at the University of "
821	        "Oslo."
822	    ),
823	    "cu_proiel": (
824	        "The Old Church Slavonic (OCS) UD treebank is based on the Old Church "
825	        "Slavonic data from the PROIEL treebank and contains the text of the "
826	        "Codex Marianus New Testament translation."
827	    ),
828	    "fro_srcmf": (
829	        "UD_Old_French-SRCMF is a conversion of (part of) the SRCMF corpus "
830	        "(Syntactic Reference Corpus of Medieval French srcmf.org)."
831	    ),
832	    "orv_birchbark": (
833	        "UD Old_East_Slavic-Birchbark is based on the RNC Corpus of Birchbark "
834	        "Letters and includes documents written in 1025-1500 in an East Slavic"
835	        " vernacular (letters, household and business records, records for "
836	        "church services, spell against diseases, and other short "
837	        "inscriptions). The treebank is manually syntactically annotated in "
838	        "the UD 2.0 scheme, morphological and lexical annotation is a "
839	        "conversion of the original RNC annotation."
840	    ),
841	    "orv_rnc": (
842	        "`UD_Old_Russian-RNC` is a sample of the Middle Russian corpus "
843	        "(1300-1700), a part of the Russian National Corpus. The data were "
844	        "originally annotated according to the RNC and extended UD-Russian "
845	        "morphological schemas and UD 2.4 dependency schema."
846	    ),
847	    "orv_torot": (
848	        "UD_Old_Russian-TOROT is a conversion of a selection of the Old East "
849	        "Slavonic and Middle Russian data in the Troms Old Russian and OCS "
850	        "Treebank (TOROT), which was originally annotated in PROIEL dependency"
851	        " format."
852	    ),
853	    "otk_tonqq": (
854	        "`UD_Old_Turkish-Tonqq` is an Old Turkish treebank built upon Turkic "
855	        "script texts or sentences that are trivially convertible."
856	    ),
857	    "fa_perdt": (
858	        "The Persian Universal Dependency Treebank (PerUDT) is the result of "
859	        "automatic coversion of Persian Dependency Treebank (PerDT) with "
860	        "extensive manual corrections. Please refer to the follwoing work, if "
861	        "you use this data: Mohammad Sadegh Rasooli, Pegah Safari, Amirsaeid "
862	        "Moloodi, and Alireza Nourian. 'The Persian Dependency Treebank Made "
863	        "Universal'. 2020 (to appear)."
864	    ),
865	    "fa_seraji": (
866	        "The Persian Universal Dependency Treebank (Persian UD) is based on "
867	        "Uppsala Persian Dependency Treebank (UPDT). The conversion of the "
868	        "UPDT to the Universal Dependencies was performed semi-automatically "
869	        "with extensive manual checks and corrections."
870	    ),
871	    "pl_lfg": (
872	        "The LFG Enhanced UD treebank of Polish is based on a corpus of LFG "
873	        "(Lexical Functional Grammar) syntactic structures generated by an LFG"
874	        " grammar of Polish, POLFIE, and manually disambiguated by human "
875	        "annotators."
876	    ),
877	    "pl_pdb": (
878	        "The Polish PDB-UD treebank is based on the Polish Dependency Bank 2.0"
879	        " (PDB 2.0), created at the Institute of Computer Science, Polish "
880	        "Academy of Sciences in Warsaw. The PDB-UD treebank is an extended and"
881	        " corrected version of the Polish SZ-UD treebank (the release 1.2 to "
882	        "2.3)."
883	    ),
884	    "pl_pud": (
885	        "This is the Polish portion of the Parallel Universal Dependencies "
886	        "(PUD) treebanks, created at the Institute of Computer Science, Polish"
887	        " Academy of Sciences in Warsaw.Re"
888	    ),
889	    "pt_bosque": (
890	        "This Universal Dependencies (UD) Portuguese treebank is based on the "
891	        "Constraint Grammar converted version of the Bosque, which is part of "
892	        "the Floresta Sint(c)tica treebank. It contains both European "
893	        "(CETEMPblico) and Brazilian (CETENFolha) variants."
894	    ),
895	    "pt_gsd": (
896	        "The Brazilian Portuguese UD is converted from the Google Universal "
897	        "Dependency Treebank v2.0 (legacy)."
898	    ),
899	    "pt_pud": (
900	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
901	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
902	        "Raw Text to Universal Dependencies."
903	    ),
904	    "ro_art": (
905	        "The UD treebank ArT is a treebank of the Aromanian dialect of the "
906	        "Romanian language in UD format."
907	    ),
908	    "ro_nonstandard": (
909	        "The Romanian Non-standard UD treebank (called UAIC-RoDia) is based on"
910	        " UAIC-RoDia Treebank. UAIC-RoDia = ISLRN 156-635-615-024-0"
911	    ),
912	    "ro_rrt": (
913	        "The Romanian UD treebank (called RoRefTrees) (Barbu Mititelu et al., "
914	        "2016) is the reference treebank in UD format for standard Romanian."
915	    ),
916	    "ro_simonero": "SiMoNERo is a medical corpus of contemporary Romanian.",
917	    "ru_gsd": (
918	        "Russian Universal Dependencies Treebank annotated and converted by "
919	        "Google."
920	    ),
921	    "ru_pud": (
922	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
923	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
924	        "Raw Text to Universal Dependencies."
925	    ),
926	    "ru_syntagrus": "Russian data from the SynTagRus corpus.",
927	    "ru_taiga": (
928	        "Universal Dependencies treebank is based on data samples extracted "
929	        "from Taiga Corpus and MorphoRuEval-2017 and GramEval-2020 shared "
930	        "tasks collections."
931	    ),
932	    "sa_ufal": (
933	        "A small Sanskrit treebank of sentences from Pacatantra, an ancient "
934	        "Indian collection of interrelated fables by Vishnu Sharma."
935	    ),
936	    "sa_vedic": (
937	        "The Treebank of Vedic Sanskrit contains 4,000 sentences with 27,000 "
938	        "words chosen from metrical and prose passages of the gveda (RV), the"
939	        " aunaka recension of the Atharvaveda (S), the Maitryasahit "
940	        "(MS), and the Aitareya- (AB) and atapatha-Brhmaas (B). Lexical "
941	        "and morpho-syntactic information has been generated using a tagging "
942	        "software and manually validated. POS tags have been induced "
943	        "automatically from the morpho-sytactic information of each word."
944	    ),
945	    "gd_arcosg": (
946	        "A treebank of Scottish Gaelic based on the Annotated Reference Corpus"
947	        " Of Scottish Gaelic (ARCOSG)."
948	    ),
949	    "sr_set": (
950	        "The Serbian UD treebank is based on the "
951	        "[SETimes-SR](http://hdl.handle.net/11356/1200) corpus and additional "
952	        "news documents from the Serbian web."
953	    ),
954	    "sms_giellagas": (
955	        "The UD Skolt Sami Giellagas treebank is based almost entirely on "
956	        "spoken Skolt Sami corpora."
957	    ),
958	    "sk_snk": (
959	        "The Slovak UD treebank is based on data originally annotated as part "
960	        "of the Slovak National Corpus, following the annotation style of the "
961	        "Prague Dependency Treebank."
962	    ),
963	    "sl_ssj": (
964	        "The Slovenian UD Treebank is a rule-based conversion of the ssj500k "
965	        "treebank, the largest collection of manually syntactically annotated "
966	        "data in Slovenian, originally annotated in the JOS annotation scheme."
967	    ),
968	    "sl_sst": (
969	        "The Spoken Slovenian UD Treebank (SST) is the first syntactically "
970	        "annotated corpus of spoken Slovenian, based on a sample of the "
971	        "reference GOS corpus, a collection of transcribed audio recordings of"
972	        " monologic, dialogic and multi-party spontaneous speech in different "
973	        "everyday situations."
974	    ),
975	    "soj_aha": (
976	        "The AHA Soi Treebank is a small treebank for contemporary Soi. Its "
977	        "corpus is collected and annotated manually. We have prepared this "
978	        "treebank based on interviews with Soi speakers."
979	    ),
980	    "ajp_madar": (
981	        "The South_Levantine_Arabic-MADAR treebank consists of 100 "
982	        "manually-annotated sentences taken from the "
983	        "[MADAR](https://camel.abudhabi.nyu.edu/madar/) (Multi-Arabic Dialect "
984	        "Applications and Resources) project. "
985	    ),
986	    "es_ancora": "Spanish data from the AnCora corpus.",
987	    "es_gsd": (
988	        "The Spanish UD is converted from the content head version of the "
989	        "universal dependency treebank v2.0 (legacy)."
990	    ),
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {
1217	        "test": "UD_Arabic-PUD/r2.10/ar_pud-ud-test.conllu",
1218	    },
1219	    "hy_armtdp": {
1220	        "train": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-train.conllu",
1221	        "dev": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-dev.conllu",
1222	        "test": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-test.conllu",
1223	    },
1224	    "aii_as": {
1225	        "test": "UD_Assyrian-AS/r2.10/aii_as-ud-test.conllu",
1226	    },
1227	    "bm_crb": {
1228	        "test": "UD_Bambara-CRB/r2.10/bm_crb-ud-test.conllu",
1229	    },
1230	    "eu_bdt": {
1231	        "train": "UD_Basque-BDT/r2.10/eu_bdt-ud-train.conllu",
1232	        "dev": "UD_Basque-BDT/r2.10/eu_bdt-ud-dev.conllu",
1233	        "test": "UD_Basque-BDT/r2.10/eu_bdt-ud-test.conllu",
1234	    },
1235	    "bej_nsc": {
1236	        "test": "UD_Beja-NSC/r2.10/bej_nsc-ud-test.conllu",
1237	    },
1238	    "be_hse": {
1239	        "train": "UD_Belarusian-HSE/r2.10/be_hse-ud-train.conllu",
1240	        "dev": "UD_Belarusian-HSE/r2.10/be_hse-ud-dev.conllu",
1241	        "test": "UD_Belarusian-HSE/r2.10/be_hse-ud-test.conllu",
1242	    },
1243	    "bn_bru": {
1244	        "test": "UD_Bengali-BRU/r2.10/bn_bru-ud-test.conllu",
1245	    },
1246	    "bho_bhtb": {
1247	        "test": "UD_Bhojpuri-BHTB/r2.10/bho_bhtb-ud-test.conllu",
1248	    },
1249	    "br_keb": {
1250	        "test": "UD_Breton-KEB/r2.10/br_keb-ud-test.conllu",
1251	    },
1252	    "bg_btb": {
1253	        "train": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-train.conllu",
1254	        "dev": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-dev.conllu",
1255	        "test": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-test.conllu",
1256	    },
1257	    "bxr_bdt": {
1258	        "train": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-train.conllu",
1259	        "test": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-test.conllu",
1260	    },
1261	    "yue_hk": {
1262	        "test": "UD_Cantonese-HK/r2.10/yue_hk-ud-test.conllu",
1263	    },
1264	    "ca_ancora": {
1265	        "train": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-train.conllu",
1266	        "dev": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-dev.conllu",
1267	        "test": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-test.conllu",
1268	    },
1269	    "ceb_gja": {
1270	        "test": "UD_Cebuano-GJA/r2.10/ceb_gja-ud-test.conllu",
1271	    },
1272	    "zh_cfl": {
1273	        "test": "UD_Chinese-CFL/r2.10/zh_cfl-ud-test.conllu",
1274	    },
1275	    "zh_gsd": {
1276	        "train": "UD_Chinese-GSD/r2.10/zh_gsd-ud-train.conllu",
1277	        "dev": "UD_Chinese-GSD/r2.10/zh_gsd-ud-dev.conllu",
1278	        "test": "UD_Chinese-GSD/r2.10/zh_gsd-ud-test.conllu",
1279	    },
1280	    "zh_gsdsimp": {
1281	        "train": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-train.conllu",
1282	        "dev": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-dev.conllu",
1283	        "test": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-test.conllu",
1284	    },
1285	    "zh_hk": {
1286	        "test": "UD_Chinese-HK/r2.10/zh_hk-ud-test.conllu",
1287	    },
1288	    "zh_pud": {
1289	        "test": "UD_Chinese-PUD/r2.10/zh_pud-ud-test.conllu",
1290	    },
1291	    "ckt_hse": {
1292	        "test": "UD_Chukchi-HSE/r2.10/ckt_hse-ud-test.conllu",
1293	    },
1294	    "lzh_kyoto": {
1295	        "train": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-train.conllu",
1296	        "dev": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-dev.conllu",
1297	        "test": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-test.conllu",
1298	    },
1299	    "cop_scriptorium": {
1300	        "train": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-train.conllu",
1301	        "dev": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-dev.conllu",
1302	        "test": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-test.conllu",
1303	    },
1304	    "hr_set": {
1305	        "train": "UD_Croatian-SET/r2.10/hr_set-ud-train.conllu",
1306	        "dev": "UD_Croatian-SET/r2.10/hr_set-ud-dev.conllu",
1307	        "test": "UD_Croatian-SET/r2.10/hr_set-ud-test.conllu",
1308	    },
1309	    "cs_cac": {
1310	        "train": "UD_Czech-CAC/r2.10/cs_cac-ud-train.conllu",
1311	        "dev": "UD_Czech-CAC/r2.10/cs_cac-ud-dev.conllu",
1312	        "test": "UD_Czech-CAC/r2.10/cs_cac-ud-test.conllu",
1313	    },
1314	    "cs_cltt": {
1315	        "train": "UD_Czech-CLTT/r2.10/cs_cltt-ud-train.conllu",
1316	        "dev": "UD_Czech-CLTT/r2.10/cs_cltt-ud-dev.conllu",
1317	        "test": "UD_Czech-CLTT/r2.10/cs_cltt-ud-test.conllu",
1318	    },
1319	    "cs_fictree": {
1320	        "train": "UD_Czech-FicTree/r2.10/cs_fictree-ud-train.conllu",
1321	        "dev": "UD_Czech-FicTree/r2.10/cs_fictree-ud-dev.conllu",
1322	        "test": "UD_Czech-FicTree/r2.10/cs_fictree-ud-test.conllu",
1323	    },
1324	    "cs_pdt": {
1325	        "train": [
1326	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-l.conllu",
1327	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-m.conllu",
1328	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-c.conllu",
1329	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-v.conllu",
1330	        ],
1331	        "dev": "UD_Czech-PDT/r2.10/cs_pdt-ud-dev.conllu",
1332	        "test": "UD_Czech-PDT/r2.10/cs_pdt-ud-test.conllu",
1333	    },
1334	    "cs_pud": {
1335	        "test": "UD_Czech-PUD/r2.10/cs_pud-ud-test.conllu",
1336	    },
1337	    "da_ddt": {
1338	        "train": "UD_Danish-DDT/r2.10/da_ddt-ud-train.conllu",
1339	        "dev": "UD_Danish-DDT/r2.10/da_ddt-ud-dev.conllu",
1340	        "test": "UD_Danish-DDT/r2.10/da_ddt-ud-test.conllu",
1341	    },
1342	    "nl_alpino": {
1343	        "train": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-train.conllu",
1344	        "dev": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-dev.conllu",
1345	        "test": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-test.conllu",
1346	    },
1347	    "nl_lassysmall": {
1348	        "train": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-train.conllu",
1349	        "dev": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-dev.conllu",
1350	        "test": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-test.conllu",
1351	    },
1352	    "en_atis": {
1353	        "train": "UD_English-Atis/r2.10/en_atis-ud-train.conllu",
1354	        "dev": "UD_English-Atis/r2.10/en_atis-ud-dev.conllu",
1355	        "test": "UD_English-Atis/r2.10/en_atis-ud-test.conllu",
1356	    },
1357	    "en_esl": {
1358	        "train": "UD_English-ESL/r2.10/en_esl-ud-train.conllu",
1359	        "dev": "UD_English-ESL/r2.10/en_esl-ud-dev.conllu",
1360	        "test": "UD_English-ESL/r2.10/en_esl-ud-test.conllu",
1361	    },
1362	    "en_ewt": {
1363	        "train": "UD_English-EWT/r2.10/en_ewt-ud-train.conllu",
1364	        "dev": "UD_English-EWT/r2.10/en_ewt-ud-dev.conllu",
1365	        "test": "UD_English-EWT/r2.10/en_ewt-ud-test.conllu",
1366	    },
1367	    "en_gum": {
1368	        "train": "UD_English-GUM/r2.10/en_gum-ud-train.conllu",
1369	        "dev": "UD_English-GUM/r2.10/en_gum-ud-dev.conllu",
1370	        "test": "UD_English-GUM/r2.10/en_gum-ud-test.conllu",
1371	    },
1372	    "en_gumreddit": {
1373	        "train": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-train.conllu",
1374	        "dev": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-dev.conllu",
1375	        "test": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-test.conllu",
1376	    },
1377	    "en_lines": {
1378	        "train": "UD_English-LinES/r2.10/en_lines-ud-train.conllu",
1379	        "dev": "UD_English-LinES/r2.10/en_lines-ud-dev.conllu",
1380	        "test": "UD_English-LinES/r2.10/en_lines-ud-test.conllu",
1381	    },
1382	    "en_partut": {
1383	        "train": "UD_English-ParTUT/r2.10/en_partut-ud-train.conllu",
1384	        "dev": "UD_English-ParTUT/r2.10/en_partut-ud-dev.conllu",
1385	        "test": "UD_English-ParTUT/r2.10/en_partut-ud-test.conllu",
1386	    },
1387	    "en_pronouns": {
1388	        "test": "UD_English-Pronouns/r2.10/en_pronouns-ud-test.conllu",
1389	    },
1390	    "en_pud": {
1391	        "test": "UD_English-PUD/r2.10/en_pud-ud-test.conllu",
1392	    },
1393	    "myv_jr": {
1394	        "test": "UD_Erzya-JR/r2.10/myv_jr-ud-test.conllu",
1395	    },
1396	    "et_edt": {
1397	        "train": "UD_Estonian-EDT/r2.10/et_edt-ud-train.conllu",
1398	        "dev": "UD_Estonian-EDT/r2.10/et_edt-ud-dev.conllu",
1399	        "test": "UD_Estonian-EDT/r2.10/et_edt-ud-test.conllu",
1400	    },
1401	    "et_ewt": {
1402	        "train": "UD_Estonian-EWT/r2.10/et_ewt-ud-train.conllu",
1403	        "dev": "UD_Estonian-EWT/r2.10/et_ewt-ud-dev.conllu",
1404	        "test": "UD_Estonian-EWT/r2.10/et_ewt-ud-test.conllu",
1405	    },
1406	    "fo_farpahc": {
1407	        "train": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-train.conllu",
1408	        "dev": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-dev.conllu",
1409	        "test": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-test.conllu",
1410	    },
1411	    "fo_oft": {
1412	        "test": "UD_Faroese-OFT/r2.10/fo_oft-ud-test.conllu",
1413	    },
1414	    "fi_ftb": {
1415	        "train": "UD_Finnish-FTB/r2.10/fi_ftb-ud-train.conllu",
1416	        "dev": "UD_Finnish-FTB/r2.10/fi_ftb-ud-dev.conllu",
1417	        "test": "UD_Finnish-FTB/r2.10/fi_ftb-ud-test.conllu",
1418	    },
1419	    "fi_ood": {
1420	        "test": "UD_Finnish-OOD/r2.10/fi_ood-ud-test.conllu",
1421	    },
1422	    "fi_pud": {
1423	        "test": "UD_Finnish-PUD/r2.10/fi_pud-ud-test.conllu",
1424	    },
1425	    "fi_tdt": {
1426	        "train": "UD_Finnish-TDT/r2.10/fi_tdt-ud-train.conllu",
1427	        "dev": "UD_Finnish-TDT/r2.10/fi_tdt-ud-dev.conllu",
1428	        "test": "UD_Finnish-TDT/r2.10/fi_tdt-ud-test.conllu",
1429	    },
1430	    "fr_parisstories": {
1431	        "train": "UD_French-ParisStories/r2.10/fr_parisstories-ud-train.conllu",
1432	        "test": "UD_French-ParisStories/r2.10/fr_parisstories-ud-test.conllu",
1433	    },
1434	    "fr_fqb": {
1435	        "test": "UD_French-FQB/r2.10/fr_fqb-ud-test.conllu",
1436	    },
1437	    "fr_rhapsodie": {
1438	        "train": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-train.conllu",
1439	        "dev": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-dev.conllu",
1440	        "test": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-test.conllu",
1441	    },
1442	    "fr_ftb": {
1443	        "train": "UD_French-FTB/r2.10/fr_ftb-ud-train.conllu",
1444	        "dev": "UD_French-FTB/r2.10/fr_ftb-ud-dev.conllu",
1445	        "test": "UD_French-FTB/r2.10/fr_ftb-ud-test.conllu",
1446	    },
1447	    "fr_gsd": {
1448	        "train": "UD_French-GSD/r2.10/fr_gsd-ud-train.conllu",
1449	        "dev": "UD_French-GSD/r2.10/fr_gsd-ud-dev.conllu",
1450	        "test": "UD_French-GSD/r2.10/fr_gsd-ud-test.conllu",
1451	    },
1452	    "fr_partut": {
1453	        "train": "UD_French-ParTUT/r2.10/fr_partut-ud-train.conllu",
1454	        "dev": "UD_French-ParTUT/r2.10/fr_partut-ud-dev.conllu",
1455	        "test": "UD_French-ParTUT/r2.10/fr_partut-ud-test.conllu",
1456	    },
1457	    "fr_pud": {
1458	        "test": "UD_French-PUD/r2.10/fr_pud-ud-test.conllu",
1459	    },
1460	    "fr_sequoia": {
1461	        "train": "UD_French-Sequoia/r2.10/fr_sequoia-ud-train.conllu",
1462	        "dev": "UD_French-Sequoia/r2.10/fr_sequoia-ud-dev.conllu",
1463	        "test": "UD_French-Sequoia/r2.10/fr_sequoia-ud-test.conllu",
1464	    },
1465	    "qfn_fame": {
1466	        "test": "UD_Frisian_Dutch-Fame/r2.10/qfn_fame-ud-test.conllu",
1467	    },
1468	    "gl_ctg": {
1469	        "train": "UD_Galician-CTG/r2.10/gl_ctg-ud-train.conllu",
1470	        "dev": "UD_Galician-CTG/r2.10/gl_ctg-ud-dev.conllu",
1471	        "test": "UD_Galician-CTG/r2.10/gl_ctg-ud-test.conllu",
1472	    },
1473	    "gl_treegal": {
1474	        "train": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-train.conllu",
1475	        "test": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-test.conllu",
1476	    },
1477	    "de_gsd": {
1478	        "train": "UD_German-GSD/r2.10/de_gsd-ud-train.conllu",
1479	        "dev": "UD_German-GSD/r2.10/de_gsd-ud-dev.conllu",
1480	        "test": "UD_German-GSD/r2.10/de_gsd-ud-test.conllu",
1481	    },
1482	    "de_hdt": {
1483	        "train": [
1484	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-1.conllu",
1485	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-2.conllu",
1486	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-1.conllu",
1487	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-2.conllu",
1488	        ],
1489	        "dev": "UD_German-HDT/r2.10/de_hdt-ud-dev.conllu",
1490	        "test": "UD_German-HDT/r2.10/de_hdt-ud-test.conllu",
1491	    },
1492	    "de_lit": {
1493	        "test": "UD_German-LIT/r2.10/de_lit-ud-test.conllu",
1494	    },
1495	    "de_pud": {
1496	        "test": "UD_German-PUD/r2.10/de_pud-ud-test.conllu",
1497	    },
1498	    "got_proiel": {
1499	        "train": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-train.conllu",
1500	        "dev": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-dev.conllu",
1501	        "test": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-test.conllu",
1502	    },
1503	    "el_gdt": {
1504	        "train": "UD_Greek-GDT/r2.10/el_gdt-ud-train.conllu",
1505	        "dev": "UD_Greek-GDT/r2.10/el_gdt-ud-dev.conllu",
1506	        "test": "UD_Greek-GDT/r2.10/el_gdt-ud-test.conllu",
1507	    },
1508	    "gub_tudet": {
1509	        "test": "UD_Guajajara-TuDeT/r2.10/gub_tudet-ud-test.conllu",
1510	    },
1511	    "gn_oldtudet": {
1512	        "test": "UD_Guarani-OldTuDeT/r2.10/gn_oldtudet-ud-test.conllu",
1513	    },
1514	    "he_iahltwiki": {
1515	        "train": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-train.conllu",
1516	        "dev": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-dev.conllu",
1517	        "test": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-test.conllu",
1518	    },
1519	    "he_htb": {
1520	        "train": "UD_Hebrew-HTB/r2.10/he_htb-ud-train.conllu",
1521	        "dev": "UD_Hebrew-HTB/r2.10/he_htb-ud-dev.conllu",
1522	        "test": "UD_Hebrew-HTB/r2.10/he_htb-ud-test.conllu",
1523	    },
1524	    "qhe_hiencs": {
1525	        "train": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-train.conllu",
1526	        "dev": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-dev.conllu",
1527	        "test": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-test.conllu",
1528	    },
1529	    "hi_hdtb": {
1530	        "train": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-train.conllu",
1531	        "dev": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-dev.conllu",
1532	        "test": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-test.conllu",
1533	    },
1534	    "hi_pud": {
1535	        "test": "UD_Hindi-PUD/r2.10/hi_pud-ud-test.conllu",
1536	    },
1537	    "hu_szeged": {
1538	        "train": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-train.conllu",
1539	        "dev": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-dev.conllu",
1540	        "test": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-test.conllu",
1541	    },
1542	    "is_modern": {
1543	        "train": "UD_Icelandic-Modern/r2.10/is_modern-ud-train.conllu",
1544	        "dev": "UD_Icelandic-Modern/r2.10/is_modern-ud-dev.conllu",
1545	        "test": "UD_Icelandic-Modern/r2.10/is_modern-ud-test.conllu",
1546	    },
1547	    "is_icepahc": {
1548	        "train": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-train.conllu",
1549	        "dev": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-dev.conllu",
1550	        "test": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-test.conllu",
1551	    },
1552	    "is_pud": {
1553	        "test": "UD_Icelandic-PUD/r2.10/is_pud-ud-test.conllu",
1554	    },
1555	    "id_csui": {
1556	        "train": "UD_Indonesian-CSUI/r2.10/id_csui-ud-train.conllu",
1557	        "test": "UD_Indonesian-CSUI/r2.10/id_csui-ud-test.conllu",
1558	    },
1559	    "id_gsd": {
1560	        "train": "UD_Indonesian-GSD/r2.10/id_gsd-ud-train.conllu",
1561	        "dev": "UD_Indonesian-GSD/r2.10/id_gsd-ud-dev.conllu",
1562	        "test": "UD_Indonesian-GSD/r2.10/id_gsd-ud-test.conllu",
1563	    },
1564	    "id_pud": {
1565	        "test": "UD_Indonesian-PUD/r2.10/id_pud-ud-test.conllu",
1566	    },
1567	    "ga_twittirish": {
1568	        "test": "UD_Irish-TwittIrish/r2.10/ga_twittirish-ud-test.conllu",
1569	    },
1570	    "ga_idt": {
1571	        "train": "UD_Irish-IDT/r2.10/ga_idt-ud-train.conllu",
1572	        "dev": "UD_Irish-IDT/r2.10/ga_idt-ud-dev.conllu",
1573	        "test": "UD_Irish-IDT/r2.10/ga_idt-ud-test.conllu",
1574	    },
1575	    "it_valico": {
1576	        "test": "UD_Italian-Valico/r2.10/it_valico-ud-test.conllu",
1577	    },
1578	    "it_markit": {
1579	        "train": "UD_Italian-MarkIT/r2.10/it_markit-ud-train.conllu",
1580	        "dev": "UD_Italian-MarkIT/r2.10/it_markit-ud-dev.conllu",
1581	        "test": "UD_Italian-MarkIT/r2.10/it_markit-ud-test.conllu",
1582	    },
1583	    "it_isdt": {
1584	        "train": "UD_Italian-ISDT/r2.10/it_isdt-ud-train.conllu",
1585	        "dev": "UD_Italian-ISDT/r2.10/it_isdt-ud-dev.conllu",
1586	        "test": "UD_Italian-ISDT/r2.10/it_isdt-ud-test.conllu",
1587	    },
1588	    "it_partut": {
1589	        "train": "UD_Italian-ParTUT/r2.10/it_partut-ud-train.conllu",
1590	        "dev": "UD_Italian-ParTUT/r2.10/it_partut-ud-dev.conllu",
1591	        "test": "UD_Italian-ParTUT/r2.10/it_partut-ud-test.conllu",
1592	    },
1593	    "it_postwita": {
1594	        "train": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-train.conllu",
1595	        "dev": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-dev.conllu",
1596	        "test": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-test.conllu",
1597	    },
1598	    "it_pud": {
1599	        "test": "UD_Italian-PUD/r2.10/it_pud-ud-test.conllu",
1600	    },
1601	    "it_twittiro": {
1602	        "train": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-train.conllu",
1603	        "dev": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-dev.conllu",
1604	        "test": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-test.conllu",
1605	    },
1606	    "it_vit": {
1607	        "train": "UD_Italian-VIT/r2.10/it_vit-ud-train.conllu",
1608	        "dev": "UD_Italian-VIT/r2.10/it_vit-ud-dev.conllu",
1609	        "test": "UD_Italian-VIT/r2.10/it_vit-ud-test.conllu",
1610	    },
1611	    "ja_gsdluw": {
1612	        "train": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-train.conllu",
1613	        "dev": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-dev.conllu",
1614	        "test": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-test.conllu",
1615	    },
1616	    "ja_pudluw": {
1617	        "test": "UD_Japanese-PUDLUW/r2.10/ja_pudluw-ud-test.conllu",
1618	    },
1619	    "ja_bccwjluw": {
1620	        "train": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-train.conllu",
1621	        "dev": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-dev.conllu",
1622	        "test": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-test.conllu",
1623	    },
1624	    "ja_bccwj": {
1625	        "train": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-train.conllu",
1626	        "dev": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-dev.conllu",
1627	        "test": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-test.conllu",
1628	    },
1629	    "ja_gsd": {
1630	        "train": "UD_Japanese-GSD/r2.10/ja_gsd-ud-train.conllu",
1631	        "dev": "UD_Japanese-GSD/r2.10/ja_gsd-ud-dev.conllu",
1632	        "test": "UD_Japanese-GSD/r2.10/ja_gsd-ud-test.conllu",
1633	    },
1634	    "ja_modern": {
1635	        "test": "UD_Japanese-Modern/r2.10/ja_modern-ud-test.conllu",
1636	    },
1637	    "ja_pud": {
1638	        "test": "UD_Japanese-PUD/r2.10/ja_pud-ud-test.conllu",
1639	    },
1640	    "jv_csui": {
1641	        "test": "UD_Javanese-CSUI/r2.10/jv_csui-ud-test.conllu",
1642	    },
1643	    "urb_tudet": {
1644	        "test": "UD_Kaapor-TuDeT/r2.10/urb_tudet-ud-test.conllu",
1645	    },
1646	    "xnr_kdtb": {
1647	        "test": "UD_Kangri-KDTB/r2.10/xnr_kdtb-ud-test.conllu",
1648	    },
1649	    "krl_kkpp": {
1650	        "test": "UD_Karelian-KKPP/r2.10/krl_kkpp-ud-test.conllu",
1651	    },
1652	    "arr_tudet": {
1653	        "test": "UD_Karo-TuDeT/r2.10/arr_tudet-ud-test.conllu",
1654	    },
1655	    "kk_ktb": {
1656	        "train": "UD_Kazakh-KTB/r2.10/kk_ktb-ud-train.conllu",
1657	        "test": "UD_Kazakh-KTB/r2.10/kk_ktb-ud-test.conllu",
1658	    },
1659	    "kfm_aha": {
1660	        "test": "UD_Khunsari-AHA/r2.10/kfm_aha-ud-test.conllu",
1661	    },
1662	    "quc_iu": {
1663	        "test": "UD_Kiche-IU/r2.10/quc_iu-ud-test.conllu",
1664	    },
1665	    "koi_uh": {
1666	        "test": "UD_Komi_Permyak-UH/r2.10/koi_uh-ud-test.conllu",
1667	    },
1668	    "kpv_ikdp": {
1669	        "test": "UD_Komi_Zyrian-IKDP/r2.10/kpv_ikdp-ud-test.conllu",
1670	    },
1671	    "kpv_lattice": {
1672	        "test": "UD_Komi_Zyrian-Lattice/r2.10/kpv_lattice-ud-test.conllu",
1673	    },
1674	    "ko_gsd": {
1675	        "train": "UD_Korean-GSD/r2.10/ko_gsd-ud-train.conllu",
1676	        "dev": "UD_Korean-GSD/r2.10/ko_gsd-ud-dev.conllu",
1677	        "test": "UD_Korean-GSD/r2.10/ko_gsd-ud-test.conllu",
1678	    },
1679	    "ko_kaist": {
1680	        "train": "UD_Korean-Kaist/r2.10/ko_kaist-ud-train.conllu",
1681	        "dev": "UD_Korean-Kaist/r2.10/ko_kaist-ud-dev.conllu",
1682	        "test": "UD_Korean-Kaist/r2.10/ko_kaist-ud-test.conllu",
1683	    },
1684	    "ko_pud": {
1685	        "test": "UD_Korean-PUD/r2.10/ko_pud-ud-test.conllu",
1686	    },
1687	    "kmr_mg": {
1688	        "train": "UD_Kurmanji-MG/r2.10/kmr_mg-ud-train.conllu",
1689	        "test": "UD_Kurmanji-MG/r2.10/kmr_mg-ud-test.conllu",
1690	    },
1691	    "la_udante": {
1692	        "train": "UD_Latin-UDante/r2.10/la_udante-ud-train.conllu",
1693	        "dev": "UD_Latin-UDante/r2.10/la_udante-ud-dev.conllu",
1694	        "test": "UD_Latin-UDante/r2.10/la_udante-ud-test.conllu",
1695	    },
1696	    "la_ittb": {
1697	        "train": "UD_Latin-ITTB/r2.10/la_ittb-ud-train.conllu",
1698	        "dev": "UD_Latin-ITTB/r2.10/la_ittb-ud-dev.conllu",
1699	        "test": "UD_Latin-ITTB/r2.10/la_ittb-ud-test.conllu",
1700	    },
1701	    "la_llct": {
1702	        "train": "UD_Latin-LLCT/r2.10/la_llct-ud-train.conllu",
1703	        "dev": "UD_Latin-LLCT/r2.10/la_llct-ud-dev.conllu",
1704	        "test": "UD_Latin-LLCT/r2.10/la_llct-ud-test.conllu",
1705	    },
1706	    "la_perseus": {
1707	        "train": "UD_Latin-Perseus/r2.10/la_perseus-ud-train.conllu",
1708	        "test": "UD_Latin-Perseus/r2.10/la_perseus-ud-test.conllu",
1709	    },
1710	    "la_proiel": {
1711	        "train": "UD_Latin-PROIEL/r2.10/la_proiel-ud-train.conllu",
1712	        "dev": "UD_Latin-PROIEL/r2.10/la_proiel-ud-dev.conllu",
1713	        "test": "UD_Latin-PROIEL/r2.10/la_proiel-ud-test.conllu",
1714	    },
1715	    "lv_lvtb": {
1716	        "train": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-train.conllu",
1717	        "dev": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-dev.conllu",
1718	        "test": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-test.conllu",
1719	    },
1720	    "lij_glt": {
1721	        "train": "UD_Ligurian-GLT/r2.10/lij_glt-ud-train.conllu",
1722	        "test": "UD_Ligurian-GLT/r2.10/lij_glt-ud-test.conllu",
1723	    },
1724	    "lt_alksnis": {
1725	        "train": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-train.conllu",
1726	        "dev": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-dev.conllu",
1727	        "test": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-test.conllu",
1728	    },
1729	    "lt_hse": {
1730	        "train": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1731	        "dev": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1732	        "test": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1733	    },
1734	    "olo_kkpp": {
1735	        "train": "UD_Livvi-KKPP/r2.10/olo_kkpp-ud-train.conllu",
1736	        "test": "UD_Livvi-KKPP/r2.10/olo_kkpp-ud-test.conllu",
1737	    },
1738	    "nds_lsdc": {
1739	        "test": "UD_Low_Saxon-LSDC/r2.10/nds_lsdc-ud-test.conllu",
1740	    },
1741	    # TODO(tfds) Add Madi Jarawara splits when it will be officially released.
1742	    # TODO(tfds) Add Makurap TuDeT splits when it will be officially released.
1743	    "mt_mudt": {
1744	        "train": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-train.conllu",
1745	        "dev": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-dev.conllu",
1746	        "test": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-test.conllu",
1747	    },
1748	    "gv_cadhan": {
1749	        "test": "UD_Manx-Cadhan/r2.10/gv_cadhan-ud-test.conllu",
1750	    },
1751	    "mr_ufal": {
1752	        "train": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-train.conllu",
1753	        "dev": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-dev.conllu",
1754	        "test": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-test.conllu",
1755	    },
1756	    "gun_dooley": {
1757	        "test": "UD_Mbya_Guarani-Dooley/r2.10/gun_dooley-ud-test.conllu",
1758	    },
1759	    "gun_thomas": {
1760	        "test": "UD_Mbya_Guarani-Thomas/r2.10/gun_thomas-ud-test.conllu",
1761	    },
1762	    "mdf_jr": {
1763	        "test": "UD_Moksha-JR/r2.10/mdf_jr-ud-test.conllu",
1764	    },
1765	    "myu_tudet": {
1766	        "test": "UD_Munduruku-TuDeT/r2.10/myu_tudet-ud-test.conllu",
1767	    },
1768	    "pcm_nsc": {
1769	        "train": "UD_Naija-NSC/r2.10/pcm_nsc-ud-train.conllu",
1770	        "dev": "UD_Naija-NSC/r2.10/pcm_nsc-ud-dev.conllu",
1771	        "test": "UD_Naija-NSC/r2.10/pcm_nsc-ud-test.conllu",
1772	    },
1773	    "nyq_aha": {
1774	        "test": "UD_Nayini-AHA/r2.10/nyq_aha-ud-test.conllu",
1775	    },
1776	    # TODO(tfds) Add Neapolitan RB splits when it will be officially released.
1777	    "sme_giella": {
1778	        "train": "UD_North_Sami-Giella/r2.10/sme_giella-ud-train.conllu",
1779	        "test": "UD_North_Sami-Giella/r2.10/sme_giella-ud-test.conllu",
1780	    },
1781	    "no_bokmaal": {
1782	        "train": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-train.conllu",
1783	        "dev": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-dev.conllu",
1784	        "test": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-test.conllu",
1785	    },
1786	    "no_nynorsk": {
1787	        "train": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-train.conllu",
1788	        "dev": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-dev.conllu",
1789	        "test": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-test.conllu",
1790	    },
1791	    "no_nynorsklia": {
1792	        "train": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-train.conllu",
1793	        "dev": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-dev.conllu",
1794	        "test": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-test.conllu",
1795	    },
1796	    "cu_proiel": {
1797	        "train": (
1798	            "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-train.conllu"
1799	        ),
1800	        "dev": "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-dev.conllu",
1801	        "test": "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-test.conllu",
1802	    },
1803	    "fro_srcmf": {
1804	        "train": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-train.conllu",
1805	        "dev": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-dev.conllu",
1806	        "test": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-test.conllu",
1807	    },
1808	    "orv_birchbark": {
1809	        "train": (
1810	            "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-train.conllu"
1811	        ),
1812	        "dev": "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-dev.conllu",
1813	        "test": (
1814	            "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-test.conllu"
1815	        ),
1816	    },
1817	    "orv_rnc": {
1818	        "train": "UD_Old_Russian-RNC/r2.10/orv_rnc-ud-train.conllu",
1819	        "test": "UD_Old_Russian-RNC/r2.10/orv_rnc-ud-test.conllu",
1820	    },
1821	    "orv_torot": {
1822	        "train": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-train.conllu",
1823	        "dev": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-dev.conllu",
1824	        "test": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-test.conllu",
1825	    },
1826	    "otk_tonqq": {
1827	        "test": "UD_Old_Turkish-Tonqq/r2.10/otk_tonqq-ud-test.conllu",
1828	    },
1829	    "fa_perdt": {
1830	        "train": "UD_Persian-PerDT/r2.10/fa_perdt-ud-train.conllu",
1831	        "dev": "UD_Persian-PerDT/r2.10/fa_perdt-ud-dev.conllu",
1832	        "test": "UD_Persian-PerDT/r2.10/fa_perdt-ud-test.conllu",
1833	    },
1834	    "fa_seraji": {
1835	        "train": "UD_Persian-Seraji/r2.10/fa_seraji-ud-train.conllu",
1836	        "dev": "UD_Persian-Seraji/r2.10/fa_seraji-ud-dev.conllu",
1837	        "test": "UD_Persian-Seraji/r2.10/fa_seraji-ud-test.conllu",
1838	    },
1839	    "pl_lfg": {
1840	        "train": "UD_Polish-LFG/r2.10/pl_lfg-ud-train.conllu",
1841	        "dev": "UD_Polish-LFG/r2.10/pl_lfg-ud-dev.conllu",
1842	        "test": "UD_Polish-LFG/r2.10/pl_lfg-ud-test.conllu",
1843	    },
1844	    "pl_pdb": {
1845	        "train": "UD_Polish-PDB/r2.10/pl_pdb-ud-train.conllu",
1846	        "dev": "UD_Polish-PDB/r2.10/pl_pdb-ud-dev.conllu",
1847	        "test": "UD_Polish-PDB/r2.10/pl_pdb-ud-test.conllu",
1848	    },
1849	    "pl_pud": {
1850	        "test": "UD_Polish-PUD/r2.10/pl_pud-ud-test.conllu",
1851	    },
1852	    "pt_bosque": {
1853	        "train": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-train.conllu",
1854	        "dev": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-dev.conllu",
1855	        "test": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-test.conllu",
1856	    },
1857	    "pt_gsd": {
1858	        "train": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-train.conllu",
1859	        "dev": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-dev.conllu",
1860	        "test": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-test.conllu",
1861	    },
1862	    "pt_pud": {
1863	        "test": "UD_Portuguese-PUD/r2.10/pt_pud-ud-test.conllu",
1864	    },
1865	    "ro_art": {
1866	        "test": "UD_Romanian-ArT/r2.10/ro_art-ud-test.conllu",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:950
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
949	    "sr_set": (
950	        "The Serbian UD treebank is based on the "
951	        "[SETimes-SR](http://hdl.handle.net/11356/1200) corpus and additional "
952	        "news documents from the Serbian web."
953	    ),
954	    "sms_giellagas": (
955	        "The UD Skolt Sami Giellagas treebank is based almost entirely on "
956	        "spoken Skolt Sami corpora."
957	    ),
958	    "sk_snk": (
959	        "The Slovak UD treebank is based on data originally annotated as part "
960	        "of the Slovak National Corpus, following the annotation style of the "
961	        "Prague Dependency Treebank."
962	    ),
963	    "sl_ssj": (
964	        "The Slovenian UD Treebank is a rule-based conversion of the ssj500k "
965	        "treebank, the largest collection of manually syntactically annotated "
966	        "data in Slovenian, originally annotated in the JOS annotation scheme."
967	    ),
968	    "sl_sst": (
969	        "The Spoken Slovenian UD Treebank (SST) is the first syntactically "
970	        "annotated corpus of spoken Slovenian, based on a sample of the "
971	        "reference GOS corpus, a collection of transcribed audio recordings of"
972	        " monologic, dialogic and multi-party spontaneous speech in different "
973	        "everyday situations."
974	    ),
975	    "soj_aha": (
976	        "The AHA Soi Treebank is a small treebank for contemporary Soi. Its "
977	        "corpus is collected and annotated manually. We have prepared this "
978	        "treebank based on interviews with Soi speakers."
979	    ),
980	    "ajp_madar": (
981	        "The South_Levantine_Arabic-MADAR treebank consists of 100 "
982	        "manually-annotated sentences taken from the "
983	        "[MADAR](https://camel.abudhabi.nyu.edu/madar/) (Multi-Arabic Dialect "
984	        "Applications and Resources) project. "
985	    ),
986	    "es_ancora": "Spanish data from the AnCora corpus.",
987	    "es_gsd": (
988	        "The Spanish UD is converted from the content head version of the "
989	        "universal dependency treebank v2.0 (legacy)."
990	    ),
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {
1217	        "test": "UD_Arabic-PUD/r2.10/ar_pud-ud-test.conllu",
1218	    },
1219	    "hy_armtdp": {
1220	        "train": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-train.conllu",
1221	        "dev": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-dev.conllu",
1222	        "test": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-test.conllu",
1223	    },
1224	    "aii_as": {
1225	        "test": "UD_Assyrian-AS/r2.10/aii_as-ud-test.conllu",
1226	    },
1227	    "bm_crb": {
1228	        "test": "UD_Bambara-CRB/r2.10/bm_crb-ud-test.conllu",
1229	    },
1230	    "eu_bdt": {
1231	        "train": "UD_Basque-BDT/r2.10/eu_bdt-ud-train.conllu",
1232	        "dev": "UD_Basque-BDT/r2.10/eu_bdt-ud-dev.conllu",
1233	        "test": "UD_Basque-BDT/r2.10/eu_bdt-ud-test.conllu",
1234	    },
1235	    "bej_nsc": {
1236	        "test": "UD_Beja-NSC/r2.10/bej_nsc-ud-test.conllu",
1237	    },
1238	    "be_hse": {
1239	        "train": "UD_Belarusian-HSE/r2.10/be_hse-ud-train.conllu",
1240	        "dev": "UD_Belarusian-HSE/r2.10/be_hse-ud-dev.conllu",
1241	        "test": "UD_Belarusian-HSE/r2.10/be_hse-ud-test.conllu",
1242	    },
1243	    "bn_bru": {
1244	        "test": "UD_Bengali-BRU/r2.10/bn_bru-ud-test.conllu",
1245	    },
1246	    "bho_bhtb": {
1247	        "test": "UD_Bhojpuri-BHTB/r2.10/bho_bhtb-ud-test.conllu",
1248	    },
1249	    "br_keb": {
1250	        "test": "UD_Breton-KEB/r2.10/br_keb-ud-test.conllu",
1251	    },
1252	    "bg_btb": {
1253	        "train": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-train.conllu",
1254	        "dev": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-dev.conllu",
1255	        "test": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-test.conllu",
1256	    },
1257	    "bxr_bdt": {
1258	        "train": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-train.conllu",
1259	        "test": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-test.conllu",
1260	    },
1261	    "yue_hk": {
1262	        "test": "UD_Cantonese-HK/r2.10/yue_hk-ud-test.conllu",
1263	    },
1264	    "ca_ancora": {
1265	        "train": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-train.conllu",
1266	        "dev": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-dev.conllu",
1267	        "test": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-test.conllu",
1268	    },
1269	    "ceb_gja": {
1270	        "test": "UD_Cebuano-GJA/r2.10/ceb_gja-ud-test.conllu",
1271	    },
1272	    "zh_cfl": {
1273	        "test": "UD_Chinese-CFL/r2.10/zh_cfl-ud-test.conllu",
1274	    },
1275	    "zh_gsd": {
1276	        "train": "UD_Chinese-GSD/r2.10/zh_gsd-ud-train.conllu",
1277	        "dev": "UD_Chinese-GSD/r2.10/zh_gsd-ud-dev.conllu",
1278	        "test": "UD_Chinese-GSD/r2.10/zh_gsd-ud-test.conllu",
1279	    },
1280	    "zh_gsdsimp": {
1281	        "train": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-train.conllu",
1282	        "dev": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-dev.conllu",
1283	        "test": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-test.conllu",
1284	    },
1285	    "zh_hk": {
1286	        "test": "UD_Chinese-HK/r2.10/zh_hk-ud-test.conllu",
1287	    },
1288	    "zh_pud": {
1289	        "test": "UD_Chinese-PUD/r2.10/zh_pud-ud-test.conllu",
1290	    },
1291	    "ckt_hse": {
1292	        "test": "UD_Chukchi-HSE/r2.10/ckt_hse-ud-test.conllu",
1293	    },
1294	    "lzh_kyoto": {
1295	        "train": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-train.conllu",
1296	        "dev": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-dev.conllu",
1297	        "test": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-test.conllu",
1298	    },
1299	    "cop_scriptorium": {
1300	        "train": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-train.conllu",
1301	        "dev": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-dev.conllu",
1302	        "test": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-test.conllu",
1303	    },
1304	    "hr_set": {
1305	        "train": "UD_Croatian-SET/r2.10/hr_set-ud-train.conllu",
1306	        "dev": "UD_Croatian-SET/r2.10/hr_set-ud-dev.conllu",
1307	        "test": "UD_Croatian-SET/r2.10/hr_set-ud-test.conllu",
1308	    },
1309	    "cs_cac": {
1310	        "train": "UD_Czech-CAC/r2.10/cs_cac-ud-train.conllu",
1311	        "dev": "UD_Czech-CAC/r2.10/cs_cac-ud-dev.conllu",
1312	        "test": "UD_Czech-CAC/r2.10/cs_cac-ud-test.conllu",
1313	    },
1314	    "cs_cltt": {
1315	        "train": "UD_Czech-CLTT/r2.10/cs_cltt-ud-train.conllu",
1316	        "dev": "UD_Czech-CLTT/r2.10/cs_cltt-ud-dev.conllu",
1317	        "test": "UD_Czech-CLTT/r2.10/cs_cltt-ud-test.conllu",
1318	    },
1319	    "cs_fictree": {
1320	        "train": "UD_Czech-FicTree/r2.10/cs_fictree-ud-train.conllu",
1321	        "dev": "UD_Czech-FicTree/r2.10/cs_fictree-ud-dev.conllu",
1322	        "test": "UD_Czech-FicTree/r2.10/cs_fictree-ud-test.conllu",
1323	    },
1324	    "cs_pdt": {
1325	        "train": [
1326	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-l.conllu",
1327	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-m.conllu",
1328	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-c.conllu",
1329	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-v.conllu",
1330	        ],
1331	        "dev": "UD_Czech-PDT/r2.10/cs_pdt-ud-dev.conllu",
1332	        "test": "UD_Czech-PDT/r2.10/cs_pdt-ud-test.conllu",
1333	    },
1334	    "cs_pud": {
1335	        "test": "UD_Czech-PUD/r2.10/cs_pud-ud-test.conllu",
1336	    },
1337	    "da_ddt": {
1338	        "train": "UD_Danish-DDT/r2.10/da_ddt-ud-train.conllu",
1339	        "dev": "UD_Danish-DDT/r2.10/da_ddt-ud-dev.conllu",
1340	        "test": "UD_Danish-DDT/r2.10/da_ddt-ud-test.conllu",
1341	    },
1342	    "nl_alpino": {
1343	        "train": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-train.conllu",
1344	        "dev": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-dev.conllu",
1345	        "test": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-test.conllu",
1346	    },
1347	    "nl_lassysmall": {
1348	        "train": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-train.conllu",
1349	        "dev": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-dev.conllu",
1350	        "test": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-test.conllu",
1351	    },
1352	    "en_atis": {
1353	        "train": "UD_English-Atis/r2.10/en_atis-ud-train.conllu",
1354	        "dev": "UD_English-Atis/r2.10/en_atis-ud-dev.conllu",
1355	        "test": "UD_English-Atis/r2.10/en_atis-ud-test.conllu",
1356	    },
1357	    "en_esl": {
1358	        "train": "UD_English-ESL/r2.10/en_esl-ud-train.conllu",
1359	        "dev": "UD_English-ESL/r2.10/en_esl-ud-dev.conllu",
1360	        "test": "UD_English-ESL/r2.10/en_esl-ud-test.conllu",
1361	    },
1362	    "en_ewt": {
1363	        "train": "UD_English-EWT/r2.10/en_ewt-ud-train.conllu",
1364	        "dev": "UD_English-EWT/r2.10/en_ewt-ud-dev.conllu",
1365	        "test": "UD_English-EWT/r2.10/en_ewt-ud-test.conllu",
1366	    },
1367	    "en_gum": {
1368	        "train": "UD_English-GUM/r2.10/en_gum-ud-train.conllu",
1369	        "dev": "UD_English-GUM/r2.10/en_gum-ud-dev.conllu",
1370	        "test": "UD_English-GUM/r2.10/en_gum-ud-test.conllu",
1371	    },
1372	    "en_gumreddit": {
1373	        "train": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-train.conllu",
1374	        "dev": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-dev.conllu",
1375	        "test": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-test.conllu",
1376	    },
1377	    "en_lines": {
1378	        "train": "UD_English-LinES/r2.10/en_lines-ud-train.conllu",
1379	        "dev": "UD_English-LinES/r2.10/en_lines-ud-dev.conllu",
1380	        "test": "UD_English-LinES/r2.10/en_lines-ud-test.conllu",
1381	    },
1382	    "en_partut": {
1383	        "train": "UD_English-ParTUT/r2.10/en_partut-ud-train.conllu",
1384	        "dev": "UD_English-ParTUT/r2.10/en_partut-ud-dev.conllu",
1385	        "test": "UD_English-ParTUT/r2.10/en_partut-ud-test.conllu",
1386	    },
1387	    "en_pronouns": {
1388	        "test": "UD_English-Pronouns/r2.10/en_pronouns-ud-test.conllu",
1389	    },
1390	    "en_pud": {
1391	        "test": "UD_English-PUD/r2.10/en_pud-ud-test.conllu",
1392	    },
1393	    "myv_jr": {
1394	        "test": "UD_Erzya-JR/r2.10/myv_jr-ud-test.conllu",
1395	    },
1396	    "et_edt": {
1397	        "train": "UD_Estonian-EDT/r2.10/et_edt-ud-train.conllu",
1398	        "dev": "UD_Estonian-EDT/r2.10/et_edt-ud-dev.conllu",
1399	        "test": "UD_Estonian-EDT/r2.10/et_edt-ud-test.conllu",
1400	    },
1401	    "et_ewt": {
1402	        "train": "UD_Estonian-EWT/r2.10/et_ewt-ud-train.conllu",
1403	        "dev": "UD_Estonian-EWT/r2.10/et_ewt-ud-dev.conllu",
1404	        "test": "UD_Estonian-EWT/r2.10/et_ewt-ud-test.conllu",
1405	    },
1406	    "fo_farpahc": {
1407	        "train": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-train.conllu",
1408	        "dev": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-dev.conllu",
1409	        "test": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-test.conllu",
1410	    },
1411	    "fo_oft": {
1412	        "test": "UD_Faroese-OFT/r2.10/fo_oft-ud-test.conllu",
1413	    },
1414	    "fi_ftb": {
1415	        "train": "UD_Finnish-FTB/r2.10/fi_ftb-ud-train.conllu",
1416	        "dev": "UD_Finnish-FTB/r2.10/fi_ftb-ud-dev.conllu",
1417	        "test": "UD_Finnish-FTB/r2.10/fi_ftb-ud-test.conllu",
1418	    },
1419	    "fi_ood": {
1420	        "test": "UD_Finnish-OOD/r2.10/fi_ood-ud-test.conllu",
1421	    },
1422	    "fi_pud": {
1423	        "test": "UD_Finnish-PUD/r2.10/fi_pud-ud-test.conllu",
1424	    },
1425	    "fi_tdt": {
1426	        "train": "UD_Finnish-TDT/r2.10/fi_tdt-ud-train.conllu",
1427	        "dev": "UD_Finnish-TDT/r2.10/fi_tdt-ud-dev.conllu",
1428	        "test": "UD_Finnish-TDT/r2.10/fi_tdt-ud-test.conllu",
1429	    },
1430	    "fr_parisstories": {
1431	        "train": "UD_French-ParisStories/r2.10/fr_parisstories-ud-train.conllu",
1432	        "test": "UD_French-ParisStories/r2.10/fr_parisstories-ud-test.conllu",
1433	    },
1434	    "fr_fqb": {
1435	        "test": "UD_French-FQB/r2.10/fr_fqb-ud-test.conllu",
1436	    },
1437	    "fr_rhapsodie": {
1438	        "train": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-train.conllu",
1439	        "dev": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-dev.conllu",
1440	        "test": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-test.conllu",
1441	    },
1442	    "fr_ftb": {
1443	        "train": "UD_French-FTB/r2.10/fr_ftb-ud-train.conllu",
1444	        "dev": "UD_French-FTB/r2.10/fr_ftb-ud-dev.conllu",
1445	        "test": "UD_French-FTB/r2.10/fr_ftb-ud-test.conllu",
1446	    },
1447	    "fr_gsd": {
1448	        "train": "UD_French-GSD/r2.10/fr_gsd-ud-train.conllu",
1449	        "dev": "UD_French-GSD/r2.10/fr_gsd-ud-dev.conllu",
1450	        "test": "UD_French-GSD/r2.10/fr_gsd-ud-test.conllu",
1451	    },
1452	    "fr_partut": {
1453	        "train": "UD_French-ParTUT/r2.10/fr_partut-ud-train.conllu",
1454	        "dev": "UD_French-ParTUT/r2.10/fr_partut-ud-dev.conllu",
1455	        "test": "UD_French-ParTUT/r2.10/fr_partut-ud-test.conllu",
1456	    },
1457	    "fr_pud": {
1458	        "test": "UD_French-PUD/r2.10/fr_pud-ud-test.conllu",
1459	    },
1460	    "fr_sequoia": {
1461	        "train": "UD_French-Sequoia/r2.10/fr_sequoia-ud-train.conllu",
1462	        "dev": "UD_French-Sequoia/r2.10/fr_sequoia-ud-dev.conllu",
1463	        "test": "UD_French-Sequoia/r2.10/fr_sequoia-ud-test.conllu",
1464	    },
1465	    "qfn_fame": {
1466	        "test": "UD_Frisian_Dutch-Fame/r2.10/qfn_fame-ud-test.conllu",
1467	    },
1468	    "gl_ctg": {
1469	        "train": "UD_Galician-CTG/r2.10/gl_ctg-ud-train.conllu",
1470	        "dev": "UD_Galician-CTG/r2.10/gl_ctg-ud-dev.conllu",
1471	        "test": "UD_Galician-CTG/r2.10/gl_ctg-ud-test.conllu",
1472	    },
1473	    "gl_treegal": {
1474	        "train": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-train.conllu",
1475	        "test": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-test.conllu",
1476	    },
1477	    "de_gsd": {
1478	        "train": "UD_German-GSD/r2.10/de_gsd-ud-train.conllu",
1479	        "dev": "UD_German-GSD/r2.10/de_gsd-ud-dev.conllu",
1480	        "test": "UD_German-GSD/r2.10/de_gsd-ud-test.conllu",
1481	    },
1482	    "de_hdt": {
1483	        "train": [
1484	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-1.conllu",
1485	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-2.conllu",
1486	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-1.conllu",
1487	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-2.conllu",
1488	        ],
1489	        "dev": "UD_German-HDT/r2.10/de_hdt-ud-dev.conllu",
1490	        "test": "UD_German-HDT/r2.10/de_hdt-ud-test.conllu",
1491	    },
1492	    "de_lit": {
1493	        "test": "UD_German-LIT/r2.10/de_lit-ud-test.conllu",
1494	    },
1495	    "de_pud": {
1496	        "test": "UD_German-PUD/r2.10/de_pud-ud-test.conllu",
1497	    },
1498	    "got_proiel": {
1499	        "train": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-train.conllu",
1500	        "dev": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-dev.conllu",
1501	        "test": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-test.conllu",
1502	    },
1503	    "el_gdt": {
1504	        "train": "UD_Greek-GDT/r2.10/el_gdt-ud-train.conllu",
1505	        "dev": "UD_Greek-GDT/r2.10/el_gdt-ud-dev.conllu",
1506	        "test": "UD_Greek-GDT/r2.10/el_gdt-ud-test.conllu",
1507	    },
1508	    "gub_tudet": {
1509	        "test": "UD_Guajajara-TuDeT/r2.10/gub_tudet-ud-test.conllu",
1510	    },
1511	    "gn_oldtudet": {
1512	        "test": "UD_Guarani-OldTuDeT/r2.10/gn_oldtudet-ud-test.conllu",
1513	    },
1514	    "he_iahltwiki": {
1515	        "train": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-train.conllu",
1516	        "dev": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-dev.conllu",
1517	        "test": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-test.conllu",
1518	    },
1519	    "he_htb": {
1520	        "train": "UD_Hebrew-HTB/r2.10/he_htb-ud-train.conllu",
1521	        "dev": "UD_Hebrew-HTB/r2.10/he_htb-ud-dev.conllu",
1522	        "test": "UD_Hebrew-HTB/r2.10/he_htb-ud-test.conllu",
1523	    },
1524	    "qhe_hiencs": {
1525	        "train": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-train.conllu",
1526	        "dev": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-dev.conllu",
1527	        "test": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-test.conllu",
1528	    },
1529	    "hi_hdtb": {
1530	        "train": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-train.conllu",
1531	        "dev": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-dev.conllu",
1532	        "test": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-test.conllu",
1533	    },
1534	    "hi_pud": {
1535	        "test": "UD_Hindi-PUD/r2.10/hi_pud-ud-test.conllu",
1536	    },
1537	    "hu_szeged": {
1538	        "train": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-train.conllu",
1539	        "dev": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-dev.conllu",
1540	        "test": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-test.conllu",
1541	    },
1542	    "is_modern": {
1543	        "train": "UD_Icelandic-Modern/r2.10/is_modern-ud-train.conllu",
1544	        "dev": "UD_Icelandic-Modern/r2.10/is_modern-ud-dev.conllu",
1545	        "test": "UD_Icelandic-Modern/r2.10/is_modern-ud-test.conllu",
1546	    },
1547	    "is_icepahc": {
1548	        "train": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-train.conllu",
1549	        "dev": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-dev.conllu",
1550	        "test": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-test.conllu",
1551	    },
1552	    "is_pud": {
1553	        "test": "UD_Icelandic-PUD/r2.10/is_pud-ud-test.conllu",
1554	    },
1555	    "id_csui": {
1556	        "train": "UD_Indonesian-CSUI/r2.10/id_csui-ud-train.conllu",
1557	        "test": "UD_Indonesian-CSUI/r2.10/id_csui-ud-test.conllu",
1558	    },
1559	    "id_gsd": {
1560	        "train": "UD_Indonesian-GSD/r2.10/id_gsd-ud-train.conllu",
1561	        "dev": "UD_Indonesian-GSD/r2.10/id_gsd-ud-dev.conllu",
1562	        "test": "UD_Indonesian-GSD/r2.10/id_gsd-ud-test.conllu",
1563	    },
1564	    "id_pud": {
1565	        "test": "UD_Indonesian-PUD/r2.10/id_pud-ud-test.conllu",
1566	    },
1567	    "ga_twittirish": {
1568	        "test": "UD_Irish-TwittIrish/r2.10/ga_twittirish-ud-test.conllu",
1569	    },
1570	    "ga_idt": {
1571	        "train": "UD_Irish-IDT/r2.10/ga_idt-ud-train.conllu",
1572	        "dev": "UD_Irish-IDT/r2.10/ga_idt-ud-dev.conllu",
1573	        "test": "UD_Irish-IDT/r2.10/ga_idt-ud-test.conllu",
1574	    },
1575	    "it_valico": {
1576	        "test": "UD_Italian-Valico/r2.10/it_valico-ud-test.conllu",
1577	    },
1578	    "it_markit": {
1579	        "train": "UD_Italian-MarkIT/r2.10/it_markit-ud-train.conllu",
1580	        "dev": "UD_Italian-MarkIT/r2.10/it_markit-ud-dev.conllu",
1581	        "test": "UD_Italian-MarkIT/r2.10/it_markit-ud-test.conllu",
1582	    },
1583	    "it_isdt": {
1584	        "train": "UD_Italian-ISDT/r2.10/it_isdt-ud-train.conllu",
1585	        "dev": "UD_Italian-ISDT/r2.10/it_isdt-ud-dev.conllu",
1586	        "test": "UD_Italian-ISDT/r2.10/it_isdt-ud-test.conllu",
1587	    },
1588	    "it_partut": {
1589	        "train": "UD_Italian-ParTUT/r2.10/it_partut-ud-train.conllu",
1590	        "dev": "UD_Italian-ParTUT/r2.10/it_partut-ud-dev.conllu",
1591	        "test": "UD_Italian-ParTUT/r2.10/it_partut-ud-test.conllu",
1592	    },
1593	    "it_postwita": {
1594	        "train": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-train.conllu",
1595	        "dev": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-dev.conllu",
1596	        "test": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-test.conllu",
1597	    },
1598	    "it_pud": {
1599	        "test": "UD_Italian-PUD/r2.10/it_pud-ud-test.conllu",
1600	    },
1601	    "it_twittiro": {
1602	        "train": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-train.conllu",
1603	        "dev": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-dev.conllu",
1604	        "test": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-test.conllu",
1605	    },
1606	    "it_vit": {
1607	        "train": "UD_Italian-VIT/r2.10/it_vit-ud-train.conllu",
1608	        "dev": "UD_Italian-VIT/r2.10/it_vit-ud-dev.conllu",
1609	        "test": "UD_Italian-VIT/r2.10/it_vit-ud-test.conllu",
1610	    },
1611	    "ja_gsdluw": {
1612	        "train": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-train.conllu",
1613	        "dev": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-dev.conllu",
1614	        "test": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-test.conllu",
1615	    },
1616	    "ja_pudluw": {
1617	        "test": "UD_Japanese-PUDLUW/r2.10/ja_pudluw-ud-test.conllu",
1618	    },
1619	    "ja_bccwjluw": {
1620	        "train": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-train.conllu",
1621	        "dev": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-dev.conllu",
1622	        "test": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-test.conllu",
1623	    },
1624	    "ja_bccwj": {
1625	        "train": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-train.conllu",
1626	        "dev": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-dev.conllu",
1627	        "test": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-test.conllu",
1628	    },
1629	    "ja_gsd": {
1630	        "train": "UD_Japanese-GSD/r2.10/ja_gsd-ud-train.conllu",
1631	        "dev": "UD_Japanese-GSD/r2.10/ja_gsd-ud-dev.conllu",
1632	        "test": "UD_Japanese-GSD/r2.10/ja_gsd-ud-test.conllu",
1633	    },
1634	    "ja_modern": {
1635	        "test": "UD_Japanese-Modern/r2.10/ja_modern-ud-test.conllu",
1636	    },
1637	    "ja_pud": {
1638	        "test": "UD_Japanese-PUD/r2.10/ja_pud-ud-test.conllu",
1639	    },
1640	    "jv_csui": {
1641	        "test": "UD_Javanese-CSUI/r2.10/jv_csui-ud-test.conllu",
1642	    },
1643	    "urb_tudet": {
1644	        "test": "UD_Kaapor-TuDeT/r2.10/urb_tudet-ud-test.conllu",
1645	    },
1646	    "xnr_kdtb": {
1647	        "test": "UD_Kangri-KDTB/r2.10/xnr_kdtb-ud-test.conllu",
1648	    },
1649	    "krl_kkpp": {
1650	        "test": "UD_Karelian-KKPP/r2.10/krl_kkpp-ud-test.conllu",
1651	    },
1652	    "arr_tudet": {
1653	        "test": "UD_Karo-TuDeT/r2.10/arr_tudet-ud-test.conllu",
1654	    },
1655	    "kk_ktb": {
1656	        "train": "UD_Kazakh-KTB/r2.10/kk_ktb-ud-train.conllu",
1657	        "test": "UD_Kazakh-KTB/r2.10/kk_ktb-ud-test.conllu",
1658	    },
1659	    "kfm_aha": {
1660	        "test": "UD_Khunsari-AHA/r2.10/kfm_aha-ud-test.conllu",
1661	    },
1662	    "quc_iu": {
1663	        "test": "UD_Kiche-IU/r2.10/quc_iu-ud-test.conllu",
1664	    },
1665	    "koi_uh": {
1666	        "test": "UD_Komi_Permyak-UH/r2.10/koi_uh-ud-test.conllu",
1667	    },
1668	    "kpv_ikdp": {
1669	        "test": "UD_Komi_Zyrian-IKDP/r2.10/kpv_ikdp-ud-test.conllu",
1670	    },
1671	    "kpv_lattice": {
1672	        "test": "UD_Komi_Zyrian-Lattice/r2.10/kpv_lattice-ud-test.conllu",
1673	    },
1674	    "ko_gsd": {
1675	        "train": "UD_Korean-GSD/r2.10/ko_gsd-ud-train.conllu",
1676	        "dev": "UD_Korean-GSD/r2.10/ko_gsd-ud-dev.conllu",
1677	        "test": "UD_Korean-GSD/r2.10/ko_gsd-ud-test.conllu",
1678	    },
1679	    "ko_kaist": {
1680	        "train": "UD_Korean-Kaist/r2.10/ko_kaist-ud-train.conllu",
1681	        "dev": "UD_Korean-Kaist/r2.10/ko_kaist-ud-dev.conllu",
1682	        "test": "UD_Korean-Kaist/r2.10/ko_kaist-ud-test.conllu",
1683	    },
1684	    "ko_pud": {
1685	        "test": "UD_Korean-PUD/r2.10/ko_pud-ud-test.conllu",
1686	    },
1687	    "kmr_mg": {
1688	        "train": "UD_Kurmanji-MG/r2.10/kmr_mg-ud-train.conllu",
1689	        "test": "UD_Kurmanji-MG/r2.10/kmr_mg-ud-test.conllu",
1690	    },
1691	    "la_udante": {
1692	        "train": "UD_Latin-UDante/r2.10/la_udante-ud-train.conllu",
1693	        "dev": "UD_Latin-UDante/r2.10/la_udante-ud-dev.conllu",
1694	        "test": "UD_Latin-UDante/r2.10/la_udante-ud-test.conllu",
1695	    },
1696	    "la_ittb": {
1697	        "train": "UD_Latin-ITTB/r2.10/la_ittb-ud-train.conllu",
1698	        "dev": "UD_Latin-ITTB/r2.10/la_ittb-ud-dev.conllu",
1699	        "test": "UD_Latin-ITTB/r2.10/la_ittb-ud-test.conllu",
1700	    },
1701	    "la_llct": {
1702	        "train": "UD_Latin-LLCT/r2.10/la_llct-ud-train.conllu",
1703	        "dev": "UD_Latin-LLCT/r2.10/la_llct-ud-dev.conllu",
1704	        "test": "UD_Latin-LLCT/r2.10/la_llct-ud-test.conllu",
1705	    },
1706	    "la_perseus": {
1707	        "train": "UD_Latin-Perseus/r2.10/la_perseus-ud-train.conllu",
1708	        "test": "UD_Latin-Perseus/r2.10/la_perseus-ud-test.conllu",
1709	    },
1710	    "la_proiel": {
1711	        "train": "UD_Latin-PROIEL/r2.10/la_proiel-ud-train.conllu",
1712	        "dev": "UD_Latin-PROIEL/r2.10/la_proiel-ud-dev.conllu",
1713	        "test": "UD_Latin-PROIEL/r2.10/la_proiel-ud-test.conllu",
1714	    },
1715	    "lv_lvtb": {
1716	        "train": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-train.conllu",
1717	        "dev": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-dev.conllu",
1718	        "test": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-test.conllu",
1719	    },
1720	    "lij_glt": {
1721	        "train": "UD_Ligurian-GLT/r2.10/lij_glt-ud-train.conllu",
1722	        "test": "UD_Ligurian-GLT/r2.10/lij_glt-ud-test.conllu",
1723	    },
1724	    "lt_alksnis": {
1725	        "train": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-train.conllu",
1726	        "dev": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-dev.conllu",
1727	        "test": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-test.conllu",
1728	    },
1729	    "lt_hse": {
1730	        "train": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1731	        "dev": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1732	        "test": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1733	    },
1734	    "olo_kkpp": {
1735	        "train": "UD_Livvi-KKPP/r2.10/olo_kkpp-ud-train.conllu",
1736	        "test": "UD_Livvi-KKPP/r2.10/olo_kkpp-ud-test.conllu",
1737	    },
1738	    "nds_lsdc": {
1739	        "test": "UD_Low_Saxon-LSDC/r2.10/nds_lsdc-ud-test.conllu",
1740	    },
1741	    # TODO(tfds) Add Madi Jarawara splits when it will be officially released.
1742	    # TODO(tfds) Add Makurap TuDeT splits when it will be officially released.
1743	    "mt_mudt": {
1744	        "train": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-train.conllu",
1745	        "dev": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-dev.conllu",
1746	        "test": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-test.conllu",
1747	    },
1748	    "gv_cadhan": {
1749	        "test": "UD_Manx-Cadhan/r2.10/gv_cadhan-ud-test.conllu",
1750	    },
1751	    "mr_ufal": {
1752	        "train": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-train.conllu",
1753	        "dev": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-dev.conllu",
1754	        "test": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-test.conllu",
1755	    },
1756	    "gun_dooley": {
1757	        "test": "UD_Mbya_Guarani-Dooley/r2.10/gun_dooley-ud-test.conllu",
1758	    },
1759	    "gun_thomas": {
1760	        "test": "UD_Mbya_Guarani-Thomas/r2.10/gun_thomas-ud-test.conllu",
1761	    },
1762	    "mdf_jr": {
1763	        "test": "UD_Moksha-JR/r2.10/mdf_jr-ud-test.conllu",
1764	    },
1765	    "myu_tudet": {
1766	        "test": "UD_Munduruku-TuDeT/r2.10/myu_tudet-ud-test.conllu",
1767	    },
1768	    "pcm_nsc": {
1769	        "train": "UD_Naija-NSC/r2.10/pcm_nsc-ud-train.conllu",
1770	        "dev": "UD_Naija-NSC/r2.10/pcm_nsc-ud-dev.conllu",
1771	        "test": "UD_Naija-NSC/r2.10/pcm_nsc-ud-test.conllu",
1772	    },
1773	    "nyq_aha": {
1774	        "test": "UD_Nayini-AHA/r2.10/nyq_aha-ud-test.conllu",
1775	    },
1776	    # TODO(tfds) Add Neapolitan RB splits when it will be officially released.
1777	    "sme_giella": {
1778	        "train": "UD_North_Sami-Giella/r2.10/sme_giella-ud-train.conllu",
1779	        "test": "UD_North_Sami-Giella/r2.10/sme_giella-ud-test.conllu",
1780	    },
1781	    "no_bokmaal": {
1782	        "train": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-train.conllu",
1783	        "dev": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-dev.conllu",
1784	        "test": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-test.conllu",
1785	    },
1786	    "no_nynorsk": {
1787	        "train": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-train.conllu",
1788	        "dev": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-dev.conllu",
1789	        "test": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-test.conllu",
1790	    },
1791	    "no_nynorsklia": {
1792	        "train": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-train.conllu",
1793	        "dev": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-dev.conllu",
1794	        "test": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-test.conllu",
1795	    },
1796	    "cu_proiel": {
1797	        "train": (
1798	            "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-train.conllu"
1799	        ),
1800	        "dev": "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-dev.conllu",
1801	        "test": "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-test.conllu",
1802	    },
1803	    "fro_srcmf": {
1804	        "train": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-train.conllu",
1805	        "dev": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-dev.conllu",
1806	        "test": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-test.conllu",
1807	    },
1808	    "orv_birchbark": {
1809	        "train": (
1810	            "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-train.conllu"
1811	        ),
1812	        "dev": "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-dev.conllu",
1813	        "test": (
1814	            "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-test.conllu"
1815	        ),
1816	    },
1817	    "orv_rnc": {
1818	        "train": "UD_Old_Russian-RNC/r2.10/orv_rnc-ud-train.conllu",
1819	        "test": "UD_Old_Russian-RNC/r2.10/orv_rnc-ud-test.conllu",
1820	    },
1821	    "orv_torot": {
1822	        "train": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-train.conllu",
1823	        "dev": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-dev.conllu",
1824	        "test": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-test.conllu",
1825	    },
1826	    "otk_tonqq": {
1827	        "test": "UD_Old_Turkish-Tonqq/r2.10/otk_tonqq-ud-test.conllu",
1828	    },
1829	    "fa_perdt": {
1830	        "train": "UD_Persian-PerDT/r2.10/fa_perdt-ud-train.conllu",
1831	        "dev": "UD_Persian-PerDT/r2.10/fa_perdt-ud-dev.conllu",
1832	        "test": "UD_Persian-PerDT/r2.10/fa_perdt-ud-test.conllu",
1833	    },
1834	    "fa_seraji": {
1835	        "train": "UD_Persian-Seraji/r2.10/fa_seraji-ud-train.conllu",
1836	        "dev": "UD_Persian-Seraji/r2.10/fa_seraji-ud-dev.conllu",
1837	        "test": "UD_Persian-Seraji/r2.10/fa_seraji-ud-test.conllu",
1838	    },
1839	    "pl_lfg": {
1840	        "train": "UD_Polish-LFG/r2.10/pl_lfg-ud-train.conllu",
1841	        "dev": "UD_Polish-LFG/r2.10/pl_lfg-ud-dev.conllu",
1842	        "test": "UD_Polish-LFG/r2.10/pl_lfg-ud-test.conllu",
1843	    },
1844	    "pl_pdb": {
1845	        "train": "UD_Polish-PDB/r2.10/pl_pdb-ud-train.conllu",
1846	        "dev": "UD_Polish-PDB/r2.10/pl_pdb-ud-dev.conllu",
1847	        "test": "UD_Polish-PDB/r2.10/pl_pdb-ud-test.conllu",
1848	    },
1849	    "pl_pud": {
1850	        "test": "UD_Polish-PUD/r2.10/pl_pud-ud-test.conllu",
1851	    },
1852	    "pt_bosque": {
1853	        "train": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-train.conllu",
1854	        "dev": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-dev.conllu",
1855	        "test": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-test.conllu",
1856	    },
1857	    "pt_gsd": {
1858	        "train": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-train.conllu",
1859	        "dev": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-dev.conllu",
1860	        "test": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-test.conllu",
1861	    },
1862	    "pt_pud": {
1863	        "test": "UD_Portuguese-PUD/r2.10/pt_pud-ud-test.conllu",
1864	    },
1865	    "ro_art": {
1866	        "test": "UD_Romanian-ArT/r2.10/ro_art-ud-test.conllu",
1867	    },
1868	    "ro_nonstandard": {
1869	        "train": "UD_Romanian-Nonstandard/r2.10/ro_nonstandard-ud-train.conllu",
1870	        "dev": "UD_Romanian-Nonstandard/r2.10/ro_nonstandard-ud-dev.conllu",
1871	        "test": "UD_Romanian-Nonstandard/r2.10/ro_nonstandard-ud-test.conllu",
1872	    },
1873	    "ro_rrt": {
1874	        "train": "UD_Romanian-RRT/r2.10/ro_rrt-ud-train.conllu",
1875	        "dev": "UD_Romanian-RRT/r2.10/ro_rrt-ud-dev.conllu",
1876	        "test": "UD_Romanian-RRT/r2.10/ro_rrt-ud-test.conllu",
1877	    },
1878	    "ro_simonero": {
1879	        "train": "UD_Romanian-SiMoNERo/r2.10/ro_simonero-ud-train.conllu",
1880	        "dev": "UD_Romanian-SiMoNERo/r2.10/ro_simonero-ud-dev.conllu",
1881	        "test": "UD_Romanian-SiMoNERo/r2.10/ro_simonero-ud-test.conllu",
1882	    },
1883	    "ru_gsd": {
1884	        "train": "UD_Russian-GSD/r2.10/ru_gsd-ud-train.conllu",
1885	        "dev": "UD_Russian-GSD/r2.10/ru_gsd-ud-dev.conllu",
1886	        "test": "UD_Russian-GSD/r2.10/ru_gsd-ud-test.conllu",
1887	    },
1888	    "ru_pud": {
1889	        "test": "UD_Russian-PUD/r2.10/ru_pud-ud-test.conllu",
1890	    },
1891	    "ru_syntagrus": {
1892	        "train": "UD_Russian-SynTagRus/r2.7/ru_syntagrus-ud-train.conllu",
1893	        "dev": "UD_Russian-SynTagRus/r2.7/ru_syntagrus-ud-dev.conllu",
1894	        "test": "UD_Russian-SynTagRus/r2.7/ru_syntagrus-ud-test.conllu",
1895	    },
1896	    "ru_taiga": {
1897	        "train": "UD_Russian-Taiga/r2.10/ru_taiga-ud-train.conllu",
1898	        "dev": "UD_Russian-Taiga/r2.10/ru_taiga-ud-dev.conllu",
1899	        "test": "UD_Russian-Taiga/r2.10/ru_taiga-ud-test.conllu",
1900	    },
1901	    "sa_ufal": {
1902	        "test": "UD_Sanskrit-UFAL/r2.10/sa_ufal-ud-test.conllu",
1903	    },
1904	    "sa_vedic": {
1905	        "train": "UD_Sanskrit-Vedic/r2.10/sa_vedic-ud-train.conllu",
1906	        "test": "UD_Sanskrit-Vedic/r2.10/sa_vedic-ud-test.conllu",
1907	    },
1908	    "gd_arcosg": {
1909	        "train": "UD_Scottish_Gaelic-ARCOSG/r2.10/gd_arcosg-ud-train.conllu",
1910	        "dev": "UD_Scottish_Gaelic-ARCOSG/r2.10/gd_arcosg-ud-dev.conllu",
1911	        "test": "UD_Scottish_Gaelic-ARCOSG/r2.10/gd_arcosg-ud-test.conllu",
1912	    },
1913	    "sr_set": {
1914	        "train": "UD_Serbian-SET/r2.10/sr_set-ud-train.conllu",
1915	        "dev": "UD_Serbian-SET/r2.10/sr_set-ud-dev.conllu",
1916	        "test": "UD_Serbian-SET/r2.10/sr_set-ud-test.conllu",
1917	    },
1918	    "sms_giellagas": {
1919	        "test": "UD_Skolt_Sami-Giellagas/r2.10/sms_giellagas-ud-test.conllu",
1920	    },
1921	    "sk_snk": {
1922	        "train": "UD_Slovak-SNK/r2.10/sk_snk-ud-train.conllu",
1923	        "dev": "UD_Slovak-SNK/r2.10/sk_snk-ud-dev.conllu",
1924	        "test": "UD_Slovak-SNK/r2.10/sk_snk-ud-test.conllu",
1925	    },
1926	    "sl_ssj": {
1927	        "train": "UD_Slovenian-SSJ/r2.10/sl_ssj-ud-train.conllu",
1928	        "dev": "UD_Slovenian-SSJ/r2.10/sl_ssj-ud-dev.conllu",
1929	        "test": "UD_Slovenian-SSJ/r2.10/sl_ssj-ud-test.conllu",
1930	    },
1931	    "sl_sst": {
1932	        "train": "UD_Slovenian-SST/r2.10/sl_sst-ud-train.conllu",
1933	        "test": "UD_Slovenian-SST/r2.10/sl_sst-ud-test.conllu",
1934	    },
1935	    "soj_aha": {
1936	        "test": "UD_Soi-AHA/r2.10/soj_aha-ud-test.conllu",
1937	    },
1938	    "ajp_madar": {
1939	        "test": (
1940	            "UD_South_Levantine_Arabic-MADAR/r2.10/ajp_madar-ud-test.conllu"
1941	        ),
1942	    },
1943	    "es_ancora": {
1944	        "train": "UD_Spanish-AnCora/r2.10/es_ancora-ud-train.conllu",
1945	        "dev": "UD_Spanish-AnCora/r2.10/es_ancora-ud-dev.conllu",
1946	        "test": "UD_Spanish-AnCora/r2.10/es_ancora-ud-test.conllu",
1947	    },
1948	    "es_gsd": {
1949	        "train": "UD_Spanish-GSD/r2.10/es_gsd-ud-train.conllu",
1950	        "dev": "UD_Spanish-GSD/r2.10/es_gsd-ud-dev.conllu",
1951	        "test": "UD_Spanish-GSD/r2.10/es_gsd-ud-test.conllu",
1952	    },
1953	    "es_pud": {
1954	        "test": "UD_Spanish-PUD/r2.10/es_pud-ud-test.conllu",
1955	    },
1956	    "swl_sslc": {
1957	        "train": "UD_Swedish_Sign_Language-SSLC/r2.10/swl_sslc-ud-train.conllu",
1958	        "dev": "UD_Swedish_Sign_Language-SSLC/r2.10/swl_sslc-ud-dev.conllu",
1959	        "test": "UD_Swedish_Sign_Language-SSLC/r2.10/swl_sslc-ud-test.conllu",
1960	    },
1961	    "sv_lines": {
1962	        "train": "UD_Swedish-LinES/r2.10/sv_lines-ud-train.conllu",
1963	        "dev": "UD_Swedish-LinES/r2.10/sv_lines-ud-dev.conllu",
1964	        "test": "UD_Swedish-LinES/r2.10/sv_lines-ud-test.conllu",
1965	    },
1966	    "sv_pud": {
1967	        "test": "UD_Swedish-PUD/r2.10/sv_pud-ud-test.conllu",
1968	    },
1969	    "sv_talbanken": {
1970	        "train": "UD_Swedish-Talbanken/r2.10/sv_talbanken-ud-train.conllu",
1971	        "dev": "UD_Swedish-Talbanken/r2.10/sv_talbanken-ud-dev.conllu",
1972	        "test": "UD_Swedish-Talbanken/r2.10/sv_talbanken-ud-test.conllu",
1973	    },
1974	    "gsw_uzh": {
1975	        "test": "UD_Swiss_German-UZH/r2.10/gsw_uzh-ud-test.conllu",
1976	    },
1977	    "tl_trg": {
1978	        "test": "UD_Tagalog-TRG/r2.10/tl_trg-ud-test.conllu",
1979	    },
1980	    "tl_ugnayan": {
1981	        "test": "UD_Tagalog-Ugnayan/r2.10/tl_ugnayan-ud-test.conllu",
1982	    },
1983	    "ta_mwtt": {
1984	        "test": "UD_Tamil-MWTT/r2.10/ta_mwtt-ud-test.conllu",
1985	    },
1986	    "ta_ttb": {
1987	        "train": "UD_Tamil-TTB/r2.10/ta_ttb-ud-train.conllu",
1988	        "dev": "UD_Tamil-TTB/r2.10/ta_ttb-ud-dev.conllu",
1989	        "test": "UD_Tamil-TTB/r2.10/ta_ttb-ud-test.conllu",
1990	    },
1991	    "te_mtg": {
1992	        "train": "UD_Telugu-MTG/r2.10/te_mtg-ud-train.conllu",
1993	        "dev": "UD_Telugu-MTG/r2.10/te_mtg-ud-dev.conllu",
1994	        "test": "UD_Telugu-MTG/r2.10/te_mtg-ud-test.conllu",
1995	    },
1996	    "th_pud": {
1997	        "test": "UD_Thai-PUD/r2.10/th_pud-ud-test.conllu",
1998	    },
1999	    "tpn_tudet": {
2000	        "test": "UD_Tupinamba-TuDeT/r2.10/tpn_tudet-ud-test.conllu",
2001	    },
2002	    "qtd_sagt": {
2003	        "train": "UD_Turkish_German-SAGT/r2.10/qtd_sagt-ud-train.conllu",
2004	        "dev": "UD_Turkish_German-SAGT/r2.10/qtd_sagt-ud-dev.conllu",
2005	        "test": "UD_Turkish_German-SAGT/r2.10/qtd_sagt-ud-test.conllu",
2006	    },
2007	    "tr_kenet": {
2008	        "train": "UD_Turkish-Kenet/r2.10/tr_kenet-ud-train.conllu",
2009	        "dev": "UD_Turkish-Kenet/r2.10/tr_kenet-ud-dev.conllu",
2010	        "test": "UD_Turkish-Kenet/r2.10/tr_kenet-ud-test.conllu",
2011	    },
2012	    "tr_atis": {
2013	        "train": "UD_Turkish-Atis/r2.10/tr_atis-ud-train.conllu",
2014	        "dev": "UD_Turkish-Atis/r2.10/tr_atis-ud-dev.conllu",
2015	        "test": "UD_Turkish-Atis/r2.10/tr_atis-ud-test.conllu",
2016	    },
2017	    "tr_penn": {
2018	        "train": "UD_Turkish-Penn/r2.10/tr_penn-ud-train.conllu",
2019	        "dev": "UD_Turkish-Penn/r2.10/tr_penn-ud-dev.conllu",
2020	        "test": "UD_Turkish-Penn/r2.10/tr_penn-ud-test.conllu",
2021	    },
2022	    "tr_tourism": {
2023	        "train": "UD_Turkish-Tourism/r2.10/tr_tourism-ud-train.conllu",
2024	        "dev": "UD_Turkish-Tourism/r2.10/tr_tourism-ud-dev.conllu",
2025	        "test": "UD_Turkish-Tourism/r2.10/tr_tourism-ud-test.conllu",
2026	    },
2027	    "tr_framenet": {
2028	        "train": "UD_Turkish-FrameNet/r2.10/tr_framenet-ud-train.conllu",
2029	        "dev": "UD_Turkish-FrameNet/r2.10/tr_framenet-ud-dev.conllu",
2030	        "test": "UD_Turkish-FrameNet/r2.10/tr_framenet-ud-test.conllu",
2031	    },
2032	    "tr_boun": {
2033	        "train": "UD_Turkish-BOUN/r2.10/tr_boun-ud-train.conllu",
2034	        "dev": "UD_Turkish-BOUN/r2.10/tr_boun-ud-dev.conllu",
2035	        "test": "UD_Turkish-BOUN/r2.10/tr_boun-ud-test.conllu",
2036	    },
2037	    "tr_gb": {
2038	        "test": "UD_Turkish-GB/r2.10/tr_gb-ud-test.conllu",
2039	    },
2040	    "tr_imst": {
2041	        "train": "UD_Turkish-IMST/r2.10/tr_imst-ud-train.conllu",
2042	        "dev": "UD_Turkish-IMST/r2.10/tr_imst-ud-dev.conllu",
2043	        "test": "UD_Turkish-IMST/r2.10/tr_imst-ud-test.conllu",
2044	    },
2045	    "tr_pud": {
2046	        "test": "UD_Turkish-PUD/r2.10/tr_pud-ud-test.conllu",
2047	    },
2048	    "uk_iu": {
2049	        "train": "UD_Ukrainian-IU/r2.10/uk_iu-ud-train.conllu",
2050	        "dev": "UD_Ukrainian-IU/r2.10/uk_iu-ud-dev.conllu",
2051	        "test": "UD_Ukrainian-IU/r2.10/uk_iu-ud-test.conllu",
2052	    },
2053	    # TODO(tfds) Add Umbrian Ikuvina split when it will be officially released.
2054	    "hsb_ufal": {
2055	        "train": "UD_Upper_Sorbian-UFAL/r2.10/hsb_ufal-ud-train.conllu",
2056	        "test": "UD_Upper_Sorbian-UFAL/r2.10/hsb_ufal-ud-test.conllu",
2057	    },
2058	    "ur_udtb": {
2059	        "train": "UD_Urdu-UDTB/r2.10/ur_udtb-ud-train.conllu",
2060	        "dev": "UD_Urdu-UDTB/r2.10/ur_udtb-ud-dev.conllu",
2061	        "test": "UD_Urdu-UDTB/r2.10/ur_udtb-ud-test.conllu",
2062	    },
2063	    "ug_udt": {
2064	        "train": "UD_Uyghur-UDT/r2.10/ug_udt-ud-train.conllu",
2065	        "dev": "UD_Uyghur-UDT/r2.10/ug_udt-ud-dev.conllu",
2066	        "test": "UD_Uyghur-UDT/r2.10/ug_udt-ud-test.conllu",
2067	    },
2068	    "vi_vtb": {
2069	        "train": "UD_Vietnamese-VTB/r2.10/vi_vtb-ud-train.conllu",
2070	        "dev": "UD_Vietnamese-VTB/r2.10/vi_vtb-ud-dev.conllu",
2071	        "test": "UD_Vietnamese-VTB/r2.10/vi_vtb-ud-test.conllu",
2072	    },
2073	    "wbp_ufal": {
2074	        "test": "UD_Warlpiri-UFAL/r2.10/wbp_ufal-ud-test.conllu",
2075	    },
2076	    "cy_ccg": {
2077	        "train": "UD_Welsh-CCG/r2.10/cy_ccg-ud-train.conllu",
2078	        "test": "UD_Welsh-CCG/r2.10/cy_ccg-ud-test.conllu",
2079	    },
2080	    "wo_wtb": {
2081	        "train": "UD_Wolof-WTB/r2.10/wo_wtb-ud-train.conllu",
2082	        "dev": "UD_Wolof-WTB/r2.10/wo_wtb-ud-dev.conllu",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:981
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
980	    "ajp_madar": (
981	        "The South_Levantine_Arabic-MADAR treebank consists of 100 "
982	        "manually-annotated sentences taken from the "
983	        "[MADAR](https://camel.abudhabi.nyu.edu/madar/) (Multi-Arabic Dialect "
984	        "Applications and Resources) project. "
985	    ),
986	    "es_ancora": "Spanish data from the AnCora corpus.",
987	    "es_gsd": (
988	        "The Spanish UD is converted from the content head version of the "
989	        "universal dependency treebank v2.0 (legacy)."
990	    ),
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {
1217	        "test": "UD_Arabic-PUD/r2.10/ar_pud-ud-test.conllu",
1218	    },
1219	    "hy_armtdp": {
1220	        "train": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-train.conllu",
1221	        "dev": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-dev.conllu",
1222	        "test": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-test.conllu",
1223	    },
1224	    "aii_as": {
1225	        "test": "UD_Assyrian-AS/r2.10/aii_as-ud-test.conllu",
1226	    },
1227	    "bm_crb": {
1228	        "test": "UD_Bambara-CRB/r2.10/bm_crb-ud-test.conllu",
1229	    },
1230	    "eu_bdt": {
1231	        "train": "UD_Basque-BDT/r2.10/eu_bdt-ud-train.conllu",
1232	        "dev": "UD_Basque-BDT/r2.10/eu_bdt-ud-dev.conllu",
1233	        "test": "UD_Basque-BDT/r2.10/eu_bdt-ud-test.conllu",
1234	    },
1235	    "bej_nsc": {
1236	        "test": "UD_Beja-NSC/r2.10/bej_nsc-ud-test.conllu",
1237	    },
1238	    "be_hse": {
1239	        "train": "UD_Belarusian-HSE/r2.10/be_hse-ud-train.conllu",
1240	        "dev": "UD_Belarusian-HSE/r2.10/be_hse-ud-dev.conllu",
1241	        "test": "UD_Belarusian-HSE/r2.10/be_hse-ud-test.conllu",
1242	    },
1243	    "bn_bru": {
1244	        "test": "UD_Bengali-BRU/r2.10/bn_bru-ud-test.conllu",
1245	    },
1246	    "bho_bhtb": {
1247	        "test": "UD_Bhojpuri-BHTB/r2.10/bho_bhtb-ud-test.conllu",
1248	    },
1249	    "br_keb": {
1250	        "test": "UD_Breton-KEB/r2.10/br_keb-ud-test.conllu",
1251	    },
1252	    "bg_btb": {
1253	        "train": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-train.conllu",
1254	        "dev": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-dev.conllu",
1255	        "test": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-test.conllu",
1256	    },
1257	    "bxr_bdt": {
1258	        "train": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-train.conllu",
1259	        "test": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-test.conllu",
1260	    },
1261	    "yue_hk": {
1262	        "test": "UD_Cantonese-HK/r2.10/yue_hk-ud-test.conllu",
1263	    },
1264	    "ca_ancora": {
1265	        "train": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-train.conllu",
1266	        "dev": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-dev.conllu",
1267	        "test": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-test.conllu",
1268	    },
1269	    "ceb_gja": {
1270	        "test": "UD_Cebuano-GJA/r2.10/ceb_gja-ud-test.conllu",
1271	    },
1272	    "zh_cfl": {
1273	        "test": "UD_Chinese-CFL/r2.10/zh_cfl-ud-test.conllu",
1274	    },
1275	    "zh_gsd": {
1276	        "train": "UD_Chinese-GSD/r2.10/zh_gsd-ud-train.conllu",
1277	        "dev": "UD_Chinese-GSD/r2.10/zh_gsd-ud-dev.conllu",
1278	        "test": "UD_Chinese-GSD/r2.10/zh_gsd-ud-test.conllu",
1279	    },
1280	    "zh_gsdsimp": {
1281	        "train": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-train.conllu",
1282	        "dev": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-dev.conllu",
1283	        "test": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-test.conllu",
1284	    },
1285	    "zh_hk": {
1286	        "test": "UD_Chinese-HK/r2.10/zh_hk-ud-test.conllu",
1287	    },
1288	    "zh_pud": {
1289	        "test": "UD_Chinese-PUD/r2.10/zh_pud-ud-test.conllu",
1290	    },
1291	    "ckt_hse": {
1292	        "test": "UD_Chukchi-HSE/r2.10/ckt_hse-ud-test.conllu",
1293	    },
1294	    "lzh_kyoto": {
1295	        "train": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-train.conllu",
1296	        "dev": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-dev.conllu",
1297	        "test": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-test.conllu",
1298	    },
1299	    "cop_scriptorium": {
1300	        "train": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-train.conllu",
1301	        "dev": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-dev.conllu",
1302	        "test": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-test.conllu",
1303	    },
1304	    "hr_set": {
1305	        "train": "UD_Croatian-SET/r2.10/hr_set-ud-train.conllu",
1306	        "dev": "UD_Croatian-SET/r2.10/hr_set-ud-dev.conllu",
1307	        "test": "UD_Croatian-SET/r2.10/hr_set-ud-test.conllu",
1308	    },
1309	    "cs_cac": {
1310	        "train": "UD_Czech-CAC/r2.10/cs_cac-ud-train.conllu",
1311	        "dev": "UD_Czech-CAC/r2.10/cs_cac-ud-dev.conllu",
1312	        "test": "UD_Czech-CAC/r2.10/cs_cac-ud-test.conllu",
1313	    },
1314	    "cs_cltt": {
1315	        "train": "UD_Czech-CLTT/r2.10/cs_cltt-ud-train.conllu",
1316	        "dev": "UD_Czech-CLTT/r2.10/cs_cltt-ud-dev.conllu",
1317	        "test": "UD_Czech-CLTT/r2.10/cs_cltt-ud-test.conllu",
1318	    },
1319	    "cs_fictree": {
1320	        "train": "UD_Czech-FicTree/r2.10/cs_fictree-ud-train.conllu",
1321	        "dev": "UD_Czech-FicTree/r2.10/cs_fictree-ud-dev.conllu",
1322	        "test": "UD_Czech-FicTree/r2.10/cs_fictree-ud-test.conllu",
1323	    },
1324	    "cs_pdt": {
1325	        "train": [
1326	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-l.conllu",
1327	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-m.conllu",
1328	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-c.conllu",
1329	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-v.conllu",
1330	        ],
1331	        "dev": "UD_Czech-PDT/r2.10/cs_pdt-ud-dev.conllu",
1332	        "test": "UD_Czech-PDT/r2.10/cs_pdt-ud-test.conllu",
1333	    },
1334	    "cs_pud": {
1335	        "test": "UD_Czech-PUD/r2.10/cs_pud-ud-test.conllu",
1336	    },
1337	    "da_ddt": {
1338	        "train": "UD_Danish-DDT/r2.10/da_ddt-ud-train.conllu",
1339	        "dev": "UD_Danish-DDT/r2.10/da_ddt-ud-dev.conllu",
1340	        "test": "UD_Danish-DDT/r2.10/da_ddt-ud-test.conllu",
1341	    },
1342	    "nl_alpino": {
1343	        "train": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-train.conllu",
1344	        "dev": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-dev.conllu",
1345	        "test": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-test.conllu",
1346	    },
1347	    "nl_lassysmall": {
1348	        "train": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-train.conllu",
1349	        "dev": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-dev.conllu",
1350	        "test": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-test.conllu",
1351	    },
1352	    "en_atis": {
1353	        "train": "UD_English-Atis/r2.10/en_atis-ud-train.conllu",
1354	        "dev": "UD_English-Atis/r2.10/en_atis-ud-dev.conllu",
1355	        "test": "UD_English-Atis/r2.10/en_atis-ud-test.conllu",
1356	    },
1357	    "en_esl": {
1358	        "train": "UD_English-ESL/r2.10/en_esl-ud-train.conllu",
1359	        "dev": "UD_English-ESL/r2.10/en_esl-ud-dev.conllu",
1360	        "test": "UD_English-ESL/r2.10/en_esl-ud-test.conllu",
1361	    },
1362	    "en_ewt": {
1363	        "train": "UD_English-EWT/r2.10/en_ewt-ud-train.conllu",
1364	        "dev": "UD_English-EWT/r2.10/en_ewt-ud-dev.conllu",
1365	        "test": "UD_English-EWT/r2.10/en_ewt-ud-test.conllu",
1366	    },
1367	    "en_gum": {
1368	        "train": "UD_English-GUM/r2.10/en_gum-ud-train.conllu",
1369	        "dev": "UD_English-GUM/r2.10/en_gum-ud-dev.conllu",
1370	        "test": "UD_English-GUM/r2.10/en_gum-ud-test.conllu",
1371	    },
1372	    "en_gumreddit": {
1373	        "train": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-train.conllu",
1374	        "dev": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-dev.conllu",
1375	        "test": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-test.conllu",
1376	    },
1377	    "en_lines": {
1378	        "train": "UD_English-LinES/r2.10/en_lines-ud-train.conllu",
1379	        "dev": "UD_English-LinES/r2.10/en_lines-ud-dev.conllu",
1380	        "test": "UD_English-LinES/r2.10/en_lines-ud-test.conllu",
1381	    },
1382	    "en_partut": {
1383	        "train": "UD_English-ParTUT/r2.10/en_partut-ud-train.conllu",
1384	        "dev": "UD_English-ParTUT/r2.10/en_partut-ud-dev.conllu",
1385	        "test": "UD_English-ParTUT/r2.10/en_partut-ud-test.conllu",
1386	    },
1387	    "en_pronouns": {
1388	        "test": "UD_English-Pronouns/r2.10/en_pronouns-ud-test.conllu",
1389	    },
1390	    "en_pud": {
1391	        "test": "UD_English-PUD/r2.10/en_pud-ud-test.conllu",
1392	    },
1393	    "myv_jr": {
1394	        "test": "UD_Erzya-JR/r2.10/myv_jr-ud-test.conllu",
1395	    },
1396	    "et_edt": {
1397	        "train": "UD_Estonian-EDT/r2.10/et_edt-ud-train.conllu",
1398	        "dev": "UD_Estonian-EDT/r2.10/et_edt-ud-dev.conllu",
1399	        "test": "UD_Estonian-EDT/r2.10/et_edt-ud-test.conllu",
1400	    },
1401	    "et_ewt": {
1402	        "train": "UD_Estonian-EWT/r2.10/et_ewt-ud-train.conllu",
1403	        "dev": "UD_Estonian-EWT/r2.10/et_ewt-ud-dev.conllu",
1404	        "test": "UD_Estonian-EWT/r2.10/et_ewt-ud-test.conllu",
1405	    },
1406	    "fo_farpahc": {
1407	        "train": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-train.conllu",
1408	        "dev": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-dev.conllu",
1409	        "test": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-test.conllu",
1410	    },
1411	    "fo_oft": {
1412	        "test": "UD_Faroese-OFT/r2.10/fo_oft-ud-test.conllu",
1413	    },
1414	    "fi_ftb": {
1415	        "train": "UD_Finnish-FTB/r2.10/fi_ftb-ud-train.conllu",
1416	        "dev": "UD_Finnish-FTB/r2.10/fi_ftb-ud-dev.conllu",
1417	        "test": "UD_Finnish-FTB/r2.10/fi_ftb-ud-test.conllu",
1418	    },
1419	    "fi_ood": {
1420	        "test": "UD_Finnish-OOD/r2.10/fi_ood-ud-test.conllu",
1421	    },
1422	    "fi_pud": {
1423	        "test": "UD_Finnish-PUD/r2.10/fi_pud-ud-test.conllu",
1424	    },
1425	    "fi_tdt": {
1426	        "train": "UD_Finnish-TDT/r2.10/fi_tdt-ud-train.conllu",
1427	        "dev": "UD_Finnish-TDT/r2.10/fi_tdt-ud-dev.conllu",
1428	        "test": "UD_Finnish-TDT/r2.10/fi_tdt-ud-test.conllu",
1429	    },
1430	    "fr_parisstories": {
1431	        "train": "UD_French-ParisStories/r2.10/fr_parisstories-ud-train.conllu",
1432	        "test": "UD_French-ParisStories/r2.10/fr_parisstories-ud-test.conllu",
1433	    },
1434	    "fr_fqb": {
1435	        "test": "UD_French-FQB/r2.10/fr_fqb-ud-test.conllu",
1436	    },
1437	    "fr_rhapsodie": {
1438	        "train": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-train.conllu",
1439	        "dev": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-dev.conllu",
1440	        "test": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-test.conllu",
1441	    },
1442	    "fr_ftb": {
1443	        "train": "UD_French-FTB/r2.10/fr_ftb-ud-train.conllu",
1444	        "dev": "UD_French-FTB/r2.10/fr_ftb-ud-dev.conllu",
1445	        "test": "UD_French-FTB/r2.10/fr_ftb-ud-test.conllu",
1446	    },
1447	    "fr_gsd": {
1448	        "train": "UD_French-GSD/r2.10/fr_gsd-ud-train.conllu",
1449	        "dev": "UD_French-GSD/r2.10/fr_gsd-ud-dev.conllu",
1450	        "test": "UD_French-GSD/r2.10/fr_gsd-ud-test.conllu",
1451	    },
1452	    "fr_partut": {
1453	        "train": "UD_French-ParTUT/r2.10/fr_partut-ud-train.conllu",
1454	        "dev": "UD_French-ParTUT/r2.10/fr_partut-ud-dev.conllu",
1455	        "test": "UD_French-ParTUT/r2.10/fr_partut-ud-test.conllu",
1456	    },
1457	    "fr_pud": {
1458	        "test": "UD_French-PUD/r2.10/fr_pud-ud-test.conllu",
1459	    },
1460	    "fr_sequoia": {
1461	        "train": "UD_French-Sequoia/r2.10/fr_sequoia-ud-train.conllu",
1462	        "dev": "UD_French-Sequoia/r2.10/fr_sequoia-ud-dev.conllu",
1463	        "test": "UD_French-Sequoia/r2.10/fr_sequoia-ud-test.conllu",
1464	    },
1465	    "qfn_fame": {
1466	        "test": "UD_Frisian_Dutch-Fame/r2.10/qfn_fame-ud-test.conllu",
1467	    },
1468	    "gl_ctg": {
1469	        "train": "UD_Galician-CTG/r2.10/gl_ctg-ud-train.conllu",
1470	        "dev": "UD_Galician-CTG/r2.10/gl_ctg-ud-dev.conllu",
1471	        "test": "UD_Galician-CTG/r2.10/gl_ctg-ud-test.conllu",
1472	    },
1473	    "gl_treegal": {
1474	        "train": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-train.conllu",
1475	        "test": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-test.conllu",
1476	    },
1477	    "de_gsd": {
1478	        "train": "UD_German-GSD/r2.10/de_gsd-ud-train.conllu",
1479	        "dev": "UD_German-GSD/r2.10/de_gsd-ud-dev.conllu",
1480	        "test": "UD_German-GSD/r2.10/de_gsd-ud-test.conllu",
1481	    },
1482	    "de_hdt": {
1483	        "train": [
1484	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-1.conllu",
1485	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-2.conllu",
1486	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-1.conllu",
1487	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-2.conllu",
1488	        ],
1489	        "dev": "UD_German-HDT/r2.10/de_hdt-ud-dev.conllu",
1490	        "test": "UD_German-HDT/r2.10/de_hdt-ud-test.conllu",
1491	    },
1492	    "de_lit": {
1493	        "test": "UD_German-LIT/r2.10/de_lit-ud-test.conllu",
1494	    },
1495	    "de_pud": {
1496	        "test": "UD_German-PUD/r2.10/de_pud-ud-test.conllu",
1497	    },
1498	    "got_proiel": {
1499	        "train": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-train.conllu",
1500	        "dev": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-dev.conllu",
1501	        "test": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-test.conllu",
1502	    },
1503	    "el_gdt": {
1504	        "train": "UD_Greek-GDT/r2.10/el_gdt-ud-train.conllu",
1505	        "dev": "UD_Greek-GDT/r2.10/el_gdt-ud-dev.conllu",
1506	        "test": "UD_Greek-GDT/r2.10/el_gdt-ud-test.conllu",
1507	    },
1508	    "gub_tudet": {
1509	        "test": "UD_Guajajara-TuDeT/r2.10/gub_tudet-ud-test.conllu",
1510	    },
1511	    "gn_oldtudet": {
1512	        "test": "UD_Guarani-OldTuDeT/r2.10/gn_oldtudet-ud-test.conllu",
1513	    },
1514	    "he_iahltwiki": {
1515	        "train": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-train.conllu",
1516	        "dev": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-dev.conllu",
1517	        "test": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-test.conllu",
1518	    },
1519	    "he_htb": {
1520	        "train": "UD_Hebrew-HTB/r2.10/he_htb-ud-train.conllu",
1521	        "dev": "UD_Hebrew-HTB/r2.10/he_htb-ud-dev.conllu",
1522	        "test": "UD_Hebrew-HTB/r2.10/he_htb-ud-test.conllu",
1523	    },
1524	    "qhe_hiencs": {
1525	        "train": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-train.conllu",
1526	        "dev": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-dev.conllu",
1527	        "test": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-test.conllu",
1528	    },
1529	    "hi_hdtb": {
1530	        "train": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-train.conllu",
1531	        "dev": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-dev.conllu",
1532	        "test": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-test.conllu",
1533	    },
1534	    "hi_pud": {
1535	        "test": "UD_Hindi-PUD/r2.10/hi_pud-ud-test.conllu",
1536	    },
1537	    "hu_szeged": {
1538	        "train": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-train.conllu",
1539	        "dev": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-dev.conllu",
1540	        "test": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-test.conllu",
1541	    },
1542	    "is_modern": {
1543	        "train": "UD_Icelandic-Modern/r2.10/is_modern-ud-train.conllu",
1544	        "dev": "UD_Icelandic-Modern/r2.10/is_modern-ud-dev.conllu",
1545	        "test": "UD_Icelandic-Modern/r2.10/is_modern-ud-test.conllu",
1546	    },
1547	    "is_icepahc": {
1548	        "train": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-train.conllu",
1549	        "dev": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-dev.conllu",
1550	        "test": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-test.conllu",
1551	    },
1552	    "is_pud": {
1553	        "test": "UD_Icelandic-PUD/r2.10/is_pud-ud-test.conllu",
1554	    },
1555	    "id_csui": {
1556	        "train": "UD_Indonesian-CSUI/r2.10/id_csui-ud-train.conllu",
1557	        "test": "UD_Indonesian-CSUI/r2.10/id_csui-ud-test.conllu",
1558	    },
1559	    "id_gsd": {
1560	        "train": "UD_Indonesian-GSD/r2.10/id_gsd-ud-train.conllu",
1561	        "dev": "UD_Indonesian-GSD/r2.10/id_gsd-ud-dev.conllu",
1562	        "test": "UD_Indonesian-GSD/r2.10/id_gsd-ud-test.conllu",
1563	    },
1564	    "id_pud": {
1565	        "test": "UD_Indonesian-PUD/r2.10/id_pud-ud-test.conllu",
1566	    },
1567	    "ga_twittirish": {
1568	        "test": "UD_Irish-TwittIrish/r2.10/ga_twittirish-ud-test.conllu",
1569	    },
1570	    "ga_idt": {
1571	        "train": "UD_Irish-IDT/r2.10/ga_idt-ud-train.conllu",
1572	        "dev": "UD_Irish-IDT/r2.10/ga_idt-ud-dev.conllu",
1573	        "test": "UD_Irish-IDT/r2.10/ga_idt-ud-test.conllu",
1574	    },
1575	    "it_valico": {
1576	        "test": "UD_Italian-Valico/r2.10/it_valico-ud-test.conllu",
1577	    },
1578	    "it_markit": {
1579	        "train": "UD_Italian-MarkIT/r2.10/it_markit-ud-train.conllu",
1580	        "dev": "UD_Italian-MarkIT/r2.10/it_markit-ud-dev.conllu",
1581	        "test": "UD_Italian-MarkIT/r2.10/it_markit-ud-test.conllu",
1582	    },
1583	    "it_isdt": {
1584	        "train": "UD_Italian-ISDT/r2.10/it_isdt-ud-train.conllu",
1585	        "dev": "UD_Italian-ISDT/r2.10/it_isdt-ud-dev.conllu",
1586	        "test": "UD_Italian-ISDT/r2.10/it_isdt-ud-test.conllu",
1587	    },
1588	    "it_partut": {
1589	        "train": "UD_Italian-ParTUT/r2.10/it_partut-ud-train.conllu",
1590	        "dev": "UD_Italian-ParTUT/r2.10/it_partut-ud-dev.conllu",
1591	        "test": "UD_Italian-ParTUT/r2.10/it_partut-ud-test.conllu",
1592	    },
1593	    "it_postwita": {
1594	        "train": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-train.conllu",
1595	        "dev": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-dev.conllu",
1596	        "test": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-test.conllu",
1597	    },
1598	    "it_pud": {
1599	        "test": "UD_Italian-PUD/r2.10/it_pud-ud-test.conllu",
1600	    },
1601	    "it_twittiro": {
1602	        "train": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-train.conllu",
1603	        "dev": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-dev.conllu",
1604	        "test": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-test.conllu",
1605	    },
1606	    "it_vit": {
1607	        "train": "UD_Italian-VIT/r2.10/it_vit-ud-train.conllu",
1608	        "dev": "UD_Italian-VIT/r2.10/it_vit-ud-dev.conllu",
1609	        "test": "UD_Italian-VIT/r2.10/it_vit-ud-test.conllu",
1610	    },
1611	    "ja_gsdluw": {
1612	        "train": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-train.conllu",
1613	        "dev": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-dev.conllu",
1614	        "test": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-test.conllu",
1615	    },
1616	    "ja_pudluw": {
1617	        "test": "UD_Japanese-PUDLUW/r2.10/ja_pudluw-ud-test.conllu",
1618	    },
1619	    "ja_bccwjluw": {
1620	        "train": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-train.conllu",
1621	        "dev": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-dev.conllu",
1622	        "test": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-test.conllu",
1623	    },
1624	    "ja_bccwj": {
1625	        "train": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-train.conllu",
1626	        "dev": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-dev.conllu",
1627	        "test": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-test.conllu",
1628	    },
1629	    "ja_gsd": {
1630	        "train": "UD_Japanese-GSD/r2.10/ja_gsd-ud-train.conllu",
1631	        "dev": "UD_Japanese-GSD/r2.10/ja_gsd-ud-dev.conllu",
1632	        "test": "UD_Japanese-GSD/r2.10/ja_gsd-ud-test.conllu",
1633	    },
1634	    "ja_modern": {
1635	        "test": "UD_Japanese-Modern/r2.10/ja_modern-ud-test.conllu",
1636	    },
1637	    "ja_pud": {
1638	        "test": "UD_Japanese-PUD/r2.10/ja_pud-ud-test.conllu",
1639	    },
1640	    "jv_csui": {
1641	        "test": "UD_Javanese-CSUI/r2.10/jv_csui-ud-test.conllu",
1642	    },
1643	    "urb_tudet": {
1644	        "test": "UD_Kaapor-TuDeT/r2.10/urb_tudet-ud-test.conllu",
1645	    },
1646	    "xnr_kdtb": {
1647	        "test": "UD_Kangri-KDTB/r2.10/xnr_kdtb-ud-test.conllu",
1648	    },
1649	    "krl_kkpp": {
1650	        "test": "UD_Karelian-KKPP/r2.10/krl_kkpp-ud-test.conllu",
1651	    },
1652	    "arr_tudet": {
1653	        "test": "UD_Karo-TuDeT/r2.10/arr_tudet-ud-test.conllu",
1654	    },
1655	    "kk_ktb": {
1656	        "train": "UD_Kazakh-KTB/r2.10/kk_ktb-ud-train.conllu",
1657	        "test": "UD_Kazakh-KTB/r2.10/kk_ktb-ud-test.conllu",
1658	    },
1659	    "kfm_aha": {
1660	        "test": "UD_Khunsari-AHA/r2.10/kfm_aha-ud-test.conllu",
1661	    },
1662	    "quc_iu": {
1663	        "test": "UD_Kiche-IU/r2.10/quc_iu-ud-test.conllu",
1664	    },
1665	    "koi_uh": {
1666	        "test": "UD_Komi_Permyak-UH/r2.10/koi_uh-ud-test.conllu",
1667	    },
1668	    "kpv_ikdp": {
1669	        "test": "UD_Komi_Zyrian-IKDP/r2.10/kpv_ikdp-ud-test.conllu",
1670	    },
1671	    "kpv_lattice": {
1672	        "test": "UD_Komi_Zyrian-Lattice/r2.10/kpv_lattice-ud-test.conllu",
1673	    },
1674	    "ko_gsd": {
1675	        "train": "UD_Korean-GSD/r2.10/ko_gsd-ud-train.conllu",
1676	        "dev": "UD_Korean-GSD/r2.10/ko_gsd-ud-dev.conllu",
1677	        "test": "UD_Korean-GSD/r2.10/ko_gsd-ud-test.conllu",
1678	    },
1679	    "ko_kaist": {
1680	        "train": "UD_Korean-Kaist/r2.10/ko_kaist-ud-train.conllu",
1681	        "dev": "UD_Korean-Kaist/r2.10/ko_kaist-ud-dev.conllu",
1682	        "test": "UD_Korean-Kaist/r2.10/ko_kaist-ud-test.conllu",
1683	    },
1684	    "ko_pud": {
1685	        "test": "UD_Korean-PUD/r2.10/ko_pud-ud-test.conllu",
1686	    },
1687	    "kmr_mg": {
1688	        "train": "UD_Kurmanji-MG/r2.10/kmr_mg-ud-train.conllu",
1689	        "test": "UD_Kurmanji-MG/r2.10/kmr_mg-ud-test.conllu",
1690	    },
1691	    "la_udante": {
1692	        "train": "UD_Latin-UDante/r2.10/la_udante-ud-train.conllu",
1693	        "dev": "UD_Latin-UDante/r2.10/la_udante-ud-dev.conllu",
1694	        "test": "UD_Latin-UDante/r2.10/la_udante-ud-test.conllu",
1695	    },
1696	    "la_ittb": {
1697	        "train": "UD_Latin-ITTB/r2.10/la_ittb-ud-train.conllu",
1698	        "dev": "UD_Latin-ITTB/r2.10/la_ittb-ud-dev.conllu",
1699	        "test": "UD_Latin-ITTB/r2.10/la_ittb-ud-test.conllu",
1700	    },
1701	    "la_llct": {
1702	        "train": "UD_Latin-LLCT/r2.10/la_llct-ud-train.conllu",
1703	        "dev": "UD_Latin-LLCT/r2.10/la_llct-ud-dev.conllu",
1704	        "test": "UD_Latin-LLCT/r2.10/la_llct-ud-test.conllu",
1705	    },
1706	    "la_perseus": {
1707	        "train": "UD_Latin-Perseus/r2.10/la_perseus-ud-train.conllu",
1708	        "test": "UD_Latin-Perseus/r2.10/la_perseus-ud-test.conllu",
1709	    },
1710	    "la_proiel": {
1711	        "train": "UD_Latin-PROIEL/r2.10/la_proiel-ud-train.conllu",
1712	        "dev": "UD_Latin-PROIEL/r2.10/la_proiel-ud-dev.conllu",
1713	        "test": "UD_Latin-PROIEL/r2.10/la_proiel-ud-test.conllu",
1714	    },
1715	    "lv_lvtb": {
1716	        "train": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-train.conllu",
1717	        "dev": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-dev.conllu",
1718	        "test": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-test.conllu",
1719	    },
1720	    "lij_glt": {
1721	        "train": "UD_Ligurian-GLT/r2.10/lij_glt-ud-train.conllu",
1722	        "test": "UD_Ligurian-GLT/r2.10/lij_glt-ud-test.conllu",
1723	    },
1724	    "lt_alksnis": {
1725	        "train": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-train.conllu",
1726	        "dev": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-dev.conllu",
1727	        "test": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-test.conllu",
1728	    },
1729	    "lt_hse": {
1730	        "train": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1731	        "dev": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1732	        "test": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1733	    },
1734	    "olo_kkpp": {
1735	        "train": "UD_Livvi-KKPP/r2.10/olo_kkpp-ud-train.conllu",
1736	        "test": "UD_Livvi-KKPP/r2.10/olo_kkpp-ud-test.conllu",
1737	    },
1738	    "nds_lsdc": {
1739	        "test": "UD_Low_Saxon-LSDC/r2.10/nds_lsdc-ud-test.conllu",
1740	    },
1741	    # TODO(tfds) Add Madi Jarawara splits when it will be officially released.
1742	    # TODO(tfds) Add Makurap TuDeT splits when it will be officially released.
1743	    "mt_mudt": {
1744	        "train": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-train.conllu",
1745	        "dev": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-dev.conllu",
1746	        "test": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-test.conllu",
1747	    },
1748	    "gv_cadhan": {
1749	        "test": "UD_Manx-Cadhan/r2.10/gv_cadhan-ud-test.conllu",
1750	    },
1751	    "mr_ufal": {
1752	        "train": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-train.conllu",
1753	        "dev": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-dev.conllu",
1754	        "test": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-test.conllu",
1755	    },
1756	    "gun_dooley": {
1757	        "test": "UD_Mbya_Guarani-Dooley/r2.10/gun_dooley-ud-test.conllu",
1758	    },
1759	    "gun_thomas": {
1760	        "test": "UD_Mbya_Guarani-Thomas/r2.10/gun_thomas-ud-test.conllu",
1761	    },
1762	    "mdf_jr": {
1763	        "test": "UD_Moksha-JR/r2.10/mdf_jr-ud-test.conllu",
1764	    },
1765	    "myu_tudet": {
1766	        "test": "UD_Munduruku-TuDeT/r2.10/myu_tudet-ud-test.conllu",
1767	    },
1768	    "pcm_nsc": {
1769	        "train": "UD_Naija-NSC/r2.10/pcm_nsc-ud-train.conllu",
1770	        "dev": "UD_Naija-NSC/r2.10/pcm_nsc-ud-dev.conllu",
1771	        "test": "UD_Naija-NSC/r2.10/pcm_nsc-ud-test.conllu",
1772	    },
1773	    "nyq_aha": {
1774	        "test": "UD_Nayini-AHA/r2.10/nyq_aha-ud-test.conllu",
1775	    },
1776	    # TODO(tfds) Add Neapolitan RB splits when it will be officially released.
1777	    "sme_giella": {
1778	        "train": "UD_North_Sami-Giella/r2.10/sme_giella-ud-train.conllu",
1779	        "test": "UD_North_Sami-Giella/r2.10/sme_giella-ud-test.conllu",
1780	    },
1781	    "no_bokmaal": {
1782	        "train": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-train.conllu",
1783	        "dev": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-dev.conllu",
1784	        "test": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-test.conllu",
1785	    },
1786	    "no_nynorsk": {
1787	        "train": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-train.conllu",
1788	        "dev": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-dev.conllu",
1789	        "test": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-test.conllu",
1790	    },
1791	    "no_nynorsklia": {
1792	        "train": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-train.conllu",
1793	        "dev": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-dev.conllu",
1794	        "test": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-test.conllu",
1795	    },
1796	    "cu_proiel": {
1797	        "train": (
1798	            "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-train.conllu"
1799	        ),
1800	        "dev": "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-dev.conllu",
1801	        "test": "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-test.conllu",
1802	    },
1803	    "fro_srcmf": {
1804	        "train": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-train.conllu",
1805	        "dev": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-dev.conllu",
1806	        "test": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-test.conllu",
1807	    },
1808	    "orv_birchbark": {
1809	        "train": (
1810	            "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-train.conllu"
1811	        ),
1812	        "dev": "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-dev.conllu",
1813	        "test": (
1814	            "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-test.conllu"
1815	        ),
1816	    },
1817	    "orv_rnc": {
1818	        "train": "UD_Old_Russian-RNC/r2.10/orv_rnc-ud-train.conllu",
1819	        "test": "UD_Old_Russian-RNC/r2.10/orv_rnc-ud-test.conllu",
1820	    },
1821	    "orv_torot": {
1822	        "train": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-train.conllu",
1823	        "dev": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-dev.conllu",
1824	        "test": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-test.conllu",
1825	    },
1826	    "otk_tonqq": {
1827	        "test": "UD_Old_Turkish-Tonqq/r2.10/otk_tonqq-ud-test.conllu",
1828	    },
1829	    "fa_perdt": {
1830	        "train": "UD_Persian-PerDT/r2.10/fa_perdt-ud-train.conllu",
1831	        "dev": "UD_Persian-PerDT/r2.10/fa_perdt-ud-dev.conllu",
1832	        "test": "UD_Persian-PerDT/r2.10/fa_perdt-ud-test.conllu",
1833	    },
1834	    "fa_seraji": {
1835	        "train": "UD_Persian-Seraji/r2.10/fa_seraji-ud-train.conllu",
1836	        "dev": "UD_Persian-Seraji/r2.10/fa_seraji-ud-dev.conllu",
1837	        "test": "UD_Persian-Seraji/r2.10/fa_seraji-ud-test.conllu",
1838	    },
1839	    "pl_lfg": {
1840	        "train": "UD_Polish-LFG/r2.10/pl_lfg-ud-train.conllu",
1841	        "dev": "UD_Polish-LFG/r2.10/pl_lfg-ud-dev.conllu",
1842	        "test": "UD_Polish-LFG/r2.10/pl_lfg-ud-test.conllu",
1843	    },
1844	    "pl_pdb": {
1845	        "train": "UD_Polish-PDB/r2.10/pl_pdb-ud-train.conllu",
1846	        "dev": "UD_Polish-PDB/r2.10/pl_pdb-ud-dev.conllu",
1847	        "test": "UD_Polish-PDB/r2.10/pl_pdb-ud-test.conllu",
1848	    },
1849	    "pl_pud": {
1850	        "test": "UD_Polish-PUD/r2.10/pl_pud-ud-test.conllu",
1851	    },
1852	    "pt_bosque": {
1853	        "train": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-train.conllu",
1854	        "dev": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-dev.conllu",
1855	        "test": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-test.conllu",
1856	    },
1857	    "pt_gsd": {
1858	        "train": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-train.conllu",
1859	        "dev": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-dev.conllu",
1860	        "test": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-test.conllu",
1861	    },
1862	    "pt_pud": {
1863	        "test": "UD_Portuguese-PUD/r2.10/pt_pud-ud-test.conllu",
1864	    },
1865	    "ro_art": {
1866	        "test": "UD_Romanian-ArT/r2.10/ro_art-ud-test.conllu",
1867	    },
1868	    "ro_nonstandard": {
1869	        "train": "UD_Romanian-Nonstandard/r2.10/ro_nonstandard-ud-train.conllu",
1870	        "dev": "UD_Romanian-Nonstandard/r2.10/ro_nonstandard-ud-dev.conllu",
1871	        "test": "UD_Romanian-Nonstandard/r2.10/ro_nonstandard-ud-test.conllu",
1872	    },
1873	    "ro_rrt": {
1874	        "train": "UD_Romanian-RRT/r2.10/ro_rrt-ud-train.conllu",
1875	        "dev": "UD_Romanian-RRT/r2.10/ro_rrt-ud-dev.conllu",
1876	        "test": "UD_Romanian-RRT/r2.10/ro_rrt-ud-test.conllu",
1877	    },
1878	    "ro_simonero": {
1879	        "train": "UD_Romanian-SiMoNERo/r2.10/ro_simonero-ud-train.conllu",
1880	        "dev": "UD_Romanian-SiMoNERo/r2.10/ro_simonero-ud-dev.conllu",
1881	        "test": "UD_Romanian-SiMoNERo/r2.10/ro_simonero-ud-test.conllu",
1882	    },
1883	    "ru_gsd": {
1884	        "train": "UD_Russian-GSD/r2.10/ru_gsd-ud-train.conllu",
1885	        "dev": "UD_Russian-GSD/r2.10/ru_gsd-ud-dev.conllu",
1886	        "test": "UD_Russian-GSD/r2.10/ru_gsd-ud-test.conllu",
1887	    },
1888	    "ru_pud": {
1889	        "test": "UD_Russian-PUD/r2.10/ru_pud-ud-test.conllu",
1890	    },
1891	    "ru_syntagrus": {
1892	        "train": "UD_Russian-SynTagRus/r2.7/ru_syntagrus-ud-train.conllu",
1893	        "dev": "UD_Russian-SynTagRus/r2.7/ru_syntagrus-ud-dev.conllu",
1894	        "test": "UD_Russian-SynTagRus/r2.7/ru_syntagrus-ud-test.conllu",
1895	    },
1896	    "ru_taiga": {
1897	        "train": "UD_Russian-Taiga/r2.10/ru_taiga-ud-train.conllu",
1898	        "dev": "UD_Russian-Taiga/r2.10/ru_taiga-ud-dev.conllu",
1899	        "test": "UD_Russian-Taiga/r2.10/ru_taiga-ud-test.conllu",
1900	    },
1901	    "sa_ufal": {
1902	        "test": "UD_Sanskrit-UFAL/r2.10/sa_ufal-ud-test.conllu",
1903	    },
1904	    "sa_vedic": {
1905	        "train": "UD_Sanskrit-Vedic/r2.10/sa_vedic-ud-train.conllu",
1906	        "test": "UD_Sanskrit-Vedic/r2.10/sa_vedic-ud-test.conllu",
1907	    },
1908	    "gd_arcosg": {
1909	        "train": "UD_Scottish_Gaelic-ARCOSG/r2.10/gd_arcosg-ud-train.conllu",
1910	        "dev": "UD_Scottish_Gaelic-ARCOSG/r2.10/gd_arcosg-ud-dev.conllu",
1911	        "test": "UD_Scottish_Gaelic-ARCOSG/r2.10/gd_arcosg-ud-test.conllu",
1912	    },
1913	    "sr_set": {
1914	        "train": "UD_Serbian-SET/r2.10/sr_set-ud-train.conllu",
1915	        "dev": "UD_Serbian-SET/r2.10/sr_set-ud-dev.conllu",
1916	        "test": "UD_Serbian-SET/r2.10/sr_set-ud-test.conllu",
1917	    },
1918	    "sms_giellagas": {
1919	        "test": "UD_Skolt_Sami-Giellagas/r2.10/sms_giellagas-ud-test.conllu",
1920	    },
1921	    "sk_snk": {
1922	        "train": "UD_Slovak-SNK/r2.10/sk_snk-ud-train.conllu",
1923	        "dev": "UD_Slovak-SNK/r2.10/sk_snk-ud-dev.conllu",
1924	        "test": "UD_Slovak-SNK/r2.10/sk_snk-ud-test.conllu",
1925	    },
1926	    "sl_ssj": {
1927	        "train": "UD_Slovenian-SSJ/r2.10/sl_ssj-ud-train.conllu",
1928	        "dev": "UD_Slovenian-SSJ/r2.10/sl_ssj-ud-dev.conllu",
1929	        "test": "UD_Slovenian-SSJ/r2.10/sl_ssj-ud-test.conllu",
1930	    },
1931	    "sl_sst": {
1932	        "train": "UD_Slovenian-SST/r2.10/sl_sst-ud-train.conllu",
1933	        "test": "UD_Slovenian-SST/r2.10/sl_sst-ud-test.conllu",
1934	    },
1935	    "soj_aha": {
1936	        "test": "UD_Soi-AHA/r2.10/soj_aha-ud-test.conllu",
1937	    },
1938	    "ajp_madar": {
1939	        "test": (
1940	            "UD_South_Levantine_Arabic-MADAR/r2.10/ajp_madar-ud-test.conllu"
1941	        ),
1942	    },
1943	    "es_ancora": {
1944	        "train": "UD_Spanish-AnCora/r2.10/es_ancora-ud-train.conllu",
1945	        "dev": "UD_Spanish-AnCora/r2.10/es_ancora-ud-dev.conllu",
1946	        "test": "UD_Spanish-AnCora/r2.10/es_ancora-ud-test.conllu",
1947	    },
1948	    "es_gsd": {
1949	        "train": "UD_Spanish-GSD/r2.10/es_gsd-ud-train.conllu",
1950	        "dev": "UD_Spanish-GSD/r2.10/es_gsd-ud-dev.conllu",
1951	        "test": "UD_Spanish-GSD/r2.10/es_gsd-ud-test.conllu",
1952	    },
1953	    "es_pud": {
1954	        "test": "UD_Spanish-PUD/r2.10/es_pud-ud-test.conllu",
1955	    },
1956	    "swl_sslc": {
1957	        "train": "UD_Swedish_Sign_Language-SSLC/r2.10/swl_sslc-ud-train.conllu",
1958	        "dev": "UD_Swedish_Sign_Language-SSLC/r2.10/swl_sslc-ud-dev.conllu",
1959	        "test": "UD_Swedish_Sign_Language-SSLC/r2.10/swl_sslc-ud-test.conllu",
1960	    },
1961	    "sv_lines": {
1962	        "train": "UD_Swedish-LinES/r2.10/sv_lines-ud-train.conllu",
1963	        "dev": "UD_Swedish-LinES/r2.10/sv_lines-ud-dev.conllu",
1964	        "test": "UD_Swedish-LinES/r2.10/sv_lines-ud-test.conllu",
1965	    },
1966	    "sv_pud": {
1967	        "test": "UD_Swedish-PUD/r2.10/sv_pud-ud-test.conllu",
1968	    },
1969	    "sv_talbanken": {
1970	        "train": "UD_Swedish-Talbanken/r2.10/sv_talbanken-ud-train.conllu",
1971	        "dev": "UD_Swedish-Talbanken/r2.10/sv_talbanken-ud-dev.conllu",
1972	        "test": "UD_Swedish-Talbanken/r2.10/sv_talbanken-ud-test.conllu",
1973	    },
1974	    "gsw_uzh": {
1975	        "test": "UD_Swiss_German-UZH/r2.10/gsw_uzh-ud-test.conllu",
1976	    },
1977	    "tl_trg": {
1978	        "test": "UD_Tagalog-TRG/r2.10/tl_trg-ud-test.conllu",
1979	    },
1980	    "tl_ugnayan": {
1981	        "test": "UD_Tagalog-Ugnayan/r2.10/tl_ugnayan-ud-test.conllu",
1982	    },
1983	    "ta_mwtt": {
1984	        "test": "UD_Tamil-MWTT/r2.10/ta_mwtt-ud-test.conllu",
1985	    },
1986	    "ta_ttb": {
1987	        "train": "UD_Tamil-TTB/r2.10/ta_ttb-ud-train.conllu",
1988	        "dev": "UD_Tamil-TTB/r2.10/ta_ttb-ud-dev.conllu",
1989	        "test": "UD_Tamil-TTB/r2.10/ta_ttb-ud-test.conllu",
1990	    },
1991	    "te_mtg": {
1992	        "train": "UD_Telugu-MTG/r2.10/te_mtg-ud-train.conllu",
1993	        "dev": "UD_Telugu-MTG/r2.10/te_mtg-ud-dev.conllu",
1994	        "test": "UD_Telugu-MTG/r2.10/te_mtg-ud-test.conllu",
1995	    },
1996	    "th_pud": {
1997	        "test": "UD_Thai-PUD/r2.10/th_pud-ud-test.conllu",
1998	    },
1999	    "tpn_tudet": {
2000	        "test": "UD_Tupinamba-TuDeT/r2.10/tpn_tudet-ud-test.conllu",
2001	    },
2002	    "qtd_sagt": {
2003	        "train": "UD_Turkish_German-SAGT/r2.10/qtd_sagt-ud-train.conllu",
2004	        "dev": "UD_Turkish_German-SAGT/r2.10/qtd_sagt-ud-dev.conllu",
2005	        "test": "UD_Turkish_German-SAGT/r2.10/qtd_sagt-ud-test.conllu",
2006	    },
2007	    "tr_kenet": {
2008	        "train": "UD_Turkish-Kenet/r2.10/tr_kenet-ud-train.conllu",
2009	        "dev": "UD_Turkish-Kenet/r2.10/tr_kenet-ud-dev.conllu",
2010	        "test": "UD_Turkish-Kenet/r2.10/tr_kenet-ud-test.conllu",
2011	    },
2012	    "tr_atis": {
2013	        "train": "UD_Turkish-Atis/r2.10/tr_atis-ud-train.conllu",
2014	        "dev": "UD_Turkish-Atis/r2.10/tr_atis-ud-dev.conllu",
2015	        "test": "UD_Turkish-Atis/r2.10/tr_atis-ud-test.conllu",
2016	    },
2017	    "tr_penn": {
2018	        "train": "UD_Turkish-Penn/r2.10/tr_penn-ud-train.conllu",
2019	        "dev": "UD_Turkish-Penn/r2.10/tr_penn-ud-dev.conllu",
2020	        "test": "UD_Turkish-Penn/r2.10/tr_penn-ud-test.conllu",
2021	    },
2022	    "tr_tourism": {
2023	        "train": "UD_Turkish-Tourism/r2.10/tr_tourism-ud-train.conllu",
2024	        "dev": "UD_Turkish-Tourism/r2.10/tr_tourism-ud-dev.conllu",
2025	        "test": "UD_Turkish-Tourism/r2.10/tr_tourism-ud-test.conllu",
2026	    },
2027	    "tr_framenet": {
2028	        "train": "UD_Turkish-FrameNet/r2.10/tr_framenet-ud-train.conllu",
2029	        "dev": "UD_Turkish-FrameNet/r2.10/tr_framenet-ud-dev.conllu",
2030	        "test": "UD_Turkish-FrameNet/r2.10/tr_framenet-ud-test.conllu",
2031	    },
2032	    "tr_boun": {
2033	        "train": "UD_Turkish-BOUN/r2.10/tr_boun-ud-train.conllu",
2034	        "dev": "UD_Turkish-BOUN/r2.10/tr_boun-ud-dev.conllu",
2035	        "test": "UD_Turkish-BOUN/r2.10/tr_boun-ud-test.conllu",
2036	    },
2037	    "tr_gb": {
2038	        "test": "UD_Turkish-GB/r2.10/tr_gb-ud-test.conllu",
2039	    },
2040	    "tr_imst": {
2041	        "train": "UD_Turkish-IMST/r2.10/tr_imst-ud-train.conllu",
2042	        "dev": "UD_Turkish-IMST/r2.10/tr_imst-ud-dev.conllu",
2043	        "test": "UD_Turkish-IMST/r2.10/tr_imst-ud-test.conllu",
2044	    },
2045	    "tr_pud": {
2046	        "test": "UD_Turkish-PUD/r2.10/tr_pud-ud-test.conllu",
2047	    },
2048	    "uk_iu": {
2049	        "train": "UD_Ukrainian-IU/r2.10/uk_iu-ud-train.conllu",
2050	        "dev": "UD_Ukrainian-IU/r2.10/uk_iu-ud-dev.conllu",
2051	        "test": "UD_Ukrainian-IU/r2.10/uk_iu-ud-test.conllu",
2052	    },
2053	    # TODO(tfds) Add Umbrian Ikuvina split when it will be officially released.
2054	    "hsb_ufal": {
2055	        "train": "UD_Upper_Sorbian-UFAL/r2.10/hsb_ufal-ud-train.conllu",
2056	        "test": "UD_Upper_Sorbian-UFAL/r2.10/hsb_ufal-ud-test.conllu",
2057	    },
2058	    "ur_udtb": {
2059	        "train": "UD_Urdu-UDTB/r2.10/ur_udtb-ud-train.conllu",
2060	        "dev": "UD_Urdu-UDTB/r2.10/ur_udtb-ud-dev.conllu",
2061	        "test": "UD_Urdu-UDTB/r2.10/ur_udtb-ud-test.conllu",
2062	    },
2063	    "ug_udt": {
2064	        "train": "UD_Uyghur-UDT/r2.10/ug_udt-ud-train.conllu",
2065	        "dev": "UD_Uyghur-UDT/r2.10/ug_udt-ud-dev.conllu",
2066	        "test": "UD_Uyghur-UDT/r2.10/ug_udt-ud-test.conllu",
2067	    },
2068	    "vi_vtb": {
2069	        "train": "UD_Vietnamese-VTB/r2.10/vi_vtb-ud-train.conllu",
2070	        "dev": "UD_Vietnamese-VTB/r2.10/vi_vtb-ud-dev.conllu",
2071	        "test": "UD_Vietnamese-VTB/r2.10/vi_vtb-ud-test.conllu",
2072	    },
2073	    "wbp_ufal": {
2074	        "test": "UD_Warlpiri-UFAL/r2.10/wbp_ufal-ud-test.conllu",
2075	    },
2076	    "cy_ccg": {
2077	        "train": "UD_Welsh-CCG/r2.10/cy_ccg-ud-train.conllu",
2078	        "test": "UD_Welsh-CCG/r2.10/cy_ccg-ud-test.conllu",
2079	    },
2080	    "wo_wtb": {
2081	        "train": "UD_Wolof-WTB/r2.10/wo_wtb-ud-train.conllu",
2082	        "dev": "UD_Wolof-WTB/r2.10/wo_wtb-ud-dev.conllu",
2083	        "test": "UD_Wolof-WTB/r2.10/wo_wtb-ud-test.conllu",
2084	    },
2085	    "sjo_xdt": {
2086	        "test": "UD_Xibe-XDT/r2.10/sjo_xdt-ud-test.conllu",
2087	    },
2088	    "sah_yktdt": {
2089	        "test": "UD_Yakut-YKTDT/r2.10/sah_yktdt-ud-test.conllu",
2090	    },
2091	    "yo_ytb": {
2092	        "test": "UD_Yoruba-YTB/r2.10/yo_ytb-ud-test.conllu",
2093	    },
2094	    "ess_sli": {
2095	        "test": "UD_Yupik-SLI/r2.10/ess_sli-ud-test.conllu",
2096	    },
2097	}
2098	
2099	LANGS = DESCRIPTIONS.keys()
2100	
2101	
2102	def prepare_ud_filepaths(
2103	    path_prefix: epath.PathLike,
2104	    filepaths: Union[epath.PathLike, List[epath.PathLike]],
2105	) -> List[epath.PathLike]:
2106	  """Prepends a path prefix to a (list of) filepaths.
2107	
2108	  Args:
2109	    path_prefix: A path which will be prepended to all paths in filepaths.
2110	    filepaths: The filepaths to be prepared. Could be a list of paths for
2111	      multiple files, or a single path.
2112	
2113	  Returns:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:992
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
991	    "es_pud": (
992	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
993	        " created for the [CoNLL 2017 shared task on Multilingual Parsing from"
994	        " Raw Text to Universal "
995	        "Dependencies](http://universaldependencies.org/conll17/)."
996	    ),
997	    "swl_sslc": (
998	        "The Universal Dependencies treebank for Swedish Sign Language (ISO "
999	        "639-3: swl) is derived from the Swedish Sign Language Corpus (SSLC) "
1000	        "from the department of linguistics, Stockholm University."
1001	    ),
1002	    "sv_lines": (
1003	        "UD Swedish_LinES is the Swedish half of the LinES Parallel Treebank "
1004	        "with UD annotations. All segments are translations from English and "
1005	        "the sources cover literary genres, online manuals and Europarl data."
1006	    ),
1007	    "sv_pud": (
1008	        "Swedish-PUD is the Swedish part of the Parallel Universal "
1009	        "Dependencies (PUD) treebanks."
1010	    ),
1011	    "sv_talbanken": (
1012	        "The Swedish-Talbanken treebank is based on Talbanken, a treebank "
1013	        "developed at Lund University in the 1970s."
1014	    ),
1015	    "gsw_uzh": (
1016	        "_UD_Swiss_German-UZH_ is a tiny manually annotated treebank of 100 "
1017	        "sentences in different Swiss German dialects and a variety of text "
1018	        "genres."
1019	    ),
1020	    "tl_trg": (
1021	        "UD_Tagalog-TRG is a UD treebank manually annotated using sentences "
1022	        "from a grammar book."
1023	    ),
1024	    "tl_ugnayan": (
1025	        "Ugnayan is a manually annotated Tagalog treebank currently composed "
1026	        "of educational fiction and nonfiction text. The treebank is under "
1027	        "development at the University of the Philippines."
1028	    ),
1029	    "ta_mwtt": (
1030	        "MWTT - Modern Written Tamil Treebank has sentences taken primarily "
1031	        "from a text called 'A Grammar of Modern Tamil' by Thomas Lehmann "
1032	        "(1993). This initial release has 536 sentences of various lengths, "
1033	        "and all of these are added as the test set."
1034	    ),
1035	    "ta_ttb": (
1036	        "The UD Tamil treebank is based on the Tamil Dependency Treebank "
1037	        "created at the Charles University in Prague by Loganathan Ramasamy."
1038	    ),
1039	    "te_mtg": (
1040	        "The Telugu UD treebank is created in UD based on manual annotations "
1041	        "of sentences from a grammar book."
1042	    ),
1043	    "th_pud": (
1044	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1045	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1046	        "Raw Text to Universal Dependencies."
1047	    ),
1048	    "tpn_tudet": (
1049	        "UD_Tupinamba-TuDeT is a collection of annotated texts in Tupi(namb)."
1050	        " Together with UD_Akuntsu-TuDeT and UD_Munduruku-TuDeT, "
1051	        "UD_Tupinamba-TuDeT is part of the TuLaR. The treebank is ongoing work"
1052	        " and is constantly being updated."
1053	    ),
1054	    "qtd_sagt": (
1055	        "UD Turkish-German SAGT is a Turkish-German code-switching treebank "
1056	        "that is developed as part of the SAGT project."
1057	    ),
1058	    "tr_atis": (
1059	        "This treebank is a translation of English ATIS (Airline Travel "
1060	        "Information System) corpus (see References). It consists of 5432 "
1061	        "sentences."
1062	    ),
1063	    "tr_tourism": (
1064	        "Turkish Tourism is a domain specific treebank consisting of 19,750 "
1065	        "manually annotated sentences and 92,200 tokens. These sentences were "
1066	        "taken from the original customer reviews of a tourism company."
1067	    ),
1068	    "tr_kenet": (
1069	        "Turkish-Kenet UD Treebank is the biggest treebank of Turkish. It "
1070	        "consists of 18,700 manually annotated sentences and 178,700 tokens. "
1071	        "Its corpus consists of dictionary examples."
1072	    ),
1073	    "tr_penn": (
1074	        "Turkish version of the Penn Treebank. It consists of a total of 9,560"
1075	        " manually annotated sentences and 87,367 tokens. (It only includes "
1076	        "sentences up to 15 words long.)"
1077	    ),
1078	    "tr_framenet": (
1079	        "Turkish FrameNet consists of 2,700 manually annotated example "
1080	        "sentences and 19,221 tokens. Its data consists of the sentences taken"
1081	        " from the Turkish FrameNet Project. The annotated sentences can be "
1082	        "filtered according to the semantic frame category of the root of the "
1083	        "sentence."
1084	    ),
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {
1217	        "test": "UD_Arabic-PUD/r2.10/ar_pud-ud-test.conllu",
1218	    },
1219	    "hy_armtdp": {
1220	        "train": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-train.conllu",
1221	        "dev": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-dev.conllu",
1222	        "test": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-test.conllu",
1223	    },
1224	    "aii_as": {
1225	        "test": "UD_Assyrian-AS/r2.10/aii_as-ud-test.conllu",
1226	    },
1227	    "bm_crb": {
1228	        "test": "UD_Bambara-CRB/r2.10/bm_crb-ud-test.conllu",
1229	    },
1230	    "eu_bdt": {
1231	        "train": "UD_Basque-BDT/r2.10/eu_bdt-ud-train.conllu",
1232	        "dev": "UD_Basque-BDT/r2.10/eu_bdt-ud-dev.conllu",
1233	        "test": "UD_Basque-BDT/r2.10/eu_bdt-ud-test.conllu",
1234	    },
1235	    "bej_nsc": {
1236	        "test": "UD_Beja-NSC/r2.10/bej_nsc-ud-test.conllu",
1237	    },
1238	    "be_hse": {
1239	        "train": "UD_Belarusian-HSE/r2.10/be_hse-ud-train.conllu",
1240	        "dev": "UD_Belarusian-HSE/r2.10/be_hse-ud-dev.conllu",
1241	        "test": "UD_Belarusian-HSE/r2.10/be_hse-ud-test.conllu",
1242	    },
1243	    "bn_bru": {
1244	        "test": "UD_Bengali-BRU/r2.10/bn_bru-ud-test.conllu",
1245	    },
1246	    "bho_bhtb": {
1247	        "test": "UD_Bhojpuri-BHTB/r2.10/bho_bhtb-ud-test.conllu",
1248	    },
1249	    "br_keb": {
1250	        "test": "UD_Breton-KEB/r2.10/br_keb-ud-test.conllu",
1251	    },
1252	    "bg_btb": {
1253	        "train": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-train.conllu",
1254	        "dev": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-dev.conllu",
1255	        "test": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-test.conllu",
1256	    },
1257	    "bxr_bdt": {
1258	        "train": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-train.conllu",
1259	        "test": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-test.conllu",
1260	    },
1261	    "yue_hk": {
1262	        "test": "UD_Cantonese-HK/r2.10/yue_hk-ud-test.conllu",
1263	    },
1264	    "ca_ancora": {
1265	        "train": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-train.conllu",
1266	        "dev": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-dev.conllu",
1267	        "test": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-test.conllu",
1268	    },
1269	    "ceb_gja": {
1270	        "test": "UD_Cebuano-GJA/r2.10/ceb_gja-ud-test.conllu",
1271	    },
1272	    "zh_cfl": {
1273	        "test": "UD_Chinese-CFL/r2.10/zh_cfl-ud-test.conllu",
1274	    },
1275	    "zh_gsd": {
1276	        "train": "UD_Chinese-GSD/r2.10/zh_gsd-ud-train.conllu",
1277	        "dev": "UD_Chinese-GSD/r2.10/zh_gsd-ud-dev.conllu",
1278	        "test": "UD_Chinese-GSD/r2.10/zh_gsd-ud-test.conllu",
1279	    },
1280	    "zh_gsdsimp": {
1281	        "train": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-train.conllu",
1282	        "dev": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-dev.conllu",
1283	        "test": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-test.conllu",
1284	    },
1285	    "zh_hk": {
1286	        "test": "UD_Chinese-HK/r2.10/zh_hk-ud-test.conllu",
1287	    },
1288	    "zh_pud": {
1289	        "test": "UD_Chinese-PUD/r2.10/zh_pud-ud-test.conllu",
1290	    },
1291	    "ckt_hse": {
1292	        "test": "UD_Chukchi-HSE/r2.10/ckt_hse-ud-test.conllu",
1293	    },
1294	    "lzh_kyoto": {
1295	        "train": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-train.conllu",
1296	        "dev": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-dev.conllu",
1297	        "test": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-test.conllu",
1298	    },
1299	    "cop_scriptorium": {
1300	        "train": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-train.conllu",
1301	        "dev": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-dev.conllu",
1302	        "test": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-test.conllu",
1303	    },
1304	    "hr_set": {
1305	        "train": "UD_Croatian-SET/r2.10/hr_set-ud-train.conllu",
1306	        "dev": "UD_Croatian-SET/r2.10/hr_set-ud-dev.conllu",
1307	        "test": "UD_Croatian-SET/r2.10/hr_set-ud-test.conllu",
1308	    },
1309	    "cs_cac": {
1310	        "train": "UD_Czech-CAC/r2.10/cs_cac-ud-train.conllu",
1311	        "dev": "UD_Czech-CAC/r2.10/cs_cac-ud-dev.conllu",
1312	        "test": "UD_Czech-CAC/r2.10/cs_cac-ud-test.conllu",
1313	    },
1314	    "cs_cltt": {
1315	        "train": "UD_Czech-CLTT/r2.10/cs_cltt-ud-train.conllu",
1316	        "dev": "UD_Czech-CLTT/r2.10/cs_cltt-ud-dev.conllu",
1317	        "test": "UD_Czech-CLTT/r2.10/cs_cltt-ud-test.conllu",
1318	    },
1319	    "cs_fictree": {
1320	        "train": "UD_Czech-FicTree/r2.10/cs_fictree-ud-train.conllu",
1321	        "dev": "UD_Czech-FicTree/r2.10/cs_fictree-ud-dev.conllu",
1322	        "test": "UD_Czech-FicTree/r2.10/cs_fictree-ud-test.conllu",
1323	    },
1324	    "cs_pdt": {
1325	        "train": [
1326	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-l.conllu",
1327	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-m.conllu",
1328	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-c.conllu",
1329	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-v.conllu",
1330	        ],
1331	        "dev": "UD_Czech-PDT/r2.10/cs_pdt-ud-dev.conllu",
1332	        "test": "UD_Czech-PDT/r2.10/cs_pdt-ud-test.conllu",
1333	    },
1334	    "cs_pud": {
1335	        "test": "UD_Czech-PUD/r2.10/cs_pud-ud-test.conllu",
1336	    },
1337	    "da_ddt": {
1338	        "train": "UD_Danish-DDT/r2.10/da_ddt-ud-train.conllu",
1339	        "dev": "UD_Danish-DDT/r2.10/da_ddt-ud-dev.conllu",
1340	        "test": "UD_Danish-DDT/r2.10/da_ddt-ud-test.conllu",
1341	    },
1342	    "nl_alpino": {
1343	        "train": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-train.conllu",
1344	        "dev": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-dev.conllu",
1345	        "test": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-test.conllu",
1346	    },
1347	    "nl_lassysmall": {
1348	        "train": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-train.conllu",
1349	        "dev": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-dev.conllu",
1350	        "test": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-test.conllu",
1351	    },
1352	    "en_atis": {
1353	        "train": "UD_English-Atis/r2.10/en_atis-ud-train.conllu",
1354	        "dev": "UD_English-Atis/r2.10/en_atis-ud-dev.conllu",
1355	        "test": "UD_English-Atis/r2.10/en_atis-ud-test.conllu",
1356	    },
1357	    "en_esl": {
1358	        "train": "UD_English-ESL/r2.10/en_esl-ud-train.conllu",
1359	        "dev": "UD_English-ESL/r2.10/en_esl-ud-dev.conllu",
1360	        "test": "UD_English-ESL/r2.10/en_esl-ud-test.conllu",
1361	    },
1362	    "en_ewt": {
1363	        "train": "UD_English-EWT/r2.10/en_ewt-ud-train.conllu",
1364	        "dev": "UD_English-EWT/r2.10/en_ewt-ud-dev.conllu",
1365	        "test": "UD_English-EWT/r2.10/en_ewt-ud-test.conllu",
1366	    },
1367	    "en_gum": {
1368	        "train": "UD_English-GUM/r2.10/en_gum-ud-train.conllu",
1369	        "dev": "UD_English-GUM/r2.10/en_gum-ud-dev.conllu",
1370	        "test": "UD_English-GUM/r2.10/en_gum-ud-test.conllu",
1371	    },
1372	    "en_gumreddit": {
1373	        "train": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-train.conllu",
1374	        "dev": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-dev.conllu",
1375	        "test": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-test.conllu",
1376	    },
1377	    "en_lines": {
1378	        "train": "UD_English-LinES/r2.10/en_lines-ud-train.conllu",
1379	        "dev": "UD_English-LinES/r2.10/en_lines-ud-dev.conllu",
1380	        "test": "UD_English-LinES/r2.10/en_lines-ud-test.conllu",
1381	    },
1382	    "en_partut": {
1383	        "train": "UD_English-ParTUT/r2.10/en_partut-ud-train.conllu",
1384	        "dev": "UD_English-ParTUT/r2.10/en_partut-ud-dev.conllu",
1385	        "test": "UD_English-ParTUT/r2.10/en_partut-ud-test.conllu",
1386	    },
1387	    "en_pronouns": {
1388	        "test": "UD_English-Pronouns/r2.10/en_pronouns-ud-test.conllu",
1389	    },
1390	    "en_pud": {
1391	        "test": "UD_English-PUD/r2.10/en_pud-ud-test.conllu",
1392	    },
1393	    "myv_jr": {
1394	        "test": "UD_Erzya-JR/r2.10/myv_jr-ud-test.conllu",
1395	    },
1396	    "et_edt": {
1397	        "train": "UD_Estonian-EDT/r2.10/et_edt-ud-train.conllu",
1398	        "dev": "UD_Estonian-EDT/r2.10/et_edt-ud-dev.conllu",
1399	        "test": "UD_Estonian-EDT/r2.10/et_edt-ud-test.conllu",
1400	    },
1401	    "et_ewt": {
1402	        "train": "UD_Estonian-EWT/r2.10/et_ewt-ud-train.conllu",
1403	        "dev": "UD_Estonian-EWT/r2.10/et_ewt-ud-dev.conllu",
1404	        "test": "UD_Estonian-EWT/r2.10/et_ewt-ud-test.conllu",
1405	    },
1406	    "fo_farpahc": {
1407	        "train": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-train.conllu",
1408	        "dev": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-dev.conllu",
1409	        "test": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-test.conllu",
1410	    },
1411	    "fo_oft": {
1412	        "test": "UD_Faroese-OFT/r2.10/fo_oft-ud-test.conllu",
1413	    },
1414	    "fi_ftb": {
1415	        "train": "UD_Finnish-FTB/r2.10/fi_ftb-ud-train.conllu",
1416	        "dev": "UD_Finnish-FTB/r2.10/fi_ftb-ud-dev.conllu",
1417	        "test": "UD_Finnish-FTB/r2.10/fi_ftb-ud-test.conllu",
1418	    },
1419	    "fi_ood": {
1420	        "test": "UD_Finnish-OOD/r2.10/fi_ood-ud-test.conllu",
1421	    },
1422	    "fi_pud": {
1423	        "test": "UD_Finnish-PUD/r2.10/fi_pud-ud-test.conllu",
1424	    },
1425	    "fi_tdt": {
1426	        "train": "UD_Finnish-TDT/r2.10/fi_tdt-ud-train.conllu",
1427	        "dev": "UD_Finnish-TDT/r2.10/fi_tdt-ud-dev.conllu",
1428	        "test": "UD_Finnish-TDT/r2.10/fi_tdt-ud-test.conllu",
1429	    },
1430	    "fr_parisstories": {
1431	        "train": "UD_French-ParisStories/r2.10/fr_parisstories-ud-train.conllu",
1432	        "test": "UD_French-ParisStories/r2.10/fr_parisstories-ud-test.conllu",
1433	    },
1434	    "fr_fqb": {
1435	        "test": "UD_French-FQB/r2.10/fr_fqb-ud-test.conllu",
1436	    },
1437	    "fr_rhapsodie": {
1438	        "train": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-train.conllu",
1439	        "dev": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-dev.conllu",
1440	        "test": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-test.conllu",
1441	    },
1442	    "fr_ftb": {
1443	        "train": "UD_French-FTB/r2.10/fr_ftb-ud-train.conllu",
1444	        "dev": "UD_French-FTB/r2.10/fr_ftb-ud-dev.conllu",
1445	        "test": "UD_French-FTB/r2.10/fr_ftb-ud-test.conllu",
1446	    },
1447	    "fr_gsd": {
1448	        "train": "UD_French-GSD/r2.10/fr_gsd-ud-train.conllu",
1449	        "dev": "UD_French-GSD/r2.10/fr_gsd-ud-dev.conllu",
1450	        "test": "UD_French-GSD/r2.10/fr_gsd-ud-test.conllu",
1451	    },
1452	    "fr_partut": {
1453	        "train": "UD_French-ParTUT/r2.10/fr_partut-ud-train.conllu",
1454	        "dev": "UD_French-ParTUT/r2.10/fr_partut-ud-dev.conllu",
1455	        "test": "UD_French-ParTUT/r2.10/fr_partut-ud-test.conllu",
1456	    },
1457	    "fr_pud": {
1458	        "test": "UD_French-PUD/r2.10/fr_pud-ud-test.conllu",
1459	    },
1460	    "fr_sequoia": {
1461	        "train": "UD_French-Sequoia/r2.10/fr_sequoia-ud-train.conllu",
1462	        "dev": "UD_French-Sequoia/r2.10/fr_sequoia-ud-dev.conllu",
1463	        "test": "UD_French-Sequoia/r2.10/fr_sequoia-ud-test.conllu",
1464	    },
1465	    "qfn_fame": {
1466	        "test": "UD_Frisian_Dutch-Fame/r2.10/qfn_fame-ud-test.conllu",
1467	    },
1468	    "gl_ctg": {
1469	        "train": "UD_Galician-CTG/r2.10/gl_ctg-ud-train.conllu",
1470	        "dev": "UD_Galician-CTG/r2.10/gl_ctg-ud-dev.conllu",
1471	        "test": "UD_Galician-CTG/r2.10/gl_ctg-ud-test.conllu",
1472	    },
1473	    "gl_treegal": {
1474	        "train": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-train.conllu",
1475	        "test": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-test.conllu",
1476	    },
1477	    "de_gsd": {
1478	        "train": "UD_German-GSD/r2.10/de_gsd-ud-train.conllu",
1479	        "dev": "UD_German-GSD/r2.10/de_gsd-ud-dev.conllu",
1480	        "test": "UD_German-GSD/r2.10/de_gsd-ud-test.conllu",
1481	    },
1482	    "de_hdt": {
1483	        "train": [
1484	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-1.conllu",
1485	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-2.conllu",
1486	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-1.conllu",
1487	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-2.conllu",
1488	        ],
1489	        "dev": "UD_German-HDT/r2.10/de_hdt-ud-dev.conllu",
1490	        "test": "UD_German-HDT/r2.10/de_hdt-ud-test.conllu",
1491	    },
1492	    "de_lit": {
1493	        "test": "UD_German-LIT/r2.10/de_lit-ud-test.conllu",
1494	    },
1495	    "de_pud": {
1496	        "test": "UD_German-PUD/r2.10/de_pud-ud-test.conllu",
1497	    },
1498	    "got_proiel": {
1499	        "train": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-train.conllu",
1500	        "dev": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-dev.conllu",
1501	        "test": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-test.conllu",
1502	    },
1503	    "el_gdt": {
1504	        "train": "UD_Greek-GDT/r2.10/el_gdt-ud-train.conllu",
1505	        "dev": "UD_Greek-GDT/r2.10/el_gdt-ud-dev.conllu",
1506	        "test": "UD_Greek-GDT/r2.10/el_gdt-ud-test.conllu",
1507	    },
1508	    "gub_tudet": {
1509	        "test": "UD_Guajajara-TuDeT/r2.10/gub_tudet-ud-test.conllu",
1510	    },
1511	    "gn_oldtudet": {
1512	        "test": "UD_Guarani-OldTuDeT/r2.10/gn_oldtudet-ud-test.conllu",
1513	    },
1514	    "he_iahltwiki": {
1515	        "train": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-train.conllu",
1516	        "dev": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-dev.conllu",
1517	        "test": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-test.conllu",
1518	    },
1519	    "he_htb": {
1520	        "train": "UD_Hebrew-HTB/r2.10/he_htb-ud-train.conllu",
1521	        "dev": "UD_Hebrew-HTB/r2.10/he_htb-ud-dev.conllu",
1522	        "test": "UD_Hebrew-HTB/r2.10/he_htb-ud-test.conllu",
1523	    },
1524	    "qhe_hiencs": {
1525	        "train": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-train.conllu",
1526	        "dev": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-dev.conllu",
1527	        "test": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-test.conllu",
1528	    },
1529	    "hi_hdtb": {
1530	        "train": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-train.conllu",
1531	        "dev": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-dev.conllu",
1532	        "test": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-test.conllu",
1533	    },
1534	    "hi_pud": {
1535	        "test": "UD_Hindi-PUD/r2.10/hi_pud-ud-test.conllu",
1536	    },
1537	    "hu_szeged": {
1538	        "train": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-train.conllu",
1539	        "dev": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-dev.conllu",
1540	        "test": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-test.conllu",
1541	    },
1542	    "is_modern": {
1543	        "train": "UD_Icelandic-Modern/r2.10/is_modern-ud-train.conllu",
1544	        "dev": "UD_Icelandic-Modern/r2.10/is_modern-ud-dev.conllu",
1545	        "test": "UD_Icelandic-Modern/r2.10/is_modern-ud-test.conllu",
1546	    },
1547	    "is_icepahc": {
1548	        "train": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-train.conllu",
1549	        "dev": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-dev.conllu",
1550	        "test": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-test.conllu",
1551	    },
1552	    "is_pud": {
1553	        "test": "UD_Icelandic-PUD/r2.10/is_pud-ud-test.conllu",
1554	    },
1555	    "id_csui": {
1556	        "train": "UD_Indonesian-CSUI/r2.10/id_csui-ud-train.conllu",
1557	        "test": "UD_Indonesian-CSUI/r2.10/id_csui-ud-test.conllu",
1558	    },
1559	    "id_gsd": {
1560	        "train": "UD_Indonesian-GSD/r2.10/id_gsd-ud-train.conllu",
1561	        "dev": "UD_Indonesian-GSD/r2.10/id_gsd-ud-dev.conllu",
1562	        "test": "UD_Indonesian-GSD/r2.10/id_gsd-ud-test.conllu",
1563	    },
1564	    "id_pud": {
1565	        "test": "UD_Indonesian-PUD/r2.10/id_pud-ud-test.conllu",
1566	    },
1567	    "ga_twittirish": {
1568	        "test": "UD_Irish-TwittIrish/r2.10/ga_twittirish-ud-test.conllu",
1569	    },
1570	    "ga_idt": {
1571	        "train": "UD_Irish-IDT/r2.10/ga_idt-ud-train.conllu",
1572	        "dev": "UD_Irish-IDT/r2.10/ga_idt-ud-dev.conllu",
1573	        "test": "UD_Irish-IDT/r2.10/ga_idt-ud-test.conllu",
1574	    },
1575	    "it_valico": {
1576	        "test": "UD_Italian-Valico/r2.10/it_valico-ud-test.conllu",
1577	    },
1578	    "it_markit": {
1579	        "train": "UD_Italian-MarkIT/r2.10/it_markit-ud-train.conllu",
1580	        "dev": "UD_Italian-MarkIT/r2.10/it_markit-ud-dev.conllu",
1581	        "test": "UD_Italian-MarkIT/r2.10/it_markit-ud-test.conllu",
1582	    },
1583	    "it_isdt": {
1584	        "train": "UD_Italian-ISDT/r2.10/it_isdt-ud-train.conllu",
1585	        "dev": "UD_Italian-ISDT/r2.10/it_isdt-ud-dev.conllu",
1586	        "test": "UD_Italian-ISDT/r2.10/it_isdt-ud-test.conllu",
1587	    },
1588	    "it_partut": {
1589	        "train": "UD_Italian-ParTUT/r2.10/it_partut-ud-train.conllu",
1590	        "dev": "UD_Italian-ParTUT/r2.10/it_partut-ud-dev.conllu",
1591	        "test": "UD_Italian-ParTUT/r2.10/it_partut-ud-test.conllu",
1592	    },
1593	    "it_postwita": {
1594	        "train": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-train.conllu",
1595	        "dev": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-dev.conllu",
1596	        "test": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-test.conllu",
1597	    },
1598	    "it_pud": {
1599	        "test": "UD_Italian-PUD/r2.10/it_pud-ud-test.conllu",
1600	    },
1601	    "it_twittiro": {
1602	        "train": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-train.conllu",
1603	        "dev": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-dev.conllu",
1604	        "test": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-test.conllu",
1605	    },
1606	    "it_vit": {
1607	        "train": "UD_Italian-VIT/r2.10/it_vit-ud-train.conllu",
1608	        "dev": "UD_Italian-VIT/r2.10/it_vit-ud-dev.conllu",
1609	        "test": "UD_Italian-VIT/r2.10/it_vit-ud-test.conllu",
1610	    },
1611	    "ja_gsdluw": {
1612	        "train": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-train.conllu",
1613	        "dev": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-dev.conllu",
1614	        "test": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-test.conllu",
1615	    },
1616	    "ja_pudluw": {
1617	        "test": "UD_Japanese-PUDLUW/r2.10/ja_pudluw-ud-test.conllu",
1618	    },
1619	    "ja_bccwjluw": {
1620	        "train": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-train.conllu",
1621	        "dev": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-dev.conllu",
1622	        "test": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-test.conllu",
1623	    },
1624	    "ja_bccwj": {
1625	        "train": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-train.conllu",
1626	        "dev": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-dev.conllu",
1627	        "test": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-test.conllu",
1628	    },
1629	    "ja_gsd": {
1630	        "train": "UD_Japanese-GSD/r2.10/ja_gsd-ud-train.conllu",
1631	        "dev": "UD_Japanese-GSD/r2.10/ja_gsd-ud-dev.conllu",
1632	        "test": "UD_Japanese-GSD/r2.10/ja_gsd-ud-test.conllu",
1633	    },
1634	    "ja_modern": {
1635	        "test": "UD_Japanese-Modern/r2.10/ja_modern-ud-test.conllu",
1636	    },
1637	    "ja_pud": {
1638	        "test": "UD_Japanese-PUD/r2.10/ja_pud-ud-test.conllu",
1639	    },
1640	    "jv_csui": {
1641	        "test": "UD_Javanese-CSUI/r2.10/jv_csui-ud-test.conllu",
1642	    },
1643	    "urb_tudet": {
1644	        "test": "UD_Kaapor-TuDeT/r2.10/urb_tudet-ud-test.conllu",
1645	    },
1646	    "xnr_kdtb": {
1647	        "test": "UD_Kangri-KDTB/r2.10/xnr_kdtb-ud-test.conllu",
1648	    },
1649	    "krl_kkpp": {
1650	        "test": "UD_Karelian-KKPP/r2.10/krl_kkpp-ud-test.conllu",
1651	    },
1652	    "arr_tudet": {
1653	        "test": "UD_Karo-TuDeT/r2.10/arr_tudet-ud-test.conllu",
1654	    },
1655	    "kk_ktb": {
1656	        "train": "UD_Kazakh-KTB/r2.10/kk_ktb-ud-train.conllu",
1657	        "test": "UD_Kazakh-KTB/r2.10/kk_ktb-ud-test.conllu",
1658	    },
1659	    "kfm_aha": {
1660	        "test": "UD_Khunsari-AHA/r2.10/kfm_aha-ud-test.conllu",
1661	    },
1662	    "quc_iu": {
1663	        "test": "UD_Kiche-IU/r2.10/quc_iu-ud-test.conllu",
1664	    },
1665	    "koi_uh": {
1666	        "test": "UD_Komi_Permyak-UH/r2.10/koi_uh-ud-test.conllu",
1667	    },
1668	    "kpv_ikdp": {
1669	        "test": "UD_Komi_Zyrian-IKDP/r2.10/kpv_ikdp-ud-test.conllu",
1670	    },
1671	    "kpv_lattice": {
1672	        "test": "UD_Komi_Zyrian-Lattice/r2.10/kpv_lattice-ud-test.conllu",
1673	    },
1674	    "ko_gsd": {
1675	        "train": "UD_Korean-GSD/r2.10/ko_gsd-ud-train.conllu",
1676	        "dev": "UD_Korean-GSD/r2.10/ko_gsd-ud-dev.conllu",
1677	        "test": "UD_Korean-GSD/r2.10/ko_gsd-ud-test.conllu",
1678	    },
1679	    "ko_kaist": {
1680	        "train": "UD_Korean-Kaist/r2.10/ko_kaist-ud-train.conllu",
1681	        "dev": "UD_Korean-Kaist/r2.10/ko_kaist-ud-dev.conllu",
1682	        "test": "UD_Korean-Kaist/r2.10/ko_kaist-ud-test.conllu",
1683	    },
1684	    "ko_pud": {
1685	        "test": "UD_Korean-PUD/r2.10/ko_pud-ud-test.conllu",
1686	    },
1687	    "kmr_mg": {
1688	        "train": "UD_Kurmanji-MG/r2.10/kmr_mg-ud-train.conllu",
1689	        "test": "UD_Kurmanji-MG/r2.10/kmr_mg-ud-test.conllu",
1690	    },
1691	    "la_udante": {
1692	        "train": "UD_Latin-UDante/r2.10/la_udante-ud-train.conllu",
1693	        "dev": "UD_Latin-UDante/r2.10/la_udante-ud-dev.conllu",
1694	        "test": "UD_Latin-UDante/r2.10/la_udante-ud-test.conllu",
1695	    },
1696	    "la_ittb": {
1697	        "train": "UD_Latin-ITTB/r2.10/la_ittb-ud-train.conllu",
1698	        "dev": "UD_Latin-ITTB/r2.10/la_ittb-ud-dev.conllu",
1699	        "test": "UD_Latin-ITTB/r2.10/la_ittb-ud-test.conllu",
1700	    },
1701	    "la_llct": {
1702	        "train": "UD_Latin-LLCT/r2.10/la_llct-ud-train.conllu",
1703	        "dev": "UD_Latin-LLCT/r2.10/la_llct-ud-dev.conllu",
1704	        "test": "UD_Latin-LLCT/r2.10/la_llct-ud-test.conllu",
1705	    },
1706	    "la_perseus": {
1707	        "train": "UD_Latin-Perseus/r2.10/la_perseus-ud-train.conllu",
1708	        "test": "UD_Latin-Perseus/r2.10/la_perseus-ud-test.conllu",
1709	    },
1710	    "la_proiel": {
1711	        "train": "UD_Latin-PROIEL/r2.10/la_proiel-ud-train.conllu",
1712	        "dev": "UD_Latin-PROIEL/r2.10/la_proiel-ud-dev.conllu",
1713	        "test": "UD_Latin-PROIEL/r2.10/la_proiel-ud-test.conllu",
1714	    },
1715	    "lv_lvtb": {
1716	        "train": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-train.conllu",
1717	        "dev": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-dev.conllu",
1718	        "test": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-test.conllu",
1719	    },
1720	    "lij_glt": {
1721	        "train": "UD_Ligurian-GLT/r2.10/lij_glt-ud-train.conllu",
1722	        "test": "UD_Ligurian-GLT/r2.10/lij_glt-ud-test.conllu",
1723	    },
1724	    "lt_alksnis": {
1725	        "train": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-train.conllu",
1726	        "dev": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-dev.conllu",
1727	        "test": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-test.conllu",
1728	    },
1729	    "lt_hse": {
1730	        "train": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1731	        "dev": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1732	        "test": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1733	    },
1734	    "olo_kkpp": {
1735	        "train": "UD_Livvi-KKPP/r2.10/olo_kkpp-ud-train.conllu",
1736	        "test": "UD_Livvi-KKPP/r2.10/olo_kkpp-ud-test.conllu",
1737	    },
1738	    "nds_lsdc": {
1739	        "test": "UD_Low_Saxon-LSDC/r2.10/nds_lsdc-ud-test.conllu",
1740	    },
1741	    # TODO(tfds) Add Madi Jarawara splits when it will be officially released.
1742	    # TODO(tfds) Add Makurap TuDeT splits when it will be officially released.
1743	    "mt_mudt": {
1744	        "train": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-train.conllu",
1745	        "dev": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-dev.conllu",
1746	        "test": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-test.conllu",
1747	    },
1748	    "gv_cadhan": {
1749	        "test": "UD_Manx-Cadhan/r2.10/gv_cadhan-ud-test.conllu",
1750	    },
1751	    "mr_ufal": {
1752	        "train": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-train.conllu",
1753	        "dev": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-dev.conllu",
1754	        "test": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-test.conllu",
1755	    },
1756	    "gun_dooley": {
1757	        "test": "UD_Mbya_Guarani-Dooley/r2.10/gun_dooley-ud-test.conllu",
1758	    },
1759	    "gun_thomas": {
1760	        "test": "UD_Mbya_Guarani-Thomas/r2.10/gun_thomas-ud-test.conllu",
1761	    },
1762	    "mdf_jr": {
1763	        "test": "UD_Moksha-JR/r2.10/mdf_jr-ud-test.conllu",
1764	    },
1765	    "myu_tudet": {
1766	        "test": "UD_Munduruku-TuDeT/r2.10/myu_tudet-ud-test.conllu",
1767	    },
1768	    "pcm_nsc": {
1769	        "train": "UD_Naija-NSC/r2.10/pcm_nsc-ud-train.conllu",
1770	        "dev": "UD_Naija-NSC/r2.10/pcm_nsc-ud-dev.conllu",
1771	        "test": "UD_Naija-NSC/r2.10/pcm_nsc-ud-test.conllu",
1772	    },
1773	    "nyq_aha": {
1774	        "test": "UD_Nayini-AHA/r2.10/nyq_aha-ud-test.conllu",
1775	    },
1776	    # TODO(tfds) Add Neapolitan RB splits when it will be officially released.
1777	    "sme_giella": {
1778	        "train": "UD_North_Sami-Giella/r2.10/sme_giella-ud-train.conllu",
1779	        "test": "UD_North_Sami-Giella/r2.10/sme_giella-ud-test.conllu",
1780	    },
1781	    "no_bokmaal": {
1782	        "train": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-train.conllu",
1783	        "dev": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-dev.conllu",
1784	        "test": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-test.conllu",
1785	    },
1786	    "no_nynorsk": {
1787	        "train": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-train.conllu",
1788	        "dev": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-dev.conllu",
1789	        "test": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-test.conllu",
1790	    },
1791	    "no_nynorsklia": {
1792	        "train": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-train.conllu",
1793	        "dev": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-dev.conllu",
1794	        "test": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-test.conllu",
1795	    },
1796	    "cu_proiel": {
1797	        "train": (
1798	            "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-train.conllu"
1799	        ),
1800	        "dev": "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-dev.conllu",
1801	        "test": "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-test.conllu",
1802	    },
1803	    "fro_srcmf": {
1804	        "train": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-train.conllu",
1805	        "dev": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-dev.conllu",
1806	        "test": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-test.conllu",
1807	    },
1808	    "orv_birchbark": {
1809	        "train": (
1810	            "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-train.conllu"
1811	        ),
1812	        "dev": "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-dev.conllu",
1813	        "test": (
1814	            "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-test.conllu"
1815	        ),
1816	    },
1817	    "orv_rnc": {
1818	        "train": "UD_Old_Russian-RNC/r2.10/orv_rnc-ud-train.conllu",
1819	        "test": "UD_Old_Russian-RNC/r2.10/orv_rnc-ud-test.conllu",
1820	    },
1821	    "orv_torot": {
1822	        "train": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-train.conllu",
1823	        "dev": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-dev.conllu",
1824	        "test": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-test.conllu",
1825	    },
1826	    "otk_tonqq": {
1827	        "test": "UD_Old_Turkish-Tonqq/r2.10/otk_tonqq-ud-test.conllu",
1828	    },
1829	    "fa_perdt": {
1830	        "train": "UD_Persian-PerDT/r2.10/fa_perdt-ud-train.conllu",
1831	        "dev": "UD_Persian-PerDT/r2.10/fa_perdt-ud-dev.conllu",
1832	        "test": "UD_Persian-PerDT/r2.10/fa_perdt-ud-test.conllu",
1833	    },
1834	    "fa_seraji": {
1835	        "train": "UD_Persian-Seraji/r2.10/fa_seraji-ud-train.conllu",
1836	        "dev": "UD_Persian-Seraji/r2.10/fa_seraji-ud-dev.conllu",
1837	        "test": "UD_Persian-Seraji/r2.10/fa_seraji-ud-test.conllu",
1838	    },
1839	    "pl_lfg": {
1840	        "train": "UD_Polish-LFG/r2.10/pl_lfg-ud-train.conllu",
1841	        "dev": "UD_Polish-LFG/r2.10/pl_lfg-ud-dev.conllu",
1842	        "test": "UD_Polish-LFG/r2.10/pl_lfg-ud-test.conllu",
1843	    },
1844	    "pl_pdb": {
1845	        "train": "UD_Polish-PDB/r2.10/pl_pdb-ud-train.conllu",
1846	        "dev": "UD_Polish-PDB/r2.10/pl_pdb-ud-dev.conllu",
1847	        "test": "UD_Polish-PDB/r2.10/pl_pdb-ud-test.conllu",
1848	    },
1849	    "pl_pud": {
1850	        "test": "UD_Polish-PUD/r2.10/pl_pud-ud-test.conllu",
1851	    },
1852	    "pt_bosque": {
1853	        "train": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-train.conllu",
1854	        "dev": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-dev.conllu",
1855	        "test": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-test.conllu",
1856	    },
1857	    "pt_gsd": {
1858	        "train": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-train.conllu",
1859	        "dev": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-dev.conllu",
1860	        "test": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-test.conllu",
1861	    },
1862	    "pt_pud": {
1863	        "test": "UD_Portuguese-PUD/r2.10/pt_pud-ud-test.conllu",
1864	    },
1865	    "ro_art": {
1866	        "test": "UD_Romanian-ArT/r2.10/ro_art-ud-test.conllu",
1867	    },
1868	    "ro_nonstandard": {
1869	        "train": "UD_Romanian-Nonstandard/r2.10/ro_nonstandard-ud-train.conllu",
1870	        "dev": "UD_Romanian-Nonstandard/r2.10/ro_nonstandard-ud-dev.conllu",
1871	        "test": "UD_Romanian-Nonstandard/r2.10/ro_nonstandard-ud-test.conllu",
1872	    },
1873	    "ro_rrt": {
1874	        "train": "UD_Romanian-RRT/r2.10/ro_rrt-ud-train.conllu",
1875	        "dev": "UD_Romanian-RRT/r2.10/ro_rrt-ud-dev.conllu",
1876	        "test": "UD_Romanian-RRT/r2.10/ro_rrt-ud-test.conllu",
1877	    },
1878	    "ro_simonero": {
1879	        "train": "UD_Romanian-SiMoNERo/r2.10/ro_simonero-ud-train.conllu",
1880	        "dev": "UD_Romanian-SiMoNERo/r2.10/ro_simonero-ud-dev.conllu",
1881	        "test": "UD_Romanian-SiMoNERo/r2.10/ro_simonero-ud-test.conllu",
1882	    },
1883	    "ru_gsd": {
1884	        "train": "UD_Russian-GSD/r2.10/ru_gsd-ud-train.conllu",
1885	        "dev": "UD_Russian-GSD/r2.10/ru_gsd-ud-dev.conllu",
1886	        "test": "UD_Russian-GSD/r2.10/ru_gsd-ud-test.conllu",
1887	    },
1888	    "ru_pud": {
1889	        "test": "UD_Russian-PUD/r2.10/ru_pud-ud-test.conllu",
1890	    },
1891	    "ru_syntagrus": {
1892	        "train": "UD_Russian-SynTagRus/r2.7/ru_syntagrus-ud-train.conllu",
1893	        "dev": "UD_Russian-SynTagRus/r2.7/ru_syntagrus-ud-dev.conllu",
1894	        "test": "UD_Russian-SynTagRus/r2.7/ru_syntagrus-ud-test.conllu",
1895	    },
1896	    "ru_taiga": {
1897	        "train": "UD_Russian-Taiga/r2.10/ru_taiga-ud-train.conllu",
1898	        "dev": "UD_Russian-Taiga/r2.10/ru_taiga-ud-dev.conllu",
1899	        "test": "UD_Russian-Taiga/r2.10/ru_taiga-ud-test.conllu",
1900	    },
1901	    "sa_ufal": {
1902	        "test": "UD_Sanskrit-UFAL/r2.10/sa_ufal-ud-test.conllu",
1903	    },
1904	    "sa_vedic": {
1905	        "train": "UD_Sanskrit-Vedic/r2.10/sa_vedic-ud-train.conllu",
1906	        "test": "UD_Sanskrit-Vedic/r2.10/sa_vedic-ud-test.conllu",
1907	    },
1908	    "gd_arcosg": {
1909	        "train": "UD_Scottish_Gaelic-ARCOSG/r2.10/gd_arcosg-ud-train.conllu",
1910	        "dev": "UD_Scottish_Gaelic-ARCOSG/r2.10/gd_arcosg-ud-dev.conllu",
1911	        "test": "UD_Scottish_Gaelic-ARCOSG/r2.10/gd_arcosg-ud-test.conllu",
1912	    },
1913	    "sr_set": {
1914	        "train": "UD_Serbian-SET/r2.10/sr_set-ud-train.conllu",
1915	        "dev": "UD_Serbian-SET/r2.10/sr_set-ud-dev.conllu",
1916	        "test": "UD_Serbian-SET/r2.10/sr_set-ud-test.conllu",
1917	    },
1918	    "sms_giellagas": {
1919	        "test": "UD_Skolt_Sami-Giellagas/r2.10/sms_giellagas-ud-test.conllu",
1920	    },
1921	    "sk_snk": {
1922	        "train": "UD_Slovak-SNK/r2.10/sk_snk-ud-train.conllu",
1923	        "dev": "UD_Slovak-SNK/r2.10/sk_snk-ud-dev.conllu",
1924	        "test": "UD_Slovak-SNK/r2.10/sk_snk-ud-test.conllu",
1925	    },
1926	    "sl_ssj": {
1927	        "train": "UD_Slovenian-SSJ/r2.10/sl_ssj-ud-train.conllu",
1928	        "dev": "UD_Slovenian-SSJ/r2.10/sl_ssj-ud-dev.conllu",
1929	        "test": "UD_Slovenian-SSJ/r2.10/sl_ssj-ud-test.conllu",
1930	    },
1931	    "sl_sst": {
1932	        "train": "UD_Slovenian-SST/r2.10/sl_sst-ud-train.conllu",
1933	        "test": "UD_Slovenian-SST/r2.10/sl_sst-ud-test.conllu",
1934	    },
1935	    "soj_aha": {
1936	        "test": "UD_Soi-AHA/r2.10/soj_aha-ud-test.conllu",
1937	    },
1938	    "ajp_madar": {
1939	        "test": (
1940	            "UD_South_Levantine_Arabic-MADAR/r2.10/ajp_madar-ud-test.conllu"
1941	        ),
1942	    },
1943	    "es_ancora": {
1944	        "train": "UD_Spanish-AnCora/r2.10/es_ancora-ud-train.conllu",
1945	        "dev": "UD_Spanish-AnCora/r2.10/es_ancora-ud-dev.conllu",
1946	        "test": "UD_Spanish-AnCora/r2.10/es_ancora-ud-test.conllu",
1947	    },
1948	    "es_gsd": {
1949	        "train": "UD_Spanish-GSD/r2.10/es_gsd-ud-train.conllu",
1950	        "dev": "UD_Spanish-GSD/r2.10/es_gsd-ud-dev.conllu",
1951	        "test": "UD_Spanish-GSD/r2.10/es_gsd-ud-test.conllu",
1952	    },
1953	    "es_pud": {
1954	        "test": "UD_Spanish-PUD/r2.10/es_pud-ud-test.conllu",
1955	    },
1956	    "swl_sslc": {
1957	        "train": "UD_Swedish_Sign_Language-SSLC/r2.10/swl_sslc-ud-train.conllu",
1958	        "dev": "UD_Swedish_Sign_Language-SSLC/r2.10/swl_sslc-ud-dev.conllu",
1959	        "test": "UD_Swedish_Sign_Language-SSLC/r2.10/swl_sslc-ud-test.conllu",
1960	    },
1961	    "sv_lines": {
1962	        "train": "UD_Swedish-LinES/r2.10/sv_lines-ud-train.conllu",
1963	        "dev": "UD_Swedish-LinES/r2.10/sv_lines-ud-dev.conllu",
1964	        "test": "UD_Swedish-LinES/r2.10/sv_lines-ud-test.conllu",
1965	    },
1966	    "sv_pud": {
1967	        "test": "UD_Swedish-PUD/r2.10/sv_pud-ud-test.conllu",
1968	    },
1969	    "sv_talbanken": {
1970	        "train": "UD_Swedish-Talbanken/r2.10/sv_talbanken-ud-train.conllu",
1971	        "dev": "UD_Swedish-Talbanken/r2.10/sv_talbanken-ud-dev.conllu",
1972	        "test": "UD_Swedish-Talbanken/r2.10/sv_talbanken-ud-test.conllu",
1973	    },
1974	    "gsw_uzh": {
1975	        "test": "UD_Swiss_German-UZH/r2.10/gsw_uzh-ud-test.conllu",
1976	    },
1977	    "tl_trg": {
1978	        "test": "UD_Tagalog-TRG/r2.10/tl_trg-ud-test.conllu",
1979	    },
1980	    "tl_ugnayan": {
1981	        "test": "UD_Tagalog-Ugnayan/r2.10/tl_ugnayan-ud-test.conllu",
1982	    },
1983	    "ta_mwtt": {
1984	        "test": "UD_Tamil-MWTT/r2.10/ta_mwtt-ud-test.conllu",
1985	    },
1986	    "ta_ttb": {
1987	        "train": "UD_Tamil-TTB/r2.10/ta_ttb-ud-train.conllu",
1988	        "dev": "UD_Tamil-TTB/r2.10/ta_ttb-ud-dev.conllu",
1989	        "test": "UD_Tamil-TTB/r2.10/ta_ttb-ud-test.conllu",
1990	    },
1991	    "te_mtg": {
1992	        "train": "UD_Telugu-MTG/r2.10/te_mtg-ud-train.conllu",
1993	        "dev": "UD_Telugu-MTG/r2.10/te_mtg-ud-dev.conllu",
1994	        "test": "UD_Telugu-MTG/r2.10/te_mtg-ud-test.conllu",
1995	    },
1996	    "th_pud": {
1997	        "test": "UD_Thai-PUD/r2.10/th_pud-ud-test.conllu",
1998	    },
1999	    "tpn_tudet": {
2000	        "test": "UD_Tupinamba-TuDeT/r2.10/tpn_tudet-ud-test.conllu",
2001	    },
2002	    "qtd_sagt": {
2003	        "train": "UD_Turkish_German-SAGT/r2.10/qtd_sagt-ud-train.conllu",
2004	        "dev": "UD_Turkish_German-SAGT/r2.10/qtd_sagt-ud-dev.conllu",
2005	        "test": "UD_Turkish_German-SAGT/r2.10/qtd_sagt-ud-test.conllu",
2006	    },
2007	    "tr_kenet": {
2008	        "train": "UD_Turkish-Kenet/r2.10/tr_kenet-ud-train.conllu",
2009	        "dev": "UD_Turkish-Kenet/r2.10/tr_kenet-ud-dev.conllu",
2010	        "test": "UD_Turkish-Kenet/r2.10/tr_kenet-ud-test.conllu",
2011	    },
2012	    "tr_atis": {
2013	        "train": "UD_Turkish-Atis/r2.10/tr_atis-ud-train.conllu",
2014	        "dev": "UD_Turkish-Atis/r2.10/tr_atis-ud-dev.conllu",
2015	        "test": "UD_Turkish-Atis/r2.10/tr_atis-ud-test.conllu",
2016	    },
2017	    "tr_penn": {
2018	        "train": "UD_Turkish-Penn/r2.10/tr_penn-ud-train.conllu",
2019	        "dev": "UD_Turkish-Penn/r2.10/tr_penn-ud-dev.conllu",
2020	        "test": "UD_Turkish-Penn/r2.10/tr_penn-ud-test.conllu",
2021	    },
2022	    "tr_tourism": {
2023	        "train": "UD_Turkish-Tourism/r2.10/tr_tourism-ud-train.conllu",
2024	        "dev": "UD_Turkish-Tourism/r2.10/tr_tourism-ud-dev.conllu",
2025	        "test": "UD_Turkish-Tourism/r2.10/tr_tourism-ud-test.conllu",
2026	    },
2027	    "tr_framenet": {
2028	        "train": "UD_Turkish-FrameNet/r2.10/tr_framenet-ud-train.conllu",
2029	        "dev": "UD_Turkish-FrameNet/r2.10/tr_framenet-ud-dev.conllu",
2030	        "test": "UD_Turkish-FrameNet/r2.10/tr_framenet-ud-test.conllu",
2031	    },
2032	    "tr_boun": {
2033	        "train": "UD_Turkish-BOUN/r2.10/tr_boun-ud-train.conllu",
2034	        "dev": "UD_Turkish-BOUN/r2.10/tr_boun-ud-dev.conllu",
2035	        "test": "UD_Turkish-BOUN/r2.10/tr_boun-ud-test.conllu",
2036	    },
2037	    "tr_gb": {
2038	        "test": "UD_Turkish-GB/r2.10/tr_gb-ud-test.conllu",
2039	    },
2040	    "tr_imst": {
2041	        "train": "UD_Turkish-IMST/r2.10/tr_imst-ud-train.conllu",
2042	        "dev": "UD_Turkish-IMST/r2.10/tr_imst-ud-dev.conllu",
2043	        "test": "UD_Turkish-IMST/r2.10/tr_imst-ud-test.conllu",
2044	    },
2045	    "tr_pud": {
2046	        "test": "UD_Turkish-PUD/r2.10/tr_pud-ud-test.conllu",
2047	    },
2048	    "uk_iu": {
2049	        "train": "UD_Ukrainian-IU/r2.10/uk_iu-ud-train.conllu",
2050	        "dev": "UD_Ukrainian-IU/r2.10/uk_iu-ud-dev.conllu",
2051	        "test": "UD_Ukrainian-IU/r2.10/uk_iu-ud-test.conllu",
2052	    },
2053	    # TODO(tfds) Add Umbrian Ikuvina split when it will be officially released.
2054	    "hsb_ufal": {
2055	        "train": "UD_Upper_Sorbian-UFAL/r2.10/hsb_ufal-ud-train.conllu",
2056	        "test": "UD_Upper_Sorbian-UFAL/r2.10/hsb_ufal-ud-test.conllu",
2057	    },
2058	    "ur_udtb": {
2059	        "train": "UD_Urdu-UDTB/r2.10/ur_udtb-ud-train.conllu",
2060	        "dev": "UD_Urdu-UDTB/r2.10/ur_udtb-ud-dev.conllu",
2061	        "test": "UD_Urdu-UDTB/r2.10/ur_udtb-ud-test.conllu",
2062	    },
2063	    "ug_udt": {
2064	        "train": "UD_Uyghur-UDT/r2.10/ug_udt-ud-train.conllu",
2065	        "dev": "UD_Uyghur-UDT/r2.10/ug_udt-ud-dev.conllu",
2066	        "test": "UD_Uyghur-UDT/r2.10/ug_udt-ud-test.conllu",
2067	    },
2068	    "vi_vtb": {
2069	        "train": "UD_Vietnamese-VTB/r2.10/vi_vtb-ud-train.conllu",
2070	        "dev": "UD_Vietnamese-VTB/r2.10/vi_vtb-ud-dev.conllu",
2071	        "test": "UD_Vietnamese-VTB/r2.10/vi_vtb-ud-test.conllu",
2072	    },
2073	    "wbp_ufal": {
2074	        "test": "UD_Warlpiri-UFAL/r2.10/wbp_ufal-ud-test.conllu",
2075	    },
2076	    "cy_ccg": {
2077	        "train": "UD_Welsh-CCG/r2.10/cy_ccg-ud-train.conllu",
2078	        "test": "UD_Welsh-CCG/r2.10/cy_ccg-ud-test.conllu",
2079	    },
2080	    "wo_wtb": {
2081	        "train": "UD_Wolof-WTB/r2.10/wo_wtb-ud-train.conllu",
2082	        "dev": "UD_Wolof-WTB/r2.10/wo_wtb-ud-dev.conllu",
2083	        "test": "UD_Wolof-WTB/r2.10/wo_wtb-ud-test.conllu",
2084	    },
2085	    "sjo_xdt": {
2086	        "test": "UD_Xibe-XDT/r2.10/sjo_xdt-ud-test.conllu",
2087	    },
2088	    "sah_yktdt": {
2089	        "test": "UD_Yakut-YKTDT/r2.10/sah_yktdt-ud-test.conllu",
2090	    },
2091	    "yo_ytb": {
2092	        "test": "UD_Yoruba-YTB/r2.10/yo_ytb-ud-test.conllu",
2093	    },
2094	    "ess_sli": {
2095	        "test": "UD_Yupik-SLI/r2.10/ess_sli-ud-test.conllu",
2096	    },
2097	}
2098	
2099	LANGS = DESCRIPTIONS.keys()
2100	
2101	
2102	def prepare_ud_filepaths(
2103	    path_prefix: epath.PathLike,
2104	    filepaths: Union[epath.PathLike, List[epath.PathLike]],
2105	) -> List[epath.PathLike]:
2106	  """Prepends a path prefix to a (list of) filepaths.
2107	
2108	  Args:
2109	    path_prefix: A path which will be prepended to all paths in filepaths.
2110	    filepaths: The filepaths to be prepared. Could be a list of paths for
2111	      multiple files, or a single path.
2112	
2113	  Returns:
2114	    A list with the resulting filepath(s). In case a single path was given as
2115	    `filepath`, it will returns a one-item list containing the resulting path.
2116	  """
2117	  paths = filepaths if isinstance(filepaths, list) else [filepaths]
2118	  return [os.path.join(path_prefix, filepath) for filepath in paths]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:1086
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1085	    "tr_boun": (
1086	        "The largest Turkish dependency treebank annotated in UD style. "
1087	        "Created by the members of "
1088	        "[TABILAB](http://http://tabilab.cmpe.boun.edu.tr/) from Boazii "
1089	        "University."
1090	    ),
1091	    "tr_gb": (
1092	        "This is a treebank annotating example sentences from a comprehensive "
1093	        "grammar book of Turkish."
1094	    ),
1095	    "tr_imst": (
1096	        "The UD Turkish Treebank, also called the IMST-UD Treebank, is a "
1097	        "semi-automatic conversion of the IMST Treebank (Sulubacak et al., "
1098	        "2016)."
1099	    ),
1100	    "tr_pud": (
1101	        "This is a part of the Parallel Universal Dependencies (PUD) treebanks"
1102	        " created for the CoNLL 2017 shared task on Multilingual Parsing from "
1103	        "Raw Text to Universal Dependencies."
1104	    ),
1105	    "uk_iu": (
1106	        "Gold standard Universal Dependencies corpus for Ukrainian, developed "
1107	        "for UD originally, by Institute for Ukrainian, NGO. []"
1108	    ),
1109	    "hsb_ufal": "A small treebank of Upper Sorbian based mostly on Wikipedia.",
1110	    "ur_udtb": (
1111	        "The Urdu Universal Dependency Treebank was automatically converted "
1112	        "from Urdu Dependency Treebank (UDTB) which is part of an ongoing "
1113	        "effort of creating multi-layered treebanks for Hindi and Urdu."
1114	    ),
1115	    "ug_udt": (
1116	        "The Uyghur UD treebank is based on the Uyghur Dependency Treebank "
1117	        "(UDT), created at the Xinjiang University in rmqi, China."
1118	    ),
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {
1217	        "test": "UD_Arabic-PUD/r2.10/ar_pud-ud-test.conllu",
1218	    },
1219	    "hy_armtdp": {
1220	        "train": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-train.conllu",
1221	        "dev": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-dev.conllu",
1222	        "test": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-test.conllu",
1223	    },
1224	    "aii_as": {
1225	        "test": "UD_Assyrian-AS/r2.10/aii_as-ud-test.conllu",
1226	    },
1227	    "bm_crb": {
1228	        "test": "UD_Bambara-CRB/r2.10/bm_crb-ud-test.conllu",
1229	    },
1230	    "eu_bdt": {
1231	        "train": "UD_Basque-BDT/r2.10/eu_bdt-ud-train.conllu",
1232	        "dev": "UD_Basque-BDT/r2.10/eu_bdt-ud-dev.conllu",
1233	        "test": "UD_Basque-BDT/r2.10/eu_bdt-ud-test.conllu",
1234	    },
1235	    "bej_nsc": {
1236	        "test": "UD_Beja-NSC/r2.10/bej_nsc-ud-test.conllu",
1237	    },
1238	    "be_hse": {
1239	        "train": "UD_Belarusian-HSE/r2.10/be_hse-ud-train.conllu",
1240	        "dev": "UD_Belarusian-HSE/r2.10/be_hse-ud-dev.conllu",
1241	        "test": "UD_Belarusian-HSE/r2.10/be_hse-ud-test.conllu",
1242	    },
1243	    "bn_bru": {
1244	        "test": "UD_Bengali-BRU/r2.10/bn_bru-ud-test.conllu",
1245	    },
1246	    "bho_bhtb": {
1247	        "test": "UD_Bhojpuri-BHTB/r2.10/bho_bhtb-ud-test.conllu",
1248	    },
1249	    "br_keb": {
1250	        "test": "UD_Breton-KEB/r2.10/br_keb-ud-test.conllu",
1251	    },
1252	    "bg_btb": {
1253	        "train": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-train.conllu",
1254	        "dev": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-dev.conllu",
1255	        "test": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-test.conllu",
1256	    },
1257	    "bxr_bdt": {
1258	        "train": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-train.conllu",
1259	        "test": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-test.conllu",
1260	    },
1261	    "yue_hk": {
1262	        "test": "UD_Cantonese-HK/r2.10/yue_hk-ud-test.conllu",
1263	    },
1264	    "ca_ancora": {
1265	        "train": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-train.conllu",
1266	        "dev": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-dev.conllu",
1267	        "test": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-test.conllu",
1268	    },
1269	    "ceb_gja": {
1270	        "test": "UD_Cebuano-GJA/r2.10/ceb_gja-ud-test.conllu",
1271	    },
1272	    "zh_cfl": {
1273	        "test": "UD_Chinese-CFL/r2.10/zh_cfl-ud-test.conllu",
1274	    },
1275	    "zh_gsd": {
1276	        "train": "UD_Chinese-GSD/r2.10/zh_gsd-ud-train.conllu",
1277	        "dev": "UD_Chinese-GSD/r2.10/zh_gsd-ud-dev.conllu",
1278	        "test": "UD_Chinese-GSD/r2.10/zh_gsd-ud-test.conllu",
1279	    },
1280	    "zh_gsdsimp": {
1281	        "train": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-train.conllu",
1282	        "dev": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-dev.conllu",
1283	        "test": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-test.conllu",
1284	    },
1285	    "zh_hk": {
1286	        "test": "UD_Chinese-HK/r2.10/zh_hk-ud-test.conllu",
1287	    },
1288	    "zh_pud": {
1289	        "test": "UD_Chinese-PUD/r2.10/zh_pud-ud-test.conllu",
1290	    },
1291	    "ckt_hse": {
1292	        "test": "UD_Chukchi-HSE/r2.10/ckt_hse-ud-test.conllu",
1293	    },
1294	    "lzh_kyoto": {
1295	        "train": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-train.conllu",
1296	        "dev": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-dev.conllu",
1297	        "test": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-test.conllu",
1298	    },
1299	    "cop_scriptorium": {
1300	        "train": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-train.conllu",
1301	        "dev": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-dev.conllu",
1302	        "test": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-test.conllu",
1303	    },
1304	    "hr_set": {
1305	        "train": "UD_Croatian-SET/r2.10/hr_set-ud-train.conllu",
1306	        "dev": "UD_Croatian-SET/r2.10/hr_set-ud-dev.conllu",
1307	        "test": "UD_Croatian-SET/r2.10/hr_set-ud-test.conllu",
1308	    },
1309	    "cs_cac": {
1310	        "train": "UD_Czech-CAC/r2.10/cs_cac-ud-train.conllu",
1311	        "dev": "UD_Czech-CAC/r2.10/cs_cac-ud-dev.conllu",
1312	        "test": "UD_Czech-CAC/r2.10/cs_cac-ud-test.conllu",
1313	    },
1314	    "cs_cltt": {
1315	        "train": "UD_Czech-CLTT/r2.10/cs_cltt-ud-train.conllu",
1316	        "dev": "UD_Czech-CLTT/r2.10/cs_cltt-ud-dev.conllu",
1317	        "test": "UD_Czech-CLTT/r2.10/cs_cltt-ud-test.conllu",
1318	    },
1319	    "cs_fictree": {
1320	        "train": "UD_Czech-FicTree/r2.10/cs_fictree-ud-train.conllu",
1321	        "dev": "UD_Czech-FicTree/r2.10/cs_fictree-ud-dev.conllu",
1322	        "test": "UD_Czech-FicTree/r2.10/cs_fictree-ud-test.conllu",
1323	    },
1324	    "cs_pdt": {
1325	        "train": [
1326	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-l.conllu",
1327	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-m.conllu",
1328	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-c.conllu",
1329	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-v.conllu",
1330	        ],
1331	        "dev": "UD_Czech-PDT/r2.10/cs_pdt-ud-dev.conllu",
1332	        "test": "UD_Czech-PDT/r2.10/cs_pdt-ud-test.conllu",
1333	    },
1334	    "cs_pud": {
1335	        "test": "UD_Czech-PUD/r2.10/cs_pud-ud-test.conllu",
1336	    },
1337	    "da_ddt": {
1338	        "train": "UD_Danish-DDT/r2.10/da_ddt-ud-train.conllu",
1339	        "dev": "UD_Danish-DDT/r2.10/da_ddt-ud-dev.conllu",
1340	        "test": "UD_Danish-DDT/r2.10/da_ddt-ud-test.conllu",
1341	    },
1342	    "nl_alpino": {
1343	        "train": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-train.conllu",
1344	        "dev": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-dev.conllu",
1345	        "test": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-test.conllu",
1346	    },
1347	    "nl_lassysmall": {
1348	        "train": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-train.conllu",
1349	        "dev": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-dev.conllu",
1350	        "test": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-test.conllu",
1351	    },
1352	    "en_atis": {
1353	        "train": "UD_English-Atis/r2.10/en_atis-ud-train.conllu",
1354	        "dev": "UD_English-Atis/r2.10/en_atis-ud-dev.conllu",
1355	        "test": "UD_English-Atis/r2.10/en_atis-ud-test.conllu",
1356	    },
1357	    "en_esl": {
1358	        "train": "UD_English-ESL/r2.10/en_esl-ud-train.conllu",
1359	        "dev": "UD_English-ESL/r2.10/en_esl-ud-dev.conllu",
1360	        "test": "UD_English-ESL/r2.10/en_esl-ud-test.conllu",
1361	    },
1362	    "en_ewt": {
1363	        "train": "UD_English-EWT/r2.10/en_ewt-ud-train.conllu",
1364	        "dev": "UD_English-EWT/r2.10/en_ewt-ud-dev.conllu",
1365	        "test": "UD_English-EWT/r2.10/en_ewt-ud-test.conllu",
1366	    },
1367	    "en_gum": {
1368	        "train": "UD_English-GUM/r2.10/en_gum-ud-train.conllu",
1369	        "dev": "UD_English-GUM/r2.10/en_gum-ud-dev.conllu",
1370	        "test": "UD_English-GUM/r2.10/en_gum-ud-test.conllu",
1371	    },
1372	    "en_gumreddit": {
1373	        "train": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-train.conllu",
1374	        "dev": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-dev.conllu",
1375	        "test": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-test.conllu",
1376	    },
1377	    "en_lines": {
1378	        "train": "UD_English-LinES/r2.10/en_lines-ud-train.conllu",
1379	        "dev": "UD_English-LinES/r2.10/en_lines-ud-dev.conllu",
1380	        "test": "UD_English-LinES/r2.10/en_lines-ud-test.conllu",
1381	    },
1382	    "en_partut": {
1383	        "train": "UD_English-ParTUT/r2.10/en_partut-ud-train.conllu",
1384	        "dev": "UD_English-ParTUT/r2.10/en_partut-ud-dev.conllu",
1385	        "test": "UD_English-ParTUT/r2.10/en_partut-ud-test.conllu",
1386	    },
1387	    "en_pronouns": {
1388	        "test": "UD_English-Pronouns/r2.10/en_pronouns-ud-test.conllu",
1389	    },
1390	    "en_pud": {
1391	        "test": "UD_English-PUD/r2.10/en_pud-ud-test.conllu",
1392	    },
1393	    "myv_jr": {
1394	        "test": "UD_Erzya-JR/r2.10/myv_jr-ud-test.conllu",
1395	    },
1396	    "et_edt": {
1397	        "train": "UD_Estonian-EDT/r2.10/et_edt-ud-train.conllu",
1398	        "dev": "UD_Estonian-EDT/r2.10/et_edt-ud-dev.conllu",
1399	        "test": "UD_Estonian-EDT/r2.10/et_edt-ud-test.conllu",
1400	    },
1401	    "et_ewt": {
1402	        "train": "UD_Estonian-EWT/r2.10/et_ewt-ud-train.conllu",
1403	        "dev": "UD_Estonian-EWT/r2.10/et_ewt-ud-dev.conllu",
1404	        "test": "UD_Estonian-EWT/r2.10/et_ewt-ud-test.conllu",
1405	    },
1406	    "fo_farpahc": {
1407	        "train": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-train.conllu",
1408	        "dev": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-dev.conllu",
1409	        "test": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-test.conllu",
1410	    },
1411	    "fo_oft": {
1412	        "test": "UD_Faroese-OFT/r2.10/fo_oft-ud-test.conllu",
1413	    },
1414	    "fi_ftb": {
1415	        "train": "UD_Finnish-FTB/r2.10/fi_ftb-ud-train.conllu",
1416	        "dev": "UD_Finnish-FTB/r2.10/fi_ftb-ud-dev.conllu",
1417	        "test": "UD_Finnish-FTB/r2.10/fi_ftb-ud-test.conllu",
1418	    },
1419	    "fi_ood": {
1420	        "test": "UD_Finnish-OOD/r2.10/fi_ood-ud-test.conllu",
1421	    },
1422	    "fi_pud": {
1423	        "test": "UD_Finnish-PUD/r2.10/fi_pud-ud-test.conllu",
1424	    },
1425	    "fi_tdt": {
1426	        "train": "UD_Finnish-TDT/r2.10/fi_tdt-ud-train.conllu",
1427	        "dev": "UD_Finnish-TDT/r2.10/fi_tdt-ud-dev.conllu",
1428	        "test": "UD_Finnish-TDT/r2.10/fi_tdt-ud-test.conllu",
1429	    },
1430	    "fr_parisstories": {
1431	        "train": "UD_French-ParisStories/r2.10/fr_parisstories-ud-train.conllu",
1432	        "test": "UD_French-ParisStories/r2.10/fr_parisstories-ud-test.conllu",
1433	    },
1434	    "fr_fqb": {
1435	        "test": "UD_French-FQB/r2.10/fr_fqb-ud-test.conllu",
1436	    },
1437	    "fr_rhapsodie": {
1438	        "train": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-train.conllu",
1439	        "dev": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-dev.conllu",
1440	        "test": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-test.conllu",
1441	    },
1442	    "fr_ftb": {
1443	        "train": "UD_French-FTB/r2.10/fr_ftb-ud-train.conllu",
1444	        "dev": "UD_French-FTB/r2.10/fr_ftb-ud-dev.conllu",
1445	        "test": "UD_French-FTB/r2.10/fr_ftb-ud-test.conllu",
1446	    },
1447	    "fr_gsd": {
1448	        "train": "UD_French-GSD/r2.10/fr_gsd-ud-train.conllu",
1449	        "dev": "UD_French-GSD/r2.10/fr_gsd-ud-dev.conllu",
1450	        "test": "UD_French-GSD/r2.10/fr_gsd-ud-test.conllu",
1451	    },
1452	    "fr_partut": {
1453	        "train": "UD_French-ParTUT/r2.10/fr_partut-ud-train.conllu",
1454	        "dev": "UD_French-ParTUT/r2.10/fr_partut-ud-dev.conllu",
1455	        "test": "UD_French-ParTUT/r2.10/fr_partut-ud-test.conllu",
1456	    },
1457	    "fr_pud": {
1458	        "test": "UD_French-PUD/r2.10/fr_pud-ud-test.conllu",
1459	    },
1460	    "fr_sequoia": {
1461	        "train": "UD_French-Sequoia/r2.10/fr_sequoia-ud-train.conllu",
1462	        "dev": "UD_French-Sequoia/r2.10/fr_sequoia-ud-dev.conllu",
1463	        "test": "UD_French-Sequoia/r2.10/fr_sequoia-ud-test.conllu",
1464	    },
1465	    "qfn_fame": {
1466	        "test": "UD_Frisian_Dutch-Fame/r2.10/qfn_fame-ud-test.conllu",
1467	    },
1468	    "gl_ctg": {
1469	        "train": "UD_Galician-CTG/r2.10/gl_ctg-ud-train.conllu",
1470	        "dev": "UD_Galician-CTG/r2.10/gl_ctg-ud-dev.conllu",
1471	        "test": "UD_Galician-CTG/r2.10/gl_ctg-ud-test.conllu",
1472	    },
1473	    "gl_treegal": {
1474	        "train": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-train.conllu",
1475	        "test": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-test.conllu",
1476	    },
1477	    "de_gsd": {
1478	        "train": "UD_German-GSD/r2.10/de_gsd-ud-train.conllu",
1479	        "dev": "UD_German-GSD/r2.10/de_gsd-ud-dev.conllu",
1480	        "test": "UD_German-GSD/r2.10/de_gsd-ud-test.conllu",
1481	    },
1482	    "de_hdt": {
1483	        "train": [
1484	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-1.conllu",
1485	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-2.conllu",
1486	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-1.conllu",
1487	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-2.conllu",
1488	        ],
1489	        "dev": "UD_German-HDT/r2.10/de_hdt-ud-dev.conllu",
1490	        "test": "UD_German-HDT/r2.10/de_hdt-ud-test.conllu",
1491	    },
1492	    "de_lit": {
1493	        "test": "UD_German-LIT/r2.10/de_lit-ud-test.conllu",
1494	    },
1495	    "de_pud": {
1496	        "test": "UD_German-PUD/r2.10/de_pud-ud-test.conllu",
1497	    },
1498	    "got_proiel": {
1499	        "train": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-train.conllu",
1500	        "dev": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-dev.conllu",
1501	        "test": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-test.conllu",
1502	    },
1503	    "el_gdt": {
1504	        "train": "UD_Greek-GDT/r2.10/el_gdt-ud-train.conllu",
1505	        "dev": "UD_Greek-GDT/r2.10/el_gdt-ud-dev.conllu",
1506	        "test": "UD_Greek-GDT/r2.10/el_gdt-ud-test.conllu",
1507	    },
1508	    "gub_tudet": {
1509	        "test": "UD_Guajajara-TuDeT/r2.10/gub_tudet-ud-test.conllu",
1510	    },
1511	    "gn_oldtudet": {
1512	        "test": "UD_Guarani-OldTuDeT/r2.10/gn_oldtudet-ud-test.conllu",
1513	    },
1514	    "he_iahltwiki": {
1515	        "train": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-train.conllu",
1516	        "dev": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-dev.conllu",
1517	        "test": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-test.conllu",
1518	    },
1519	    "he_htb": {
1520	        "train": "UD_Hebrew-HTB/r2.10/he_htb-ud-train.conllu",
1521	        "dev": "UD_Hebrew-HTB/r2.10/he_htb-ud-dev.conllu",
1522	        "test": "UD_Hebrew-HTB/r2.10/he_htb-ud-test.conllu",
1523	    },
1524	    "qhe_hiencs": {
1525	        "train": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-train.conllu",
1526	        "dev": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-dev.conllu",
1527	        "test": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-test.conllu",
1528	    },
1529	    "hi_hdtb": {
1530	        "train": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-train.conllu",
1531	        "dev": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-dev.conllu",
1532	        "test": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-test.conllu",
1533	    },
1534	    "hi_pud": {
1535	        "test": "UD_Hindi-PUD/r2.10/hi_pud-ud-test.conllu",
1536	    },
1537	    "hu_szeged": {
1538	        "train": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-train.conllu",
1539	        "dev": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-dev.conllu",
1540	        "test": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-test.conllu",
1541	    },
1542	    "is_modern": {
1543	        "train": "UD_Icelandic-Modern/r2.10/is_modern-ud-train.conllu",
1544	        "dev": "UD_Icelandic-Modern/r2.10/is_modern-ud-dev.conllu",
1545	        "test": "UD_Icelandic-Modern/r2.10/is_modern-ud-test.conllu",
1546	    },
1547	    "is_icepahc": {
1548	        "train": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-train.conllu",
1549	        "dev": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-dev.conllu",
1550	        "test": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-test.conllu",
1551	    },
1552	    "is_pud": {
1553	        "test": "UD_Icelandic-PUD/r2.10/is_pud-ud-test.conllu",
1554	    },
1555	    "id_csui": {
1556	        "train": "UD_Indonesian-CSUI/r2.10/id_csui-ud-train.conllu",
1557	        "test": "UD_Indonesian-CSUI/r2.10/id_csui-ud-test.conllu",
1558	    },
1559	    "id_gsd": {
1560	        "train": "UD_Indonesian-GSD/r2.10/id_gsd-ud-train.conllu",
1561	        "dev": "UD_Indonesian-GSD/r2.10/id_gsd-ud-dev.conllu",
1562	        "test": "UD_Indonesian-GSD/r2.10/id_gsd-ud-test.conllu",
1563	    },
1564	    "id_pud": {
1565	        "test": "UD_Indonesian-PUD/r2.10/id_pud-ud-test.conllu",
1566	    },
1567	    "ga_twittirish": {
1568	        "test": "UD_Irish-TwittIrish/r2.10/ga_twittirish-ud-test.conllu",
1569	    },
1570	    "ga_idt": {
1571	        "train": "UD_Irish-IDT/r2.10/ga_idt-ud-train.conllu",
1572	        "dev": "UD_Irish-IDT/r2.10/ga_idt-ud-dev.conllu",
1573	        "test": "UD_Irish-IDT/r2.10/ga_idt-ud-test.conllu",
1574	    },
1575	    "it_valico": {
1576	        "test": "UD_Italian-Valico/r2.10/it_valico-ud-test.conllu",
1577	    },
1578	    "it_markit": {
1579	        "train": "UD_Italian-MarkIT/r2.10/it_markit-ud-train.conllu",
1580	        "dev": "UD_Italian-MarkIT/r2.10/it_markit-ud-dev.conllu",
1581	        "test": "UD_Italian-MarkIT/r2.10/it_markit-ud-test.conllu",
1582	    },
1583	    "it_isdt": {
1584	        "train": "UD_Italian-ISDT/r2.10/it_isdt-ud-train.conllu",
1585	        "dev": "UD_Italian-ISDT/r2.10/it_isdt-ud-dev.conllu",
1586	        "test": "UD_Italian-ISDT/r2.10/it_isdt-ud-test.conllu",
1587	    },
1588	    "it_partut": {
1589	        "train": "UD_Italian-ParTUT/r2.10/it_partut-ud-train.conllu",
1590	        "dev": "UD_Italian-ParTUT/r2.10/it_partut-ud-dev.conllu",
1591	        "test": "UD_Italian-ParTUT/r2.10/it_partut-ud-test.conllu",
1592	    },
1593	    "it_postwita": {
1594	        "train": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-train.conllu",
1595	        "dev": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-dev.conllu",
1596	        "test": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-test.conllu",
1597	    },
1598	    "it_pud": {
1599	        "test": "UD_Italian-PUD/r2.10/it_pud-ud-test.conllu",
1600	    },
1601	    "it_twittiro": {
1602	        "train": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-train.conllu",
1603	        "dev": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-dev.conllu",
1604	        "test": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-test.conllu",
1605	    },
1606	    "it_vit": {
1607	        "train": "UD_Italian-VIT/r2.10/it_vit-ud-train.conllu",
1608	        "dev": "UD_Italian-VIT/r2.10/it_vit-ud-dev.conllu",
1609	        "test": "UD_Italian-VIT/r2.10/it_vit-ud-test.conllu",
1610	    },
1611	    "ja_gsdluw": {
1612	        "train": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-train.conllu",
1613	        "dev": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-dev.conllu",
1614	        "test": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-test.conllu",
1615	    },
1616	    "ja_pudluw": {
1617	        "test": "UD_Japanese-PUDLUW/r2.10/ja_pudluw-ud-test.conllu",
1618	    },
1619	    "ja_bccwjluw": {
1620	        "train": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-train.conllu",
1621	        "dev": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-dev.conllu",
1622	        "test": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-test.conllu",
1623	    },
1624	    "ja_bccwj": {
1625	        "train": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-train.conllu",
1626	        "dev": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-dev.conllu",
1627	        "test": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-test.conllu",
1628	    },
1629	    "ja_gsd": {
1630	        "train": "UD_Japanese-GSD/r2.10/ja_gsd-ud-train.conllu",
1631	        "dev": "UD_Japanese-GSD/r2.10/ja_gsd-ud-dev.conllu",
1632	        "test": "UD_Japanese-GSD/r2.10/ja_gsd-ud-test.conllu",
1633	    },
1634	    "ja_modern": {
1635	        "test": "UD_Japanese-Modern/r2.10/ja_modern-ud-test.conllu",
1636	    },
1637	    "ja_pud": {
1638	        "test": "UD_Japanese-PUD/r2.10/ja_pud-ud-test.conllu",
1639	    },
1640	    "jv_csui": {
1641	        "test": "UD_Javanese-CSUI/r2.10/jv_csui-ud-test.conllu",
1642	    },
1643	    "urb_tudet": {
1644	        "test": "UD_Kaapor-TuDeT/r2.10/urb_tudet-ud-test.conllu",
1645	    },
1646	    "xnr_kdtb": {
1647	        "test": "UD_Kangri-KDTB/r2.10/xnr_kdtb-ud-test.conllu",
1648	    },
1649	    "krl_kkpp": {
1650	        "test": "UD_Karelian-KKPP/r2.10/krl_kkpp-ud-test.conllu",
1651	    },
1652	    "arr_tudet": {
1653	        "test": "UD_Karo-TuDeT/r2.10/arr_tudet-ud-test.conllu",
1654	    },
1655	    "kk_ktb": {
1656	        "train": "UD_Kazakh-KTB/r2.10/kk_ktb-ud-train.conllu",
1657	        "test": "UD_Kazakh-KTB/r2.10/kk_ktb-ud-test.conllu",
1658	    },
1659	    "kfm_aha": {
1660	        "test": "UD_Khunsari-AHA/r2.10/kfm_aha-ud-test.conllu",
1661	    },
1662	    "quc_iu": {
1663	        "test": "UD_Kiche-IU/r2.10/quc_iu-ud-test.conllu",
1664	    },
1665	    "koi_uh": {
1666	        "test": "UD_Komi_Permyak-UH/r2.10/koi_uh-ud-test.conllu",
1667	    },
1668	    "kpv_ikdp": {
1669	        "test": "UD_Komi_Zyrian-IKDP/r2.10/kpv_ikdp-ud-test.conllu",
1670	    },
1671	    "kpv_lattice": {
1672	        "test": "UD_Komi_Zyrian-Lattice/r2.10/kpv_lattice-ud-test.conllu",
1673	    },
1674	    "ko_gsd": {
1675	        "train": "UD_Korean-GSD/r2.10/ko_gsd-ud-train.conllu",
1676	        "dev": "UD_Korean-GSD/r2.10/ko_gsd-ud-dev.conllu",
1677	        "test": "UD_Korean-GSD/r2.10/ko_gsd-ud-test.conllu",
1678	    },
1679	    "ko_kaist": {
1680	        "train": "UD_Korean-Kaist/r2.10/ko_kaist-ud-train.conllu",
1681	        "dev": "UD_Korean-Kaist/r2.10/ko_kaist-ud-dev.conllu",
1682	        "test": "UD_Korean-Kaist/r2.10/ko_kaist-ud-test.conllu",
1683	    },
1684	    "ko_pud": {
1685	        "test": "UD_Korean-PUD/r2.10/ko_pud-ud-test.conllu",
1686	    },
1687	    "kmr_mg": {
1688	        "train": "UD_Kurmanji-MG/r2.10/kmr_mg-ud-train.conllu",
1689	        "test": "UD_Kurmanji-MG/r2.10/kmr_mg-ud-test.conllu",
1690	    },
1691	    "la_udante": {
1692	        "train": "UD_Latin-UDante/r2.10/la_udante-ud-train.conllu",
1693	        "dev": "UD_Latin-UDante/r2.10/la_udante-ud-dev.conllu",
1694	        "test": "UD_Latin-UDante/r2.10/la_udante-ud-test.conllu",
1695	    },
1696	    "la_ittb": {
1697	        "train": "UD_Latin-ITTB/r2.10/la_ittb-ud-train.conllu",
1698	        "dev": "UD_Latin-ITTB/r2.10/la_ittb-ud-dev.conllu",
1699	        "test": "UD_Latin-ITTB/r2.10/la_ittb-ud-test.conllu",
1700	    },
1701	    "la_llct": {
1702	        "train": "UD_Latin-LLCT/r2.10/la_llct-ud-train.conllu",
1703	        "dev": "UD_Latin-LLCT/r2.10/la_llct-ud-dev.conllu",
1704	        "test": "UD_Latin-LLCT/r2.10/la_llct-ud-test.conllu",
1705	    },
1706	    "la_perseus": {
1707	        "train": "UD_Latin-Perseus/r2.10/la_perseus-ud-train.conllu",
1708	        "test": "UD_Latin-Perseus/r2.10/la_perseus-ud-test.conllu",
1709	    },
1710	    "la_proiel": {
1711	        "train": "UD_Latin-PROIEL/r2.10/la_proiel-ud-train.conllu",
1712	        "dev": "UD_Latin-PROIEL/r2.10/la_proiel-ud-dev.conllu",
1713	        "test": "UD_Latin-PROIEL/r2.10/la_proiel-ud-test.conllu",
1714	    },
1715	    "lv_lvtb": {
1716	        "train": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-train.conllu",
1717	        "dev": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-dev.conllu",
1718	        "test": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-test.conllu",
1719	    },
1720	    "lij_glt": {
1721	        "train": "UD_Ligurian-GLT/r2.10/lij_glt-ud-train.conllu",
1722	        "test": "UD_Ligurian-GLT/r2.10/lij_glt-ud-test.conllu",
1723	    },
1724	    "lt_alksnis": {
1725	        "train": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-train.conllu",
1726	        "dev": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-dev.conllu",
1727	        "test": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-test.conllu",
1728	    },
1729	    "lt_hse": {
1730	        "train": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1731	        "dev": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1732	        "test": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1733	    },
1734	    "olo_kkpp": {
1735	        "train": "UD_Livvi-KKPP/r2.10/olo_kkpp-ud-train.conllu",
1736	        "test": "UD_Livvi-KKPP/r2.10/olo_kkpp-ud-test.conllu",
1737	    },
1738	    "nds_lsdc": {
1739	        "test": "UD_Low_Saxon-LSDC/r2.10/nds_lsdc-ud-test.conllu",
1740	    },
1741	    # TODO(tfds) Add Madi Jarawara splits when it will be officially released.
1742	    # TODO(tfds) Add Makurap TuDeT splits when it will be officially released.
1743	    "mt_mudt": {
1744	        "train": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-train.conllu",
1745	        "dev": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-dev.conllu",
1746	        "test": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-test.conllu",
1747	    },
1748	    "gv_cadhan": {
1749	        "test": "UD_Manx-Cadhan/r2.10/gv_cadhan-ud-test.conllu",
1750	    },
1751	    "mr_ufal": {
1752	        "train": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-train.conllu",
1753	        "dev": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-dev.conllu",
1754	        "test": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-test.conllu",
1755	    },
1756	    "gun_dooley": {
1757	        "test": "UD_Mbya_Guarani-Dooley/r2.10/gun_dooley-ud-test.conllu",
1758	    },
1759	    "gun_thomas": {
1760	        "test": "UD_Mbya_Guarani-Thomas/r2.10/gun_thomas-ud-test.conllu",
1761	    },
1762	    "mdf_jr": {
1763	        "test": "UD_Moksha-JR/r2.10/mdf_jr-ud-test.conllu",
1764	    },
1765	    "myu_tudet": {
1766	        "test": "UD_Munduruku-TuDeT/r2.10/myu_tudet-ud-test.conllu",
1767	    },
1768	    "pcm_nsc": {
1769	        "train": "UD_Naija-NSC/r2.10/pcm_nsc-ud-train.conllu",
1770	        "dev": "UD_Naija-NSC/r2.10/pcm_nsc-ud-dev.conllu",
1771	        "test": "UD_Naija-NSC/r2.10/pcm_nsc-ud-test.conllu",
1772	    },
1773	    "nyq_aha": {
1774	        "test": "UD_Nayini-AHA/r2.10/nyq_aha-ud-test.conllu",
1775	    },
1776	    # TODO(tfds) Add Neapolitan RB splits when it will be officially released.
1777	    "sme_giella": {
1778	        "train": "UD_North_Sami-Giella/r2.10/sme_giella-ud-train.conllu",
1779	        "test": "UD_North_Sami-Giella/r2.10/sme_giella-ud-test.conllu",
1780	    },
1781	    "no_bokmaal": {
1782	        "train": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-train.conllu",
1783	        "dev": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-dev.conllu",
1784	        "test": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-test.conllu",
1785	    },
1786	    "no_nynorsk": {
1787	        "train": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-train.conllu",
1788	        "dev": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-dev.conllu",
1789	        "test": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-test.conllu",
1790	    },
1791	    "no_nynorsklia": {
1792	        "train": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-train.conllu",
1793	        "dev": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-dev.conllu",
1794	        "test": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-test.conllu",
1795	    },
1796	    "cu_proiel": {
1797	        "train": (
1798	            "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-train.conllu"
1799	        ),
1800	        "dev": "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-dev.conllu",
1801	        "test": "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-test.conllu",
1802	    },
1803	    "fro_srcmf": {
1804	        "train": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-train.conllu",
1805	        "dev": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-dev.conllu",
1806	        "test": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-test.conllu",
1807	    },
1808	    "orv_birchbark": {
1809	        "train": (
1810	            "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-train.conllu"
1811	        ),
1812	        "dev": "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-dev.conllu",
1813	        "test": (
1814	            "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-test.conllu"
1815	        ),
1816	    },
1817	    "orv_rnc": {
1818	        "train": "UD_Old_Russian-RNC/r2.10/orv_rnc-ud-train.conllu",
1819	        "test": "UD_Old_Russian-RNC/r2.10/orv_rnc-ud-test.conllu",
1820	    },
1821	    "orv_torot": {
1822	        "train": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-train.conllu",
1823	        "dev": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-dev.conllu",
1824	        "test": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-test.conllu",
1825	    },
1826	    "otk_tonqq": {
1827	        "test": "UD_Old_Turkish-Tonqq/r2.10/otk_tonqq-ud-test.conllu",
1828	    },
1829	    "fa_perdt": {
1830	        "train": "UD_Persian-PerDT/r2.10/fa_perdt-ud-train.conllu",
1831	        "dev": "UD_Persian-PerDT/r2.10/fa_perdt-ud-dev.conllu",
1832	        "test": "UD_Persian-PerDT/r2.10/fa_perdt-ud-test.conllu",
1833	    },
1834	    "fa_seraji": {
1835	        "train": "UD_Persian-Seraji/r2.10/fa_seraji-ud-train.conllu",
1836	        "dev": "UD_Persian-Seraji/r2.10/fa_seraji-ud-dev.conllu",
1837	        "test": "UD_Persian-Seraji/r2.10/fa_seraji-ud-test.conllu",
1838	    },
1839	    "pl_lfg": {
1840	        "train": "UD_Polish-LFG/r2.10/pl_lfg-ud-train.conllu",
1841	        "dev": "UD_Polish-LFG/r2.10/pl_lfg-ud-dev.conllu",
1842	        "test": "UD_Polish-LFG/r2.10/pl_lfg-ud-test.conllu",
1843	    },
1844	    "pl_pdb": {
1845	        "train": "UD_Polish-PDB/r2.10/pl_pdb-ud-train.conllu",
1846	        "dev": "UD_Polish-PDB/r2.10/pl_pdb-ud-dev.conllu",
1847	        "test": "UD_Polish-PDB/r2.10/pl_pdb-ud-test.conllu",
1848	    },
1849	    "pl_pud": {
1850	        "test": "UD_Polish-PUD/r2.10/pl_pud-ud-test.conllu",
1851	    },
1852	    "pt_bosque": {
1853	        "train": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-train.conllu",
1854	        "dev": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-dev.conllu",
1855	        "test": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-test.conllu",
1856	    },
1857	    "pt_gsd": {
1858	        "train": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-train.conllu",
1859	        "dev": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-dev.conllu",
1860	        "test": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-test.conllu",
1861	    },
1862	    "pt_pud": {
1863	        "test": "UD_Portuguese-PUD/r2.10/pt_pud-ud-test.conllu",
1864	    },
1865	    "ro_art": {
1866	        "test": "UD_Romanian-ArT/r2.10/ro_art-ud-test.conllu",
1867	    },
1868	    "ro_nonstandard": {
1869	        "train": "UD_Romanian-Nonstandard/r2.10/ro_nonstandard-ud-train.conllu",
1870	        "dev": "UD_Romanian-Nonstandard/r2.10/ro_nonstandard-ud-dev.conllu",
1871	        "test": "UD_Romanian-Nonstandard/r2.10/ro_nonstandard-ud-test.conllu",
1872	    },
1873	    "ro_rrt": {
1874	        "train": "UD_Romanian-RRT/r2.10/ro_rrt-ud-train.conllu",
1875	        "dev": "UD_Romanian-RRT/r2.10/ro_rrt-ud-dev.conllu",
1876	        "test": "UD_Romanian-RRT/r2.10/ro_rrt-ud-test.conllu",
1877	    },
1878	    "ro_simonero": {
1879	        "train": "UD_Romanian-SiMoNERo/r2.10/ro_simonero-ud-train.conllu",
1880	        "dev": "UD_Romanian-SiMoNERo/r2.10/ro_simonero-ud-dev.conllu",
1881	        "test": "UD_Romanian-SiMoNERo/r2.10/ro_simonero-ud-test.conllu",
1882	    },
1883	    "ru_gsd": {
1884	        "train": "UD_Russian-GSD/r2.10/ru_gsd-ud-train.conllu",
1885	        "dev": "UD_Russian-GSD/r2.10/ru_gsd-ud-dev.conllu",
1886	        "test": "UD_Russian-GSD/r2.10/ru_gsd-ud-test.conllu",
1887	    },
1888	    "ru_pud": {
1889	        "test": "UD_Russian-PUD/r2.10/ru_pud-ud-test.conllu",
1890	    },
1891	    "ru_syntagrus": {
1892	        "train": "UD_Russian-SynTagRus/r2.7/ru_syntagrus-ud-train.conllu",
1893	        "dev": "UD_Russian-SynTagRus/r2.7/ru_syntagrus-ud-dev.conllu",
1894	        "test": "UD_Russian-SynTagRus/r2.7/ru_syntagrus-ud-test.conllu",
1895	    },
1896	    "ru_taiga": {
1897	        "train": "UD_Russian-Taiga/r2.10/ru_taiga-ud-train.conllu",
1898	        "dev": "UD_Russian-Taiga/r2.10/ru_taiga-ud-dev.conllu",
1899	        "test": "UD_Russian-Taiga/r2.10/ru_taiga-ud-test.conllu",
1900	    },
1901	    "sa_ufal": {
1902	        "test": "UD_Sanskrit-UFAL/r2.10/sa_ufal-ud-test.conllu",
1903	    },
1904	    "sa_vedic": {
1905	        "train": "UD_Sanskrit-Vedic/r2.10/sa_vedic-ud-train.conllu",
1906	        "test": "UD_Sanskrit-Vedic/r2.10/sa_vedic-ud-test.conllu",
1907	    },
1908	    "gd_arcosg": {
1909	        "train": "UD_Scottish_Gaelic-ARCOSG/r2.10/gd_arcosg-ud-train.conllu",
1910	        "dev": "UD_Scottish_Gaelic-ARCOSG/r2.10/gd_arcosg-ud-dev.conllu",
1911	        "test": "UD_Scottish_Gaelic-ARCOSG/r2.10/gd_arcosg-ud-test.conllu",
1912	    },
1913	    "sr_set": {
1914	        "train": "UD_Serbian-SET/r2.10/sr_set-ud-train.conllu",
1915	        "dev": "UD_Serbian-SET/r2.10/sr_set-ud-dev.conllu",
1916	        "test": "UD_Serbian-SET/r2.10/sr_set-ud-test.conllu",
1917	    },
1918	    "sms_giellagas": {
1919	        "test": "UD_Skolt_Sami-Giellagas/r2.10/sms_giellagas-ud-test.conllu",
1920	    },
1921	    "sk_snk": {
1922	        "train": "UD_Slovak-SNK/r2.10/sk_snk-ud-train.conllu",
1923	        "dev": "UD_Slovak-SNK/r2.10/sk_snk-ud-dev.conllu",
1924	        "test": "UD_Slovak-SNK/r2.10/sk_snk-ud-test.conllu",
1925	    },
1926	    "sl_ssj": {
1927	        "train": "UD_Slovenian-SSJ/r2.10/sl_ssj-ud-train.conllu",
1928	        "dev": "UD_Slovenian-SSJ/r2.10/sl_ssj-ud-dev.conllu",
1929	        "test": "UD_Slovenian-SSJ/r2.10/sl_ssj-ud-test.conllu",
1930	    },
1931	    "sl_sst": {
1932	        "train": "UD_Slovenian-SST/r2.10/sl_sst-ud-train.conllu",
1933	        "test": "UD_Slovenian-SST/r2.10/sl_sst-ud-test.conllu",
1934	    },
1935	    "soj_aha": {
1936	        "test": "UD_Soi-AHA/r2.10/soj_aha-ud-test.conllu",
1937	    },
1938	    "ajp_madar": {
1939	        "test": (
1940	            "UD_South_Levantine_Arabic-MADAR/r2.10/ajp_madar-ud-test.conllu"
1941	        ),
1942	    },
1943	    "es_ancora": {
1944	        "train": "UD_Spanish-AnCora/r2.10/es_ancora-ud-train.conllu",
1945	        "dev": "UD_Spanish-AnCora/r2.10/es_ancora-ud-dev.conllu",
1946	        "test": "UD_Spanish-AnCora/r2.10/es_ancora-ud-test.conllu",
1947	    },
1948	    "es_gsd": {
1949	        "train": "UD_Spanish-GSD/r2.10/es_gsd-ud-train.conllu",
1950	        "dev": "UD_Spanish-GSD/r2.10/es_gsd-ud-dev.conllu",
1951	        "test": "UD_Spanish-GSD/r2.10/es_gsd-ud-test.conllu",
1952	    },
1953	    "es_pud": {
1954	        "test": "UD_Spanish-PUD/r2.10/es_pud-ud-test.conllu",
1955	    },
1956	    "swl_sslc": {
1957	        "train": "UD_Swedish_Sign_Language-SSLC/r2.10/swl_sslc-ud-train.conllu",
1958	        "dev": "UD_Swedish_Sign_Language-SSLC/r2.10/swl_sslc-ud-dev.conllu",
1959	        "test": "UD_Swedish_Sign_Language-SSLC/r2.10/swl_sslc-ud-test.conllu",
1960	    },
1961	    "sv_lines": {
1962	        "train": "UD_Swedish-LinES/r2.10/sv_lines-ud-train.conllu",
1963	        "dev": "UD_Swedish-LinES/r2.10/sv_lines-ud-dev.conllu",
1964	        "test": "UD_Swedish-LinES/r2.10/sv_lines-ud-test.conllu",
1965	    },
1966	    "sv_pud": {
1967	        "test": "UD_Swedish-PUD/r2.10/sv_pud-ud-test.conllu",
1968	    },
1969	    "sv_talbanken": {
1970	        "train": "UD_Swedish-Talbanken/r2.10/sv_talbanken-ud-train.conllu",
1971	        "dev": "UD_Swedish-Talbanken/r2.10/sv_talbanken-ud-dev.conllu",
1972	        "test": "UD_Swedish-Talbanken/r2.10/sv_talbanken-ud-test.conllu",
1973	    },
1974	    "gsw_uzh": {
1975	        "test": "UD_Swiss_German-UZH/r2.10/gsw_uzh-ud-test.conllu",
1976	    },
1977	    "tl_trg": {
1978	        "test": "UD_Tagalog-TRG/r2.10/tl_trg-ud-test.conllu",
1979	    },
1980	    "tl_ugnayan": {
1981	        "test": "UD_Tagalog-Ugnayan/r2.10/tl_ugnayan-ud-test.conllu",
1982	    },
1983	    "ta_mwtt": {
1984	        "test": "UD_Tamil-MWTT/r2.10/ta_mwtt-ud-test.conllu",
1985	    },
1986	    "ta_ttb": {
1987	        "train": "UD_Tamil-TTB/r2.10/ta_ttb-ud-train.conllu",
1988	        "dev": "UD_Tamil-TTB/r2.10/ta_ttb-ud-dev.conllu",
1989	        "test": "UD_Tamil-TTB/r2.10/ta_ttb-ud-test.conllu",
1990	    },
1991	    "te_mtg": {
1992	        "train": "UD_Telugu-MTG/r2.10/te_mtg-ud-train.conllu",
1993	        "dev": "UD_Telugu-MTG/r2.10/te_mtg-ud-dev.conllu",
1994	        "test": "UD_Telugu-MTG/r2.10/te_mtg-ud-test.conllu",
1995	    },
1996	    "th_pud": {
1997	        "test": "UD_Thai-PUD/r2.10/th_pud-ud-test.conllu",
1998	    },
1999	    "tpn_tudet": {
2000	        "test": "UD_Tupinamba-TuDeT/r2.10/tpn_tudet-ud-test.conllu",
2001	    },
2002	    "qtd_sagt": {
2003	        "train": "UD_Turkish_German-SAGT/r2.10/qtd_sagt-ud-train.conllu",
2004	        "dev": "UD_Turkish_German-SAGT/r2.10/qtd_sagt-ud-dev.conllu",
2005	        "test": "UD_Turkish_German-SAGT/r2.10/qtd_sagt-ud-test.conllu",
2006	    },
2007	    "tr_kenet": {
2008	        "train": "UD_Turkish-Kenet/r2.10/tr_kenet-ud-train.conllu",
2009	        "dev": "UD_Turkish-Kenet/r2.10/tr_kenet-ud-dev.conllu",
2010	        "test": "UD_Turkish-Kenet/r2.10/tr_kenet-ud-test.conllu",
2011	    },
2012	    "tr_atis": {
2013	        "train": "UD_Turkish-Atis/r2.10/tr_atis-ud-train.conllu",
2014	        "dev": "UD_Turkish-Atis/r2.10/tr_atis-ud-dev.conllu",
2015	        "test": "UD_Turkish-Atis/r2.10/tr_atis-ud-test.conllu",
2016	    },
2017	    "tr_penn": {
2018	        "train": "UD_Turkish-Penn/r2.10/tr_penn-ud-train.conllu",
2019	        "dev": "UD_Turkish-Penn/r2.10/tr_penn-ud-dev.conllu",
2020	        "test": "UD_Turkish-Penn/r2.10/tr_penn-ud-test.conllu",
2021	    },
2022	    "tr_tourism": {
2023	        "train": "UD_Turkish-Tourism/r2.10/tr_tourism-ud-train.conllu",
2024	        "dev": "UD_Turkish-Tourism/r2.10/tr_tourism-ud-dev.conllu",
2025	        "test": "UD_Turkish-Tourism/r2.10/tr_tourism-ud-test.conllu",
2026	    },
2027	    "tr_framenet": {
2028	        "train": "UD_Turkish-FrameNet/r2.10/tr_framenet-ud-train.conllu",
2029	        "dev": "UD_Turkish-FrameNet/r2.10/tr_framenet-ud-dev.conllu",
2030	        "test": "UD_Turkish-FrameNet/r2.10/tr_framenet-ud-test.conllu",
2031	    },
2032	    "tr_boun": {
2033	        "train": "UD_Turkish-BOUN/r2.10/tr_boun-ud-train.conllu",
2034	        "dev": "UD_Turkish-BOUN/r2.10/tr_boun-ud-dev.conllu",
2035	        "test": "UD_Turkish-BOUN/r2.10/tr_boun-ud-test.conllu",
2036	    },
2037	    "tr_gb": {
2038	        "test": "UD_Turkish-GB/r2.10/tr_gb-ud-test.conllu",
2039	    },
2040	    "tr_imst": {
2041	        "train": "UD_Turkish-IMST/r2.10/tr_imst-ud-train.conllu",
2042	        "dev": "UD_Turkish-IMST/r2.10/tr_imst-ud-dev.conllu",
2043	        "test": "UD_Turkish-IMST/r2.10/tr_imst-ud-test.conllu",
2044	    },
2045	    "tr_pud": {
2046	        "test": "UD_Turkish-PUD/r2.10/tr_pud-ud-test.conllu",
2047	    },
2048	    "uk_iu": {
2049	        "train": "UD_Ukrainian-IU/r2.10/uk_iu-ud-train.conllu",
2050	        "dev": "UD_Ukrainian-IU/r2.10/uk_iu-ud-dev.conllu",
2051	        "test": "UD_Ukrainian-IU/r2.10/uk_iu-ud-test.conllu",
2052	    },
2053	    # TODO(tfds) Add Umbrian Ikuvina split when it will be officially released.
2054	    "hsb_ufal": {
2055	        "train": "UD_Upper_Sorbian-UFAL/r2.10/hsb_ufal-ud-train.conllu",
2056	        "test": "UD_Upper_Sorbian-UFAL/r2.10/hsb_ufal-ud-test.conllu",
2057	    },
2058	    "ur_udtb": {
2059	        "train": "UD_Urdu-UDTB/r2.10/ur_udtb-ud-train.conllu",
2060	        "dev": "UD_Urdu-UDTB/r2.10/ur_udtb-ud-dev.conllu",
2061	        "test": "UD_Urdu-UDTB/r2.10/ur_udtb-ud-test.conllu",
2062	    },
2063	    "ug_udt": {
2064	        "train": "UD_Uyghur-UDT/r2.10/ug_udt-ud-train.conllu",
2065	        "dev": "UD_Uyghur-UDT/r2.10/ug_udt-ud-dev.conllu",
2066	        "test": "UD_Uyghur-UDT/r2.10/ug_udt-ud-test.conllu",
2067	    },
2068	    "vi_vtb": {
2069	        "train": "UD_Vietnamese-VTB/r2.10/vi_vtb-ud-train.conllu",
2070	        "dev": "UD_Vietnamese-VTB/r2.10/vi_vtb-ud-dev.conllu",
2071	        "test": "UD_Vietnamese-VTB/r2.10/vi_vtb-ud-test.conllu",
2072	    },
2073	    "wbp_ufal": {
2074	        "test": "UD_Warlpiri-UFAL/r2.10/wbp_ufal-ud-test.conllu",
2075	    },
2076	    "cy_ccg": {
2077	        "train": "UD_Welsh-CCG/r2.10/cy_ccg-ud-train.conllu",
2078	        "test": "UD_Welsh-CCG/r2.10/cy_ccg-ud-test.conllu",
2079	    },
2080	    "wo_wtb": {
2081	        "train": "UD_Wolof-WTB/r2.10/wo_wtb-ud-train.conllu",
2082	        "dev": "UD_Wolof-WTB/r2.10/wo_wtb-ud-dev.conllu",
2083	        "test": "UD_Wolof-WTB/r2.10/wo_wtb-ud-test.conllu",
2084	    },
2085	    "sjo_xdt": {
2086	        "test": "UD_Xibe-XDT/r2.10/sjo_xdt-ud-test.conllu",
2087	    },
2088	    "sah_yktdt": {
2089	        "test": "UD_Yakut-YKTDT/r2.10/sah_yktdt-ud-test.conllu",
2090	    },
2091	    "yo_ytb": {
2092	        "test": "UD_Yoruba-YTB/r2.10/yo_ytb-ud-test.conllu",
2093	    },
2094	    "ess_sli": {
2095	        "test": "UD_Yupik-SLI/r2.10/ess_sli-ud-test.conllu",
2096	    },
2097	}
2098	
2099	LANGS = DESCRIPTIONS.keys()
2100	
2101	
2102	def prepare_ud_filepaths(
2103	    path_prefix: epath.PathLike,
2104	    filepaths: Union[epath.PathLike, List[epath.PathLike]],
2105	) -> List[epath.PathLike]:
2106	  """Prepends a path prefix to a (list of) filepaths.
2107	
2108	  Args:
2109	    path_prefix: A path which will be prepended to all paths in filepaths.
2110	    filepaths: The filepaths to be prepared. Could be a list of paths for
2111	      multiple files, or a single path.
2112	
2113	  Returns:
2114	    A list with the resulting filepath(s). In case a single path was given as
2115	    `filepath`, it will returns a one-item list containing the resulting path.
2116	  """
2117	  paths = filepaths if isinstance(filepaths, list) else [filepaths]
2118	  return [os.path.join(path_prefix, filepath) for filepath in paths]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:1120
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1119	    "vi_vtb": (
1120	        "The Vietnamese UD treebank is a conversion of the constituent "
1121	        "treebank created in the VLSP project (https://vlsp.hpda.vn/)."
1122	    ),
1123	    "wbp_ufal": (
1124	        "A small treebank of grammatical examples in Warlpiri, taken from "
1125	        "linguistic literature."
1126	    ),
1127	    "cy_ccg": (
1128	        "UD Welsh-CCG (Corpws Cystrawennol y Gymraeg) is a treebank of Welsh, "
1129	        "annotated according to the Universal Dependencies guidelines."
1130	    ),
1131	    "hy_armtdp": (
1132	        "A Universal Dependencies treebank for Eastern Armenian developed for "
1133	        "UD originally by the ArmTDP team led by Marat M. Yavrumyan at the "
1134	        "Yerevan State University."
1135	    ),
1136	    "wo_wtb": (
1137	        "UD_Wolof-WTB is a natively manual developed treebank for Wolof. "
1138	        "Sentences were collected from encyclopedic, fictional, biographical, "
1139	        "religious texts and news."
1140	    ),
1141	    "sjo_xdt": (
1142	        "The UD Xibe Treebank is a corpus of the Xibe language (ISO "
1143	        "639-3: sjo) containing manually annotated syntactic trees under the "
1144	        "Universal Dependencies. Sentences come from three sources: grammar "
1145	        "book examples, newspaper (Cabcal News) and Xibe textbooks."
1146	    ),
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {
1217	        "test": "UD_Arabic-PUD/r2.10/ar_pud-ud-test.conllu",
1218	    },
1219	    "hy_armtdp": {
1220	        "train": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-train.conllu",
1221	        "dev": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-dev.conllu",
1222	        "test": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-test.conllu",
1223	    },
1224	    "aii_as": {
1225	        "test": "UD_Assyrian-AS/r2.10/aii_as-ud-test.conllu",
1226	    },
1227	    "bm_crb": {
1228	        "test": "UD_Bambara-CRB/r2.10/bm_crb-ud-test.conllu",
1229	    },
1230	    "eu_bdt": {
1231	        "train": "UD_Basque-BDT/r2.10/eu_bdt-ud-train.conllu",
1232	        "dev": "UD_Basque-BDT/r2.10/eu_bdt-ud-dev.conllu",
1233	        "test": "UD_Basque-BDT/r2.10/eu_bdt-ud-test.conllu",
1234	    },
1235	    "bej_nsc": {
1236	        "test": "UD_Beja-NSC/r2.10/bej_nsc-ud-test.conllu",
1237	    },
1238	    "be_hse": {
1239	        "train": "UD_Belarusian-HSE/r2.10/be_hse-ud-train.conllu",
1240	        "dev": "UD_Belarusian-HSE/r2.10/be_hse-ud-dev.conllu",
1241	        "test": "UD_Belarusian-HSE/r2.10/be_hse-ud-test.conllu",
1242	    },
1243	    "bn_bru": {
1244	        "test": "UD_Bengali-BRU/r2.10/bn_bru-ud-test.conllu",
1245	    },
1246	    "bho_bhtb": {
1247	        "test": "UD_Bhojpuri-BHTB/r2.10/bho_bhtb-ud-test.conllu",
1248	    },
1249	    "br_keb": {
1250	        "test": "UD_Breton-KEB/r2.10/br_keb-ud-test.conllu",
1251	    },
1252	    "bg_btb": {
1253	        "train": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-train.conllu",
1254	        "dev": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-dev.conllu",
1255	        "test": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-test.conllu",
1256	    },
1257	    "bxr_bdt": {
1258	        "train": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-train.conllu",
1259	        "test": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-test.conllu",
1260	    },
1261	    "yue_hk": {
1262	        "test": "UD_Cantonese-HK/r2.10/yue_hk-ud-test.conllu",
1263	    },
1264	    "ca_ancora": {
1265	        "train": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-train.conllu",
1266	        "dev": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-dev.conllu",
1267	        "test": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-test.conllu",
1268	    },
1269	    "ceb_gja": {
1270	        "test": "UD_Cebuano-GJA/r2.10/ceb_gja-ud-test.conllu",
1271	    },
1272	    "zh_cfl": {
1273	        "test": "UD_Chinese-CFL/r2.10/zh_cfl-ud-test.conllu",
1274	    },
1275	    "zh_gsd": {
1276	        "train": "UD_Chinese-GSD/r2.10/zh_gsd-ud-train.conllu",
1277	        "dev": "UD_Chinese-GSD/r2.10/zh_gsd-ud-dev.conllu",
1278	        "test": "UD_Chinese-GSD/r2.10/zh_gsd-ud-test.conllu",
1279	    },
1280	    "zh_gsdsimp": {
1281	        "train": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-train.conllu",
1282	        "dev": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-dev.conllu",
1283	        "test": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-test.conllu",
1284	    },
1285	    "zh_hk": {
1286	        "test": "UD_Chinese-HK/r2.10/zh_hk-ud-test.conllu",
1287	    },
1288	    "zh_pud": {
1289	        "test": "UD_Chinese-PUD/r2.10/zh_pud-ud-test.conllu",
1290	    },
1291	    "ckt_hse": {
1292	        "test": "UD_Chukchi-HSE/r2.10/ckt_hse-ud-test.conllu",
1293	    },
1294	    "lzh_kyoto": {
1295	        "train": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-train.conllu",
1296	        "dev": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-dev.conllu",
1297	        "test": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-test.conllu",
1298	    },
1299	    "cop_scriptorium": {
1300	        "train": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-train.conllu",
1301	        "dev": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-dev.conllu",
1302	        "test": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-test.conllu",
1303	    },
1304	    "hr_set": {
1305	        "train": "UD_Croatian-SET/r2.10/hr_set-ud-train.conllu",
1306	        "dev": "UD_Croatian-SET/r2.10/hr_set-ud-dev.conllu",
1307	        "test": "UD_Croatian-SET/r2.10/hr_set-ud-test.conllu",
1308	    },
1309	    "cs_cac": {
1310	        "train": "UD_Czech-CAC/r2.10/cs_cac-ud-train.conllu",
1311	        "dev": "UD_Czech-CAC/r2.10/cs_cac-ud-dev.conllu",
1312	        "test": "UD_Czech-CAC/r2.10/cs_cac-ud-test.conllu",
1313	    },
1314	    "cs_cltt": {
1315	        "train": "UD_Czech-CLTT/r2.10/cs_cltt-ud-train.conllu",
1316	        "dev": "UD_Czech-CLTT/r2.10/cs_cltt-ud-dev.conllu",
1317	        "test": "UD_Czech-CLTT/r2.10/cs_cltt-ud-test.conllu",
1318	    },
1319	    "cs_fictree": {
1320	        "train": "UD_Czech-FicTree/r2.10/cs_fictree-ud-train.conllu",
1321	        "dev": "UD_Czech-FicTree/r2.10/cs_fictree-ud-dev.conllu",
1322	        "test": "UD_Czech-FicTree/r2.10/cs_fictree-ud-test.conllu",
1323	    },
1324	    "cs_pdt": {
1325	        "train": [
1326	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-l.conllu",
1327	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-m.conllu",
1328	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-c.conllu",
1329	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-v.conllu",
1330	        ],
1331	        "dev": "UD_Czech-PDT/r2.10/cs_pdt-ud-dev.conllu",
1332	        "test": "UD_Czech-PDT/r2.10/cs_pdt-ud-test.conllu",
1333	    },
1334	    "cs_pud": {
1335	        "test": "UD_Czech-PUD/r2.10/cs_pud-ud-test.conllu",
1336	    },
1337	    "da_ddt": {
1338	        "train": "UD_Danish-DDT/r2.10/da_ddt-ud-train.conllu",
1339	        "dev": "UD_Danish-DDT/r2.10/da_ddt-ud-dev.conllu",
1340	        "test": "UD_Danish-DDT/r2.10/da_ddt-ud-test.conllu",
1341	    },
1342	    "nl_alpino": {
1343	        "train": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-train.conllu",
1344	        "dev": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-dev.conllu",
1345	        "test": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-test.conllu",
1346	    },
1347	    "nl_lassysmall": {
1348	        "train": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-train.conllu",
1349	        "dev": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-dev.conllu",
1350	        "test": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-test.conllu",
1351	    },
1352	    "en_atis": {
1353	        "train": "UD_English-Atis/r2.10/en_atis-ud-train.conllu",
1354	        "dev": "UD_English-Atis/r2.10/en_atis-ud-dev.conllu",
1355	        "test": "UD_English-Atis/r2.10/en_atis-ud-test.conllu",
1356	    },
1357	    "en_esl": {
1358	        "train": "UD_English-ESL/r2.10/en_esl-ud-train.conllu",
1359	        "dev": "UD_English-ESL/r2.10/en_esl-ud-dev.conllu",
1360	        "test": "UD_English-ESL/r2.10/en_esl-ud-test.conllu",
1361	    },
1362	    "en_ewt": {
1363	        "train": "UD_English-EWT/r2.10/en_ewt-ud-train.conllu",
1364	        "dev": "UD_English-EWT/r2.10/en_ewt-ud-dev.conllu",
1365	        "test": "UD_English-EWT/r2.10/en_ewt-ud-test.conllu",
1366	    },
1367	    "en_gum": {
1368	        "train": "UD_English-GUM/r2.10/en_gum-ud-train.conllu",
1369	        "dev": "UD_English-GUM/r2.10/en_gum-ud-dev.conllu",
1370	        "test": "UD_English-GUM/r2.10/en_gum-ud-test.conllu",
1371	    },
1372	    "en_gumreddit": {
1373	        "train": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-train.conllu",
1374	        "dev": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-dev.conllu",
1375	        "test": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-test.conllu",
1376	    },
1377	    "en_lines": {
1378	        "train": "UD_English-LinES/r2.10/en_lines-ud-train.conllu",
1379	        "dev": "UD_English-LinES/r2.10/en_lines-ud-dev.conllu",
1380	        "test": "UD_English-LinES/r2.10/en_lines-ud-test.conllu",
1381	    },
1382	    "en_partut": {
1383	        "train": "UD_English-ParTUT/r2.10/en_partut-ud-train.conllu",
1384	        "dev": "UD_English-ParTUT/r2.10/en_partut-ud-dev.conllu",
1385	        "test": "UD_English-ParTUT/r2.10/en_partut-ud-test.conllu",
1386	    },
1387	    "en_pronouns": {
1388	        "test": "UD_English-Pronouns/r2.10/en_pronouns-ud-test.conllu",
1389	    },
1390	    "en_pud": {
1391	        "test": "UD_English-PUD/r2.10/en_pud-ud-test.conllu",
1392	    },
1393	    "myv_jr": {
1394	        "test": "UD_Erzya-JR/r2.10/myv_jr-ud-test.conllu",
1395	    },
1396	    "et_edt": {
1397	        "train": "UD_Estonian-EDT/r2.10/et_edt-ud-train.conllu",
1398	        "dev": "UD_Estonian-EDT/r2.10/et_edt-ud-dev.conllu",
1399	        "test": "UD_Estonian-EDT/r2.10/et_edt-ud-test.conllu",
1400	    },
1401	    "et_ewt": {
1402	        "train": "UD_Estonian-EWT/r2.10/et_ewt-ud-train.conllu",
1403	        "dev": "UD_Estonian-EWT/r2.10/et_ewt-ud-dev.conllu",
1404	        "test": "UD_Estonian-EWT/r2.10/et_ewt-ud-test.conllu",
1405	    },
1406	    "fo_farpahc": {
1407	        "train": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-train.conllu",
1408	        "dev": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-dev.conllu",
1409	        "test": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-test.conllu",
1410	    },
1411	    "fo_oft": {
1412	        "test": "UD_Faroese-OFT/r2.10/fo_oft-ud-test.conllu",
1413	    },
1414	    "fi_ftb": {
1415	        "train": "UD_Finnish-FTB/r2.10/fi_ftb-ud-train.conllu",
1416	        "dev": "UD_Finnish-FTB/r2.10/fi_ftb-ud-dev.conllu",
1417	        "test": "UD_Finnish-FTB/r2.10/fi_ftb-ud-test.conllu",
1418	    },
1419	    "fi_ood": {
1420	        "test": "UD_Finnish-OOD/r2.10/fi_ood-ud-test.conllu",
1421	    },
1422	    "fi_pud": {
1423	        "test": "UD_Finnish-PUD/r2.10/fi_pud-ud-test.conllu",
1424	    },
1425	    "fi_tdt": {
1426	        "train": "UD_Finnish-TDT/r2.10/fi_tdt-ud-train.conllu",
1427	        "dev": "UD_Finnish-TDT/r2.10/fi_tdt-ud-dev.conllu",
1428	        "test": "UD_Finnish-TDT/r2.10/fi_tdt-ud-test.conllu",
1429	    },
1430	    "fr_parisstories": {
1431	        "train": "UD_French-ParisStories/r2.10/fr_parisstories-ud-train.conllu",
1432	        "test": "UD_French-ParisStories/r2.10/fr_parisstories-ud-test.conllu",
1433	    },
1434	    "fr_fqb": {
1435	        "test": "UD_French-FQB/r2.10/fr_fqb-ud-test.conllu",
1436	    },
1437	    "fr_rhapsodie": {
1438	        "train": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-train.conllu",
1439	        "dev": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-dev.conllu",
1440	        "test": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-test.conllu",
1441	    },
1442	    "fr_ftb": {
1443	        "train": "UD_French-FTB/r2.10/fr_ftb-ud-train.conllu",
1444	        "dev": "UD_French-FTB/r2.10/fr_ftb-ud-dev.conllu",
1445	        "test": "UD_French-FTB/r2.10/fr_ftb-ud-test.conllu",
1446	    },
1447	    "fr_gsd": {
1448	        "train": "UD_French-GSD/r2.10/fr_gsd-ud-train.conllu",
1449	        "dev": "UD_French-GSD/r2.10/fr_gsd-ud-dev.conllu",
1450	        "test": "UD_French-GSD/r2.10/fr_gsd-ud-test.conllu",
1451	    },
1452	    "fr_partut": {
1453	        "train": "UD_French-ParTUT/r2.10/fr_partut-ud-train.conllu",
1454	        "dev": "UD_French-ParTUT/r2.10/fr_partut-ud-dev.conllu",
1455	        "test": "UD_French-ParTUT/r2.10/fr_partut-ud-test.conllu",
1456	    },
1457	    "fr_pud": {
1458	        "test": "UD_French-PUD/r2.10/fr_pud-ud-test.conllu",
1459	    },
1460	    "fr_sequoia": {
1461	        "train": "UD_French-Sequoia/r2.10/fr_sequoia-ud-train.conllu",
1462	        "dev": "UD_French-Sequoia/r2.10/fr_sequoia-ud-dev.conllu",
1463	        "test": "UD_French-Sequoia/r2.10/fr_sequoia-ud-test.conllu",
1464	    },
1465	    "qfn_fame": {
1466	        "test": "UD_Frisian_Dutch-Fame/r2.10/qfn_fame-ud-test.conllu",
1467	    },
1468	    "gl_ctg": {
1469	        "train": "UD_Galician-CTG/r2.10/gl_ctg-ud-train.conllu",
1470	        "dev": "UD_Galician-CTG/r2.10/gl_ctg-ud-dev.conllu",
1471	        "test": "UD_Galician-CTG/r2.10/gl_ctg-ud-test.conllu",
1472	    },
1473	    "gl_treegal": {
1474	        "train": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-train.conllu",
1475	        "test": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-test.conllu",
1476	    },
1477	    "de_gsd": {
1478	        "train": "UD_German-GSD/r2.10/de_gsd-ud-train.conllu",
1479	        "dev": "UD_German-GSD/r2.10/de_gsd-ud-dev.conllu",
1480	        "test": "UD_German-GSD/r2.10/de_gsd-ud-test.conllu",
1481	    },
1482	    "de_hdt": {
1483	        "train": [
1484	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-1.conllu",
1485	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-2.conllu",
1486	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-1.conllu",
1487	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-2.conllu",
1488	        ],
1489	        "dev": "UD_German-HDT/r2.10/de_hdt-ud-dev.conllu",
1490	        "test": "UD_German-HDT/r2.10/de_hdt-ud-test.conllu",
1491	    },
1492	    "de_lit": {
1493	        "test": "UD_German-LIT/r2.10/de_lit-ud-test.conllu",
1494	    },
1495	    "de_pud": {
1496	        "test": "UD_German-PUD/r2.10/de_pud-ud-test.conllu",
1497	    },
1498	    "got_proiel": {
1499	        "train": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-train.conllu",
1500	        "dev": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-dev.conllu",
1501	        "test": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-test.conllu",
1502	    },
1503	    "el_gdt": {
1504	        "train": "UD_Greek-GDT/r2.10/el_gdt-ud-train.conllu",
1505	        "dev": "UD_Greek-GDT/r2.10/el_gdt-ud-dev.conllu",
1506	        "test": "UD_Greek-GDT/r2.10/el_gdt-ud-test.conllu",
1507	    },
1508	    "gub_tudet": {
1509	        "test": "UD_Guajajara-TuDeT/r2.10/gub_tudet-ud-test.conllu",
1510	    },
1511	    "gn_oldtudet": {
1512	        "test": "UD_Guarani-OldTuDeT/r2.10/gn_oldtudet-ud-test.conllu",
1513	    },
1514	    "he_iahltwiki": {
1515	        "train": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-train.conllu",
1516	        "dev": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-dev.conllu",
1517	        "test": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-test.conllu",
1518	    },
1519	    "he_htb": {
1520	        "train": "UD_Hebrew-HTB/r2.10/he_htb-ud-train.conllu",
1521	        "dev": "UD_Hebrew-HTB/r2.10/he_htb-ud-dev.conllu",
1522	        "test": "UD_Hebrew-HTB/r2.10/he_htb-ud-test.conllu",
1523	    },
1524	    "qhe_hiencs": {
1525	        "train": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-train.conllu",
1526	        "dev": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-dev.conllu",
1527	        "test": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-test.conllu",
1528	    },
1529	    "hi_hdtb": {
1530	        "train": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-train.conllu",
1531	        "dev": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-dev.conllu",
1532	        "test": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-test.conllu",
1533	    },
1534	    "hi_pud": {
1535	        "test": "UD_Hindi-PUD/r2.10/hi_pud-ud-test.conllu",
1536	    },
1537	    "hu_szeged": {
1538	        "train": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-train.conllu",
1539	        "dev": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-dev.conllu",
1540	        "test": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-test.conllu",
1541	    },
1542	    "is_modern": {
1543	        "train": "UD_Icelandic-Modern/r2.10/is_modern-ud-train.conllu",
1544	        "dev": "UD_Icelandic-Modern/r2.10/is_modern-ud-dev.conllu",
1545	        "test": "UD_Icelandic-Modern/r2.10/is_modern-ud-test.conllu",
1546	    },
1547	    "is_icepahc": {
1548	        "train": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-train.conllu",
1549	        "dev": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-dev.conllu",
1550	        "test": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-test.conllu",
1551	    },
1552	    "is_pud": {
1553	        "test": "UD_Icelandic-PUD/r2.10/is_pud-ud-test.conllu",
1554	    },
1555	    "id_csui": {
1556	        "train": "UD_Indonesian-CSUI/r2.10/id_csui-ud-train.conllu",
1557	        "test": "UD_Indonesian-CSUI/r2.10/id_csui-ud-test.conllu",
1558	    },
1559	    "id_gsd": {
1560	        "train": "UD_Indonesian-GSD/r2.10/id_gsd-ud-train.conllu",
1561	        "dev": "UD_Indonesian-GSD/r2.10/id_gsd-ud-dev.conllu",
1562	        "test": "UD_Indonesian-GSD/r2.10/id_gsd-ud-test.conllu",
1563	    },
1564	    "id_pud": {
1565	        "test": "UD_Indonesian-PUD/r2.10/id_pud-ud-test.conllu",
1566	    },
1567	    "ga_twittirish": {
1568	        "test": "UD_Irish-TwittIrish/r2.10/ga_twittirish-ud-test.conllu",
1569	    },
1570	    "ga_idt": {
1571	        "train": "UD_Irish-IDT/r2.10/ga_idt-ud-train.conllu",
1572	        "dev": "UD_Irish-IDT/r2.10/ga_idt-ud-dev.conllu",
1573	        "test": "UD_Irish-IDT/r2.10/ga_idt-ud-test.conllu",
1574	    },
1575	    "it_valico": {
1576	        "test": "UD_Italian-Valico/r2.10/it_valico-ud-test.conllu",
1577	    },
1578	    "it_markit": {
1579	        "train": "UD_Italian-MarkIT/r2.10/it_markit-ud-train.conllu",
1580	        "dev": "UD_Italian-MarkIT/r2.10/it_markit-ud-dev.conllu",
1581	        "test": "UD_Italian-MarkIT/r2.10/it_markit-ud-test.conllu",
1582	    },
1583	    "it_isdt": {
1584	        "train": "UD_Italian-ISDT/r2.10/it_isdt-ud-train.conllu",
1585	        "dev": "UD_Italian-ISDT/r2.10/it_isdt-ud-dev.conllu",
1586	        "test": "UD_Italian-ISDT/r2.10/it_isdt-ud-test.conllu",
1587	    },
1588	    "it_partut": {
1589	        "train": "UD_Italian-ParTUT/r2.10/it_partut-ud-train.conllu",
1590	        "dev": "UD_Italian-ParTUT/r2.10/it_partut-ud-dev.conllu",
1591	        "test": "UD_Italian-ParTUT/r2.10/it_partut-ud-test.conllu",
1592	    },
1593	    "it_postwita": {
1594	        "train": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-train.conllu",
1595	        "dev": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-dev.conllu",
1596	        "test": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-test.conllu",
1597	    },
1598	    "it_pud": {
1599	        "test": "UD_Italian-PUD/r2.10/it_pud-ud-test.conllu",
1600	    },
1601	    "it_twittiro": {
1602	        "train": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-train.conllu",
1603	        "dev": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-dev.conllu",
1604	        "test": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-test.conllu",
1605	    },
1606	    "it_vit": {
1607	        "train": "UD_Italian-VIT/r2.10/it_vit-ud-train.conllu",
1608	        "dev": "UD_Italian-VIT/r2.10/it_vit-ud-dev.conllu",
1609	        "test": "UD_Italian-VIT/r2.10/it_vit-ud-test.conllu",
1610	    },
1611	    "ja_gsdluw": {
1612	        "train": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-train.conllu",
1613	        "dev": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-dev.conllu",
1614	        "test": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-test.conllu",
1615	    },
1616	    "ja_pudluw": {
1617	        "test": "UD_Japanese-PUDLUW/r2.10/ja_pudluw-ud-test.conllu",
1618	    },
1619	    "ja_bccwjluw": {
1620	        "train": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-train.conllu",
1621	        "dev": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-dev.conllu",
1622	        "test": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-test.conllu",
1623	    },
1624	    "ja_bccwj": {
1625	        "train": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-train.conllu",
1626	        "dev": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-dev.conllu",
1627	        "test": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-test.conllu",
1628	    },
1629	    "ja_gsd": {
1630	        "train": "UD_Japanese-GSD/r2.10/ja_gsd-ud-train.conllu",
1631	        "dev": "UD_Japanese-GSD/r2.10/ja_gsd-ud-dev.conllu",
1632	        "test": "UD_Japanese-GSD/r2.10/ja_gsd-ud-test.conllu",
1633	    },
1634	    "ja_modern": {
1635	        "test": "UD_Japanese-Modern/r2.10/ja_modern-ud-test.conllu",
1636	    },
1637	    "ja_pud": {
1638	        "test": "UD_Japanese-PUD/r2.10/ja_pud-ud-test.conllu",
1639	    },
1640	    "jv_csui": {
1641	        "test": "UD_Javanese-CSUI/r2.10/jv_csui-ud-test.conllu",
1642	    },
1643	    "urb_tudet": {
1644	        "test": "UD_Kaapor-TuDeT/r2.10/urb_tudet-ud-test.conllu",
1645	    },
1646	    "xnr_kdtb": {
1647	        "test": "UD_Kangri-KDTB/r2.10/xnr_kdtb-ud-test.conllu",
1648	    },
1649	    "krl_kkpp": {
1650	        "test": "UD_Karelian-KKPP/r2.10/krl_kkpp-ud-test.conllu",
1651	    },
1652	    "arr_tudet": {
1653	        "test": "UD_Karo-TuDeT/r2.10/arr_tudet-ud-test.conllu",
1654	    },
1655	    "kk_ktb": {
1656	        "train": "UD_Kazakh-KTB/r2.10/kk_ktb-ud-train.conllu",
1657	        "test": "UD_Kazakh-KTB/r2.10/kk_ktb-ud-test.conllu",
1658	    },
1659	    "kfm_aha": {
1660	        "test": "UD_Khunsari-AHA/r2.10/kfm_aha-ud-test.conllu",
1661	    },
1662	    "quc_iu": {
1663	        "test": "UD_Kiche-IU/r2.10/quc_iu-ud-test.conllu",
1664	    },
1665	    "koi_uh": {
1666	        "test": "UD_Komi_Permyak-UH/r2.10/koi_uh-ud-test.conllu",
1667	    },
1668	    "kpv_ikdp": {
1669	        "test": "UD_Komi_Zyrian-IKDP/r2.10/kpv_ikdp-ud-test.conllu",
1670	    },
1671	    "kpv_lattice": {
1672	        "test": "UD_Komi_Zyrian-Lattice/r2.10/kpv_lattice-ud-test.conllu",
1673	    },
1674	    "ko_gsd": {
1675	        "train": "UD_Korean-GSD/r2.10/ko_gsd-ud-train.conllu",
1676	        "dev": "UD_Korean-GSD/r2.10/ko_gsd-ud-dev.conllu",
1677	        "test": "UD_Korean-GSD/r2.10/ko_gsd-ud-test.conllu",
1678	    },
1679	    "ko_kaist": {
1680	        "train": "UD_Korean-Kaist/r2.10/ko_kaist-ud-train.conllu",
1681	        "dev": "UD_Korean-Kaist/r2.10/ko_kaist-ud-dev.conllu",
1682	        "test": "UD_Korean-Kaist/r2.10/ko_kaist-ud-test.conllu",
1683	    },
1684	    "ko_pud": {
1685	        "test": "UD_Korean-PUD/r2.10/ko_pud-ud-test.conllu",
1686	    },
1687	    "kmr_mg": {
1688	        "train": "UD_Kurmanji-MG/r2.10/kmr_mg-ud-train.conllu",
1689	        "test": "UD_Kurmanji-MG/r2.10/kmr_mg-ud-test.conllu",
1690	    },
1691	    "la_udante": {
1692	        "train": "UD_Latin-UDante/r2.10/la_udante-ud-train.conllu",
1693	        "dev": "UD_Latin-UDante/r2.10/la_udante-ud-dev.conllu",
1694	        "test": "UD_Latin-UDante/r2.10/la_udante-ud-test.conllu",
1695	    },
1696	    "la_ittb": {
1697	        "train": "UD_Latin-ITTB/r2.10/la_ittb-ud-train.conllu",
1698	        "dev": "UD_Latin-ITTB/r2.10/la_ittb-ud-dev.conllu",
1699	        "test": "UD_Latin-ITTB/r2.10/la_ittb-ud-test.conllu",
1700	    },
1701	    "la_llct": {
1702	        "train": "UD_Latin-LLCT/r2.10/la_llct-ud-train.conllu",
1703	        "dev": "UD_Latin-LLCT/r2.10/la_llct-ud-dev.conllu",
1704	        "test": "UD_Latin-LLCT/r2.10/la_llct-ud-test.conllu",
1705	    },
1706	    "la_perseus": {
1707	        "train": "UD_Latin-Perseus/r2.10/la_perseus-ud-train.conllu",
1708	        "test": "UD_Latin-Perseus/r2.10/la_perseus-ud-test.conllu",
1709	    },
1710	    "la_proiel": {
1711	        "train": "UD_Latin-PROIEL/r2.10/la_proiel-ud-train.conllu",
1712	        "dev": "UD_Latin-PROIEL/r2.10/la_proiel-ud-dev.conllu",
1713	        "test": "UD_Latin-PROIEL/r2.10/la_proiel-ud-test.conllu",
1714	    },
1715	    "lv_lvtb": {
1716	        "train": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-train.conllu",
1717	        "dev": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-dev.conllu",
1718	        "test": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-test.conllu",
1719	    },
1720	    "lij_glt": {
1721	        "train": "UD_Ligurian-GLT/r2.10/lij_glt-ud-train.conllu",
1722	        "test": "UD_Ligurian-GLT/r2.10/lij_glt-ud-test.conllu",
1723	    },
1724	    "lt_alksnis": {
1725	        "train": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-train.conllu",
1726	        "dev": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-dev.conllu",
1727	        "test": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-test.conllu",
1728	    },
1729	    "lt_hse": {
1730	        "train": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1731	        "dev": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1732	        "test": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1733	    },
1734	    "olo_kkpp": {
1735	        "train": "UD_Livvi-KKPP/r2.10/olo_kkpp-ud-train.conllu",
1736	        "test": "UD_Livvi-KKPP/r2.10/olo_kkpp-ud-test.conllu",
1737	    },
1738	    "nds_lsdc": {
1739	        "test": "UD_Low_Saxon-LSDC/r2.10/nds_lsdc-ud-test.conllu",
1740	    },
1741	    # TODO(tfds) Add Madi Jarawara splits when it will be officially released.
1742	    # TODO(tfds) Add Makurap TuDeT splits when it will be officially released.
1743	    "mt_mudt": {
1744	        "train": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-train.conllu",
1745	        "dev": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-dev.conllu",
1746	        "test": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-test.conllu",
1747	    },
1748	    "gv_cadhan": {
1749	        "test": "UD_Manx-Cadhan/r2.10/gv_cadhan-ud-test.conllu",
1750	    },
1751	    "mr_ufal": {
1752	        "train": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-train.conllu",
1753	        "dev": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-dev.conllu",
1754	        "test": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-test.conllu",
1755	    },
1756	    "gun_dooley": {
1757	        "test": "UD_Mbya_Guarani-Dooley/r2.10/gun_dooley-ud-test.conllu",
1758	    },
1759	    "gun_thomas": {
1760	        "test": "UD_Mbya_Guarani-Thomas/r2.10/gun_thomas-ud-test.conllu",
1761	    },
1762	    "mdf_jr": {
1763	        "test": "UD_Moksha-JR/r2.10/mdf_jr-ud-test.conllu",
1764	    },
1765	    "myu_tudet": {
1766	        "test": "UD_Munduruku-TuDeT/r2.10/myu_tudet-ud-test.conllu",
1767	    },
1768	    "pcm_nsc": {
1769	        "train": "UD_Naija-NSC/r2.10/pcm_nsc-ud-train.conllu",
1770	        "dev": "UD_Naija-NSC/r2.10/pcm_nsc-ud-dev.conllu",
1771	        "test": "UD_Naija-NSC/r2.10/pcm_nsc-ud-test.conllu",
1772	    },
1773	    "nyq_aha": {
1774	        "test": "UD_Nayini-AHA/r2.10/nyq_aha-ud-test.conllu",
1775	    },
1776	    # TODO(tfds) Add Neapolitan RB splits when it will be officially released.
1777	    "sme_giella": {
1778	        "train": "UD_North_Sami-Giella/r2.10/sme_giella-ud-train.conllu",
1779	        "test": "UD_North_Sami-Giella/r2.10/sme_giella-ud-test.conllu",
1780	    },
1781	    "no_bokmaal": {
1782	        "train": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-train.conllu",
1783	        "dev": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-dev.conllu",
1784	        "test": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-test.conllu",
1785	    },
1786	    "no_nynorsk": {
1787	        "train": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-train.conllu",
1788	        "dev": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-dev.conllu",
1789	        "test": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-test.conllu",
1790	    },
1791	    "no_nynorsklia": {
1792	        "train": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-train.conllu",
1793	        "dev": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-dev.conllu",
1794	        "test": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-test.conllu",
1795	    },
1796	    "cu_proiel": {
1797	        "train": (
1798	            "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-train.conllu"
1799	        ),
1800	        "dev": "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-dev.conllu",
1801	        "test": "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-test.conllu",
1802	    },
1803	    "fro_srcmf": {
1804	        "train": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-train.conllu",
1805	        "dev": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-dev.conllu",
1806	        "test": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-test.conllu",
1807	    },
1808	    "orv_birchbark": {
1809	        "train": (
1810	            "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-train.conllu"
1811	        ),
1812	        "dev": "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-dev.conllu",
1813	        "test": (
1814	            "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-test.conllu"
1815	        ),
1816	    },
1817	    "orv_rnc": {
1818	        "train": "UD_Old_Russian-RNC/r2.10/orv_rnc-ud-train.conllu",
1819	        "test": "UD_Old_Russian-RNC/r2.10/orv_rnc-ud-test.conllu",
1820	    },
1821	    "orv_torot": {
1822	        "train": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-train.conllu",
1823	        "dev": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-dev.conllu",
1824	        "test": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-test.conllu",
1825	    },
1826	    "otk_tonqq": {
1827	        "test": "UD_Old_Turkish-Tonqq/r2.10/otk_tonqq-ud-test.conllu",
1828	    },
1829	    "fa_perdt": {
1830	        "train": "UD_Persian-PerDT/r2.10/fa_perdt-ud-train.conllu",
1831	        "dev": "UD_Persian-PerDT/r2.10/fa_perdt-ud-dev.conllu",
1832	        "test": "UD_Persian-PerDT/r2.10/fa_perdt-ud-test.conllu",
1833	    },
1834	    "fa_seraji": {
1835	        "train": "UD_Persian-Seraji/r2.10/fa_seraji-ud-train.conllu",
1836	        "dev": "UD_Persian-Seraji/r2.10/fa_seraji-ud-dev.conllu",
1837	        "test": "UD_Persian-Seraji/r2.10/fa_seraji-ud-test.conllu",
1838	    },
1839	    "pl_lfg": {
1840	        "train": "UD_Polish-LFG/r2.10/pl_lfg-ud-train.conllu",
1841	        "dev": "UD_Polish-LFG/r2.10/pl_lfg-ud-dev.conllu",
1842	        "test": "UD_Polish-LFG/r2.10/pl_lfg-ud-test.conllu",
1843	    },
1844	    "pl_pdb": {
1845	        "train": "UD_Polish-PDB/r2.10/pl_pdb-ud-train.conllu",
1846	        "dev": "UD_Polish-PDB/r2.10/pl_pdb-ud-dev.conllu",
1847	        "test": "UD_Polish-PDB/r2.10/pl_pdb-ud-test.conllu",
1848	    },
1849	    "pl_pud": {
1850	        "test": "UD_Polish-PUD/r2.10/pl_pud-ud-test.conllu",
1851	    },
1852	    "pt_bosque": {
1853	        "train": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-train.conllu",
1854	        "dev": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-dev.conllu",
1855	        "test": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-test.conllu",
1856	    },
1857	    "pt_gsd": {
1858	        "train": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-train.conllu",
1859	        "dev": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-dev.conllu",
1860	        "test": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-test.conllu",
1861	    },
1862	    "pt_pud": {
1863	        "test": "UD_Portuguese-PUD/r2.10/pt_pud-ud-test.conllu",
1864	    },
1865	    "ro_art": {
1866	        "test": "UD_Romanian-ArT/r2.10/ro_art-ud-test.conllu",
1867	    },
1868	    "ro_nonstandard": {
1869	        "train": "UD_Romanian-Nonstandard/r2.10/ro_nonstandard-ud-train.conllu",
1870	        "dev": "UD_Romanian-Nonstandard/r2.10/ro_nonstandard-ud-dev.conllu",
1871	        "test": "UD_Romanian-Nonstandard/r2.10/ro_nonstandard-ud-test.conllu",
1872	    },
1873	    "ro_rrt": {
1874	        "train": "UD_Romanian-RRT/r2.10/ro_rrt-ud-train.conllu",
1875	        "dev": "UD_Romanian-RRT/r2.10/ro_rrt-ud-dev.conllu",
1876	        "test": "UD_Romanian-RRT/r2.10/ro_rrt-ud-test.conllu",
1877	    },
1878	    "ro_simonero": {
1879	        "train": "UD_Romanian-SiMoNERo/r2.10/ro_simonero-ud-train.conllu",
1880	        "dev": "UD_Romanian-SiMoNERo/r2.10/ro_simonero-ud-dev.conllu",
1881	        "test": "UD_Romanian-SiMoNERo/r2.10/ro_simonero-ud-test.conllu",
1882	    },
1883	    "ru_gsd": {
1884	        "train": "UD_Russian-GSD/r2.10/ru_gsd-ud-train.conllu",
1885	        "dev": "UD_Russian-GSD/r2.10/ru_gsd-ud-dev.conllu",
1886	        "test": "UD_Russian-GSD/r2.10/ru_gsd-ud-test.conllu",
1887	    },
1888	    "ru_pud": {
1889	        "test": "UD_Russian-PUD/r2.10/ru_pud-ud-test.conllu",
1890	    },
1891	    "ru_syntagrus": {
1892	        "train": "UD_Russian-SynTagRus/r2.7/ru_syntagrus-ud-train.conllu",
1893	        "dev": "UD_Russian-SynTagRus/r2.7/ru_syntagrus-ud-dev.conllu",
1894	        "test": "UD_Russian-SynTagRus/r2.7/ru_syntagrus-ud-test.conllu",
1895	    },
1896	    "ru_taiga": {
1897	        "train": "UD_Russian-Taiga/r2.10/ru_taiga-ud-train.conllu",
1898	        "dev": "UD_Russian-Taiga/r2.10/ru_taiga-ud-dev.conllu",
1899	        "test": "UD_Russian-Taiga/r2.10/ru_taiga-ud-test.conllu",
1900	    },
1901	    "sa_ufal": {
1902	        "test": "UD_Sanskrit-UFAL/r2.10/sa_ufal-ud-test.conllu",
1903	    },
1904	    "sa_vedic": {
1905	        "train": "UD_Sanskrit-Vedic/r2.10/sa_vedic-ud-train.conllu",
1906	        "test": "UD_Sanskrit-Vedic/r2.10/sa_vedic-ud-test.conllu",
1907	    },
1908	    "gd_arcosg": {
1909	        "train": "UD_Scottish_Gaelic-ARCOSG/r2.10/gd_arcosg-ud-train.conllu",
1910	        "dev": "UD_Scottish_Gaelic-ARCOSG/r2.10/gd_arcosg-ud-dev.conllu",
1911	        "test": "UD_Scottish_Gaelic-ARCOSG/r2.10/gd_arcosg-ud-test.conllu",
1912	    },
1913	    "sr_set": {
1914	        "train": "UD_Serbian-SET/r2.10/sr_set-ud-train.conllu",
1915	        "dev": "UD_Serbian-SET/r2.10/sr_set-ud-dev.conllu",
1916	        "test": "UD_Serbian-SET/r2.10/sr_set-ud-test.conllu",
1917	    },
1918	    "sms_giellagas": {
1919	        "test": "UD_Skolt_Sami-Giellagas/r2.10/sms_giellagas-ud-test.conllu",
1920	    },
1921	    "sk_snk": {
1922	        "train": "UD_Slovak-SNK/r2.10/sk_snk-ud-train.conllu",
1923	        "dev": "UD_Slovak-SNK/r2.10/sk_snk-ud-dev.conllu",
1924	        "test": "UD_Slovak-SNK/r2.10/sk_snk-ud-test.conllu",
1925	    },
1926	    "sl_ssj": {
1927	        "train": "UD_Slovenian-SSJ/r2.10/sl_ssj-ud-train.conllu",
1928	        "dev": "UD_Slovenian-SSJ/r2.10/sl_ssj-ud-dev.conllu",
1929	        "test": "UD_Slovenian-SSJ/r2.10/sl_ssj-ud-test.conllu",
1930	    },
1931	    "sl_sst": {
1932	        "train": "UD_Slovenian-SST/r2.10/sl_sst-ud-train.conllu",
1933	        "test": "UD_Slovenian-SST/r2.10/sl_sst-ud-test.conllu",
1934	    },
1935	    "soj_aha": {
1936	        "test": "UD_Soi-AHA/r2.10/soj_aha-ud-test.conllu",
1937	    },
1938	    "ajp_madar": {
1939	        "test": (
1940	            "UD_South_Levantine_Arabic-MADAR/r2.10/ajp_madar-ud-test.conllu"
1941	        ),
1942	    },
1943	    "es_ancora": {
1944	        "train": "UD_Spanish-AnCora/r2.10/es_ancora-ud-train.conllu",
1945	        "dev": "UD_Spanish-AnCora/r2.10/es_ancora-ud-dev.conllu",
1946	        "test": "UD_Spanish-AnCora/r2.10/es_ancora-ud-test.conllu",
1947	    },
1948	    "es_gsd": {
1949	        "train": "UD_Spanish-GSD/r2.10/es_gsd-ud-train.conllu",
1950	        "dev": "UD_Spanish-GSD/r2.10/es_gsd-ud-dev.conllu",
1951	        "test": "UD_Spanish-GSD/r2.10/es_gsd-ud-test.conllu",
1952	    },
1953	    "es_pud": {
1954	        "test": "UD_Spanish-PUD/r2.10/es_pud-ud-test.conllu",
1955	    },
1956	    "swl_sslc": {
1957	        "train": "UD_Swedish_Sign_Language-SSLC/r2.10/swl_sslc-ud-train.conllu",
1958	        "dev": "UD_Swedish_Sign_Language-SSLC/r2.10/swl_sslc-ud-dev.conllu",
1959	        "test": "UD_Swedish_Sign_Language-SSLC/r2.10/swl_sslc-ud-test.conllu",
1960	    },
1961	    "sv_lines": {
1962	        "train": "UD_Swedish-LinES/r2.10/sv_lines-ud-train.conllu",
1963	        "dev": "UD_Swedish-LinES/r2.10/sv_lines-ud-dev.conllu",
1964	        "test": "UD_Swedish-LinES/r2.10/sv_lines-ud-test.conllu",
1965	    },
1966	    "sv_pud": {
1967	        "test": "UD_Swedish-PUD/r2.10/sv_pud-ud-test.conllu",
1968	    },
1969	    "sv_talbanken": {
1970	        "train": "UD_Swedish-Talbanken/r2.10/sv_talbanken-ud-train.conllu",
1971	        "dev": "UD_Swedish-Talbanken/r2.10/sv_talbanken-ud-dev.conllu",
1972	        "test": "UD_Swedish-Talbanken/r2.10/sv_talbanken-ud-test.conllu",
1973	    },
1974	    "gsw_uzh": {
1975	        "test": "UD_Swiss_German-UZH/r2.10/gsw_uzh-ud-test.conllu",
1976	    },
1977	    "tl_trg": {
1978	        "test": "UD_Tagalog-TRG/r2.10/tl_trg-ud-test.conllu",
1979	    },
1980	    "tl_ugnayan": {
1981	        "test": "UD_Tagalog-Ugnayan/r2.10/tl_ugnayan-ud-test.conllu",
1982	    },
1983	    "ta_mwtt": {
1984	        "test": "UD_Tamil-MWTT/r2.10/ta_mwtt-ud-test.conllu",
1985	    },
1986	    "ta_ttb": {
1987	        "train": "UD_Tamil-TTB/r2.10/ta_ttb-ud-train.conllu",
1988	        "dev": "UD_Tamil-TTB/r2.10/ta_ttb-ud-dev.conllu",
1989	        "test": "UD_Tamil-TTB/r2.10/ta_ttb-ud-test.conllu",
1990	    },
1991	    "te_mtg": {
1992	        "train": "UD_Telugu-MTG/r2.10/te_mtg-ud-train.conllu",
1993	        "dev": "UD_Telugu-MTG/r2.10/te_mtg-ud-dev.conllu",
1994	        "test": "UD_Telugu-MTG/r2.10/te_mtg-ud-test.conllu",
1995	    },
1996	    "th_pud": {
1997	        "test": "UD_Thai-PUD/r2.10/th_pud-ud-test.conllu",
1998	    },
1999	    "tpn_tudet": {
2000	        "test": "UD_Tupinamba-TuDeT/r2.10/tpn_tudet-ud-test.conllu",
2001	    },
2002	    "qtd_sagt": {
2003	        "train": "UD_Turkish_German-SAGT/r2.10/qtd_sagt-ud-train.conllu",
2004	        "dev": "UD_Turkish_German-SAGT/r2.10/qtd_sagt-ud-dev.conllu",
2005	        "test": "UD_Turkish_German-SAGT/r2.10/qtd_sagt-ud-test.conllu",
2006	    },
2007	    "tr_kenet": {
2008	        "train": "UD_Turkish-Kenet/r2.10/tr_kenet-ud-train.conllu",
2009	        "dev": "UD_Turkish-Kenet/r2.10/tr_kenet-ud-dev.conllu",
2010	        "test": "UD_Turkish-Kenet/r2.10/tr_kenet-ud-test.conllu",
2011	    },
2012	    "tr_atis": {
2013	        "train": "UD_Turkish-Atis/r2.10/tr_atis-ud-train.conllu",
2014	        "dev": "UD_Turkish-Atis/r2.10/tr_atis-ud-dev.conllu",
2015	        "test": "UD_Turkish-Atis/r2.10/tr_atis-ud-test.conllu",
2016	    },
2017	    "tr_penn": {
2018	        "train": "UD_Turkish-Penn/r2.10/tr_penn-ud-train.conllu",
2019	        "dev": "UD_Turkish-Penn/r2.10/tr_penn-ud-dev.conllu",
2020	        "test": "UD_Turkish-Penn/r2.10/tr_penn-ud-test.conllu",
2021	    },
2022	    "tr_tourism": {
2023	        "train": "UD_Turkish-Tourism/r2.10/tr_tourism-ud-train.conllu",
2024	        "dev": "UD_Turkish-Tourism/r2.10/tr_tourism-ud-dev.conllu",
2025	        "test": "UD_Turkish-Tourism/r2.10/tr_tourism-ud-test.conllu",
2026	    },
2027	    "tr_framenet": {
2028	        "train": "UD_Turkish-FrameNet/r2.10/tr_framenet-ud-train.conllu",
2029	        "dev": "UD_Turkish-FrameNet/r2.10/tr_framenet-ud-dev.conllu",
2030	        "test": "UD_Turkish-FrameNet/r2.10/tr_framenet-ud-test.conllu",
2031	    },
2032	    "tr_boun": {
2033	        "train": "UD_Turkish-BOUN/r2.10/tr_boun-ud-train.conllu",
2034	        "dev": "UD_Turkish-BOUN/r2.10/tr_boun-ud-dev.conllu",
2035	        "test": "UD_Turkish-BOUN/r2.10/tr_boun-ud-test.conllu",
2036	    },
2037	    "tr_gb": {
2038	        "test": "UD_Turkish-GB/r2.10/tr_gb-ud-test.conllu",
2039	    },
2040	    "tr_imst": {
2041	        "train": "UD_Turkish-IMST/r2.10/tr_imst-ud-train.conllu",
2042	        "dev": "UD_Turkish-IMST/r2.10/tr_imst-ud-dev.conllu",
2043	        "test": "UD_Turkish-IMST/r2.10/tr_imst-ud-test.conllu",
2044	    },
2045	    "tr_pud": {
2046	        "test": "UD_Turkish-PUD/r2.10/tr_pud-ud-test.conllu",
2047	    },
2048	    "uk_iu": {
2049	        "train": "UD_Ukrainian-IU/r2.10/uk_iu-ud-train.conllu",
2050	        "dev": "UD_Ukrainian-IU/r2.10/uk_iu-ud-dev.conllu",
2051	        "test": "UD_Ukrainian-IU/r2.10/uk_iu-ud-test.conllu",
2052	    },
2053	    # TODO(tfds) Add Umbrian Ikuvina split when it will be officially released.
2054	    "hsb_ufal": {
2055	        "train": "UD_Upper_Sorbian-UFAL/r2.10/hsb_ufal-ud-train.conllu",
2056	        "test": "UD_Upper_Sorbian-UFAL/r2.10/hsb_ufal-ud-test.conllu",
2057	    },
2058	    "ur_udtb": {
2059	        "train": "UD_Urdu-UDTB/r2.10/ur_udtb-ud-train.conllu",
2060	        "dev": "UD_Urdu-UDTB/r2.10/ur_udtb-ud-dev.conllu",
2061	        "test": "UD_Urdu-UDTB/r2.10/ur_udtb-ud-test.conllu",
2062	    },
2063	    "ug_udt": {
2064	        "train": "UD_Uyghur-UDT/r2.10/ug_udt-ud-train.conllu",
2065	        "dev": "UD_Uyghur-UDT/r2.10/ug_udt-ud-dev.conllu",
2066	        "test": "UD_Uyghur-UDT/r2.10/ug_udt-ud-test.conllu",
2067	    },
2068	    "vi_vtb": {
2069	        "train": "UD_Vietnamese-VTB/r2.10/vi_vtb-ud-train.conllu",
2070	        "dev": "UD_Vietnamese-VTB/r2.10/vi_vtb-ud-dev.conllu",
2071	        "test": "UD_Vietnamese-VTB/r2.10/vi_vtb-ud-test.conllu",
2072	    },
2073	    "wbp_ufal": {
2074	        "test": "UD_Warlpiri-UFAL/r2.10/wbp_ufal-ud-test.conllu",
2075	    },
2076	    "cy_ccg": {
2077	        "train": "UD_Welsh-CCG/r2.10/cy_ccg-ud-train.conllu",
2078	        "test": "UD_Welsh-CCG/r2.10/cy_ccg-ud-test.conllu",
2079	    },
2080	    "wo_wtb": {
2081	        "train": "UD_Wolof-WTB/r2.10/wo_wtb-ud-train.conllu",
2082	        "dev": "UD_Wolof-WTB/r2.10/wo_wtb-ud-dev.conllu",
2083	        "test": "UD_Wolof-WTB/r2.10/wo_wtb-ud-test.conllu",
2084	    },
2085	    "sjo_xdt": {
2086	        "test": "UD_Xibe-XDT/r2.10/sjo_xdt-ud-test.conllu",
2087	    },
2088	    "sah_yktdt": {
2089	        "test": "UD_Yakut-YKTDT/r2.10/sah_yktdt-ud-test.conllu",
2090	    },
2091	    "yo_ytb": {
2092	        "test": "UD_Yoruba-YTB/r2.10/yo_ytb-ud-test.conllu",
2093	    },
2094	    "ess_sli": {
2095	        "test": "UD_Yupik-SLI/r2.10/ess_sli-ud-test.conllu",
2096	    },
2097	}
2098	
2099	LANGS = DESCRIPTIONS.keys()
2100	
2101	
2102	def prepare_ud_filepaths(
2103	    path_prefix: epath.PathLike,
2104	    filepaths: Union[epath.PathLike, List[epath.PathLike]],
2105	) -> List[epath.PathLike]:
2106	  """Prepends a path prefix to a (list of) filepaths.
2107	
2108	  Args:
2109	    path_prefix: A path which will be prepended to all paths in filepaths.
2110	    filepaths: The filepaths to be prepared. Could be a list of paths for
2111	      multiple files, or a single path.
2112	
2113	  Returns:
2114	    A list with the resulting filepath(s). In case a single path was given as
2115	    `filepath`, it will returns a one-item list containing the resulting path.
2116	  """
2117	  paths = filepaths if isinstance(filepaths, list) else [filepaths]
2118	  return [os.path.join(path_prefix, filepath) for filepath in paths]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils.py:1148
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1147	    "sah_yktdt": (
1148	        "UD_Yakut-YKTDT is a collection Yakut ([Sakha]) sentences "
1149	        "(https://glottolog.org/resource/languoid/id/yaku1245). The project is "
1150	        "work-in-progress and the treebank is being updated on a regular basis"
1151	    ),
1152	    "yo_ytb": (
1153	        "Parts of the Yoruba Bible and of the Yoruba edition of Wikipedia, "
1154	        "hand-annotated natively in Universal Dependencies."
1155	    ),
1156	    "ess_sli": (
1157	        "UD_Yupik-SLI is a treebank of St. Lawrence Island Yupik (ISO 639-3: "
1158	        "ess) that has been manually annotated at the morpheme level, based on "
1159	        "a finite-state morphological analyzer by Chen et al., 2020. The "
1160	        "word-level annotation, merging multiword expressions, is provided in "
1161	        "not-to-release/ess_sli-ud-test.merged.conllu. More information about "
1162	        "the treebank can be found in our publication (AmericasNLP, 2021)."
1163	    ),
1164	}
1165	
1166	UD_FILEPATHS = {
1167	    "af_afribooms": {
1168	        "train": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-train.conllu",
1169	        "dev": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-dev.conllu",
1170	        "test": "UD_Afrikaans-AfriBooms/r2.10/af_afribooms-ud-test.conllu",
1171	    },
1172	    "akk_pisandub": {
1173	        "test": "UD_Akkadian-PISANDUB/r2.10/akk_pisandub-ud-test.conllu",
1174	    },
1175	    "akk_riao": {
1176	        "test": "UD_Akkadian-RIAO/r2.10/akk_riao-ud-test.conllu",
1177	    },
1178	    "aqz_tudet": {
1179	        "test": "UD_Akuntsu-TuDeT/r2.10/aqz_tudet-ud-test.conllu",
1180	    },
1181	    "sq_tsa": {
1182	        "test": "UD_Albanian-TSA/r2.10/sq_tsa-ud-test.conllu",
1183	    },
1184	    "am_att": {
1185	        "test": "UD_Amharic-ATT/r2.10/am_att-ud-test.conllu",
1186	    },
1187	    "grc_perseus": {
1188	        "train": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-train.conllu",
1189	        "dev": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-dev.conllu",
1190	        "test": "UD_Ancient_Greek-Perseus/r2.10/grc_perseus-ud-test.conllu",
1191	    },
1192	    "grc_proiel": {
1193	        "train": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-train.conllu",
1194	        "dev": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-dev.conllu",
1195	        "test": "UD_Ancient_Greek-PROIEL/r2.10/grc_proiel-ud-test.conllu",
1196	    },
1197	    "apu_ufpa": {
1198	        "test": "UD_Apurina-UFPA/r2.10/apu_ufpa-ud-test.conllu",
1199	    },
1200	    "ar_nyuad": {
1201	        "train": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-train.conllu",
1202	        "dev": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-dev.conllu",
1203	        "test": "UD_Arabic-NYUAD/r2.10/ar_nyuad-ud-test.conllu",
1204	    },
1205	    "hbo_ptnk": {
1206	        "train": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-train.conllu",
1207	        "dev": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-dev.conllu",
1208	        "test": "UD_Ancient_Hebrew-PTNK/r2.10/hbo_ptnk-ud-test.conllu",
1209	    },
1210	    "ar_padt": {
1211	        "train": "UD_Arabic-PADT/r2.10/ar_padt-ud-train.conllu",
1212	        "dev": "UD_Arabic-PADT/r2.10/ar_padt-ud-dev.conllu",
1213	        "test": "UD_Arabic-PADT/r2.10/ar_padt-ud-test.conllu",
1214	    },
1215	    # TODO(tfds) Add Armenian BSUT splits when it will be officially released.
1216	    "ar_pud": {
1217	        "test": "UD_Arabic-PUD/r2.10/ar_pud-ud-test.conllu",
1218	    },
1219	    "hy_armtdp": {
1220	        "train": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-train.conllu",
1221	        "dev": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-dev.conllu",
1222	        "test": "UD_Armenian-ArmTDP/r2.10/hy_armtdp-ud-test.conllu",
1223	    },
1224	    "aii_as": {
1225	        "test": "UD_Assyrian-AS/r2.10/aii_as-ud-test.conllu",
1226	    },
1227	    "bm_crb": {
1228	        "test": "UD_Bambara-CRB/r2.10/bm_crb-ud-test.conllu",
1229	    },
1230	    "eu_bdt": {
1231	        "train": "UD_Basque-BDT/r2.10/eu_bdt-ud-train.conllu",
1232	        "dev": "UD_Basque-BDT/r2.10/eu_bdt-ud-dev.conllu",
1233	        "test": "UD_Basque-BDT/r2.10/eu_bdt-ud-test.conllu",
1234	    },
1235	    "bej_nsc": {
1236	        "test": "UD_Beja-NSC/r2.10/bej_nsc-ud-test.conllu",
1237	    },
1238	    "be_hse": {
1239	        "train": "UD_Belarusian-HSE/r2.10/be_hse-ud-train.conllu",
1240	        "dev": "UD_Belarusian-HSE/r2.10/be_hse-ud-dev.conllu",
1241	        "test": "UD_Belarusian-HSE/r2.10/be_hse-ud-test.conllu",
1242	    },
1243	    "bn_bru": {
1244	        "test": "UD_Bengali-BRU/r2.10/bn_bru-ud-test.conllu",
1245	    },
1246	    "bho_bhtb": {
1247	        "test": "UD_Bhojpuri-BHTB/r2.10/bho_bhtb-ud-test.conllu",
1248	    },
1249	    "br_keb": {
1250	        "test": "UD_Breton-KEB/r2.10/br_keb-ud-test.conllu",
1251	    },
1252	    "bg_btb": {
1253	        "train": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-train.conllu",
1254	        "dev": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-dev.conllu",
1255	        "test": "UD_Bulgarian-BTB/r2.10/bg_btb-ud-test.conllu",
1256	    },
1257	    "bxr_bdt": {
1258	        "train": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-train.conllu",
1259	        "test": "UD_Buryat-BDT/r2.10/bxr_bdt-ud-test.conllu",
1260	    },
1261	    "yue_hk": {
1262	        "test": "UD_Cantonese-HK/r2.10/yue_hk-ud-test.conllu",
1263	    },
1264	    "ca_ancora": {
1265	        "train": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-train.conllu",
1266	        "dev": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-dev.conllu",
1267	        "test": "UD_Catalan-AnCora/r2.10/ca_ancora-ud-test.conllu",
1268	    },
1269	    "ceb_gja": {
1270	        "test": "UD_Cebuano-GJA/r2.10/ceb_gja-ud-test.conllu",
1271	    },
1272	    "zh_cfl": {
1273	        "test": "UD_Chinese-CFL/r2.10/zh_cfl-ud-test.conllu",
1274	    },
1275	    "zh_gsd": {
1276	        "train": "UD_Chinese-GSD/r2.10/zh_gsd-ud-train.conllu",
1277	        "dev": "UD_Chinese-GSD/r2.10/zh_gsd-ud-dev.conllu",
1278	        "test": "UD_Chinese-GSD/r2.10/zh_gsd-ud-test.conllu",
1279	    },
1280	    "zh_gsdsimp": {
1281	        "train": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-train.conllu",
1282	        "dev": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-dev.conllu",
1283	        "test": "UD_Chinese-GSDSimp/r2.10/zh_gsdsimp-ud-test.conllu",
1284	    },
1285	    "zh_hk": {
1286	        "test": "UD_Chinese-HK/r2.10/zh_hk-ud-test.conllu",
1287	    },
1288	    "zh_pud": {
1289	        "test": "UD_Chinese-PUD/r2.10/zh_pud-ud-test.conllu",
1290	    },
1291	    "ckt_hse": {
1292	        "test": "UD_Chukchi-HSE/r2.10/ckt_hse-ud-test.conllu",
1293	    },
1294	    "lzh_kyoto": {
1295	        "train": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-train.conllu",
1296	        "dev": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-dev.conllu",
1297	        "test": "UD_Classical_Chinese-Kyoto/r2.10/lzh_kyoto-ud-test.conllu",
1298	    },
1299	    "cop_scriptorium": {
1300	        "train": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-train.conllu",
1301	        "dev": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-dev.conllu",
1302	        "test": "UD_Coptic-Scriptorium/r2.10/cop_scriptorium-ud-test.conllu",
1303	    },
1304	    "hr_set": {
1305	        "train": "UD_Croatian-SET/r2.10/hr_set-ud-train.conllu",
1306	        "dev": "UD_Croatian-SET/r2.10/hr_set-ud-dev.conllu",
1307	        "test": "UD_Croatian-SET/r2.10/hr_set-ud-test.conllu",
1308	    },
1309	    "cs_cac": {
1310	        "train": "UD_Czech-CAC/r2.10/cs_cac-ud-train.conllu",
1311	        "dev": "UD_Czech-CAC/r2.10/cs_cac-ud-dev.conllu",
1312	        "test": "UD_Czech-CAC/r2.10/cs_cac-ud-test.conllu",
1313	    },
1314	    "cs_cltt": {
1315	        "train": "UD_Czech-CLTT/r2.10/cs_cltt-ud-train.conllu",
1316	        "dev": "UD_Czech-CLTT/r2.10/cs_cltt-ud-dev.conllu",
1317	        "test": "UD_Czech-CLTT/r2.10/cs_cltt-ud-test.conllu",
1318	    },
1319	    "cs_fictree": {
1320	        "train": "UD_Czech-FicTree/r2.10/cs_fictree-ud-train.conllu",
1321	        "dev": "UD_Czech-FicTree/r2.10/cs_fictree-ud-dev.conllu",
1322	        "test": "UD_Czech-FicTree/r2.10/cs_fictree-ud-test.conllu",
1323	    },
1324	    "cs_pdt": {
1325	        "train": [
1326	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-l.conllu",
1327	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-m.conllu",
1328	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-c.conllu",
1329	            "UD_Czech-PDT/r2.10/cs_pdt-ud-train-v.conllu",
1330	        ],
1331	        "dev": "UD_Czech-PDT/r2.10/cs_pdt-ud-dev.conllu",
1332	        "test": "UD_Czech-PDT/r2.10/cs_pdt-ud-test.conllu",
1333	    },
1334	    "cs_pud": {
1335	        "test": "UD_Czech-PUD/r2.10/cs_pud-ud-test.conllu",
1336	    },
1337	    "da_ddt": {
1338	        "train": "UD_Danish-DDT/r2.10/da_ddt-ud-train.conllu",
1339	        "dev": "UD_Danish-DDT/r2.10/da_ddt-ud-dev.conllu",
1340	        "test": "UD_Danish-DDT/r2.10/da_ddt-ud-test.conllu",
1341	    },
1342	    "nl_alpino": {
1343	        "train": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-train.conllu",
1344	        "dev": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-dev.conllu",
1345	        "test": "UD_Dutch-Alpino/r2.10/nl_alpino-ud-test.conllu",
1346	    },
1347	    "nl_lassysmall": {
1348	        "train": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-train.conllu",
1349	        "dev": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-dev.conllu",
1350	        "test": "UD_Dutch-LassySmall/r2.10/nl_lassysmall-ud-test.conllu",
1351	    },
1352	    "en_atis": {
1353	        "train": "UD_English-Atis/r2.10/en_atis-ud-train.conllu",
1354	        "dev": "UD_English-Atis/r2.10/en_atis-ud-dev.conllu",
1355	        "test": "UD_English-Atis/r2.10/en_atis-ud-test.conllu",
1356	    },
1357	    "en_esl": {
1358	        "train": "UD_English-ESL/r2.10/en_esl-ud-train.conllu",
1359	        "dev": "UD_English-ESL/r2.10/en_esl-ud-dev.conllu",
1360	        "test": "UD_English-ESL/r2.10/en_esl-ud-test.conllu",
1361	    },
1362	    "en_ewt": {
1363	        "train": "UD_English-EWT/r2.10/en_ewt-ud-train.conllu",
1364	        "dev": "UD_English-EWT/r2.10/en_ewt-ud-dev.conllu",
1365	        "test": "UD_English-EWT/r2.10/en_ewt-ud-test.conllu",
1366	    },
1367	    "en_gum": {
1368	        "train": "UD_English-GUM/r2.10/en_gum-ud-train.conllu",
1369	        "dev": "UD_English-GUM/r2.10/en_gum-ud-dev.conllu",
1370	        "test": "UD_English-GUM/r2.10/en_gum-ud-test.conllu",
1371	    },
1372	    "en_gumreddit": {
1373	        "train": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-train.conllu",
1374	        "dev": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-dev.conllu",
1375	        "test": "UD_English-GUMReddit/r2.10/en_gumreddit-ud-test.conllu",
1376	    },
1377	    "en_lines": {
1378	        "train": "UD_English-LinES/r2.10/en_lines-ud-train.conllu",
1379	        "dev": "UD_English-LinES/r2.10/en_lines-ud-dev.conllu",
1380	        "test": "UD_English-LinES/r2.10/en_lines-ud-test.conllu",
1381	    },
1382	    "en_partut": {
1383	        "train": "UD_English-ParTUT/r2.10/en_partut-ud-train.conllu",
1384	        "dev": "UD_English-ParTUT/r2.10/en_partut-ud-dev.conllu",
1385	        "test": "UD_English-ParTUT/r2.10/en_partut-ud-test.conllu",
1386	    },
1387	    "en_pronouns": {
1388	        "test": "UD_English-Pronouns/r2.10/en_pronouns-ud-test.conllu",
1389	    },
1390	    "en_pud": {
1391	        "test": "UD_English-PUD/r2.10/en_pud-ud-test.conllu",
1392	    },
1393	    "myv_jr": {
1394	        "test": "UD_Erzya-JR/r2.10/myv_jr-ud-test.conllu",
1395	    },
1396	    "et_edt": {
1397	        "train": "UD_Estonian-EDT/r2.10/et_edt-ud-train.conllu",
1398	        "dev": "UD_Estonian-EDT/r2.10/et_edt-ud-dev.conllu",
1399	        "test": "UD_Estonian-EDT/r2.10/et_edt-ud-test.conllu",
1400	    },
1401	    "et_ewt": {
1402	        "train": "UD_Estonian-EWT/r2.10/et_ewt-ud-train.conllu",
1403	        "dev": "UD_Estonian-EWT/r2.10/et_ewt-ud-dev.conllu",
1404	        "test": "UD_Estonian-EWT/r2.10/et_ewt-ud-test.conllu",
1405	    },
1406	    "fo_farpahc": {
1407	        "train": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-train.conllu",
1408	        "dev": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-dev.conllu",
1409	        "test": "UD_Faroese-FarPaHC/r2.10/fo_farpahc-ud-test.conllu",
1410	    },
1411	    "fo_oft": {
1412	        "test": "UD_Faroese-OFT/r2.10/fo_oft-ud-test.conllu",
1413	    },
1414	    "fi_ftb": {
1415	        "train": "UD_Finnish-FTB/r2.10/fi_ftb-ud-train.conllu",
1416	        "dev": "UD_Finnish-FTB/r2.10/fi_ftb-ud-dev.conllu",
1417	        "test": "UD_Finnish-FTB/r2.10/fi_ftb-ud-test.conllu",
1418	    },
1419	    "fi_ood": {
1420	        "test": "UD_Finnish-OOD/r2.10/fi_ood-ud-test.conllu",
1421	    },
1422	    "fi_pud": {
1423	        "test": "UD_Finnish-PUD/r2.10/fi_pud-ud-test.conllu",
1424	    },
1425	    "fi_tdt": {
1426	        "train": "UD_Finnish-TDT/r2.10/fi_tdt-ud-train.conllu",
1427	        "dev": "UD_Finnish-TDT/r2.10/fi_tdt-ud-dev.conllu",
1428	        "test": "UD_Finnish-TDT/r2.10/fi_tdt-ud-test.conllu",
1429	    },
1430	    "fr_parisstories": {
1431	        "train": "UD_French-ParisStories/r2.10/fr_parisstories-ud-train.conllu",
1432	        "test": "UD_French-ParisStories/r2.10/fr_parisstories-ud-test.conllu",
1433	    },
1434	    "fr_fqb": {
1435	        "test": "UD_French-FQB/r2.10/fr_fqb-ud-test.conllu",
1436	    },
1437	    "fr_rhapsodie": {
1438	        "train": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-train.conllu",
1439	        "dev": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-dev.conllu",
1440	        "test": "UD_French-Rhapsodie/r2.10/fr_rhapsodie-ud-test.conllu",
1441	    },
1442	    "fr_ftb": {
1443	        "train": "UD_French-FTB/r2.10/fr_ftb-ud-train.conllu",
1444	        "dev": "UD_French-FTB/r2.10/fr_ftb-ud-dev.conllu",
1445	        "test": "UD_French-FTB/r2.10/fr_ftb-ud-test.conllu",
1446	    },
1447	    "fr_gsd": {
1448	        "train": "UD_French-GSD/r2.10/fr_gsd-ud-train.conllu",
1449	        "dev": "UD_French-GSD/r2.10/fr_gsd-ud-dev.conllu",
1450	        "test": "UD_French-GSD/r2.10/fr_gsd-ud-test.conllu",
1451	    },
1452	    "fr_partut": {
1453	        "train": "UD_French-ParTUT/r2.10/fr_partut-ud-train.conllu",
1454	        "dev": "UD_French-ParTUT/r2.10/fr_partut-ud-dev.conllu",
1455	        "test": "UD_French-ParTUT/r2.10/fr_partut-ud-test.conllu",
1456	    },
1457	    "fr_pud": {
1458	        "test": "UD_French-PUD/r2.10/fr_pud-ud-test.conllu",
1459	    },
1460	    "fr_sequoia": {
1461	        "train": "UD_French-Sequoia/r2.10/fr_sequoia-ud-train.conllu",
1462	        "dev": "UD_French-Sequoia/r2.10/fr_sequoia-ud-dev.conllu",
1463	        "test": "UD_French-Sequoia/r2.10/fr_sequoia-ud-test.conllu",
1464	    },
1465	    "qfn_fame": {
1466	        "test": "UD_Frisian_Dutch-Fame/r2.10/qfn_fame-ud-test.conllu",
1467	    },
1468	    "gl_ctg": {
1469	        "train": "UD_Galician-CTG/r2.10/gl_ctg-ud-train.conllu",
1470	        "dev": "UD_Galician-CTG/r2.10/gl_ctg-ud-dev.conllu",
1471	        "test": "UD_Galician-CTG/r2.10/gl_ctg-ud-test.conllu",
1472	    },
1473	    "gl_treegal": {
1474	        "train": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-train.conllu",
1475	        "test": "UD_Galician-TreeGal/r2.10/gl_treegal-ud-test.conllu",
1476	    },
1477	    "de_gsd": {
1478	        "train": "UD_German-GSD/r2.10/de_gsd-ud-train.conllu",
1479	        "dev": "UD_German-GSD/r2.10/de_gsd-ud-dev.conllu",
1480	        "test": "UD_German-GSD/r2.10/de_gsd-ud-test.conllu",
1481	    },
1482	    "de_hdt": {
1483	        "train": [
1484	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-1.conllu",
1485	            "UD_German-HDT/r2.10/de_hdt-ud-train-a-2.conllu",
1486	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-1.conllu",
1487	            "UD_German-HDT/r2.10/de_hdt-ud-train-b-2.conllu",
1488	        ],
1489	        "dev": "UD_German-HDT/r2.10/de_hdt-ud-dev.conllu",
1490	        "test": "UD_German-HDT/r2.10/de_hdt-ud-test.conllu",
1491	    },
1492	    "de_lit": {
1493	        "test": "UD_German-LIT/r2.10/de_lit-ud-test.conllu",
1494	    },
1495	    "de_pud": {
1496	        "test": "UD_German-PUD/r2.10/de_pud-ud-test.conllu",
1497	    },
1498	    "got_proiel": {
1499	        "train": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-train.conllu",
1500	        "dev": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-dev.conllu",
1501	        "test": "UD_Gothic-PROIEL/r2.10/got_proiel-ud-test.conllu",
1502	    },
1503	    "el_gdt": {
1504	        "train": "UD_Greek-GDT/r2.10/el_gdt-ud-train.conllu",
1505	        "dev": "UD_Greek-GDT/r2.10/el_gdt-ud-dev.conllu",
1506	        "test": "UD_Greek-GDT/r2.10/el_gdt-ud-test.conllu",
1507	    },
1508	    "gub_tudet": {
1509	        "test": "UD_Guajajara-TuDeT/r2.10/gub_tudet-ud-test.conllu",
1510	    },
1511	    "gn_oldtudet": {
1512	        "test": "UD_Guarani-OldTuDeT/r2.10/gn_oldtudet-ud-test.conllu",
1513	    },
1514	    "he_iahltwiki": {
1515	        "train": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-train.conllu",
1516	        "dev": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-dev.conllu",
1517	        "test": "UD_Hebrew-IAHLTwiki/r2.10/he_iahltwiki-ud-test.conllu",
1518	    },
1519	    "he_htb": {
1520	        "train": "UD_Hebrew-HTB/r2.10/he_htb-ud-train.conllu",
1521	        "dev": "UD_Hebrew-HTB/r2.10/he_htb-ud-dev.conllu",
1522	        "test": "UD_Hebrew-HTB/r2.10/he_htb-ud-test.conllu",
1523	    },
1524	    "qhe_hiencs": {
1525	        "train": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-train.conllu",
1526	        "dev": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-dev.conllu",
1527	        "test": "UD_Hindi_English-HIENCS/r2.10/qhe_hiencs-ud-test.conllu",
1528	    },
1529	    "hi_hdtb": {
1530	        "train": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-train.conllu",
1531	        "dev": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-dev.conllu",
1532	        "test": "UD_Hindi-HDTB/r2.10/hi_hdtb-ud-test.conllu",
1533	    },
1534	    "hi_pud": {
1535	        "test": "UD_Hindi-PUD/r2.10/hi_pud-ud-test.conllu",
1536	    },
1537	    "hu_szeged": {
1538	        "train": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-train.conllu",
1539	        "dev": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-dev.conllu",
1540	        "test": "UD_Hungarian-Szeged/r2.10/hu_szeged-ud-test.conllu",
1541	    },
1542	    "is_modern": {
1543	        "train": "UD_Icelandic-Modern/r2.10/is_modern-ud-train.conllu",
1544	        "dev": "UD_Icelandic-Modern/r2.10/is_modern-ud-dev.conllu",
1545	        "test": "UD_Icelandic-Modern/r2.10/is_modern-ud-test.conllu",
1546	    },
1547	    "is_icepahc": {
1548	        "train": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-train.conllu",
1549	        "dev": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-dev.conllu",
1550	        "test": "UD_Icelandic-IcePaHC/r2.10/is_icepahc-ud-test.conllu",
1551	    },
1552	    "is_pud": {
1553	        "test": "UD_Icelandic-PUD/r2.10/is_pud-ud-test.conllu",
1554	    },
1555	    "id_csui": {
1556	        "train": "UD_Indonesian-CSUI/r2.10/id_csui-ud-train.conllu",
1557	        "test": "UD_Indonesian-CSUI/r2.10/id_csui-ud-test.conllu",
1558	    },
1559	    "id_gsd": {
1560	        "train": "UD_Indonesian-GSD/r2.10/id_gsd-ud-train.conllu",
1561	        "dev": "UD_Indonesian-GSD/r2.10/id_gsd-ud-dev.conllu",
1562	        "test": "UD_Indonesian-GSD/r2.10/id_gsd-ud-test.conllu",
1563	    },
1564	    "id_pud": {
1565	        "test": "UD_Indonesian-PUD/r2.10/id_pud-ud-test.conllu",
1566	    },
1567	    "ga_twittirish": {
1568	        "test": "UD_Irish-TwittIrish/r2.10/ga_twittirish-ud-test.conllu",
1569	    },
1570	    "ga_idt": {
1571	        "train": "UD_Irish-IDT/r2.10/ga_idt-ud-train.conllu",
1572	        "dev": "UD_Irish-IDT/r2.10/ga_idt-ud-dev.conllu",
1573	        "test": "UD_Irish-IDT/r2.10/ga_idt-ud-test.conllu",
1574	    },
1575	    "it_valico": {
1576	        "test": "UD_Italian-Valico/r2.10/it_valico-ud-test.conllu",
1577	    },
1578	    "it_markit": {
1579	        "train": "UD_Italian-MarkIT/r2.10/it_markit-ud-train.conllu",
1580	        "dev": "UD_Italian-MarkIT/r2.10/it_markit-ud-dev.conllu",
1581	        "test": "UD_Italian-MarkIT/r2.10/it_markit-ud-test.conllu",
1582	    },
1583	    "it_isdt": {
1584	        "train": "UD_Italian-ISDT/r2.10/it_isdt-ud-train.conllu",
1585	        "dev": "UD_Italian-ISDT/r2.10/it_isdt-ud-dev.conllu",
1586	        "test": "UD_Italian-ISDT/r2.10/it_isdt-ud-test.conllu",
1587	    },
1588	    "it_partut": {
1589	        "train": "UD_Italian-ParTUT/r2.10/it_partut-ud-train.conllu",
1590	        "dev": "UD_Italian-ParTUT/r2.10/it_partut-ud-dev.conllu",
1591	        "test": "UD_Italian-ParTUT/r2.10/it_partut-ud-test.conllu",
1592	    },
1593	    "it_postwita": {
1594	        "train": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-train.conllu",
1595	        "dev": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-dev.conllu",
1596	        "test": "UD_Italian-PoSTWITA/r2.10/it_postwita-ud-test.conllu",
1597	    },
1598	    "it_pud": {
1599	        "test": "UD_Italian-PUD/r2.10/it_pud-ud-test.conllu",
1600	    },
1601	    "it_twittiro": {
1602	        "train": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-train.conllu",
1603	        "dev": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-dev.conllu",
1604	        "test": "UD_Italian-TWITTIRO/r2.10/it_twittiro-ud-test.conllu",
1605	    },
1606	    "it_vit": {
1607	        "train": "UD_Italian-VIT/r2.10/it_vit-ud-train.conllu",
1608	        "dev": "UD_Italian-VIT/r2.10/it_vit-ud-dev.conllu",
1609	        "test": "UD_Italian-VIT/r2.10/it_vit-ud-test.conllu",
1610	    },
1611	    "ja_gsdluw": {
1612	        "train": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-train.conllu",
1613	        "dev": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-dev.conllu",
1614	        "test": "UD_Japanese-GSDLUW/r2.10/ja_gsdluw-ud-test.conllu",
1615	    },
1616	    "ja_pudluw": {
1617	        "test": "UD_Japanese-PUDLUW/r2.10/ja_pudluw-ud-test.conllu",
1618	    },
1619	    "ja_bccwjluw": {
1620	        "train": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-train.conllu",
1621	        "dev": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-dev.conllu",
1622	        "test": "UD_Japanese-BCCWJLUW/r2.10/ja_bccwjluw-ud-test.conllu",
1623	    },
1624	    "ja_bccwj": {
1625	        "train": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-train.conllu",
1626	        "dev": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-dev.conllu",
1627	        "test": "UD_Japanese-BCCWJ/r2.10/ja_bccwj-ud-test.conllu",
1628	    },
1629	    "ja_gsd": {
1630	        "train": "UD_Japanese-GSD/r2.10/ja_gsd-ud-train.conllu",
1631	        "dev": "UD_Japanese-GSD/r2.10/ja_gsd-ud-dev.conllu",
1632	        "test": "UD_Japanese-GSD/r2.10/ja_gsd-ud-test.conllu",
1633	    },
1634	    "ja_modern": {
1635	        "test": "UD_Japanese-Modern/r2.10/ja_modern-ud-test.conllu",
1636	    },
1637	    "ja_pud": {
1638	        "test": "UD_Japanese-PUD/r2.10/ja_pud-ud-test.conllu",
1639	    },
1640	    "jv_csui": {
1641	        "test": "UD_Javanese-CSUI/r2.10/jv_csui-ud-test.conllu",
1642	    },
1643	    "urb_tudet": {
1644	        "test": "UD_Kaapor-TuDeT/r2.10/urb_tudet-ud-test.conllu",
1645	    },
1646	    "xnr_kdtb": {
1647	        "test": "UD_Kangri-KDTB/r2.10/xnr_kdtb-ud-test.conllu",
1648	    },
1649	    "krl_kkpp": {
1650	        "test": "UD_Karelian-KKPP/r2.10/krl_kkpp-ud-test.conllu",
1651	    },
1652	    "arr_tudet": {
1653	        "test": "UD_Karo-TuDeT/r2.10/arr_tudet-ud-test.conllu",
1654	    },
1655	    "kk_ktb": {
1656	        "train": "UD_Kazakh-KTB/r2.10/kk_ktb-ud-train.conllu",
1657	        "test": "UD_Kazakh-KTB/r2.10/kk_ktb-ud-test.conllu",
1658	    },
1659	    "kfm_aha": {
1660	        "test": "UD_Khunsari-AHA/r2.10/kfm_aha-ud-test.conllu",
1661	    },
1662	    "quc_iu": {
1663	        "test": "UD_Kiche-IU/r2.10/quc_iu-ud-test.conllu",
1664	    },
1665	    "koi_uh": {
1666	        "test": "UD_Komi_Permyak-UH/r2.10/koi_uh-ud-test.conllu",
1667	    },
1668	    "kpv_ikdp": {
1669	        "test": "UD_Komi_Zyrian-IKDP/r2.10/kpv_ikdp-ud-test.conllu",
1670	    },
1671	    "kpv_lattice": {
1672	        "test": "UD_Komi_Zyrian-Lattice/r2.10/kpv_lattice-ud-test.conllu",
1673	    },
1674	    "ko_gsd": {
1675	        "train": "UD_Korean-GSD/r2.10/ko_gsd-ud-train.conllu",
1676	        "dev": "UD_Korean-GSD/r2.10/ko_gsd-ud-dev.conllu",
1677	        "test": "UD_Korean-GSD/r2.10/ko_gsd-ud-test.conllu",
1678	    },
1679	    "ko_kaist": {
1680	        "train": "UD_Korean-Kaist/r2.10/ko_kaist-ud-train.conllu",
1681	        "dev": "UD_Korean-Kaist/r2.10/ko_kaist-ud-dev.conllu",
1682	        "test": "UD_Korean-Kaist/r2.10/ko_kaist-ud-test.conllu",
1683	    },
1684	    "ko_pud": {
1685	        "test": "UD_Korean-PUD/r2.10/ko_pud-ud-test.conllu",
1686	    },
1687	    "kmr_mg": {
1688	        "train": "UD_Kurmanji-MG/r2.10/kmr_mg-ud-train.conllu",
1689	        "test": "UD_Kurmanji-MG/r2.10/kmr_mg-ud-test.conllu",
1690	    },
1691	    "la_udante": {
1692	        "train": "UD_Latin-UDante/r2.10/la_udante-ud-train.conllu",
1693	        "dev": "UD_Latin-UDante/r2.10/la_udante-ud-dev.conllu",
1694	        "test": "UD_Latin-UDante/r2.10/la_udante-ud-test.conllu",
1695	    },
1696	    "la_ittb": {
1697	        "train": "UD_Latin-ITTB/r2.10/la_ittb-ud-train.conllu",
1698	        "dev": "UD_Latin-ITTB/r2.10/la_ittb-ud-dev.conllu",
1699	        "test": "UD_Latin-ITTB/r2.10/la_ittb-ud-test.conllu",
1700	    },
1701	    "la_llct": {
1702	        "train": "UD_Latin-LLCT/r2.10/la_llct-ud-train.conllu",
1703	        "dev": "UD_Latin-LLCT/r2.10/la_llct-ud-dev.conllu",
1704	        "test": "UD_Latin-LLCT/r2.10/la_llct-ud-test.conllu",
1705	    },
1706	    "la_perseus": {
1707	        "train": "UD_Latin-Perseus/r2.10/la_perseus-ud-train.conllu",
1708	        "test": "UD_Latin-Perseus/r2.10/la_perseus-ud-test.conllu",
1709	    },
1710	    "la_proiel": {
1711	        "train": "UD_Latin-PROIEL/r2.10/la_proiel-ud-train.conllu",
1712	        "dev": "UD_Latin-PROIEL/r2.10/la_proiel-ud-dev.conllu",
1713	        "test": "UD_Latin-PROIEL/r2.10/la_proiel-ud-test.conllu",
1714	    },
1715	    "lv_lvtb": {
1716	        "train": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-train.conllu",
1717	        "dev": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-dev.conllu",
1718	        "test": "UD_Latvian-LVTB/r2.10/lv_lvtb-ud-test.conllu",
1719	    },
1720	    "lij_glt": {
1721	        "train": "UD_Ligurian-GLT/r2.10/lij_glt-ud-train.conllu",
1722	        "test": "UD_Ligurian-GLT/r2.10/lij_glt-ud-test.conllu",
1723	    },
1724	    "lt_alksnis": {
1725	        "train": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-train.conllu",
1726	        "dev": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-dev.conllu",
1727	        "test": "UD_Lithuanian-ALKSNIS/r2.10/lt_alksnis-ud-test.conllu",
1728	    },
1729	    "lt_hse": {
1730	        "train": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1731	        "dev": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1732	        "test": "UD_Lithuanian-HSE/r2.10/lt_hse-ud-train.conllu",
1733	    },
1734	    "olo_kkpp": {
1735	        "train": "UD_Livvi-KKPP/r2.10/olo_kkpp-ud-train.conllu",
1736	        "test": "UD_Livvi-KKPP/r2.10/olo_kkpp-ud-test.conllu",
1737	    },
1738	    "nds_lsdc": {
1739	        "test": "UD_Low_Saxon-LSDC/r2.10/nds_lsdc-ud-test.conllu",
1740	    },
1741	    # TODO(tfds) Add Madi Jarawara splits when it will be officially released.
1742	    # TODO(tfds) Add Makurap TuDeT splits when it will be officially released.
1743	    "mt_mudt": {
1744	        "train": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-train.conllu",
1745	        "dev": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-dev.conllu",
1746	        "test": "UD_Maltese-MUDT/r2.10/mt_mudt-ud-test.conllu",
1747	    },
1748	    "gv_cadhan": {
1749	        "test": "UD_Manx-Cadhan/r2.10/gv_cadhan-ud-test.conllu",
1750	    },
1751	    "mr_ufal": {
1752	        "train": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-train.conllu",
1753	        "dev": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-dev.conllu",
1754	        "test": "UD_Marathi-UFAL/r2.10/mr_ufal-ud-test.conllu",
1755	    },
1756	    "gun_dooley": {
1757	        "test": "UD_Mbya_Guarani-Dooley/r2.10/gun_dooley-ud-test.conllu",
1758	    },
1759	    "gun_thomas": {
1760	        "test": "UD_Mbya_Guarani-Thomas/r2.10/gun_thomas-ud-test.conllu",
1761	    },
1762	    "mdf_jr": {
1763	        "test": "UD_Moksha-JR/r2.10/mdf_jr-ud-test.conllu",
1764	    },
1765	    "myu_tudet": {
1766	        "test": "UD_Munduruku-TuDeT/r2.10/myu_tudet-ud-test.conllu",
1767	    },
1768	    "pcm_nsc": {
1769	        "train": "UD_Naija-NSC/r2.10/pcm_nsc-ud-train.conllu",
1770	        "dev": "UD_Naija-NSC/r2.10/pcm_nsc-ud-dev.conllu",
1771	        "test": "UD_Naija-NSC/r2.10/pcm_nsc-ud-test.conllu",
1772	    },
1773	    "nyq_aha": {
1774	        "test": "UD_Nayini-AHA/r2.10/nyq_aha-ud-test.conllu",
1775	    },
1776	    # TODO(tfds) Add Neapolitan RB splits when it will be officially released.
1777	    "sme_giella": {
1778	        "train": "UD_North_Sami-Giella/r2.10/sme_giella-ud-train.conllu",
1779	        "test": "UD_North_Sami-Giella/r2.10/sme_giella-ud-test.conllu",
1780	    },
1781	    "no_bokmaal": {
1782	        "train": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-train.conllu",
1783	        "dev": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-dev.conllu",
1784	        "test": "UD_Norwegian-Bokmaal/r2.10/no_bokmaal-ud-test.conllu",
1785	    },
1786	    "no_nynorsk": {
1787	        "train": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-train.conllu",
1788	        "dev": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-dev.conllu",
1789	        "test": "UD_Norwegian-Nynorsk/r2.10/no_nynorsk-ud-test.conllu",
1790	    },
1791	    "no_nynorsklia": {
1792	        "train": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-train.conllu",
1793	        "dev": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-dev.conllu",
1794	        "test": "UD_Norwegian-NynorskLIA/r2.10/no_nynorsklia-ud-test.conllu",
1795	    },
1796	    "cu_proiel": {
1797	        "train": (
1798	            "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-train.conllu"
1799	        ),
1800	        "dev": "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-dev.conllu",
1801	        "test": "UD_Old_Church_Slavonic-PROIEL/r2.10/cu_proiel-ud-test.conllu",
1802	    },
1803	    "fro_srcmf": {
1804	        "train": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-train.conllu",
1805	        "dev": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-dev.conllu",
1806	        "test": "UD_Old_French-SRCMF/r2.10/fro_srcmf-ud-test.conllu",
1807	    },
1808	    "orv_birchbark": {
1809	        "train": (
1810	            "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-train.conllu"
1811	        ),
1812	        "dev": "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-dev.conllu",
1813	        "test": (
1814	            "UD_Old_East_Slavic-Birchbark/r2.10/orv_birchbark-ud-test.conllu"
1815	        ),
1816	    },
1817	    "orv_rnc": {
1818	        "train": "UD_Old_Russian-RNC/r2.10/orv_rnc-ud-train.conllu",
1819	        "test": "UD_Old_Russian-RNC/r2.10/orv_rnc-ud-test.conllu",
1820	    },
1821	    "orv_torot": {
1822	        "train": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-train.conllu",
1823	        "dev": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-dev.conllu",
1824	        "test": "UD_Old_Russian-TOROT/r2.10/orv_torot-ud-test.conllu",
1825	    },
1826	    "otk_tonqq": {
1827	        "test": "UD_Old_Turkish-Tonqq/r2.10/otk_tonqq-ud-test.conllu",
1828	    },
1829	    "fa_perdt": {
1830	        "train": "UD_Persian-PerDT/r2.10/fa_perdt-ud-train.conllu",
1831	        "dev": "UD_Persian-PerDT/r2.10/fa_perdt-ud-dev.conllu",
1832	        "test": "UD_Persian-PerDT/r2.10/fa_perdt-ud-test.conllu",
1833	    },
1834	    "fa_seraji": {
1835	        "train": "UD_Persian-Seraji/r2.10/fa_seraji-ud-train.conllu",
1836	        "dev": "UD_Persian-Seraji/r2.10/fa_seraji-ud-dev.conllu",
1837	        "test": "UD_Persian-Seraji/r2.10/fa_seraji-ud-test.conllu",
1838	    },
1839	    "pl_lfg": {
1840	        "train": "UD_Polish-LFG/r2.10/pl_lfg-ud-train.conllu",
1841	        "dev": "UD_Polish-LFG/r2.10/pl_lfg-ud-dev.conllu",
1842	        "test": "UD_Polish-LFG/r2.10/pl_lfg-ud-test.conllu",
1843	    },
1844	    "pl_pdb": {
1845	        "train": "UD_Polish-PDB/r2.10/pl_pdb-ud-train.conllu",
1846	        "dev": "UD_Polish-PDB/r2.10/pl_pdb-ud-dev.conllu",
1847	        "test": "UD_Polish-PDB/r2.10/pl_pdb-ud-test.conllu",
1848	    },
1849	    "pl_pud": {
1850	        "test": "UD_Polish-PUD/r2.10/pl_pud-ud-test.conllu",
1851	    },
1852	    "pt_bosque": {
1853	        "train": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-train.conllu",
1854	        "dev": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-dev.conllu",
1855	        "test": "UD_Portuguese-Bosque/r2.10/pt_bosque-ud-test.conllu",
1856	    },
1857	    "pt_gsd": {
1858	        "train": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-train.conllu",
1859	        "dev": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-dev.conllu",
1860	        "test": "UD_Portuguese-GSD/r2.10/pt_gsd-ud-test.conllu",
1861	    },
1862	    "pt_pud": {
1863	        "test": "UD_Portuguese-PUD/r2.10/pt_pud-ud-test.conllu",
1864	    },
1865	    "ro_art": {
1866	        "test": "UD_Romanian-ArT/r2.10/ro_art-ud-test.conllu",
1867	    },
1868	    "ro_nonstandard": {
1869	        "train": "UD_Romanian-Nonstandard/r2.10/ro_nonstandard-ud-train.conllu",
1870	        "dev": "UD_Romanian-Nonstandard/r2.10/ro_nonstandard-ud-dev.conllu",
1871	        "test": "UD_Romanian-Nonstandard/r2.10/ro_nonstandard-ud-test.conllu",
1872	    },
1873	    "ro_rrt": {
1874	        "train": "UD_Romanian-RRT/r2.10/ro_rrt-ud-train.conllu",
1875	        "dev": "UD_Romanian-RRT/r2.10/ro_rrt-ud-dev.conllu",
1876	        "test": "UD_Romanian-RRT/r2.10/ro_rrt-ud-test.conllu",
1877	    },
1878	    "ro_simonero": {
1879	        "train": "UD_Romanian-SiMoNERo/r2.10/ro_simonero-ud-train.conllu",
1880	        "dev": "UD_Romanian-SiMoNERo/r2.10/ro_simonero-ud-dev.conllu",
1881	        "test": "UD_Romanian-SiMoNERo/r2.10/ro_simonero-ud-test.conllu",
1882	    },
1883	    "ru_gsd": {
1884	        "train": "UD_Russian-GSD/r2.10/ru_gsd-ud-train.conllu",
1885	        "dev": "UD_Russian-GSD/r2.10/ru_gsd-ud-dev.conllu",
1886	        "test": "UD_Russian-GSD/r2.10/ru_gsd-ud-test.conllu",
1887	    },
1888	    "ru_pud": {
1889	        "test": "UD_Russian-PUD/r2.10/ru_pud-ud-test.conllu",
1890	    },
1891	    "ru_syntagrus": {
1892	        "train": "UD_Russian-SynTagRus/r2.7/ru_syntagrus-ud-train.conllu",
1893	        "dev": "UD_Russian-SynTagRus/r2.7/ru_syntagrus-ud-dev.conllu",
1894	        "test": "UD_Russian-SynTagRus/r2.7/ru_syntagrus-ud-test.conllu",
1895	    },
1896	    "ru_taiga": {
1897	        "train": "UD_Russian-Taiga/r2.10/ru_taiga-ud-train.conllu",
1898	        "dev": "UD_Russian-Taiga/r2.10/ru_taiga-ud-dev.conllu",
1899	        "test": "UD_Russian-Taiga/r2.10/ru_taiga-ud-test.conllu",
1900	    },
1901	    "sa_ufal": {
1902	        "test": "UD_Sanskrit-UFAL/r2.10/sa_ufal-ud-test.conllu",
1903	    },
1904	    "sa_vedic": {
1905	        "train": "UD_Sanskrit-Vedic/r2.10/sa_vedic-ud-train.conllu",
1906	        "test": "UD_Sanskrit-Vedic/r2.10/sa_vedic-ud-test.conllu",
1907	    },
1908	    "gd_arcosg": {
1909	        "train": "UD_Scottish_Gaelic-ARCOSG/r2.10/gd_arcosg-ud-train.conllu",
1910	        "dev": "UD_Scottish_Gaelic-ARCOSG/r2.10/gd_arcosg-ud-dev.conllu",
1911	        "test": "UD_Scottish_Gaelic-ARCOSG/r2.10/gd_arcosg-ud-test.conllu",
1912	    },
1913	    "sr_set": {
1914	        "train": "UD_Serbian-SET/r2.10/sr_set-ud-train.conllu",
1915	        "dev": "UD_Serbian-SET/r2.10/sr_set-ud-dev.conllu",
1916	        "test": "UD_Serbian-SET/r2.10/sr_set-ud-test.conllu",
1917	    },
1918	    "sms_giellagas": {
1919	        "test": "UD_Skolt_Sami-Giellagas/r2.10/sms_giellagas-ud-test.conllu",
1920	    },
1921	    "sk_snk": {
1922	        "train": "UD_Slovak-SNK/r2.10/sk_snk-ud-train.conllu",
1923	        "dev": "UD_Slovak-SNK/r2.10/sk_snk-ud-dev.conllu",
1924	        "test": "UD_Slovak-SNK/r2.10/sk_snk-ud-test.conllu",
1925	    },
1926	    "sl_ssj": {
1927	        "train": "UD_Slovenian-SSJ/r2.10/sl_ssj-ud-train.conllu",
1928	        "dev": "UD_Slovenian-SSJ/r2.10/sl_ssj-ud-dev.conllu",
1929	        "test": "UD_Slovenian-SSJ/r2.10/sl_ssj-ud-test.conllu",
1930	    },
1931	    "sl_sst": {
1932	        "train": "UD_Slovenian-SST/r2.10/sl_sst-ud-train.conllu",
1933	        "test": "UD_Slovenian-SST/r2.10/sl_sst-ud-test.conllu",
1934	    },
1935	    "soj_aha": {
1936	        "test": "UD_Soi-AHA/r2.10/soj_aha-ud-test.conllu",
1937	    },
1938	    "ajp_madar": {
1939	        "test": (
1940	            "UD_South_Levantine_Arabic-MADAR/r2.10/ajp_madar-ud-test.conllu"
1941	        ),
1942	    },
1943	    "es_ancora": {
1944	        "train": "UD_Spanish-AnCora/r2.10/es_ancora-ud-train.conllu",
1945	        "dev": "UD_Spanish-AnCora/r2.10/es_ancora-ud-dev.conllu",
1946	        "test": "UD_Spanish-AnCora/r2.10/es_ancora-ud-test.conllu",
1947	    },
1948	    "es_gsd": {
1949	        "train": "UD_Spanish-GSD/r2.10/es_gsd-ud-train.conllu",
1950	        "dev": "UD_Spanish-GSD/r2.10/es_gsd-ud-dev.conllu",
1951	        "test": "UD_Spanish-GSD/r2.10/es_gsd-ud-test.conllu",
1952	    },
1953	    "es_pud": {
1954	        "test": "UD_Spanish-PUD/r2.10/es_pud-ud-test.conllu",
1955	    },
1956	    "swl_sslc": {
1957	        "train": "UD_Swedish_Sign_Language-SSLC/r2.10/swl_sslc-ud-train.conllu",
1958	        "dev": "UD_Swedish_Sign_Language-SSLC/r2.10/swl_sslc-ud-dev.conllu",
1959	        "test": "UD_Swedish_Sign_Language-SSLC/r2.10/swl_sslc-ud-test.conllu",
1960	    },
1961	    "sv_lines": {
1962	        "train": "UD_Swedish-LinES/r2.10/sv_lines-ud-train.conllu",
1963	        "dev": "UD_Swedish-LinES/r2.10/sv_lines-ud-dev.conllu",
1964	        "test": "UD_Swedish-LinES/r2.10/sv_lines-ud-test.conllu",
1965	    },
1966	    "sv_pud": {
1967	        "test": "UD_Swedish-PUD/r2.10/sv_pud-ud-test.conllu",
1968	    },
1969	    "sv_talbanken": {
1970	        "train": "UD_Swedish-Talbanken/r2.10/sv_talbanken-ud-train.conllu",
1971	        "dev": "UD_Swedish-Talbanken/r2.10/sv_talbanken-ud-dev.conllu",
1972	        "test": "UD_Swedish-Talbanken/r2.10/sv_talbanken-ud-test.conllu",
1973	    },
1974	    "gsw_uzh": {
1975	        "test": "UD_Swiss_German-UZH/r2.10/gsw_uzh-ud-test.conllu",
1976	    },
1977	    "tl_trg": {
1978	        "test": "UD_Tagalog-TRG/r2.10/tl_trg-ud-test.conllu",
1979	    },
1980	    "tl_ugnayan": {
1981	        "test": "UD_Tagalog-Ugnayan/r2.10/tl_ugnayan-ud-test.conllu",
1982	    },
1983	    "ta_mwtt": {
1984	        "test": "UD_Tamil-MWTT/r2.10/ta_mwtt-ud-test.conllu",
1985	    },
1986	    "ta_ttb": {
1987	        "train": "UD_Tamil-TTB/r2.10/ta_ttb-ud-train.conllu",
1988	        "dev": "UD_Tamil-TTB/r2.10/ta_ttb-ud-dev.conllu",
1989	        "test": "UD_Tamil-TTB/r2.10/ta_ttb-ud-test.conllu",
1990	    },
1991	    "te_mtg": {
1992	        "train": "UD_Telugu-MTG/r2.10/te_mtg-ud-train.conllu",
1993	        "dev": "UD_Telugu-MTG/r2.10/te_mtg-ud-dev.conllu",
1994	        "test": "UD_Telugu-MTG/r2.10/te_mtg-ud-test.conllu",
1995	    },
1996	    "th_pud": {
1997	        "test": "UD_Thai-PUD/r2.10/th_pud-ud-test.conllu",
1998	    },
1999	    "tpn_tudet": {
2000	        "test": "UD_Tupinamba-TuDeT/r2.10/tpn_tudet-ud-test.conllu",
2001	    },
2002	    "qtd_sagt": {
2003	        "train": "UD_Turkish_German-SAGT/r2.10/qtd_sagt-ud-train.conllu",
2004	        "dev": "UD_Turkish_German-SAGT/r2.10/qtd_sagt-ud-dev.conllu",
2005	        "test": "UD_Turkish_German-SAGT/r2.10/qtd_sagt-ud-test.conllu",
2006	    },
2007	    "tr_kenet": {
2008	        "train": "UD_Turkish-Kenet/r2.10/tr_kenet-ud-train.conllu",
2009	        "dev": "UD_Turkish-Kenet/r2.10/tr_kenet-ud-dev.conllu",
2010	        "test": "UD_Turkish-Kenet/r2.10/tr_kenet-ud-test.conllu",
2011	    },
2012	    "tr_atis": {
2013	        "train": "UD_Turkish-Atis/r2.10/tr_atis-ud-train.conllu",
2014	        "dev": "UD_Turkish-Atis/r2.10/tr_atis-ud-dev.conllu",
2015	        "test": "UD_Turkish-Atis/r2.10/tr_atis-ud-test.conllu",
2016	    },
2017	    "tr_penn": {
2018	        "train": "UD_Turkish-Penn/r2.10/tr_penn-ud-train.conllu",
2019	        "dev": "UD_Turkish-Penn/r2.10/tr_penn-ud-dev.conllu",
2020	        "test": "UD_Turkish-Penn/r2.10/tr_penn-ud-test.conllu",
2021	    },
2022	    "tr_tourism": {
2023	        "train": "UD_Turkish-Tourism/r2.10/tr_tourism-ud-train.conllu",
2024	        "dev": "UD_Turkish-Tourism/r2.10/tr_tourism-ud-dev.conllu",
2025	        "test": "UD_Turkish-Tourism/r2.10/tr_tourism-ud-test.conllu",
2026	    },
2027	    "tr_framenet": {
2028	        "train": "UD_Turkish-FrameNet/r2.10/tr_framenet-ud-train.conllu",
2029	        "dev": "UD_Turkish-FrameNet/r2.10/tr_framenet-ud-dev.conllu",
2030	        "test": "UD_Turkish-FrameNet/r2.10/tr_framenet-ud-test.conllu",
2031	    },
2032	    "tr_boun": {
2033	        "train": "UD_Turkish-BOUN/r2.10/tr_boun-ud-train.conllu",
2034	        "dev": "UD_Turkish-BOUN/r2.10/tr_boun-ud-dev.conllu",
2035	        "test": "UD_Turkish-BOUN/r2.10/tr_boun-ud-test.conllu",
2036	    },
2037	    "tr_gb": {
2038	        "test": "UD_Turkish-GB/r2.10/tr_gb-ud-test.conllu",
2039	    },
2040	    "tr_imst": {
2041	        "train": "UD_Turkish-IMST/r2.10/tr_imst-ud-train.conllu",
2042	        "dev": "UD_Turkish-IMST/r2.10/tr_imst-ud-dev.conllu",
2043	        "test": "UD_Turkish-IMST/r2.10/tr_imst-ud-test.conllu",
2044	    },
2045	    "tr_pud": {
2046	        "test": "UD_Turkish-PUD/r2.10/tr_pud-ud-test.conllu",
2047	    },
2048	    "uk_iu": {
2049	        "train": "UD_Ukrainian-IU/r2.10/uk_iu-ud-train.conllu",
2050	        "dev": "UD_Ukrainian-IU/r2.10/uk_iu-ud-dev.conllu",
2051	        "test": "UD_Ukrainian-IU/r2.10/uk_iu-ud-test.conllu",
2052	    },
2053	    # TODO(tfds) Add Umbrian Ikuvina split when it will be officially released.
2054	    "hsb_ufal": {
2055	        "train": "UD_Upper_Sorbian-UFAL/r2.10/hsb_ufal-ud-train.conllu",
2056	        "test": "UD_Upper_Sorbian-UFAL/r2.10/hsb_ufal-ud-test.conllu",
2057	    },
2058	    "ur_udtb": {
2059	        "train": "UD_Urdu-UDTB/r2.10/ur_udtb-ud-train.conllu",
2060	        "dev": "UD_Urdu-UDTB/r2.10/ur_udtb-ud-dev.conllu",
2061	        "test": "UD_Urdu-UDTB/r2.10/ur_udtb-ud-test.conllu",
2062	    },
2063	    "ug_udt": {
2064	        "train": "UD_Uyghur-UDT/r2.10/ug_udt-ud-train.conllu",
2065	        "dev": "UD_Uyghur-UDT/r2.10/ug_udt-ud-dev.conllu",
2066	        "test": "UD_Uyghur-UDT/r2.10/ug_udt-ud-test.conllu",
2067	    },
2068	    "vi_vtb": {
2069	        "train": "UD_Vietnamese-VTB/r2.10/vi_vtb-ud-train.conllu",
2070	        "dev": "UD_Vietnamese-VTB/r2.10/vi_vtb-ud-dev.conllu",
2071	        "test": "UD_Vietnamese-VTB/r2.10/vi_vtb-ud-test.conllu",
2072	    },
2073	    "wbp_ufal": {
2074	        "test": "UD_Warlpiri-UFAL/r2.10/wbp_ufal-ud-test.conllu",
2075	    },
2076	    "cy_ccg": {
2077	        "train": "UD_Welsh-CCG/r2.10/cy_ccg-ud-train.conllu",
2078	        "test": "UD_Welsh-CCG/r2.10/cy_ccg-ud-test.conllu",
2079	    },
2080	    "wo_wtb": {
2081	        "train": "UD_Wolof-WTB/r2.10/wo_wtb-ud-train.conllu",
2082	        "dev": "UD_Wolof-WTB/r2.10/wo_wtb-ud-dev.conllu",
2083	        "test": "UD_Wolof-WTB/r2.10/wo_wtb-ud-test.conllu",
2084	    },
2085	    "sjo_xdt": {
2086	        "test": "UD_Xibe-XDT/r2.10/sjo_xdt-ud-test.conllu",
2087	    },
2088	    "sah_yktdt": {
2089	        "test": "UD_Yakut-YKTDT/r2.10/sah_yktdt-ud-test.conllu",
2090	    },
2091	    "yo_ytb": {
2092	        "test": "UD_Yoruba-YTB/r2.10/yo_ytb-ud-test.conllu",
2093	    },
2094	    "ess_sli": {
2095	        "test": "UD_Yupik-SLI/r2.10/ess_sli-ud-test.conllu",
2096	    },
2097	}
2098	
2099	LANGS = DESCRIPTIONS.keys()
2100	
2101	
2102	def prepare_ud_filepaths(
2103	    path_prefix: epath.PathLike,
2104	    filepaths: Union[epath.PathLike, List[epath.PathLike]],
2105	) -> List[epath.PathLike]:
2106	  """Prepends a path prefix to a (list of) filepaths.
2107	
2108	  Args:
2109	    path_prefix: A path which will be prepended to all paths in filepaths.
2110	    filepaths: The filepaths to be prepared. Could be a list of paths for
2111	      multiple files, or a single path.
2112	
2113	  Returns:
2114	    A list with the resulting filepath(s). In case a single path was given as
2115	    `filepath`, it will returns a one-item list containing the resulting path.
2116	  """
2117	  paths = filepaths if isinstance(filepaths, list) else [filepaths]
2118	  return [os.path.join(path_prefix, filepath) for filepath in paths]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils_test.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	def test_prepare_ud_filepaths():
22	  expected_output = ["https://a/b"]
23	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils_test.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	  filepaths_1 = ud_utils.prepare_ud_filepaths(
25	      path_prefix="https://a", filepaths="b"
26	  )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/universal_dependencies/universal_dependencies_utils_test.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	  filepaths_2 = ud_utils.prepare_ud_filepaths(
28	      path_prefix="https://a", filepaths=["b"]
29	  )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/wake_vision/wake_vision_dataset_builder.py:117
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
116	_URL_TEMPLATE = (
117	    'https://dataverse.harvard.edu/api/access/datafile/{id}?format=original'
118	)
119	_IMAGE_URLS = {
120	    'train_images': [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/wake_vision/wake_vision_dataset_builder.py:210
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
209	        supervised_keys=('image', 'person'),
210	        homepage='https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910%2FDVN%2F1HOPXC',
211	        license='See homepage for license information.',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/webvid/webvid_dataset_builder.py:132
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
131	
132	  MANUAL_DOWNLOAD_INSTRUCTIONS = """
133	  Follow the download instructions in https://m-bain.github.io/webvid-dataset/
134	  to get the data. Place the csv files and the video directories in
135	  `manual_dir/webvid`, such that mp4 files are placed in
136	  `manual_dir/webvid/*/*_*/*.mp4`.
137	
138	  First directory typically being an arbitrary part directory (for sharded
139	  downloading), second directory is the page directory (two numbers around
140	  underscore), inside of which there is one or more mp4 files.
141	  """
142	
143	  VERSION = tfds.core.Version('1.0.0')

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/webvid/webvid_dataset_builder.py:147
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
146	  }
147	  _homepage: str = 'https://m-bain.github.io/webvid-dataset/'
148	  _description: str = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/xtreme_pos/xtreme_pos_dataset_builder.py:60
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
59	
60	_DATA_URLS = "https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3105/ud-treebanks-v2.5.tgz"
61	
62	
63	class Builder(tfds.dataset_builders.ConllUDatasetBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/datasets/xtreme_pos/xtreme_pos_dataset_builder.py:84
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
83	    return self.create_dataset_info(
84	        homepage="https://universaldependencies.org/"
85	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/graphs/cardiotox/cardiotox.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_DESCRIPTION = """
24	Drug Cardiotoxicity dataset [1-2] is a molecule classification task to detect
25	cardiotoxicity caused by binding hERG target, a protein associated with heart
26	beat rhythm. The data covers over 9000 molecules with hERG activity.
27	
28	Note:
29	
30	1. The data is split into four splits: train, test-iid, test-ood1, test-ood2.
31	
32	2. Each molecule in the dataset has 2D graph annotations which is designed to
33	facilitate graph neural network modeling. Nodes are the atoms of the molecule
34	and edges are the bonds. Each atom is represented as a vector encoding basic
35	atom information such as atom type. Similar logic applies to bonds.
36	
37	3. We include Tanimoto fingerprint distance (to training data) for each molecule
38	in the test sets to facilitate research on distributional shift in graph domain.
39	
40	For each example, the features include:
41	  atoms: a 2D tensor with shape (60, 27) storing node features. Molecules with
42	    less than 60 atoms are padded with zeros. Each atom has 27 atom features.
43	  pairs: a 3D tensor with shape (60, 60, 12) storing edge features. Each edge
44	    has 12 edge features.
45	  atom_mask: a 1D tensor with shape (60, ) storing node masks. 1 indicates the
46	    corresponding atom is real, othewise a padded one.
47	  pair_mask: a 2D tensor with shape (60, 60) storing edge masks. 1 indicates the
48	    corresponding edge is real, othewise a padded one.
49	  active: a one-hot vector indicating if the molecule is toxic or not. [0, 1]
50	    indicates it's toxic, otherwise [1, 0] non-toxic.
51	
52	
53	## References
54	[1]: V. B. Siramshetty et al. Critical Assessment of Artificial Intelligence
55	Methods for Prediction of hERG Channel Inhibition in the Big Data Era.
56	    JCIM, 2020. https://pubs.acs.org/doi/10.1021/acs.jcim.0c00884
57	
58	[2]: K. Han et al. Reliable Graph Neural Networks for Drug Discovery Under
59	Distributional Shift.
60	    NeurIPS DistShift Workshop 2021. https://arxiv.org/abs/2111.12951
61	"""
62	
63	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/graphs/cardiotox/cardiotox.py:132
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
131	        supervised_keys=None,
132	        homepage='https://github.com/google/uncertainty-baselines/tree/main/baselines/drug_cardiotoxicity',
133	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image/lsun.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	LSUN_SCENE_URL = "http://dl.yf.io/lsun/scenes/%s_%s_lmdb.zip"
28	LSUN_OBJECT_URL = "http://dl.yf.io/lsun/objects/%s.zip"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image/lsun.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	LSUN_SCENE_URL = "http://dl.yf.io/lsun/scenes/%s_%s_lmdb.zip"
28	LSUN_OBJECT_URL = "http://dl.yf.io/lsun/objects/%s.zip"
29	
30	_CITATION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image/lsun.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	
30	_CITATION = """\
31	@article{journals/corr/YuZSSX15,
32	  added-at = {2018-08-13T00:00:00.000+0200},
33	  author = {Yu, Fisher and Zhang, Yinda and Song, Shuran and Seff, Ari and Xiao, Jianxiong},
34	  biburl = {https://www.bibsonomy.org/bibtex/2446d4ffb99a5d7d2ab6e5417a12e195f/dblp},
35	  ee = {http://arxiv.org/abs/1506.03365},
36	  interhash = {3e9306c4ce2ead125f3b2ab0e25adc85},
37	  intrahash = {446d4ffb99a5d7d2ab6e5417a12e195f},
38	  journal = {CoRR},
39	  keywords = {dblp},
40	  timestamp = {2018-08-14T15:08:59.000+0200},
41	  title = {LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop.},
42	  url = {http://dblp.uni-trier.de/db/journals/corr/corr1506.html#YuZSSX15},
43	  volume = {abs/1506.03365},
44	  year = 2015
45	}
46	"""
47	
48	# From http://dl.yf.io/lsun/categories.txt minus "test"
49	_SCENES_CATEGORIES = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image/lsun.py:96
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
95	          release_notes={
96	              "3.0.0": "New split API (https://tensorflow.org/datasets/splits)",
97	              "3.1.0": (
98	                  "Add builder config for missing `person` object category, "
99	                  "and add `id` to the feature dict"
100	              ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image/lsun.py:117
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
116	        }),
117	        homepage="https://www.yf.io/p/lsun",
118	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/caltech_birds.py:36
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
35	
36	_URL = "https://www.vision.caltech.edu/datasets/cub_200_2011/"
37	_CITATION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/caltech_birds.py:61
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
60	        name=self.name,
61	        images_url="https://drive.google.com/uc?export=download&id=1GDr1OkoXdhaXWGA8S3MAq3a522Tak-nx",
62	        split_url="https://drive.google.com/uc?export=download&id=1vZuZPqha0JjmwkdaS_XtYryE3Jf5Q1AC",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/caltech_birds.py:62
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
61	        images_url="https://drive.google.com/uc?export=download&id=1GDr1OkoXdhaXWGA8S3MAq3a522Tak-nx",
62	        split_url="https://drive.google.com/uc?export=download&id=1vZuZPqha0JjmwkdaS_XtYryE3Jf5Q1AC",
63	        annotations_url="https://drive.google.com/uc?export=download&id=16NsbTpMs5L6hT4hUJAmpW2u7wH326WTR",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/caltech_birds.py:63
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
62	        split_url="https://drive.google.com/uc?export=download&id=1vZuZPqha0JjmwkdaS_XtYryE3Jf5Q1AC",
63	        annotations_url="https://drive.google.com/uc?export=download&id=16NsbTpMs5L6hT4hUJAmpW2u7wH326WTR",
64	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/caltech_birds.py:102
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
101	    with epath.Path(train_path).open() as f:
102	      train_list = f.read().splitlines()
103	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/caltech_birds.py:105
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
104	    with epath.Path(test_path).open() as f:
105	      test_list = f.read().splitlines()
106	

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/caltech_birds.py:121
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
120	
121	    with concurrent.futures.ThreadPoolExecutor(
122	        max_workers=_WORKERS
123	    ) as executor:

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/caltech_birds.py:219
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
218	
219	    with concurrent.futures.ThreadPoolExecutor(
220	        max_workers=_WORKERS
221	    ) as executor:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/caltech_birds.py:236
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
235	        name=self.name,
236	        images_url="https://drive.google.com/uc?export=download&id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45",
237	        split_url=None,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/caltech_birds.py:238
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
237	        split_url=None,
238	        annotations_url="https://drive.google.com/uc?export=download&id=1EamOKGLoTuZdtcVYbHMWNpkn3iAVj8TP",
239	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/caltech_birds.py:302
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
301	        mask = tfds.core.lazy_imports.cv2.imdecode(
302	            np.frombuffer(png_f.read(), dtype=np.uint8), flags=0
303	        )

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/caltech_birds.py:306
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
305	
306	    with concurrent.futures.ThreadPoolExecutor(
307	        max_workers=_WORKERS
308	    ) as executor:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cars196.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_URL = 'http://ai.stanford.edu/~jkrause/car196/'
24	_EXTRA_URL = 'https://ai.stanford.edu/~jkrause/cars/car_devkit.tgz'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cars196.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	_URL = 'http://ai.stanford.edu/~jkrause/car196/'
24	_EXTRA_URL = 'https://ai.stanford.edu/~jkrause/cars/car_devkit.tgz'
25	
26	_DESCRIPTION = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cars196.py:257
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
256	      '2.0.1': 'Website URL update',
257	      '2.1.0': 'Fixing bug https://github.com/tensorflow/datasets/issues/3927',
258	  }
259	
260	  def _info(self):
261	    """Define the dataset info."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cars196.py:274
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
273	        supervised_keys=('image', 'label'),
274	        homepage='https://ai.stanford.edu/~jkrause/cars/car_dataset.html',
275	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cassava.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	
44	_BASE_URL = "https://storage.googleapis.com/emcassavadata/cassavaleafdata.zip"
45	_LABELS = ["cbb", "cbsd", "cgm", "cmd", "healthy"]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cassava.py:63
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
62	        supervised_keys=("image", "label"),
63	        homepage="https://www.kaggle.com/c/cassava-disease/overview",
64	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cats_vs_dogs.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_CITATION = """\
28	@Inproceedings (Conference){asirra-a-captcha-that-exploits-interest-aligned-manual-image-categorization,
29	author = {Elson, Jeremy and Douceur, John (JD) and Howell, Jon and Saul, Jared},
30	title = {Asirra: A CAPTCHA that Exploits Interest-Aligned Manual Image Categorization},
31	booktitle = {Proceedings of 14th ACM Conference on Computer and Communications Security (CCS)},
32	year = {2007},
33	month = {October},
34	publisher = {Association for Computing Machinery, Inc.},
35	url = {https://www.microsoft.com/en-us/research/publication/asirra-a-captcha-that-exploits-interest-aligned-manual-image-categorization/},
36	edition = {Proceedings of 14th ACM Conference on Computer and Communications Security (CCS)},
37	}
38	"""
39	
40	_URL = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cats_vs_dogs.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	_URL = (
41	    "https://download.microsoft.com/download/3/E/1/"
42	    "3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip"
43	)
44	_NUM_CORRUPT_IMAGES = 1738
45	_DESCRIPTION = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cats_vs_dogs.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
57	  RELEASE_NOTES = {
58	      "4.0.0": "New split API (https://tensorflow.org/datasets/splits)",
59	      "4.0.1": (
60	          "Recoding images in generator to fix corrupt JPEG data warnings"
61	          " (https://github.com/tensorflow/datasets/issues/2188)"
62	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cats_vs_dogs.py:60
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
59	      "4.0.1": (
60	          "Recoding images in generator to fix corrupt JPEG data warnings"
61	          " (https://github.com/tensorflow/datasets/issues/2188)"
62	      ),
63	  }
64	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cats_vs_dogs.py:76
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
75	        homepage=(
76	            "https://www.microsoft.com/en-us/download/details.aspx?id=54765"
77	        ),
78	        citation=_CITATION,
79	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cats_vs_dogs.py:111
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
110	      # Those messages are now displayed when generating the dataset instead.
111	      img_data = fobj.read()
112	      img_tensor = tf.image.decode_image(img_data)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cbis_ddsm.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_DESCRIPTION = """\
28	The CBIS-DDSM (Curated Breast Imaging Subset of DDSM) is an updated and
29	standardized version of the Digital Database for Screening Mammography (DDSM).
30	The DDSM is a database of 2,620 scanned film mammography studies.
31	It contains normal, benign, and malignant cases with verified pathology
32	information.
33	
34	The default config is made of patches extracted from the original mammograms,
35	following the description from (http://arxiv.org/abs/1708.09427), in order to
36	frame the task to solve in a traditional image classification setting.
37	
38	"""
39	
40	_CITATION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cbis_ddsm.py:40
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
39	
40	_CITATION = """\
41	@misc{CBIS_DDSM_Citation,
42	  doi = {10.7937/k9/tcia.2016.7o02s9cy},
43	  url = {https://wiki.cancerimagingarchive.net/x/lZNXAQ},
44	  author = {Sawyer-Lee,  Rebecca and Gimenez,  Francisco and Hoogi,  Assaf and Rubin,  Daniel},
45	  title = {Curated Breast Imaging Subset of DDSM},
46	  publisher = {The Cancer Imaging Archive},
47	  year = {2016},
48	}
49	@article{TCIA_Citation,
50	  author = {
51	    K. Clark and B. Vendt and K. Smith and J. Freymann and J. Kirby and
52	    P. Koppel and S. Moore and S. Phillips and D. Maffitt and M. Pringle and
53	    L. Tarbox and F. Prior
54	  },
55	  title = {{The Cancer Imaging Archive (TCIA): Maintaining and Operating a
56	  Public Information Repository}},
57	  journal = {Journal of Digital Imaging},
58	  volume = {26},
59	  month = {December},
60	  year = {2013},
61	  pages = {1045-1057},
62	}
63	@article{DBLP:journals/corr/abs-1708-09427,
64	  author    = {Li Shen},
65	  title     = {End-to-end Training for Whole Image Breast Cancer Diagnosis using
66	               An All Convolutional Design},
67	  journal   = {CoRR},
68	  volume    = {abs/1708.09427},
69	  year      = {2017},
70	  url       = {http://arxiv.org/abs/1708.09427},
71	  archivePrefix = {arXiv},
72	  eprint    = {1708.09427},
73	  timestamp = {Mon, 13 Aug 2018 16:48:35 +0200},
74	  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-09427},
75	  bibsource = {dblp computer science bibliography, https://dblp.org}
76	}
77	"""
78	
79	_CALC_TEST_CSV_URL = 'https://wiki.cancerimagingarchive.net/download/attachments/22516629/calc_case_description_test_set.csv?version=1&modificationDate=1506796343686&api=v2'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cbis_ddsm.py:79
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
78	
79	_CALC_TEST_CSV_URL = 'https://wiki.cancerimagingarchive.net/download/attachments/22516629/calc_case_description_test_set.csv?version=1&modificationDate=1506796343686&api=v2'
80	_CALC_TRAIN_CSV_URL = 'https://wiki.cancerimagingarchive.net/download/attachments/22516629/calc_case_description_train_set.csv?version=1&modificationDate=1506796349666&api=v2'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cbis_ddsm.py:80
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
79	_CALC_TEST_CSV_URL = 'https://wiki.cancerimagingarchive.net/download/attachments/22516629/calc_case_description_test_set.csv?version=1&modificationDate=1506796343686&api=v2'
80	_CALC_TRAIN_CSV_URL = 'https://wiki.cancerimagingarchive.net/download/attachments/22516629/calc_case_description_train_set.csv?version=1&modificationDate=1506796349666&api=v2'
81	_MASS_TEST_CSV_URL = 'https://wiki.cancerimagingarchive.net/download/attachments/22516629/mass_case_description_test_set.csv?version=1&modificationDate=1506796343175&api=v2'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cbis_ddsm.py:81
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
80	_CALC_TRAIN_CSV_URL = 'https://wiki.cancerimagingarchive.net/download/attachments/22516629/calc_case_description_train_set.csv?version=1&modificationDate=1506796349666&api=v2'
81	_MASS_TEST_CSV_URL = 'https://wiki.cancerimagingarchive.net/download/attachments/22516629/mass_case_description_test_set.csv?version=1&modificationDate=1506796343175&api=v2'
82	_MASS_TRAIN_CSV_URL = 'https://wiki.cancerimagingarchive.net/download/attachments/22516629/mass_case_description_train_set.csv?version=1&modificationDate=1506796355038&api=v2'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cbis_ddsm.py:82
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
81	_MASS_TEST_CSV_URL = 'https://wiki.cancerimagingarchive.net/download/attachments/22516629/mass_case_description_test_set.csv?version=1&modificationDate=1506796343175&api=v2'
82	_MASS_TRAIN_CSV_URL = 'https://wiki.cancerimagingarchive.net/download/attachments/22516629/mass_case_description_train_set.csv?version=1&modificationDate=1506796355038&api=v2'
83	
84	_IMAGE_VIEW_LABELS = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cbis_ddsm.py:111
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
110	    kwargs['release_notes'] = {
111	        '3.0.0': """
112	        Better cropping sampling
113	        (https://github.com/tensorflow/datasets/pull/2502)
114	        """,
115	        '2.0.1': 'New split API (https://tensorflow.org/datasets/splits)',
116	    }
117	    super(CuratedBreastImagingDDSMConfig, self).__init__(**kwargs)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cbis_ddsm.py:115
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
114	        """,
115	        '2.0.1': 'New split API (https://tensorflow.org/datasets/splits)',
116	    }
117	    super(CuratedBreastImagingDDSMConfig, self).__init__(**kwargs)
118	    self.image_size = image_size
119	    self.patch_size = patch_size
120	
121	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cbis_ddsm.py:125
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
124	
125	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
126	  You can download the images from
127	  https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM
128	
129	  Because special software and libraries are needed to download and read the
130	  images contained in the dataset, TFDS assumes that the user has downloaded the
131	  original DCIM files and converted them to PNG.
132	
133	  The following commands (or equivalent) should be used to generate the PNG
134	  files, in order to guarantee reproducible results:
135	
136	  ```sh
137	  find $DATASET_DCIM_DIR -name '*.dcm' | \\
138	  xargs -n1 -P8 -I{} bash -c 'f={}; dcmj2pnm $f | convert - ${f/.dcm/.png}'
139	  ```
140	
141	  Resulting images should be put in `manual_dir`, like:
142	  `<manual_dir>/Mass-Training_P_01981_RIGHT_MLO_1/1.3.6.../000000.png`.
143	  """
144	
145	  BUILDER_CONFIGS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cbis_ddsm.py:189
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
188	        homepage=(
189	            'https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM'
190	        ),
191	        citation=_CITATION,
192	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cbis_ddsm.py:644
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
643	    image = cv2.imdecode(
644	        np.frombuffer(f.read(), dtype=np.uint8), flags=cv2.IMREAD_GRAYSCALE
645	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/chexpert.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_DESCRIPTION = """\
27	CheXpert is a large dataset of chest X-rays and competition for automated chest 
28	x-ray interpretation, which features uncertainty labels and radiologist-labeled 
29	reference standard evaluation sets. It consists of 224,316 chest radiographs 
30	of 65,240 patients, where the chest radiographic examinations and the associated 
31	radiology reports were retrospectively collected from Stanford Hospital. Each 
32	report was labeled for the presence of 14 observations as positive, negative, 
33	or uncertain. We decided on the 14 observations based on the prevalence in the 
34	reports and clinical relevance.
35	
36	The CheXpert dataset must be downloaded separately after reading and agreeing 
37	to a Research Use Agreement. To do so, please follow the instructions on the 
38	website, https://stanfordmlgroup.github.io/competitions/chexpert/.
39	"""
40	
41	_CITATION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/chexpert.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	
41	_CITATION = """\
42	@article{DBLP:journals/corr/abs-1901-07031,
43	  author    = {Jeremy Irvin and Pranav Rajpurkar and Michael Ko and Yifan Yu and Silviana Ciurea{-}Ilcus and Chris Chute and Henrik Marklund and Behzad Haghgoo and Robyn L. Ball and Katie Shpanskaya and Jayne Seekins and David A. Mong and Safwan S. Halabi and Jesse K. Sandberg and Ricky Jones and David B. Larson and Curtis P. Langlotz and Bhavik N. Patel and Matthew P. Lungren and Andrew Y. Ng},
44	  title     = {CheXpert: {A} Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison},
45	  journal   = {CoRR},
46	  volume    = {abs/1901.07031},
47	  year      = {2019},
48	  url       = {http://arxiv.org/abs/1901.07031},
49	  archivePrefix = {arXiv},
50	  eprint    = {1901.07031},
51	  timestamp = {Fri, 01 Feb 2019 13:39:59 +0100},
52	  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-07031},
53	  bibsource = {dblp computer science bibliography, https://dblp.org}
54	}
55	"""
56	
57	# Path to images and category labels in data dir
58	_DATA_DIR = "CheXpert-v1.0-small"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/chexpert.py:79
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
78	
79	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
80	  You must register and agree to user agreement on the dataset page:
81	  https://stanfordmlgroup.github.io/competitions/chexpert/
82	  Afterwards, you have to put the CheXpert-v1.0-small directory in the
83	  manual_dir. It should contain subdirectories: train/ and valid/ with images
84	  and also train.csv and valid.csv files.
85	  """
86	
87	  def _info(self):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/chexpert.py:102
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
101	        supervised_keys=("image", "label"),
102	        homepage="https://stanfordmlgroup.github.io/competitions/chexpert/",
103	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar.py:59
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
58	        supervised_keys=("image", "label"),
59	        homepage="https://www.cs.toronto.edu/~kriz/cifar.html",
60	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar.py:67
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
66	        name=self.name,
67	        url="https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz",
68	        train_files=[

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar.py:94
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
93	      with epath.Path(labels_path).open() as label_f:
94	        label_names = [name for name in label_f.read().split("\n") if name]
95	      self.info.features[label_key].names = label_names

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar.py:147
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
146	        name=self.name,
147	        url="https://www.cs.toronto.edu/~kriz/cifar-100-binary.tar.gz",
148	        train_files=["train.bin"],

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar.py:174
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
173	        supervised_keys=("image", "label"),
174	        homepage="https://www.cs.toronto.edu/~kriz/cifar.html",
175	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar.py:210
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
209	  with tf.io.gfile.GFile(path, "rb") as f:
210	    data = f.read()
211	  offset = 0

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar100_n/cifar100_n.py:38
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
37	
38	_CITATION = """
39	@inproceedings{wei2022learning,
40	  title={Learning with Noisy Labels Revisited: A Study Using Real-World Human 
41	  Annotations},
42	  author={Jiaheng Wei and Zhaowei Zhu and Hao Cheng and Tongliang Liu and Gang 
43	  Niu and Yang Liu},
44	  booktitle={International Conference on Learning Representations},
45	  year={2022},
46	  url={https://openreview.net/forum?id=TBWA6PLJZQm}
47	}
48	"""
49	
50	
51	class Cifar100N(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar100_n/cifar100_n.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	
54	  MANUAL_DOWNLOAD_INSTRUCTIONS = """
55	  Download 'side_info_cifar100N.csv', 'CIFAR-100_human_ordered.npy' and
56	  'image_order_c100.npy' from https://github.com/UCSC-REAL/cifar-10-100n.
57	  
58	  Then convert 'CIFAR-100_human_ordered.npy' into a CSV file 
59	  'CIFAR-100_human_annotations.csv'. This can be done with the following code:
60	  
61	  ```
62	  import numpy as np
63	  from tensorflow_datasets.core.utils.lazy_imports_utils import pandas as pd
64	  from tensorflow_datasets.core.utils.lazy_imports_utils import tensorflow as tf
65	  
66	  human_labels_np_path = '<local_path>/CIFAR-100_human_ordered.npy'
67	  human_labels_csv_path = '<local_path>/CIFAR-100_human_annotations.csv'
68	
69	  with tf.io.gfile.GFile(human_labels_np_path, "rb") as f:
70	    human_annotations = np.load(f, allow_pickle=True)
71	
72	  df = pd.DataFrame(human_annotations[()])
73	
74	  with tf.io.gfile.GFile(human_labels_csv_path, "w") as f:
75	    df.to_csv(f, index=False)
76	  ```
77	  """
78	
79	  VERSION = tfds.core.Version('1.0.1')

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar100_n/cifar100_n.py:100
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
99	        supervised_keys=None,
100	        homepage='https://www.cs.toronto.edu/~kriz/cifar.html',
101	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar100_n/cifar100_n.py:108
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
107	        name=self.name,
108	        url='https://www.cs.toronto.edu/~kriz/cifar-100-binary.tar.gz',
109	        train_files=['train.bin'],

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar100_n/cifar100_n.py:140
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
139	      with epath.Path(labels_path).open() as label_f:
140	        label_names = [name for name in label_f.read().split('\n') if name]
141	      self.info.features[label_key].names = label_names

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar100_n/cifar100_n.py:249
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
248	  with tf.io.gfile.GFile(path, 'rb') as f:
249	    data = f.read()
250	  offset = 0

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_1.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	
22	_CITATION = """\
23	@article{recht2018cifar10.1,
24	  author = {Benjamin Recht and Rebecca Roelofs and Ludwig Schmidt and Vaishaal Shankar},
25	  title = {Do CIFAR-10 Classifiers Generalize to CIFAR-10?},
26	  year = {2018},
27	  note = {\\url{https://arxiv.org/abs/1806.00451}},
28	}
29	
30	@article{torralba2008tinyimages, 
31	  author = {Antonio Torralba and Rob Fergus and William T. Freeman}, 
32	  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
33	  title = {80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition}, 
34	  year = {2008}, 
35	  volume = {30}, 
36	  number = {11}, 
37	  pages = {1958-1970}
38	}
39	"""
40	
41	_DESCRIPTION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_1.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	
50	_DL_URL_IMAGES = "https://github.com/modestyachts/CIFAR-10.1/blob/master/datasets/cifar10.1_{}_data.npy?raw=true"
51	_DL_URL_LABELS = "https://github.com/modestyachts/CIFAR-10.1/blob/master/datasets/cifar10.1_{}_labels.npy?raw=true"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_1.py:51
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
50	_DL_URL_IMAGES = "https://github.com/modestyachts/CIFAR-10.1/blob/master/datasets/cifar10.1_{}_data.npy?raw=true"
51	_DL_URL_LABELS = "https://github.com/modestyachts/CIFAR-10.1/blob/master/datasets/cifar10.1_{}_labels.npy?raw=true"
52	
53	_DATA_OPTIONS = ["v4", "v6"]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_1.py:112
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
111	        supervised_keys=("image", "label"),
112	        homepage="https://github.com/modestyachts/CIFAR-10.1",
113	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_corrupted.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	
35	_CITATION = """\
36	@inproceedings{
37	  hendrycks2018benchmarking,
38	  title={Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
39	  author={Dan Hendrycks and Thomas Dietterich},
40	  booktitle={International Conference on Learning Representations},
41	  year={2019},
42	  url={https://openreview.net/forum?id=HJz6tiCqYm},
43	}
44	"""
45	
46	_CIFAR_IMAGE_SIZE = (32, 32, 3)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_corrupted.py:48
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
47	_CIFAR_CLASSES = 10
48	_DOWNLOAD_URL = 'https://zenodo.org/record/2535967/files/CIFAR-10-C.tar'
49	_CORRUPTIONS_TO_FILENAMES = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_corrupted.py:147
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
146	  RELEASE_NOTES = {
147	      '1.0.0': 'New split API (https://tensorflow.org/datasets/splits)',
148	  }
149	  BUILDER_CONFIGS = _make_builder_configs()

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_corrupted.py:165
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
164	        supervised_keys=('image', 'label'),
165	        homepage='https://github.com/hendrycks/robustness',
166	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_h/cifar10_h.py:79
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
78	        supervised_keys=None,
79	        homepage="https://github.com/jcpeterson/cifar-10h",
80	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_h/cifar10_h.py:87
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
86	        name=self.name,
87	        url="https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz",
88	        human_annotations_url="https://github.com/jcpeterson/cifar-10h/raw/master/data/cifar10h-raw.zip",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_h/cifar10_h.py:88
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
87	        url="https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz",
88	        human_annotations_url="https://github.com/jcpeterson/cifar-10h/raw/master/data/cifar10h-raw.zip",
89	        train_files=[

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_h/cifar10_h.py:122
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
121	      with epath.Path(labels_path).open() as label_f:
122	        label_names = [name for name in label_f.read().split("\n") if name]
123	      self.info.features[label_key].names = label_names

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_h/cifar10_h.py:237
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
236	  with tf.io.gfile.GFile(path, "rb") as f:
237	    data = f.read()
238	  offset = 0

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_n/cifar10_n.py:38
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
37	
38	_CITATION = """
39	@inproceedings{wei2022learning,
40	  title={Learning with Noisy Labels Revisited: A Study Using Real-World Human 
41	  Annotations},
42	  author={Jiaheng Wei and Zhaowei Zhu and Hao Cheng and Tongliang Liu and Gang 
43	  Niu and Yang Liu},
44	  booktitle={International Conference on Learning Representations},
45	  year={2022},
46	  url={https://openreview.net/forum?id=TBWA6PLJZQm}
47	}
48	"""
49	
50	
51	class Cifar10N(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_n/cifar10_n.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	
54	  MANUAL_DOWNLOAD_INSTRUCTIONS = """
55	  Download 'side_info_cifar10N.csv', 'CIFAR-10_human_ordered.npy' and 
56	  'image_order_c10.npy' from https://github.com/UCSC-REAL/cifar-10-100n.
57	
58	  Then convert 'CIFAR-10_human_ordered.npy' into a CSV file 
59	  'CIFAR-10_human_annotations.csv'. This can be done with the following code:
60	
61	  ```
62	  import numpy as np
63	  from tensorflow_datasets.core.utils.lazy_imports_utils import pandas as pd
64	  from tensorflow_datasets.core.utils.lazy_imports_utils import tensorflow as tf
65	
66	  human_labels_np_path = '<local_path>/CIFAR-10_human_ordered.npy'
67	  human_labels_csv_path = '<local_path>/CIFAR-10_human_annotations.csv'
68	
69	  with tf.io.gfile.GFile(human_labels_np_path, "rb") as f:
70	    human_annotations = np.load(f, allow_pickle=True)
71	
72	  df = pd.DataFrame(human_annotations[()])
73	
74	  with tf.io.gfile.GFile(human_labels_csv_path, "w") as f:
75	    df.to_csv(f, index=False)
76	  ```
77	  """
78	
79	  VERSION = tfds.core.Version('1.0.4')

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_n/cifar10_n.py:110
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
109	        supervised_keys=None,
110	        homepage='https://ucsc-real.soe.ucsc.edu:1995/Home.html/',
111	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_n/cifar10_n.py:118
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
117	        name=self.name,
118	        url='https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz',
119	        train_files=[

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_n/cifar10_n.py:156
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
155	      with epath.Path(labels_path).open() as label_f:
156	        label_names = [name for name in label_f.read().split('\n') if name]
157	      self.info.features[label_key].names = label_names

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cifar10_n/cifar10_n.py:291
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
290	  with tf.io.gfile.GFile(path, 'rb') as f:
291	    data = f.read()
292	  offset = 0

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/citrus.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	
37	_DESCRIPTION = """
38	The original citrus dataset contains 759 images of healthy and unhealthy citrus
39	fruits and leaves. However, for now we only export 594 images of citrus leaves
40	with the following labels: Black Spot, Canker, Greening, and Healthy. The
41	exported images are in PNG format and have 256x256 pixels.
42	
43	NOTE: Leaf images with Melanose label were dropped due to very small count and
44	other non-leaf images being present in the same directory.
45	
46	Dataset URL: https://data.mendeley.com/datasets/3f83gxmv57/2
47	License: http://creativecommons.org/licenses/by/4.0
48	"""
49	
50	_URL = "https://data.mendeley.com/public-files/datasets/3f83gxmv57/files/53398b67-6f0e-4a67-8384-e2b574b2ebf4/file_downloaded"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/citrus.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	
50	_URL = "https://data.mendeley.com/public-files/datasets/3f83gxmv57/files/53398b67-6f0e-4a67-8384-e2b574b2ebf4/file_downloaded"
51	_LEAVES_LABELS = ["Black spot", "canker", "greening", "healthy"]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/citrus.py:73
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
72	        supervised_keys=("image", "label"),
73	        homepage="https://data.mendeley.com/datasets/3f83gxmv57/2",
74	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cmaterdb.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	_CMATERDB_TRAINING_URL = (
27	    "https://raw.githubusercontent.com/prabhuomkar/CMATERdb/master/"
28	    "datasets/{type}-numerals/training-images.npz"
29	)
30	_CMATERDB_TESTING_URL = (
31	    "https://raw.githubusercontent.com/prabhuomkar/CMATERdb/master/"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cmaterdb.py:31
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
30	_CMATERDB_TESTING_URL = (
31	    "https://raw.githubusercontent.com/prabhuomkar/CMATERdb/master/"
32	    "datasets/{type}-numerals/testing-images.npz"
33	)
34	
35	_CITATION = """\
36	@article{Das:2012:GAB:2161007.2161320,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cmaterdb.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	
35	_CITATION = """\
36	@article{Das:2012:GAB:2161007.2161320,
37	  author = {Das, Nibaran and Sarkar, Ram and Basu, Subhadip and Kundu, Mahantapas 
38	            and Nasipuri, Mita and Basu, Dipak Kumar},
39	  title = {A Genetic Algorithm Based Region Sampling for Selection of Local Features 
40	          in Handwritten Digit Recognition Application},
41	  journal = {Appl. Soft Comput.},
42	  issue_date = {May, 2012},
43	  volume = {12},
44	  number = {5},
45	  month = may,
46	  year = {2012},
47	  issn = {1568-4946},
48	  pages = {1592--1606},
49	  numpages = {15},
50	  url = {http://dx.doi.org/10.1016/j.asoc.2011.11.030},
51	  doi = {10.1016/j.asoc.2011.11.030},
52	  acmid = {2161320},
53	  publisher = {Elsevier Science Publishers B. V.},
54	  address = {Amsterdam, The Netherlands, The Netherlands},
55	  keywords = {Feature selection, Genetic algorithm, N-Quality consensus, 
56	  Optimal local regions, Region sampling, Variable sized local regions},
57	}
58	@article{Das:2012:SFC:2240301.2240421,
59	  author = {Das, Nibaran and Reddy, Jagan Mohan and Sarkar, Ram and Basu, Subhadip and Kundu, 
60	            Mahantapas and Nasipuri, Mita and Basu, Dipak Kumar},
61	  title = {A Statistical-topological Feature Combination for Recognition of Handwritten Numerals},
62	  journal = {Appl. Soft Comput.},
63	  issue_date = {August, 2012},
64	  volume = {12},
65	  number = {8},
66	  month = aug,
67	  year = {2012},
68	  issn = {1568-4946},
69	  pages = {2486--2495},
70	  numpages = {10},
71	  url = {http://dx.doi.org/10.1016/j.asoc.2012.03.039},
72	  doi = {10.1016/j.asoc.2012.03.039},
73	  acmid = {2240421},
74	  publisher = {Elsevier Science Publishers B. V.},
75	  address = {Amsterdam, The Netherlands, The Netherlands},
76	  keywords = {Character recognition, Feature combination, MPCA, PCA, SVM, Statistical, Topological},
77	}
78	"""
79	
80	_DESCRIPTION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cmaterdb.py:124
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
123	        supervised_keys=("image", "label"),
124	        homepage="https://code.google.com/archive/p/cmaterdb/",
125	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/colorectal_histology.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_URL = "https://zenodo.org/record/53169#.XGZemKwzbmG"
25	_TILES_DL_URL = "https://zenodo.org/record/53169/files/Kather_texture_2016_image_tiles_5000.zip"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/colorectal_histology.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	_URL = "https://zenodo.org/record/53169#.XGZemKwzbmG"
25	_TILES_DL_URL = "https://zenodo.org/record/53169/files/Kather_texture_2016_image_tiles_5000.zip"
26	_LARGE_DL_URL = "https://zenodo.org/record/53169/files/Kather_texture_2016_larger_images_10.zip"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/colorectal_histology.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	_TILES_DL_URL = "https://zenodo.org/record/53169/files/Kather_texture_2016_image_tiles_5000.zip"
26	_LARGE_DL_URL = "https://zenodo.org/record/53169/files/Kather_texture_2016_larger_images_10.zip"
27	
28	_TILES_SUBDIR = "Kather_texture_2016_image_tiles_5000"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/colorectal_histology.py:72
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
71	  RELEASE_NOTES = {
72	      "2.0.0": "New split API (https://tensorflow.org/datasets/splits)",
73	  }
74	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/colorectal_histology.py:136
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
135	  RELEASE_NOTES = {
136	      "2.0.0": "New split API (https://tensorflow.org/datasets/splits)",
137	  }
138	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/controlled_noisy_web_labels/controlled_noisy_web_labels.py:179
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
178	
179	MINI_IMAGENET_TRAIN = 'https://raw.githubusercontent.com/yaoyao-liu/mini-imagenet-tools/main/csv_files/train.csv'
180	MINI_IMAGENET_VAL = 'https://raw.githubusercontent.com/yaoyao-liu/mini-imagenet-tools/main/csv_files/val.csv'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/controlled_noisy_web_labels/controlled_noisy_web_labels.py:180
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
179	MINI_IMAGENET_TRAIN = 'https://raw.githubusercontent.com/yaoyao-liu/mini-imagenet-tools/main/csv_files/train.csv'
180	MINI_IMAGENET_VAL = 'https://raw.githubusercontent.com/yaoyao-liu/mini-imagenet-tools/main/csv_files/val.csv'
181	MINI_IMAGENET_TEST = 'https://raw.githubusercontent.com/yaoyao-liu/mini-imagenet-tools/main/csv_files/test.csv'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/controlled_noisy_web_labels/controlled_noisy_web_labels.py:181
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
180	MINI_IMAGENET_VAL = 'https://raw.githubusercontent.com/yaoyao-liu/mini-imagenet-tools/main/csv_files/val.csv'
181	MINI_IMAGENET_TEST = 'https://raw.githubusercontent.com/yaoyao-liu/mini-imagenet-tools/main/csv_files/test.csv'
182	
183	
184	@dataclasses.dataclass
185	class ControlledNoisyWebLabelsConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/controlled_noisy_web_labels/controlled_noisy_web_labels.py:199
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
198	  }
199	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
200	  In order to manually download this data, a user must perform the
201	  following operations:
202	  
203	  1. Download the splits and the annotations [here](https://storage.googleapis.com/cnlw/dataset_no_images.zip)
204	  2. Extract dataset_no_images.zip to dataset_no_images/.
205	  3. Download all images in dataset_no_images/mini-imagenet-annotations.json into
206	  a new folder named dataset_no_images/noisy_images/. The output filename must
207	  agree with the image id provided in mini-imagenet-annotations.json. For example,
208	  if "image/id": "5922767e5677aef4", then the downloaded image should be
209	  dataset_no_images/noisy_images/5922767e5677aef4.jpg.
210	  4.Register on https://image-net.org/download-images and download
211	  ILSVRC2012_img_train.tar and ILSVRC2012_img_val.tar.
212	
213	  The resulting directory structure may then be processed by TFDS:
214	
215	  - dataset_no_images/
216	    - mini-imagenet/
217	      - class_name.txt
218	      - split/
219	        - blue_noise_nl_0.0
220	        - blue_noise_nl_0.1
221	        - ...
222	        - red_noise_nl_0.0
223	        - red_noise_nl_0.1
224	        - ...
225	        - clean_validation
226	    - mini-imagenet-annotations.json
227	  - ILSVRC2012_img_train.tar
228	  - ILSVRC2012_img_val.tar
229	  - noisy_images/
230	    - 5922767e5677aef4.jpg
231	
232	  """
233	  # pytype: disable=wrong-keyword-args
234	  BUILDER_CONFIGS = [

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/controlled_noisy_web_labels/controlled_noisy_web_labels.py:304
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
303	        continue
304	      fobj_mem = io.BytesIO(fobj.read())
305	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/controlled_noisy_web_labels/controlled_noisy_web_labels.py:341
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
340	      fobj.seek(0)
341	      fobj_mem = io.BytesIO(fobj.read())
342	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/controlled_noisy_web_labels/controlled_noisy_web_labels.py:351
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
350	          new_image_id = label + image_index + '.jpg'
351	          buf = image.read()
352	          clean_image_dict[new_image_id] = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/controlled_noisy_web_labels/controlled_noisy_web_labels.py:378
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
377	        homepage=(
378	            'https://google.github.io/controlled-noisy-web-labels/index.html'
379	        ),
380	        citation=_CITATION,
381	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/controlled_noisy_web_labels/controlled_noisy_web_labels.py:445
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
444	    with epath.Path(split_file).open() as fp:
445	      split_info = fp.read().splitlines()
446	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/controlled_noisy_web_labels/controlled_noisy_web_labels.py:458
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
457	        example['is_clean'] = 1
458	        buf = fobj.read()
459	        example['image'] = self._resize_image(buf)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/controlled_noisy_web_labels/controlled_noisy_web_labels.py:471
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
470	    with epath.Path(split_file).open() as fp:
471	      split_info = fp.read().splitlines()
472	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/controlled_noisy_web_labels/controlled_noisy_web_labels.py:488
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
487	        ) as f:
488	          example['image'] = self._resize_image(f.read())
489	      else:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cycle_gan.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	# From https://arxiv.org/abs/1703.10593
24	_CITATION = """\
25	@article{DBLP:journals/corr/ZhuPIE17,
26	  author    = {Jun{-}Yan Zhu and
27	               Taesung Park and
28	               Phillip Isola and
29	               Alexei A. Efros},
30	  title     = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial
31	               Networks},
32	  journal   = {CoRR},
33	  volume    = {abs/1703.10593},
34	  year      = {2017},
35	  url       = {http://arxiv.org/abs/1703.10593},
36	  archivePrefix = {arXiv},
37	  eprint    = {1703.10593},
38	  timestamp = {Mon, 13 Aug 2018 16:48:06 +0200},
39	  biburl    = {https://dblp.org/rec/bib/journals/corr/ZhuPIE17},
40	  bibsource = {dblp computer science bibliography, https://dblp.org}
41	}
42	"""
43	
44	_DL_URL = "https://efrosgans.eecs.berkeley.edu/cyclegan/datasets/"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cycle_gan.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	
44	_DL_URL = "https://efrosgans.eecs.berkeley.edu/cyclegan/datasets/"
45	
46	# "ae_photos" : Not added because trainA and trainB are missing.
47	# "cityscapes" : Removed due to a licensing issue. See
48	#   https://github.com/junyanz/CycleGAN/blob/master/datasets/download_dataset.sh
49	_DATA_OPTIONS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/cycle_gan.py:109
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
108	        supervised_keys=("image", "label"),
109	        homepage="https://junyanz.github.io/CycleGAN/",
110	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/deep_weeds.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	
25	_URL = "https://drive.google.com/uc?export=download&id=1xnK3B6K6KekDI55vwJ0vnc2IGoDga9cj"
26	_URL_LABELS = "https://raw.githubusercontent.com/AlexOlsen/DeepWeeds/master/labels/labels.csv"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/deep_weeds.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	_URL = "https://drive.google.com/uc?export=download&id=1xnK3B6K6KekDI55vwJ0vnc2IGoDga9cj"
26	_URL_LABELS = "https://raw.githubusercontent.com/AlexOlsen/DeepWeeds/master/labels/labels.csv"
27	
28	_DESCRIPTION = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/deep_weeds.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	
37	_CITATION = """\
38	 @article{DeepWeeds2019,
39	  author = {Alex Olsen and
40	    Dmitry A. Konovalov and
41	    Bronson Philippa and
42	    Peter Ridd and
43	    Jake C. Wood and
44	    Jamie Johns and
45	    Wesley Banks and
46	    Benjamin Girgenti and
47	    Owen Kenny and
48	    James Whinney and
49	    Brendan Calvert and
50	    Mostafa {Rahimi Azghadi} and
51	    Ronald D. White},
52	  title = {{DeepWeeds: A Multiclass Weed Species Image Dataset for Deep Learning}},
53	  journal = {Scientific Reports},
54	  year = 2019,
55	  number = 2058,
56	  month = 2,
57	  volume = 9,
58	  issue = 1,
59	  day = 14,
60	  url = "https://doi.org/10.1038/s41598-018-38343-3",
61	  doi = "10.1038/s41598-018-38343-3"
62	}
63	"""
64	
65	
66	class DeepWeeds(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/deep_weeds.py:86
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
85	        supervised_keys=("image", "label"),
86	        homepage="https://github.com/AlexOlsen/DeepWeeds",
87	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/diabetic_retinopathy_detection.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_CITATION = """\
29	@ONLINE {kaggle-diabetic-retinopathy,
30	    author = "Kaggle and EyePacs",
31	    title  = "Kaggle Diabetic Retinopathy Detection",
32	    month  = "jul",
33	    year   = "2015",
34	    url    = "https://www.kaggle.com/c/diabetic-retinopathy-detection/data"
35	}
36	"""
37	_URL_TEST_LABELS = "https://storage.googleapis.com/kaggle-forum-message-attachments/90528/2877/retinopathy_solution.csv"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/diabetic_retinopathy_detection.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	"""
37	_URL_TEST_LABELS = "https://storage.googleapis.com/kaggle-forum-message-attachments/90528/2877/retinopathy_solution.csv"
38	_BTGRAHAM_DESCRIPTION_PATTERN = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/diabetic_retinopathy_detection.py:60
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
59	        release_notes={
60	            "3.0.0": "New split API (https://tensorflow.org/datasets/splits)",
61	        },
62	        **kwargs,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/diabetic_retinopathy_detection.py:74
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
73	
74	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
75	  You have to download this dataset from Kaggle.
76	  https://www.kaggle.com/c/diabetic-retinopathy-detection/data
77	  After downloading, unpack the test.zip file into test/ directory in manual_dir
78	  and sample.zip to sample/. Also unpack the sampleSubmissions.csv and
79	  trainLabels.csv.
80	  """
81	
82	  BUILDER_CONFIGS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/diabetic_retinopathy_detection.py:117
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
116	        }),
117	        homepage="https://www.kaggle.com/c/diabetic-retinopathy-detection/data",
118	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/diabetic_retinopathy_detection.py:235
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
234	  image = cv2.imdecode(
235	      np.frombuffer(image_fobj.read(), dtype=np.uint8), flags=3
236	  )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/diabetic_retinopathy_detection.py:265
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
264	  image = cv2.imdecode(
265	      np.frombuffer(image_fobj.read(), dtype=np.uint8), flags=3
266	  )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/dmlab.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_URL = "https://storage.googleapis.com/dmlab-vtab/dmlab.tar.gz"
28	
29	
30	class Dmlab(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/dmlab.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	        }),
54	        homepage="https://github.com/google-research/task_adaptation",
55	        citation=r"""@article{zhai2019visual,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/dmlab.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	        homepage="https://github.com/google-research/task_adaptation",
55	        citation=r"""@article{zhai2019visual,
56	        title={The Visual Task Adaptation Benchmark},
57	        author={Xiaohua Zhai and Joan Puigcerver and Alexander Kolesnikov and
58	               Pierre Ruyssen and Carlos Riquelme and Mario Lucic and
59	               Josip Djolonga and Andre Susano Pinto and Maxim Neumann and
60	               Alexey Dosovitskiy and Lucas Beyer and Olivier Bachem and
61	               Michael Tschannen and Marcin Michalski and Olivier Bousquet and
62	               Sylvain Gelly and Neil Houlsby},
63	                              year={2019},
64	                              eprint={1910.04867},
65	                              archivePrefix={arXiv},
66	                              primaryClass={cs.CV},
67	                              url = {https://arxiv.org/abs/1910.04867}
68	                          }""",
69	        supervised_keys=("image", "label"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/domainnet/domainnet.py:32
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
31	
32	_HOMEPAGE = 'http://ai.bu.edu/DomainNet/'
33	_CITATION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/domainnet/domainnet.py:414
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
413	
414	  _BASE_URL = 'http://csr.bu.edu/ftp/visda/2019/multi-source'
415	
416	  img_path: Any

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/domainnet/domainnet.py:456
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
455	    with tf.io.gfile.GFile(self.splits[split]) as split_file:  # pytype: disable=attribute-error  # gen-stub-imports
456	      for i, img_class_line in enumerate(split_file.read().split('\n')):
457	        if not img_class_line:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/dtd.py:40
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
39	"""
40	_URL = "https://www.robots.ox.ac.uk/~vgg/data/dtd/index.html"
41	_DATA_URL = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/dtd.py:42
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
41	_DATA_URL = (
42	    "https://www.robots.ox.ac.uk/~vgg/data/dtd/download/dtd-r1.0.1.tar.gz"
43	)
44	
45	
46	class Dtd(tfds.core.GeneratorBasedBuilder):
47	  """Describable Textures Dataset (DTD)."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/eurosat.py:36
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
35	
36	_DESCRIPTION = """\
37	EuroSAT dataset is based on Sentinel-2 satellite images covering 13 spectral
38	bands and consisting of 10 classes with 27000 labeled and
39	geo-referenced samples.
40	
41	Two datasets are offered:
42	- rgb: Contains only the optical R, G, B frequency bands encoded as JPEG image.
43	- all: Contains all 13 bands in the original value range (float32).
44	
45	URL: https://github.com/phelber/eurosat
46	"""
47	
48	_LABELS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/eurosat.py:61
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
60	
61	_URL = 'https://github.com/phelber/eurosat'
62	
63	_DATA_OPTIONS = ['rgb', 'all']

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/eurosat.py:96
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
95	          name='rgb',
96	          download_url='http://madm.dfki.de/files/sentinel/EuroSAT.zip',
97	          subdir='2750',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/eurosat.py:103
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
102	          name='all',
103	          download_url='http://madm.dfki.de/files/sentinel/EuroSATallBands.zip',
104	          subdir='ds/images/remote_sensing/otherDatasets/sentinel_2/tif',

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/eurosat.py:171
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
170	  with tf.io.gfile.GFile(filename, 'rb') as f:
171	    arr = tfds.core.lazy_imports.tifffile.imread(io.BytesIO(f.read()))
172	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/eurosat.py:171
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
170	  with tf.io.gfile.GFile(filename, 'rb') as f:
171	    arr = tfds.core.lazy_imports.tifffile.imread(io.BytesIO(f.read()))
172	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/food101.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_BASE_URL = "http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz"
25	
26	_DESCRIPTION = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/food101.py:74
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
73	        supervised_keys=("image", "label"),
74	        homepage="https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/",
75	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/food101.py:105
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
104	    with epath.Path(json_file_path).open() as f:
105	      data = json.loads(f.read())
106	    for label, images in data.items():

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/geirhos_conflict_stimuli.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_CITATION = """\
27	@inproceedings{
28	  geirhos2018imagenettrained,
29	  title={ImageNet-trained {CNN}s are biased towards texture; increasing shape
30	         bias improves accuracy and robustness.},
31	  author={Robert Geirhos and Patricia Rubisch and Claudio Michaelis and
32	          Matthias Bethge and Felix A. Wichmann and Wieland Brendel},
33	  booktitle={International Conference on Learning Representations},
34	  year={2019},
35	  url={https://openreview.net/forum?id=Bygh9j09KX},
36	}
37	"""
38	
39	_DESCRIPTION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/geirhos_conflict_stimuli.py:48
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
47	
48	_BASE_URL = "https://github.com/rgeirhos/texture-vs-shape"
49	_DOWNLOAD_URL = "https://github.com/rgeirhos/texture-vs-shape/archive/1b69c6a445c3348927139edb30a5134521fd4b03.zip"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/geirhos_conflict_stimuli.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
48	_BASE_URL = "https://github.com/rgeirhos/texture-vs-shape"
49	_DOWNLOAD_URL = "https://github.com/rgeirhos/texture-vs-shape/archive/1b69c6a445c3348927139edb30a5134521fd4b03.zip"
50	_IMAGENET_MAPPING_URL = "https://raw.githubusercontent.com/rgeirhos/generalisation-humans-DNNs/5bbe08f6821e1eb2bdbe98acebf3586d36be00ab/16-class-ImageNet/MSCOCO_to_ImageNet_category_mapping.txt"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/geirhos_conflict_stimuli.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	_DOWNLOAD_URL = "https://github.com/rgeirhos/texture-vs-shape/archive/1b69c6a445c3348927139edb30a5134521fd4b03.zip"
50	_IMAGENET_MAPPING_URL = "https://raw.githubusercontent.com/rgeirhos/generalisation-humans-DNNs/5bbe08f6821e1eb2bdbe98acebf3586d36be00ab/16-class-ImageNet/MSCOCO_to_ImageNet_category_mapping.txt"
51	_DATA_DIR_PATH = "texture-vs-shape-1b69c6a445c3348927139edb30a5134521fd4b03/stimuli/style-transfer-preprocessed-512"

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/geirhos_conflict_stimuli.py:128
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
127	    with epath.Path(imagenet_mapping_path).open() as f:
128	      mapping_txt = f.read()
129	    mapping = {}

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/horses_or_humans.py:21
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
20	
21	_CITATION = """\
22	@ONLINE {horses_or_humans,
23	author = "Laurence Moroney",
24	title = "Horses or Humans Dataset",
25	month = "feb",
26	year = "2019",
27	url = "http://laurencemoroney.com/horses-or-humans-dataset"
28	}
29	"""
30	
31	_TRAIN_URL = "https://storage.googleapis.com/download.tensorflow.org/data/horse-or-human.zip"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/horses_or_humans.py:31
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
30	
31	_TRAIN_URL = "https://storage.googleapis.com/download.tensorflow.org/data/horse-or-human.zip"
32	_TEST_URL = "https://storage.googleapis.com/download.tensorflow.org/data/validation-horse-or-human.zip"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/horses_or_humans.py:32
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
31	_TRAIN_URL = "https://storage.googleapis.com/download.tensorflow.org/data/horse-or-human.zip"
32	_TEST_URL = "https://storage.googleapis.com/download.tensorflow.org/data/validation-horse-or-human.zip"
33	
34	_IMAGE_SIZE = 300

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/horses_or_humans.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	  RELEASE_NOTES = {
45	      "3.0.0": "New split API (https://tensorflow.org/datasets/splits)",
46	  }
47	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/horses_or_humans.py:57
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
56	        supervised_keys=("image", "label"),
57	        homepage="http://laurencemoroney.com/horses-or-humans-dataset",
58	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/i_naturalist2018/i_naturalist2018.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	"""
29	_CITATION = r"""\
30	@misc{inaturalist18,
31	    Howpublished = {~\url{https://github.com/visipedia/inat_comp/tree/master/2018}},
32	    Title = {{iNaturalist} 2018 competition dataset.},
33	    Year = {2018},
34	    key = {{iNaturalist} 2018 competition dataset},
35	    }
36	"""
37	_URL = "https://ml-inat-competition-datasets.s3.amazonaws.com/2018/"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/i_naturalist2018/i_naturalist2018.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	"""
37	_URL = "https://ml-inat-competition-datasets.s3.amazonaws.com/2018/"
38	
39	
40	class INaturalist2018(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/i_naturalist2018/i_naturalist2018.py:76
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
75	        supervised_keys=("image", "label"),
76	        homepage="https://github.com/visipedia/inat_comp/tree/master/2018",
77	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/i_naturalist2021/i_naturalist2021.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	
35	_CITATION = r"""\
36	@misc{inaturalist21,
37	    Howpublished = {~\url{https://github.com/visipedia/inat_comp/tree/master/2021}},
38	    Title = {{iNaturalist} 2021 competition dataset.},
39	    Year = {2021},
40	    key = {{iNaturalist} 2021 competition dataset},
41	    }
42	"""
43	
44	_URL = 'https://ml-inat-competition-datasets.s3.amazonaws.com/2021'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/i_naturalist2021/i_naturalist2021.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	
44	_URL = 'https://ml-inat-competition-datasets.s3.amazonaws.com/2021'
45	_HOMEPAGE = 'https://github.com/visipedia/inat_comp/tree/master/2021'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/i_naturalist2021/i_naturalist2021.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	_URL = 'https://ml-inat-competition-datasets.s3.amazonaws.com/2021'
45	_HOMEPAGE = 'https://github.com/visipedia/inat_comp/tree/master/2021'
46	_SPLIT_FILENAMES = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/inaturalist.py:48
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
47	"""
48	_URL = "http://www.vision.caltech.edu/~gvanhorn/datasets/inaturalist/fgvc4_competition/"
49	
50	
51	class INaturalist2017(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/inaturalist.py:81
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
80	        supervised_keys=("image", "label"),
81	        homepage="https://github.com/visipedia/inat_comp/tree/master/2017",
82	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/mnist.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	# CVDF mirror of http://yann.lecun.com/exdb/mnist/
27	_MNIST_URL = "https://storage.googleapis.com/cvdf-datasets/mnist/"
28	_MNIST_TRAIN_DATA_FILENAME = "train-images-idx3-ubyte.gz"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/mnist.py:38
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
37	
38	_MNIST_CITATION = """\
39	@article{lecun2010mnist,
40	  title={MNIST handwritten digit database},
41	  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
42	  journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
43	  volume={2},
44	  year={2010}
45	}
46	"""
47	
48	_FASHION_MNIST_CITATION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/mnist.py:48
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
47	
48	_FASHION_MNIST_CITATION = """\
49	@article{DBLP:journals/corr/abs-1708-07747,
50	  author    = {Han Xiao and
51	               Kashif Rasul and
52	               Roland Vollgraf},
53	  title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning
54	               Algorithms},
55	  journal   = {CoRR},
56	  volume    = {abs/1708.07747},
57	  year      = {2017},
58	  url       = {http://arxiv.org/abs/1708.07747},
59	  archivePrefix = {arXiv},
60	  eprint    = {1708.07747},
61	  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
62	  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-07747},
63	  bibsource = {dblp computer science bibliography, https://dblp.org}
64	}
65	"""
66	
67	_K_MNIST_CITATION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/mnist.py:106
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
105	        supervised_keys=("image", "label"),
106	        homepage="http://yann.lecun.com/exdb/mnist/",
107	        citation=_MNIST_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/mnist.py:166
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
165	class FashionMNIST(MNIST):
166	  URL = "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/"
167	
168	  # TODO(afrozm): Try to inherit from MNIST's _info and mutate things as needed.
169	  def _info(self):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/mnist.py:197
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
196	        supervised_keys=("image", "label"),
197	        homepage="https://github.com/zalandoresearch/fashion-mnist",
198	        citation=_FASHION_MNIST_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/mnist.py:203
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
202	class KMNIST(MNIST):
203	  URL = "http://codh.rois.ac.jp/kmnist/dataset/kmnist/"
204	
205	  def _info(self):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/mnist.py:234
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
233	        supervised_keys=("image", "label"),
234	        homepage="http://codh.rois.ac.jp/kmnist/index.html.en",
235	        citation=_K_MNIST_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/mnist.py:261
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
260	
261	  URL = "https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip"
262	  VERSION = tfds.core.Version("3.1.0")

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/mnist.py:264
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
263	  RELEASE_NOTES = {
264	      "3.0.0": "New split API (https://tensorflow.org/datasets/splits)",
265	      "3.1.0": "Updated broken download URL",
266	  }
267	  BUILDER_CONFIGS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/mnist.py:333
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
332	        homepage=(
333	            "https://www.nist.gov/itl/products-and-services/emnist-dataset"
334	        ),
335	        citation=_EMNIST_CITATION,
336	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/mnist.py:381
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
380	  with tf.io.gfile.GFile(image_filepath, "rb") as f:
381	    f.read(16)  # header
382	    buf = f.read(_MNIST_IMAGE_SIZE * _MNIST_IMAGE_SIZE * num_images)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/mnist.py:382
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
381	    f.read(16)  # header
382	    buf = f.read(_MNIST_IMAGE_SIZE * _MNIST_IMAGE_SIZE * num_images)
383	    data = np.frombuffer(

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/mnist.py:392
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
391	  with tf.io.gfile.GFile(labels_filepath, "rb") as f:
392	    f.read(8)  # header
393	    buf = f.read(num_labels)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/mnist.py:393
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
392	    f.read(8)  # header
393	    buf = f.read(num_labels)
394	    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/mnist_corrupted.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	
45	_DOWNLOAD_URL = 'https://zenodo.org/record/3239543/files/mnist_c.zip'
46	_CORRUPTIONS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/mnist_corrupted.py:131
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
130	        supervised_keys=('image', 'label'),
131	        homepage='https://github.com/google-research/mnist-c',
132	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/omniglot.py:42
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
41	
42	_BASE_URL = "https://github.com/brendenlake/omniglot/"
43	_DL_URL = _BASE_URL + "raw/master/python/"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/omniglot.py:60
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
59	  RELEASE_NOTES = {
60	      "3.0.0": "New split API (https://tensorflow.org/datasets/splits)",
61	  }
62	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/uc_merced.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	
41	_URL = "http://weegee.vision.ucmerced.edu/datasets/landuse.html"
42	
43	_LABELS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/uc_merced.py:67
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
66	
67	_ZIP_URL = "http://weegee.vision.ucmerced.edu/datasets/UCMerced_LandUse.zip"
68	_ZIP_SUBDIR = "UCMerced_LandUse/Images"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/uc_merced.py:76
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
75	  RELEASE_NOTES = {
76	      "2.0.0": "New split API (https://tensorflow.org/datasets/splits)",
77	  }
78	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/visual_domain_decathlon.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	
37	_CITATION = """\
38	@ONLINE{hakanbilensylvestrerebuffitomasjakab2017,
39	    author = "Hakan Bilen, Sylvestre Rebuffi, Tomas Jakab",
40	    title  = "Visual Domain Decathlon",
41	    year   = "2017",
42	    url    = "https://www.robots.ox.ac.uk/~vgg/decathlon/"
43	}
44	"""
45	
46	_URL_PREFIX_VGG = 'http://www.robots.ox.ac.uk/~vgg/share/'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/visual_domain_decathlon.py:46
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
45	
46	_URL_PREFIX_VGG = 'http://www.robots.ox.ac.uk/~vgg/share/'
47	_URL_PREFIX_IMAGENET = 'http://www.image-net.org/image/decathlon/'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/visual_domain_decathlon.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	_URL_PREFIX_VGG = 'http://www.robots.ox.ac.uk/~vgg/share/'
47	_URL_PREFIX_IMAGENET = 'http://www.image-net.org/image/decathlon/'
48	_CONFIG_DESCRIPTION_PATTERN = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/image_classification/visual_domain_decathlon.py:105
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
104	        supervised_keys=('image', 'label'),
105	        homepage='https://www.robots.ox.ac.uk/~vgg/decathlon/',
106	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/nearest_neighbors/deep1b/deep1b.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	
44	_URL = 'http://ann-benchmarks.com/deep-image-96-angular.hdf5'
45	
46	
47	class Deep1b(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/nearest_neighbors/deep1b/deep1b.py:80
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
79	        }),
80	        homepage='http://sites.skoltech.ru/compvision/noimi/',
81	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/nearest_neighbors/glove_100_angular/glove_100_angular.py:34
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
33	
34	_CITATION = """
35	@inproceedings{pennington2014glove,
36	  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
37	  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
38	  title = {GloVe: Global Vectors for Word Representation},
39	  year = {2014},
40	  pages = {1532--1543},
41	  url = {http://www.aclweb.org/anthology/D14-1162},
42	}
43	"""
44	
45	_URL = 'http://ann-benchmarks.com/glove-100-angular.hdf5'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/nearest_neighbors/glove_100_angular/glove_100_angular.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	
45	_URL = 'http://ann-benchmarks.com/glove-100-angular.hdf5'
46	
47	
48	class Glove100Angular(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/nearest_neighbors/glove_100_angular/glove_100_angular.py:81
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
80	        }),
81	        homepage='https://nlp.stanford.edu/projects/glove/',
82	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/object_detection/coco.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	
30	_CITATION = """\
31	@article{DBLP:journals/corr/LinMBHPRDZ14,
32	  author    = {Tsung{-}Yi Lin and
33	               Michael Maire and
34	               Serge J. Belongie and
35	               Lubomir D. Bourdev and
36	               Ross B. Girshick and
37	               James Hays and
38	               Pietro Perona and
39	               Deva Ramanan and
40	               Piotr Doll{\'{a}}r and
41	               C. Lawrence Zitnick},
42	  title     = {Microsoft {COCO:} Common Objects in Context},
43	  journal   = {CoRR},
44	  volume    = {abs/1405.0312},
45	  year      = {2014},
46	  url       = {http://arxiv.org/abs/1405.0312},
47	  archivePrefix = {arXiv},
48	  eprint    = {1405.0312},
49	  timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},
50	  biburl    = {https://dblp.org/rec/bib/journals/corr/LinMBHPRDZ14},
51	  bibsource = {dblp computer science bibliography, https://dblp.org}
52	}
53	"""
54	
55	_DESCRIPTION = """COCO is a large-scale object detection, segmentation, and

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/object_detection/coco.py:227
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
226	        features=tfds.features.FeaturesDict(features),
227	        homepage='http://cocodataset.org/#home',
228	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/object_detection/coco.py:244
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
243	    # once.
244	    root_url = 'http://images.cocodataset.org/'
245	    extracted_paths = dl_manager.download_and_extract(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/object_detection/coco_captions.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	
30	_EXTRA_CITATION = """\
31	@inproceedings{DBLP:conf/cvpr/KarpathyL15,
32	  author    = {Andrej Karpathy and
33	               Fei{-}Fei Li},
34	  title     = {Deep visual-semantic alignments for generating image
35	               descriptions},
36	  booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition,
37	               {CVPR} 2015, Boston, MA, USA, June 7-12, 2015},
38	  pages     = {3128--3137},
39	  publisher = {{IEEE} Computer Society},
40	  year      = {2015},
41	  url       = {https://doi.org/10.1109/CVPR.2015.7298932},
42	  doi       = {10.1109/CVPR.2015.7298932},
43	  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
44	  biburl    = {https://dblp.org/rec/conf/cvpr/KarpathyL15.bib},
45	  bibsource = {dblp computer science bibliography, https://dblp.org}
46	}
47	"""
48	
49	_DESCRIPTION = """COCO is a large-scale object detection, segmentation, and

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/object_detection/coco_captions.py:93
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
92	    urls['karpathy_and_li_splits'] = (
93	        'https://cs.stanford.edu/people/karpathy/deepimagesent/'
94	        'caption_datasets.zip'
95	    )
96	    extracted_paths = dl_manager.download_and_extract(urls)
97	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/object_detection/open_images_challenge2019_beam.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
42	      if filename.endswith(".jpg"):
43	        yield filename, file.read()
44	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/object_detection/voc.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_VOC_CITATION = """\
28	@misc{{pascal-voc-{year},
29		author = "Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.",
30		title = "The {{PASCAL}} {{V}}isual {{O}}bject {{C}}lasses {{C}}hallenge {year} {{(VOC{year})}} {{R}}esults",
31		howpublished = "http://www.pascal-network.org/challenges/VOC/voc{year}/workshop/index.html"}}
32	"""
33	
34	_VOC_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/object_detection/voc.py:53
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
52	"""
53	_VOC_URL = "http://host.robots.ox.ac.uk/pascal/VOC/voc{year}/"
54	# Original site, it is down very often.
55	# _VOC_DATA_URL = "http://host.robots.ox.ac.uk/pascal/VOC/voc{year}/"
56	# Data mirror:
57	_VOC_DATA_URL = "http://pjreddie.com/media/files/"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/object_detection/voc.py:57
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
56	# Data mirror:
57	_VOC_DATA_URL = "http://pjreddie.com/media/files/"
58	_VOC_LABELS = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/object_detection/waymo_open_dataset.py:38
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
37	
38	_DESCRIPTION = """\
39	The Waymo Open Dataset is comprised of high resolution sensor data
40	collected by Waymo self-driving cars in a wide variety of conditions.
41	This data is licensed for non-commercial use.
42	
43	WARNING: this dataset requires additional authorization and registration.
44	Please look at tfds documentation for accessing GCS, and
45	afterwards, please register via https://waymo.com/open/licensing/
46	"""
47	
48	_GCS_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/object_detection/waymo_open_dataset.py:59
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
58	
59	_HOMEPAGE_URL = "http://www.waymo.com/open/"
60	_OBJECT_LABELS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/object_detection/wider_face.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_PROJECT_URL = 'http://shuoyang1213.me/WIDERFACE/'
28	
29	_WIDER_TRAIN_URL = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/object_detection/wider_face.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	_WIDER_TRAIN_URL = (
30	    'https://drive.google.com/uc?export=download&'
31	    'id=15hGDLhsx8bLgLcIRD5DhYt5iBxnjNF1M'
32	)
33	
34	_WIDER_VAL_URL = (
35	    'https://drive.google.com/uc?export=download&'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/object_detection/wider_face.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	_WIDER_VAL_URL = (
35	    'https://drive.google.com/uc?export=download&'
36	    'id=1GUCogbp16PMGa39thoMMeWxp7Rp5oM8Q'
37	)
38	
39	_WIDER_TEST_URL = (
40	    'https://drive.google.com/uc?export=download&'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/object_detection/wider_face.py:40
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
39	_WIDER_TEST_URL = (
40	    'https://drive.google.com/uc?export=download&'
41	    'id=1HIfDbVEWKmsYKJZm4lchTBDLW5N7dY5T'
42	)
43	
44	_WIDER_ANNOT_URL = (
45	    'https://drive.google.com/uc?export=download&'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/object_detection/wider_face.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	_WIDER_ANNOT_URL = (
45	    'https://drive.google.com/uc?export=download&'
46	    'id=1sAl2oml7hK6aZRdgRjqQJsjV5CEr7nl4'
47	)
48	
49	_CITATION = """
50	@inproceedings{yang2016wider,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/proto/build_tf_proto.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	  """Downloads the proto from Github and formats it."""
50	  url = f'https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/core/example/{tf_proto.value}.proto'
51	  request = requests.get(url)

--------------------------------------------------
>> Issue: [B820:get] requests.get
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/proto/build_tf_proto.py:51
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b820_get.html
50	  url = f'https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/core/example/{tf_proto.value}.proto'
51	  request = requests.get(url)
52	  content = request.content

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/coqa/coqa.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	
41	_TRAIN_DATA_URL = "https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json"
42	_DEV_DATA_URL = "https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/coqa/coqa.py:42
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
41	_TRAIN_DATA_URL = "https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json"
42	_DEV_DATA_URL = "https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json"
43	_HOMEPAGE_URL = "https://stanfordnlp.github.io/coqa/"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/coqa/coqa.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	_DEV_DATA_URL = "https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json"
43	_HOMEPAGE_URL = "https://stanfordnlp.github.io/coqa/"
44	
45	
46	class Coqa(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/cosmos_qa.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_CITATION = """\
25	@inproceedings{huang-etal-2019-cosmos,
26	    title = "Cosmos {QA}: Machine Reading Comprehension with Contextual Commonsense Reasoning",
27	    author = "Huang, Lifu  and
28	      Le Bras, Ronan  and
29	      Bhagavatula, Chandra  and
30	      Choi, Yejin",
31	    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
32	    year = "2019",
33	    url = "https://www.aclweb.org/anthology/D19-1243"
34	}
35	"""
36	
37	_DESCRIPTION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/cosmos_qa.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	_SPLIT_DOWNLOAD_URL = {
47	    'train': 'https://raw.githubusercontent.com/wilburOne/cosmosqa/master/data/train.csv',
48	    'validation': 'https://raw.githubusercontent.com/wilburOne/cosmosqa/master/data/valid.csv',
49	    'test': 'https://raw.githubusercontent.com/wilburOne/cosmosqa/master/data/test.jsonl',
50	}
51	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/cosmos_qa.py:48
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
47	    'train': 'https://raw.githubusercontent.com/wilburOne/cosmosqa/master/data/train.csv',
48	    'validation': 'https://raw.githubusercontent.com/wilburOne/cosmosqa/master/data/valid.csv',
49	    'test': 'https://raw.githubusercontent.com/wilburOne/cosmosqa/master/data/test.jsonl',
50	}
51	
52	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/cosmos_qa.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
48	    'validation': 'https://raw.githubusercontent.com/wilburOne/cosmosqa/master/data/valid.csv',
49	    'test': 'https://raw.githubusercontent.com/wilburOne/cosmosqa/master/data/test.jsonl',
50	}
51	
52	
53	class CosmosQA(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/cosmos_qa.py:75
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
74	        supervised_keys=None,
75	        homepage='https://wilburone.github.io/cosmos/',
76	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/tydi_qa.py:34
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
33	
34	_DESCRIPTION = """\
35	TyDi QA is a question answering dataset covering 11 typologically diverse \
36	languages with 204K question-answer pairs. The languages of TyDi QA are \
37	diverse with regard to their typology -- the set of linguistic features that \
38	each language expresses -- such that we expect models performing well on this \
39	set to generalize across a large number of the languages in the world. It \
40	contains language phenomena that would not be found in English-only corpora. \
41	To provide a realistic information-seeking task and avoid priming effects, \
42	questions are written by people who want to know the answer, but don't know \
43	the answer yet, (unlike SQuAD and its descendents) and the data is collected \
44	directly in each language without the use of translation (unlike MLQA and \
45	XQuAD).
46	
47	IMPORTANT:  Please choose your training split carefully.
48	
49	Training splits:
50	
51	'train': This is the GoldP task from the original TyDi QA paper \
52	[https://arxiv.org/abs/2003.05002] that has original-language labeled \
53	training data.
54	
55	'translate-train-*': These splits are the automatic translations from English \
56	to each target language used in the translate-train baselines in the XTREME \
57	paper [https://arxiv.org/abs/2003.11080]. This purposefully ignores the \
58	non-English TyDiQA-GoldP training data to simulate the transfer learning \
59	scenario where original-language data is not available and system builders \
60	must rely on labeled English data plus existing machine translation systems.
61	
62	Typically, you should use EITHER the train or translate-train split, but not both.
63	"""
64	
65	LANGUAGES = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/tydi_qa.py:78
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
77	_GOLD_URL_PREFIX = (
78	    "https://storage.googleapis.com/tydiqa/v1.1/tydiqa-goldp-v1.1-"
79	)
80	_GOLD_TRANSLATE_URL_FORMAT = "https://storage.googleapis.com/xtreme_translations/TyDiQA-GoldP/translate-train/tydiqa.translate.train.en-{lang_iso}.json"
81	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/tydi_qa.py:80
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
79	)
80	_GOLD_TRANSLATE_URL_FORMAT = "https://storage.googleapis.com/xtreme_translations/TyDiQA-GoldP/translate-train/tydiqa.translate.train.en-{lang_iso}.json"
81	
82	
83	class TydiQAConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/tydi_qa.py:94
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
93	          description=(
94	              "Gold passage (GoldP) task"
95	              " (https://github.com/google-research-datasets/tydiqa/tree/master/gold_passage_baseline)."
96	          ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/tydi_qa.py:117
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
116	        supervised_keys=None,
117	        homepage="https://github.com/google-research-datasets/tydiqa",
118	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/web_questions.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_CITATION = """
25	@inproceedings{berant-etal-2013-semantic,
26	    title = "Semantic Parsing on {F}reebase from Question-Answer Pairs",
27	    author = "Berant, Jonathan  and
28	      Chou, Andrew  and
29	      Frostig, Roy  and
30	      Liang, Percy",
31	    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
32	    month = oct,
33	    year = "2013",
34	    address = "Seattle, Washington, USA",
35	    publisher = "Association for Computational Linguistics",
36	    url = "https://www.aclweb.org/anthology/D13-1160",
37	    pages = "1533--1544",
38	}
39	"""
40	_SPLIT_DOWNLOAD_URL = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/web_questions.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	_SPLIT_DOWNLOAD_URL = {
41	    'train': 'https://worksheets.codalab.org/rest/bundles/0x4a763f8cde224c2da592b75f29e2f5c2/contents/blob/',
42	    'test': 'https://worksheets.codalab.org/rest/bundles/0xe7bac352fce7448c9ef238fb0a297ec2/contents/blob/',
43	}
44	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/web_questions.py:42
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
41	    'train': 'https://worksheets.codalab.org/rest/bundles/0x4a763f8cde224c2da592b75f29e2f5c2/contents/blob/',
42	    'test': 'https://worksheets.codalab.org/rest/bundles/0xe7bac352fce7448c9ef238fb0a297ec2/contents/blob/',
43	}
44	
45	_DESCRIPTION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/web_questions.py:68
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
67	        supervised_keys=None,
68	        homepage='https://worksheets.codalab.org/worksheets/0xba659fe363cb46e7a505c5b6a774dc8a',
69	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/xquad.py:33
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
32	
33	_DESCRIPTION = """\
34	XQuAD (Cross-lingual Question Answering Dataset) is a benchmark dataset for \
35	evaluating cross-lingual question answering performance. The dataset consists \
36	of a subset of 240 paragraphs and 1190 question-answer pairs from the \
37	development set of SQuAD v1.1 (Rajpurkar et al., 2016) together with their \
38	professional translations into ten languages: Spanish, German, Greek, Russian, \
39	Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi. Consequently, the \
40	dataset is entirely parallel across 11 languages. \
41	\
42	To run XQuAD in the default zero-shot setting, use the SQuAD v1.1 training and \
43	validation data here: https://www.tensorflow.org/datasets/catalog/squad
44	
45	We also include "translate-train", "translate-dev", and "translate-test" \
46	splits for each non-English language from XTREME (Hu et al., 2020). These can \
47	be used to run XQuAD in the "translate-train" or "translate-test" settings.
48	"""
49	
50	LANGUAGES = ["ar", "de", "el", "en", "es", "hi", "ru", "th", "tr", "vi", "zh"]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/xquad.py:52
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
51	
52	_URL_FORMAT = "https://github.com/deepmind/xquad/raw/master/xquad.{lang}.json"
53	_XTREME_SQUAD_URL_FORMAT = "https://storage.googleapis.com/xtreme_translations/SQuAD/translate-{split}/squad.translate.{split}.en-{lang}.json"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/xquad.py:53
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
52	_URL_FORMAT = "https://github.com/deepmind/xquad/raw/master/xquad.{lang}.json"
53	_XTREME_SQUAD_URL_FORMAT = "https://storage.googleapis.com/xtreme_translations/SQuAD/translate-{split}/squad.translate.{split}.en-{lang}.json"
54	_XTREME_XQUAD_URL_FORMAT = "https://storage.googleapis.com/xtreme_translations/XQuAD/translate-test/xquad.translate.test.{lang}-en.json"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/xquad.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	_XTREME_SQUAD_URL_FORMAT = "https://storage.googleapis.com/xtreme_translations/SQuAD/translate-{split}/squad.translate.{split}.en-{lang}.json"
54	_XTREME_XQUAD_URL_FORMAT = "https://storage.googleapis.com/xtreme_translations/XQuAD/translate-test/xquad.translate.test.{lang}-en.json"
55	
56	
57	class XquadConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/question_answering/xquad.py:111
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
110	        supervised_keys=None,
111	        homepage="https://github.com/deepmind/xquad",
112	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/ranking/istella/istella.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	
55	_CITATION = """
56	@article{10.1145/2987380,
57	  author = {Dato, Domenico and Lucchese, Claudio and Nardini, Franco Maria and Orlando, Salvatore and Perego, Raffaele and Tonellotto, Nicola and Venturini, Rossano},
58	  title = {Fast Ranking with Additive Ensembles of Oblivious and Non-Oblivious Regression Trees},
59	  year = {2016},
60	  publisher = {ACM},
61	  address = {New York, NY, USA},
62	  volume = {35},
63	  number = {2},
64	  issn = {1046-8188},
65	  url = {https://doi.org/10.1145/2987380},
66	  doi = {10.1145/2987380},
67	  journal = {ACM Transactions on Information Systems},
68	  articleno = {15},
69	  numpages = {31},
70	}
71	"""
72	
73	_URLS = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/ranking/istella/istella.py:74
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
73	_URLS = {
74	    "main": "http://library.istella.it/dataset/istella-letor.tar.gz",
75	    "s": "http://library.istella.it/dataset/istella-s-letor.tar.gz",
76	    "x": (
77	        "http://quickrank.isti.cnr.it/istella-datasets-mirror/istella-X.tar.gz"
78	    ),
79	}

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/ranking/istella/istella.py:75
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
74	    "main": "http://library.istella.it/dataset/istella-letor.tar.gz",
75	    "s": "http://library.istella.it/dataset/istella-s-letor.tar.gz",
76	    "x": (
77	        "http://quickrank.isti.cnr.it/istella-datasets-mirror/istella-X.tar.gz"
78	    ),
79	}
80	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/ranking/istella/istella.py:77
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
76	    "x": (
77	        "http://quickrank.isti.cnr.it/istella-datasets-mirror/istella-X.tar.gz"
78	    ),
79	}
80	
81	_FEATURE_NAMES = {n: f"feature_{n}" for n in range(1, 221)}
82	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/ranking/istella/istella.py:133
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
132	        features=tfds.features.FeaturesDict(features),
133	        homepage="http://quickrank.isti.cnr.it/istella-dataset/",
134	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/ranking/mslr_web/mslr_web.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
48	
49	_CITATION = """
50	@article{DBLP:journals/corr/QinL13,
51	  author    = {Tao Qin and Tie{-}Yan Liu},
52	  title     = {Introducing {LETOR} 4.0 Datasets},
53	  journal   = {CoRR},
54	  volume    = {abs/1306.2597},
55	  year      = {2013},
56	  url       = {http://arxiv.org/abs/1306.2597},
57	  timestamp = {Mon, 01 Jul 2013 20:31:25 +0200},
58	  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/QinL13},
59	  bibsource = {dblp computer science bibliography, http://dblp.org}
60	}
61	"""
62	
63	_URLS = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/ranking/mslr_web/mslr_web.py:64
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
63	_URLS = {
64	    "10k": "https://api.onedrive.com/v1.0/shares/s!AtsMfWUz5l8nbOIoJ6Ks0bEMp78/root/content",
65	    "30k": "https://api.onedrive.com/v1.0/shares/s!AtsMfWUz5l8nbXGPBlwD1rnFdBY/root/content",
66	}
67	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/ranking/mslr_web/mslr_web.py:65
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
64	    "10k": "https://api.onedrive.com/v1.0/shares/s!AtsMfWUz5l8nbOIoJ6Ks0bEMp78/root/content",
65	    "30k": "https://api.onedrive.com/v1.0/shares/s!AtsMfWUz5l8nbXGPBlwD1rnFdBY/root/content",
66	}
67	
68	_FEATURE_NAMES = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/ranking/mslr_web/mslr_web.py:263
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
262	        features=tfds.features.FeaturesDict(features),
263	        homepage="https://www.microsoft.com/en-us/research/project/mslr/",
264	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/ranking/yahoo_ltrc/yahoo_ltrc.py:91
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
90	
91	  MANUAL_DOWNLOAD_INSTRUCTIONS = """
92	  Request access for the C14 Yahoo Learning To Rank Challenge dataset on
93	  https://research.yahoo.com/datasets. Extract the downloaded `dataset.tgz` file
94	  and place the `ltrc_yahoo.tar.bz2` file in `manual_dir/`.
95	  """
96	
97	  def _info(self) -> tfds.core.DatasetInfo:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/ranking/yahoo_ltrc/yahoo_ltrc.py:119
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
118	        features=tfds.features.FeaturesDict(features),
119	        homepage="https://research.yahoo.com/datasets",
120	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/recommendation/criteo/criteo.py:132
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
131	        ),
132	        homepage='https://ailab.criteo.com/criteo-uplift-prediction-dataset/',
133	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/recommendation/criteo/criteo.py:139
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
138	    path = dl_manager.download_and_extract(
139	        'http://go.criteo.net/criteo-research-uplift-v2.1.csv.gz'
140	    )
141	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/recommendation/hillstrom/hillstrom.py:88
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
87	        ),
88	        homepage='https://blog.minethatdata.com/2008/03/minethatdata-e-mail-analytics-and-data.html',
89	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/recommendation/hillstrom/hillstrom.py:95
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
94	    path = dl_manager.download_and_extract(
95	        'http://www.minethatdata.com/Kevin_Hillstrom_MineThatData_E-MailAnalytics_DataMiningChallenge_2008.03.20.csv'
96	    )
97	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rl_unplugged/atari_utils.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_ATARI_DESCRIPTION = """
28	We are releasing a large and diverse dataset of gameplay following the protocol
29	described by [Agarwal et al., 2020](https://arxiv.org/abs/1907.04543), which can
30	be used to evaluate several discrete offline RL algorithms. The dataset is
31	generated by running an online DQN agent and recording transitions from its
32	replay during training with sticky actions
33	[Machado et al., 2018](https://arxiv.org/abs/1709.06009). As stated in
34	[Agarwal et al., 2020](https://arxiv.org/abs/1907.04543), for each game we use
35	data from five runs with 50 million transitions each. We release datasets for 46
36	Atari games. For details on how the dataset was generated, please refer to the
37	paper. Please see [this note](https://github.com/google-research/batch_rl#important-notes-on-atari-rom-versions)
38	about the ROM versions used to generate the datasets.
39	
40	Atari is a standard RL benchmark. We recommend you to try offline RL methods on
41	Atari if you are interested in comparing your approach to other state of the art
42	offline RL methods with discrete actions.
43	
44	The reward of each step is clipped (obtained with [-1, 1] clipping) and the 
45	episode includes the sum of the clipped reward per episode.
46	"""
47	
48	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rl_unplugged/dmlab_dataset.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_DMLAB_DESCRIPTION = """
29	DeepMind Lab dataset has several levels from the challenging, partially
30	observable [Deepmind Lab suite](https://github.com/deepmind/lab). DeepMind Lab
31	dataset is collected by training distributed R2D2 by [Kapturowski et al., 2018]
32	(https://openreview.net/forum?id=r1lyTjAqYX) agents from scratch on individual
33	tasks. We recorded the experience across all actors during entire training runs
34	a few times for every task. The details of the dataset generation process is
35	described in [Gulcehre et al., 2021](https://arxiv.org/abs/2103.09575).
36	
37	We release datasets for five different DeepMind Lab levels: `seekavoid_arena_01`,
38	`explore_rewards_few`, `explore_rewards_many`, `rooms_watermaze`,
39	`rooms_select_nonmatching_object`. We also release the snapshot datasets for
40	`seekavoid_arena_01` level that we generated the datasets from a trained R2D2
41	snapshot with different levels of epsilons for the epsilon-greedy algorithm
42	when evaluating the agent in the environment.
43	
44	DeepMind Lab dataset is fairly large-scale. We recommend you to try it if you
45	are interested in large-scale offline RL models with memory.
46	"""
47	
48	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rl_unplugged/dmlab_dataset.py:48
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
47	
48	_CITATION = """
49	@article{gulcehre2021rbve,
50	    title={Regularized Behavior Value Estimation},
51	    author={{\\c{C}}aglar G{\\"{u}}l{\\c{c}}ehre and
52	               Sergio G{\\'{o}}mez Colmenarejo and
53	               Ziyu Wang and
54	               Jakub Sygnowski and
55	               Thomas Paine and
56	               Konrad Zolna and
57	               Yutian Chen and
58	               Matthew W. Hoffman and
59	               Razvan Pascanu and
60	               Nando de Freitas},
61	    year={2021},
62	    journal   = {CoRR},
63	    url       = {https://arxiv.org/abs/2103.09575},
64	    eprint={2103.09575},
65	    archivePrefix={arXiv},
66	}
67	"""
68	
69	_PIXELS_HEIGHT = 72

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rl_unplugged/rlu_atari_checkpoints_ordered/rlu_atari_checkpoints_ordered.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_EXTRA_DESCRIPTION = """
28	  Each of the configurations is broken into splits. Splits correspond to
29	  checkpoints of 1M steps (note that the number of episodes may difer).
30	  Checkpoints are ordered in time (so checkpoint 0 ran before checkpoint 1).
31	
32	  Episodes within each split are ordered. Check
33	  https://www.tensorflow.org/datasets/determinism if you want to ensure
34	  that you read episodes in order.
35	
36	  This dataset corresponds to the one used in the DQN replay paper.
37	  https://research.google/tools/datasets/dqn-replay/
38	"""
39	
40	
41	class RluAtariCheckpointsOrdered(rlu_common.RLUBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rl_unplugged/rlu_common.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_DESCRIPTION = """
29	RL Unplugged is suite of benchmarks for offline reinforcement learning. The RL
30	Unplugged is designed around the following considerations: to facilitate ease of
31	use, we provide the datasets with a unified API which makes it easy for the
32	practitioner to work with all data in the suite once a general pipeline has been
33	established.
34	
35	The datasets follow the [RLDS format](https://github.com/google-research/rlds)
36	to represent steps and episodes.
37	
38	"""
39	
40	
41	_HOMEPAGE = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rl_unplugged/rlu_common.py:42
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
41	_HOMEPAGE = (
42	    'https://github.com/deepmind/deepmind-research/tree/master/rl_unplugged'
43	)
44	
45	
46	def filename(prefix: str, num_shards: int, shard_id: int):
47	  return os.fspath(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rl_unplugged/rlu_control_suite/rlu_control_suite.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_CONTROL_SUITE_DESCRIPTION = """
29	DeepMind Control Suite [Tassa et al., 2018](https://arxiv.org/abs/1801.00690)
30	is a set of control tasks implemented in MuJoCo
31	[Todorov et al., 2012](https://homes.cs.washington.edu/~todorov/papers/TodorovIROS12.pdf).
32	We consider a subset of the tasks provided in the suite that cover a wide range
33	of difficulties.
34	
35	Most of the datasets in this domain are generated using D4PG. For the
36	environments Manipulator insert ball and Manipulator insert peg we use V-MPO
37	[Song et al., 2020](https://arxiv.org/abs/1909.12238) to generate the data as
38	D4PG is unable to solve these tasks. We release datasets for 9 control suite
39	tasks. For details on how the dataset was generated, please refer to the paper.
40	
41	DeepMind Control Suite is a traditional continuous action RL benchmark.
42	In particular, we recommend you test your approach in DeepMind Control Suite if
43	you are interested in comparing against other state of the art offline RL
44	methods.
45	"""
46	
47	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rl_unplugged/rlu_locomotion/rlu_locomotion.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_LOCOMOTION_DESCRIPTION = """
28	These tasks are made up of the corridor locomotion tasks involving the CMU
29	Humanoid, for which prior efforts have either used motion capture data
30	[Merel et al., 2019a](https://arxiv.org/abs/1811.09656),
31	[Merel et al., 2019b](https://arxiv.org/abs/1811.11711) or training from scratch
32	[Song et al., 2020](https://arxiv.org/abs/1909.12238). In addition, the DM
33	Locomotion repository contains a set of tasks adapted to be suited to a virtual
34	rodent [Merel et al., 2020](https://arxiv.org/abs/1911.09451). We emphasize that
35	the DM Locomotion tasks feature the combination of challenging high-DoF
36	continuous control along with perception from rich egocentric observations.
37	For details on how the dataset was generated, please refer to the paper.
38	
39	We recommend you to try offline RL methods on DeepMind Locomotion dataset, if
40	you are interested in very challenging offline RL dataset with continuous action
41	space.
42	"""
43	
44	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rl_unplugged/rlu_rwrl/rlu_rwrl.py:31
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
30	
31	_DESCRIPTION = """
32	Examples in the dataset represent SAR transitions stored when running a
33	partially online trained agent as described in https://arxiv.org/abs/1904.12901.
34	We follow the RLDS dataset format, as specified in
35	https://github.com/google-research/rlds#dataset-format.
36	
37	
38	We release 40 datasets on 8 tasks in total -- with no combined challenge and
39	easy combined challenge on the cartpole, walker, quadruped, and humanoid tasks.
40	Each task contains 5 different sizes of datasets, 1%, 5%, 20%, 40%, and 100%.
41	Note that the smaller dataset is not guaranteed to be a subset of the larger
42	ones. For details on how the dataset was generated, please refer to the paper.
43	"""
44	
45	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rl_unplugged/rlu_rwrl/rlu_rwrl.py:61
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
60	_HOMEPAGE = (
61	    'https://github.com/deepmind/deepmind-research/tree/master/rl_unplugged'
62	)
63	
64	# Env constants
65	DOMAIN_NAMES = (
66	    'cartpole',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rlds/datasets/locomotion/locomotion.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_DESCRIPTION = """
24	The datasets were created with a SAC agent trained on the environment reward of
25	MuJoCo locomotion tasks. These datasets are used in
26	[What Matters for Adversarial Imitation Learning? Orsini et al. 2021](https://arxiv.org/pdf/2106.00672.pdf).
27	
28	The datasets follow the [RLDS format](https://github.com/google-research/rlds)
29	to represent steps and episodes.s
30	"""
31	
32	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rlds/datasets/locomotion/locomotion.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	
41	_HOMEPAGE = 'https://github.com/google-research/rlds'
42	
43	
44	class Locomotion(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rlds/datasets/locomotion/locomotion.py:53
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
52	  _DATA_PATHS = {
53	      'ant_sac_1M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/ant_sac_1M_single_policy_stochastic.tar.gz',
54	      'walker2d_sac_1M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/walker2d_sac_1M_single_policy_stochastic.tar.gz',
55	      'humanoid_sac_15M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/humanoid_sac_15M_single_policy_stochastic.tar.gz',
56	      'hopper_sac_1M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/hopper_sac_1M_single_policy_stochastic.tar.gz',
57	      'halfcheetah_sac_1M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/halfcheetah_sac_1M_single_policy_stochastic.tar.gz',
58	  }
59	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rlds/datasets/locomotion/locomotion.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	      'ant_sac_1M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/ant_sac_1M_single_policy_stochastic.tar.gz',
54	      'walker2d_sac_1M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/walker2d_sac_1M_single_policy_stochastic.tar.gz',
55	      'humanoid_sac_15M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/humanoid_sac_15M_single_policy_stochastic.tar.gz',
56	      'hopper_sac_1M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/hopper_sac_1M_single_policy_stochastic.tar.gz',
57	      'halfcheetah_sac_1M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/halfcheetah_sac_1M_single_policy_stochastic.tar.gz',
58	  }
59	
60	  BUILDER_CONFIGS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rlds/datasets/locomotion/locomotion.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	      'walker2d_sac_1M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/walker2d_sac_1M_single_policy_stochastic.tar.gz',
55	      'humanoid_sac_15M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/humanoid_sac_15M_single_policy_stochastic.tar.gz',
56	      'hopper_sac_1M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/hopper_sac_1M_single_policy_stochastic.tar.gz',
57	      'halfcheetah_sac_1M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/halfcheetah_sac_1M_single_policy_stochastic.tar.gz',
58	  }
59	
60	  BUILDER_CONFIGS = [
61	      rlds_base.DatasetConfig(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rlds/datasets/locomotion/locomotion.py:56
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
55	      'humanoid_sac_15M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/humanoid_sac_15M_single_policy_stochastic.tar.gz',
56	      'hopper_sac_1M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/hopper_sac_1M_single_policy_stochastic.tar.gz',
57	      'halfcheetah_sac_1M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/halfcheetah_sac_1M_single_policy_stochastic.tar.gz',
58	  }
59	
60	  BUILDER_CONFIGS = [
61	      rlds_base.DatasetConfig(
62	          name='ant_sac_1M_single_policy_stochastic',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rlds/datasets/locomotion/locomotion.py:57
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
56	      'hopper_sac_1M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/hopper_sac_1M_single_policy_stochastic.tar.gz',
57	      'halfcheetah_sac_1M_single_policy_stochastic': 'https://storage.googleapis.com/rlds_external_data_release/halfcheetah_sac_1M_single_policy_stochastic.tar.gz',
58	  }
59	
60	  BUILDER_CONFIGS = [
61	      rlds_base.DatasetConfig(
62	          name='ant_sac_1M_single_policy_stochastic',
63	          observation_info=tfds.features.Tensor(shape=(111,), dtype=np.float32),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rlds/datasets/locomotion/locomotion_test.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	    reason=(
27	        '`envlogger` library might not be available for Python'
28	        f' {sys.version} or platform {sys.platform}; see'
29	        ' https://pypi.org/project/envlogger/#files'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rlds/datasets/robosuite_panda_pick_place_can/robosuite_panda_pick_place_can.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_DESCRIPTION = """
27	These datasets have been created with the PickPlaceCan environment of the
28	[robosuite robotic arm simulator](https://robosuite.ai/). The human datasets
29	were recorded by a single operator using
30	the [RLDS Creator](https://github.com/google-research/rlds-creator) and a
31	gamepad controller.
32	
33	The synthetic datasets have been recorded using the
34	[EnvLogger library](https://github.com/deepmind/envlogger).
35	
36	The datasets follow the [RLDS format](https://github.com/google-research/rlds)
37	to represent steps and episodes.
38	
39	Episodes consist of 400 steps. In each episode, a tag is
40	added when the task is completed, this tag is stored as part of the custom step
41	metadata.
42	
43	Note that, due to the EnvLogger dependency, generation of this dataset is
44	currently supported on Linux environments only.
45	"""
46	
47	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rlds/datasets/robosuite_panda_pick_place_can/robosuite_panda_pick_place_can.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
57	
58	_HOMEPAGE = 'https://github.com/google-research/rlds'
59	
60	
61	class RobosuitePandaPickPlaceCan(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rlds/datasets/robosuite_panda_pick_place_can/robosuite_panda_pick_place_can.py:70
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
69	  _DATA_PATHS = {
70	      'human_dc29b40a': 'https://storage.googleapis.com/rlds_external_data_release/rlds_robosuite_panda_pick_place_can_human_state_only_dc29b40a.tar.gz',
71	      'human_images_dc29b40a': 'https://storage.googleapis.com/rlds_external_data_release/rlds_robosuite_panda_pick_place_can_human_dc29b40a.tar.gz',
72	      'synthetic_stochastic_sac_afe13968': 'https://storage.googleapis.com/rlds_external_data_release/rlds_robosuite_panda_pick_place_can_synthetic_stochastic_sac_afe13968.tar.gz',
73	  }
74	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rlds/datasets/robosuite_panda_pick_place_can/robosuite_panda_pick_place_can.py:71
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
70	      'human_dc29b40a': 'https://storage.googleapis.com/rlds_external_data_release/rlds_robosuite_panda_pick_place_can_human_state_only_dc29b40a.tar.gz',
71	      'human_images_dc29b40a': 'https://storage.googleapis.com/rlds_external_data_release/rlds_robosuite_panda_pick_place_can_human_dc29b40a.tar.gz',
72	      'synthetic_stochastic_sac_afe13968': 'https://storage.googleapis.com/rlds_external_data_release/rlds_robosuite_panda_pick_place_can_synthetic_stochastic_sac_afe13968.tar.gz',
73	  }
74	
75	  # pytype: disable=wrong-keyword-args

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rlds/datasets/robosuite_panda_pick_place_can/robosuite_panda_pick_place_can.py:72
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
71	      'human_images_dc29b40a': 'https://storage.googleapis.com/rlds_external_data_release/rlds_robosuite_panda_pick_place_can_human_dc29b40a.tar.gz',
72	      'synthetic_stochastic_sac_afe13968': 'https://storage.googleapis.com/rlds_external_data_release/rlds_robosuite_panda_pick_place_can_synthetic_stochastic_sac_afe13968.tar.gz',
73	  }
74	
75	  # pytype: disable=wrong-keyword-args
76	  BUILDER_CONFIGS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/rlds/datasets/robosuite_panda_pick_place_can/robosuite_panda_pick_place_can_test.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	    reason=(
27	        '`envlogger` library might not be available for Python'
28	        f' {sys.version} or platform {sys.platform}; see'
29	        ' https://pypi.org/project/envlogger/#files'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robomimic/dataset_utils.py:230
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
229	        supervised_keys=None,
230	        homepage='https://arise-initiative.github.io/robomimic-web/',
231	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robomimic/dataset_utils.py:328
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
327	    filepath = (
328	        'http://downloads.cs.stanford.edu/downloads/rt_benchmark/'
329	        f'{self.builder_config.task}/{self.builder_config.dataset}/'
330	        f'{self.builder_config.filename}{ext}.hdf5'
331	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/asimov/asimov.py:20
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
19	
20	ASIMOV_CITATION = """
21	@article{sermanet2025asimov,
22	  author    = {Pierre Sermanet and Anirudha Majumdar and Alex Irpan and Dmitry Kalashnikov and Vikas Sindhwani},
23	  title     = {Generating Robot Constitutions & Benchmarks for Semantic Safety},
24	  journal   = {arXiv preprint arXiv:2503.08663},
25	  url       = {https://arxiv.org/abs/2503.08663},
26	  year      = {2025},
27	}
28	"""
29	
30	ASIMOV_HOMEPAGE = 'https://asimov-benchmark.github.io/'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/asimov/asimov.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	
30	ASIMOV_HOMEPAGE = 'https://asimov-benchmark.github.io/'
31	
32	
33	class AsimovDilemmasAutoVal(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/mt_opt/mt_opt.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_DESCRIPTION = """
29	Datasets for the [MT-Opt paper](https://arxiv.org/abs/2104.08212).
30	"""
31	
32	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/mt_opt/mt_opt.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	        description=(
47	            'This dataset contains task episodes collected across afleet of'
48	            ' real robots. It follows the [RLDS'
49	            ' format](https://github.com/google-research/rlds)to represent'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/mt_opt/mt_opt.py:163
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
162	        supervised_keys=None,
163	        homepage='https://karolhausman.github.io/mt-opt/',
164	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	  def get_homepage(self):
37	    return 'https://rail-berkeley.github.io/bridgedata/'
38	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:65
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
64	  def get_homepage(self):
65	    return 'https://www.kaggle.com/datasets/oiermees/taco-robot'
66	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:78
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
77	  def get_citation(self):
78	    return """@software{dass2023jacoplay,
79	  author = {Dass, Shivin and Yapeter, Jullian and Zhang, Jesse and Zhang, Jiahui

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:88
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
87	  def get_homepage(self):
88	    return 'https://github.com/clvrai/clvr_jaco_play_dataset'
89	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:101
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
100	  def get_citation(self):
101	    return """@article{luo2023multistage,
102	  author    = {Jianlan Luo and Charles Xu and Xinyang Geng and Gilbert Feng and Kuan Fang and Liam Tan and Stefan Schaal and Sergey Levine},

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:110
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
109	  def get_homepage(self):
110	    return 'https://sites.google.com/view/cablerouting/home'
111	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:133
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
132	  def get_homepage(self):
133	    return 'https://roboturk.stanford.edu/dataset_real.html'
134	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:158
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
157	  def get_homepage(self):
158	    return 'https://jyopari.github.io/VINN/'
159	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:179
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
178	  def get_homepage(self):
179	    return 'https://ut-austin-rpl.github.io/VIOLA/'
180	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:192
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
191	  def get_citation(self):
192	    return """@misc{BerkeleyUR5Website,
193	  title = {Berkeley {UR5} Demonstration Dataset},

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:199
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
198	  def get_homepage(self):
199	    return 'https://sites.google.com/view/berkeley-ur5/home'
200	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:220
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
219	  def get_homepage(self):
220	    return 'https://toto-benchmark.org/'
221	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:265
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
264	  def get_homepage(self):
265	    return 'https://arxiv.org/abs/2203.06173'
266	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:288
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
287	  def get_homepage(self):
288	    return 'https://www.researchsquare.com/article/rs-3289569/v1'
289	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:313
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
312	  def get_homepage(self):
313	    return 'https://rot-robot.github.io/'
314	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:334
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
333	  def get_homepage(self):
334	    return 'https://sites.google.com/view/SACSoN-review'
335	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:357
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
356	  def get_homepage(self):
357	    return 'https://github.com/haosulab/ManiSkill2'
358	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:370
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
369	  def get_citation(self):
370	    return r"""@inproceedings{
371	    shah2023mutex,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:380
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
379	  def get_homepage(self):
380	    return 'https://ut-austin-rpl.github.io/MUTEX/'
381	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:393
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
392	  def get_citation(self):
393	    return """@inproceedings{jang2021bc,
394	title={{BC}-Z: Zero-Shot Task Generalization with Robotic Imitation Learning},

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:401
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
400	  def get_homepage(self):
401	    return 'https://www.kaggle.com/datasets/google/bc-z-robot/discussion/309201'
402	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:428
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
427	  def get_homepage(self):
428	    return 'https://ut-austin-rpl.github.io/rpl-BUDS/'
429	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:449
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
448	  def get_homepage(self):
449	    return 'https://robopil.github.io/d3fields/'
450	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:476
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
475	  def get_homepage(self):
476	    return 'https://uscresl.github.io/dmfd/'
477	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:509
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
508	  def get_homepage(self):
509	    return 'https://link.springer.com/article/10.1007/s10514-023-10129-1'
510	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:535
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
534	  def get_homepage(self):
535	    return 'https://play-fusion.github.io/'
536	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:583
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
582	  def get_homepage(self):
583	    return 'https://elib.dlr.de/193739/1/padalkar2023rlsct.pdf'
584	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:610
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
609	  def get_homepage(self):
610	    return 'https://robo-affordances.github.io/'
611	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:625
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
624	  def get_citation(self):
625	    return """@article{saytap2023,
626	  author = {Yujin Tang and Wenhao Yu and Jie Tan and Heiga Zen and Aleksandra Faust and

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:636
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
635	  def get_homepage(self):
636	    return 'https://saytap.github.io/'
637	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:651
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
650	  def get_citation(self):
651	    return """@misc{oh2023pr2utokyodatasets,
652	  author={Jihoon Oh and Naoaki Kanazawa and Kento Kawaharazuka},

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:683
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
682	  def get_homepage(self):
683	    return 'https://arxiv.org/abs/2306.10007'
684	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:709
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
708	  def get_homepage(self):
709	    return 'https://journals.sagepub.com/doi/full/10.1177/02783649211044405'
710	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:724
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
723	  def get_citation(self):
724	    return """@inproceedings{lee2019icra,
725	  title={Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks},

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:733
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
732	  def get_homepage(self):
733	    return 'https://sites.google.com/view/visionandtouch'
734	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:755
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
754	  def get_homepage(self):
755	    return 'https://sites.google.com/berkeley.edu/fanuc-manipulation'
756	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:778
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
777	  def get_homepage(self):
778	    return 'https://arxiv.org/abs/1709.10489'
779	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:799
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
798	  def get_homepage(self):
799	    return 'https://arxiv.org/abs/1806.10293'
800	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:842
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
841	  def get_homepage(self):
842	    return 'https://github.com/columbia-ai-robotics/diffusion_policy'
843	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:865
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
864	  def get_homepage(self):
865	    return 'https://human-world-model.github.io/'
866	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:878
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
877	  def get_citation(self):
878	    return """@inproceedings{
879	shah2021rapid,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:888
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
887	  def get_homepage(self):
888	    return 'https://sites.google.com/view/recon-robot'
889	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:903
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
902	  def get_citation(self):
903	    return """@misc{oh2023pr2utokyodatasets,
904	  author={Jihoon Oh and Naoaki Kanazawa and Kento Kawaharazuka},

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:934
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
933	  def get_homepage(self):
934	    return 'https://ieeexplore.ieee.org/iel7/10160211/10160212/10160747.pdf'
935	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:957
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
956	  def get_homepage(self):
957	    return 'https://ut-austin-rpl.github.io/sirius/'
958	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:980
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
979	  def get_homepage(self):
980	    return 'https://hshi74.github.io/robocook/'
981	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1003
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1002	  def get_homepage(self):
1003	    return 'https://play-to-policy.github.io/'
1004	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1026
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1025	  def get_homepage(self):
1026	    return 'https://arxiv.org/abs/2206.11894'
1027	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1059
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1058	  def get_homepage(self):
1059	    return 'https://ieeexplore.ieee.org/document/9341156'
1060	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1098
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1097	  def get_citation(self):
1098	    return """@inproceedings{
1099	saxena2023multiresolution,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1108
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1107	  def get_homepage(self):
1108	    return 'https://openreview.net/forum?id=WuBv9-IGDUA'
1109	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1131
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1130	  def get_homepage(self):
1131	    return 'https://ut-austin-rpl.github.io/sailor/'
1132	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1153
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1152	  def get_homepage(self):
1153	    return 'https://owmcorl.github.io'
1154	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1176
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1175	  def get_homepage(self):
1176	    return 'https://sites.google.com/view/hydra-il-2023'
1177	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1215
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1214	  def get_homepage(self):
1215	    return 'https://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html'
1216	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1228
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1227	  def get_citation(self):
1228	    return r"""@misc{ConqHoseManipData,
1229	author={Peter Mitrano and Dmitry Berenson},

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1237
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1236	    return (
1237	        'https://sites.google.com/corp/view/conq-hose-manipulation-dataset/home'
1238	    )
1239	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1254
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1253	  def get_homepage(self):
1254	    return 'https://github.com/notmahi/dobb-e'
1255	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1267
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1266	  def get_citation(self):
1267	    return 'https://doi.org/10.48550/arXiv.2401.08553'
1268	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1270
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1269	  def get_homepage(self):
1270	    return 'https://functional-manipulation-benchmark.github.io/'
1271	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1286
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1285	  def get_homepage(self):
1286	    return 'https://github.com/ioai-tech/rlds_dataset_builder'
1287	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1302
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1301	  def get_homepage(self):
1302	    return 'https://mimic-play.github.io/'
1303	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1318
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1317	  def get_homepage(self):
1318	    return 'https://mobile-aloha.github.io'
1319	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1334
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1333	  def get_homepage(self):
1334	    return 'https://robopen.github.io/'
1335	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1350
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1349	  def get_homepage(self):
1350	    return 'https://github.com/jimmyyhwu/tidybot'
1351	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1368
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1367	  def get_homepage(self):
1368	    return r"""https://vimalabs.github.io/"""
1369	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1379
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1378	    return (
1379	        'A set of object manipulation trajectories collected at Microsoft'
1380	        ' Research on a WidowX-250 robot in a setup and format compatible with'
1381	        " UC Berkeley's BridgeData V2"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1389
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1388	  def get_homepage(self):
1389	    return 'https://www.microsoft.com/en-us/download/details.aspx?id=105937'
1390	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1406
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1405	  def get_citation(self):
1406	    return 'https://doi.org/10.48550/arXiv.2303.08789'
1407	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1409
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1408	  def get_homepage(self):
1409	    return 'https://microsoft.github.io/PLEX/'
1410	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/robotics/rtx/rtx.py:1431
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1430	  def get_homepage(self):
1431	    return 'https://spoc-robot.github.io/'
1432	

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cleanup/delete_old_versions.py:174
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
173	  # Parse all existing dirs.
174	  with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
175	    future = executor.submit(

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cleanup/refactor_dataset_as_folder.py:98
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
97	  shutil.copytree(src, dst)
98	  shutil.rmtree(src)
99	

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cleanup/refactor_dataset_as_folder.py:161
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
160	    print(f'Cleanup existing {code_info.dst}')
161	    shutil.rmtree(code_info.dst)
162	  code_info.dst.mkdir()

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cleanup/url_filename_recorder.py:115
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
114	  logging.info('Start fetching filenames.')
115	  with futures.ThreadPoolExecutor(max_workers=100) as executor:
116	    # Query all filenames in parallel

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cleanup/url_status_checker.py:57
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
56	
57	  with futures.ThreadPoolExecutor(max_workers=100) as executor:
58	    all_codes = executor.map(_get_status_code, urls)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cli/build.py:51
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
50	      help=(
51	          'Name(s) of the dataset(s) to build. Default to current dir. '
52	          'See https://www.tensorflow.org/datasets/cli for accepted values.'
53	      ),

--------------------------------------------------
>> Issue: [B839:pool] multiprocessing.Pool
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cli/build.py:129
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b839_pool.html
128	  else:
129	    with multiprocessing.Pool(args.num_processes) as pool:
130	      pool.map(process_builder_fn, builders)

--------------------------------------------------
>> Issue: [B323:blacklist] multiprocessing_Pool
   Severity: Medium   Confidence: High
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cli/build.py:129
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b323-multiprocessing-pool
128	  else:
129	    with multiprocessing.Pool(args.num_processes) as pool:
130	      pool.map(process_builder_fn, builders)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cli/build_test.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	    return {
50	        'http://data.org/file1.zip': download.checksums.UrlInfo(
51	            size=42,
52	            checksum='d45899d9a6a0e48afb250aac7ee3dc50e73e263687f15761d754515cd8284e0a',
53	            filename='file1.zip',
54	        ),
55	    }

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cli/build_test.py:295
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
294	    assert not _build('dummy_dataset_no_generate --download_only')
295	    mock_download.assert_called_with({'file0': 'http://data.org/file1.zip'})
296	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cli/builder_templates.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
57	  content = textwrap.dedent(
58	      f'''\
59	      """{info.name} dataset."""
60	
61	      import {info.tfds_api} as tfds
62	
63	
64	      class Builder(tfds.core.GeneratorBasedBuilder):
65	        """DatasetBuilder for {info.name} dataset."""
66	
67	        VERSION = tfds.core.Version('1.0.0')
68	        RELEASE_NOTES = {{
69	            '1.0.0': 'Initial release.',
70	        }}
71	
72	        def _info(self) -> tfds.core.DatasetInfo:
73	          """Returns the dataset metadata."""
74	          # {info.todo}: Specifies the tfds.core.DatasetInfo object
75	          return self.dataset_info_from_configs(
76	              features=tfds.features.FeaturesDict({{
77	                  # These are the features of your dataset like images, labels ...
78	                  'image': tfds.features.Image(shape=(None, None, 3)),
79	                  'label': tfds.features.ClassLabel(names=['no', 'yes']),
80	              }}),
81	              # If there's a common (input, target) tuple from the
82	              # features, specify them here. They'll be used if
83	              # `as_supervised=True` in `builder.as_dataset`.
84	              supervised_keys=('image', 'label'),  # Set to `None` to disable
85	              homepage='https://dataset-homepage/',
86	          )
87	
88	        def _split_generators(self, dl_manager: tfds.download.DownloadManager):
89	          """Returns SplitGenerators."""
90	          # {info.todo}: Downloads the data and defines the splits
91	          path = dl_manager.download_and_extract('https://todo-data-url')
92	
93	          # {info.todo}: Returns the Dict[split names, Iterator[Key, Example]]
94	          return {{
95	              'train': self._generate_examples(path / 'train_imgs'),
96	          }}
97	
98	        def _generate_examples(self, path):
99	          """Yields examples."""
100	          # {info.todo}: Yields (key, example) tuples from the dataset
101	          for f in path.glob('*.jpeg'):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cli/builder_templates.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
57	  content = textwrap.dedent(
58	      f'''\
59	      """{info.name} dataset."""
60	
61	      import {info.tfds_api} as tfds
62	
63	
64	      class Builder(tfds.core.GeneratorBasedBuilder):
65	        """DatasetBuilder for {info.name} dataset."""
66	
67	        VERSION = tfds.core.Version('1.0.0')
68	        RELEASE_NOTES = {{
69	            '1.0.0': 'Initial release.',
70	        }}
71	
72	        def _info(self) -> tfds.core.DatasetInfo:
73	          """Returns the dataset metadata."""
74	          # {info.todo}: Specifies the tfds.core.DatasetInfo object
75	          return self.dataset_info_from_configs(
76	              features=tfds.features.FeaturesDict({{
77	                  # These are the features of your dataset like images, labels ...
78	                  'image': tfds.features.Image(shape=(None, None, 3)),
79	                  'label': tfds.features.ClassLabel(names=['no', 'yes']),
80	              }}),
81	              # If there's a common (input, target) tuple from the
82	              # features, specify them here. They'll be used if
83	              # `as_supervised=True` in `builder.as_dataset`.
84	              supervised_keys=('image', 'label'),  # Set to `None` to disable
85	              homepage='https://dataset-homepage/',
86	          )
87	
88	        def _split_generators(self, dl_manager: tfds.download.DownloadManager):
89	          """Returns SplitGenerators."""
90	          # {info.todo}: Downloads the data and defines the splits
91	          path = dl_manager.download_and_extract('https://todo-data-url')
92	
93	          # {info.todo}: Returns the Dict[split names, Iterator[Key, Example]]
94	          return {{
95	              'train': self._generate_examples(path / 'train_imgs'),
96	          }}
97	
98	        def _generate_examples(self, path):
99	          """Yields examples."""
100	          # {info.todo}: Yields (key, example) tuples from the dataset
101	          for f in path.glob('*.jpeg'):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cli/builder_templates.py:115
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
114	  content = textwrap.dedent(
115	      f'''\
116	      """{info.name} dataset."""
117	
118	      from tensorflow_datasets.core.dataset_builders.conll import conll_dataset_builder_utils as conll_lib
119	      import {info.tfds_api} as tfds
120	
121	
122	      class {info.cls_name}(tfds.dataset_builders.ConllDatasetBuilder):
123	        """DatasetBuilder for {info.name} dataset."""
124	
125	        VERSION = tfds.core.Version('1.0.0')
126	        RELEASE_NOTES = {{
127	            '1.0.0': 'Initial release.',
128	        }}
129	        # {info.todo}: Add details about the dataset's features.
130	        # conll_lib contains a set of ready-to-use features.
131	        BUILDER_CONFIGS = [conll_lib.CONLL_2003_CONFIG]
132	
133	        def _info(self) -> tfds.core.DatasetInfo:
134	          """Returns the dataset metadata."""
135	          # {info.todo}: Specifies the dataset infos.
136	          return self.create_dataset_info(
137	            homepage="",
138	          )
139	
140	        def _split_generators(self, dl_manager: tfds.download.DownloadManager):
141	          """Returns SplitGenerators."""
142	          # {info.todo}: Downloads the data and defines the splits
143	          path = dl_manager.download_and_extract('https://todo-data-url')
144	
145	          return {{
146	            'train': self._generate_examples(path / 'train.txt',)
147	          }}
148	
149	        # {info.todo}: If you need a customized _generate_examples function,
150	        # comment out the following.

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cli/builder_templates.py:166
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
165	  content = textwrap.dedent(
166	      f'''\
167	      """{info.name} dataset."""
168	
169	      from tensorflow_datasets.core.dataset_builders.conll import conllu_dataset_builder_utils as conllu_lib
170	      import {info.tfds_api} as tfds
171	
172	
173	      class {info.cls_name}(tfds.dataset_builders.ConllUDatasetBuilder):
174	        """DatasetBuilder for {info.name} dataset."""
175	
176	        VERSION = tfds.core.Version('1.0.0')
177	        RELEASE_NOTES = {{
178	            '1.0.0': 'Initial release.',
179	        }}
180	        # {info.todo}: Add details about the dataset's config to use to
181	        # BUILDER_CONFIGS. conllu_lib contains a set of ready-to-use features.
182	        BUILDER_CONFIGS = [
183	            conllu_lib.get_universal_morphology_config(
184	                language='',
185	                features=conllu_lib.UNIVERSAL_DEPENDENCIES_FEATURES,
186	                name='')
187	        ]
188	
189	        def _info(self) -> tfds.core.DatasetInfo:
190	          """Returns the dataset metadata."""
191	          # {info.todo}: Specifies the dataset infos.
192	          return self.create_dataset_info(
193	              homepage='',
194	          )
195	
196	        def _split_generators(self, dl_manager: tfds.download.DownloadManager):
197	          """Returns SplitGenerators."""
198	          # {info.todo}: Downloads the data and defines the splits
199	          path = dl_manager.download_and_extract('https://todo-data-url')
200	
201	          # {info.todo}: Specify the process_example_fn to be used in the
202	          # `_generate_examples` method to process examples.
203	          # The default `process_example_fn` processes a conllu-annotated
204	          # example using the features specified in BUILDER_CONFIGS.
205	          # `conllu_dataset_builder` already provides a number of ready-to-use
206	          # process functions.
207	          return {{
208	              'train':
209	                  self._generate_examples(
210	                      filepaths=path / 'train.txt',
211	                      # Remove if you want to use the default `process_example_fn`.
212	                      # process_example_fn=
213	                  )
214	          }}
215	
216	        # {info.todo}: If you need a customized _generate_examples function,
217	        # comment out the following, otherwise remove it.

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cli/cli_utils.py:240
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
239	      help=(
240	          'A (comma-separated) list of flags to pass to `PipelineOptions` when'
241	          ' preparing with Apache Beam. (see:'
242	          ' https://www.tensorflow.org/datasets/beam_datasets). Example:'

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cli/convert_format_utils.py:577
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
576	
577	    with concurrent.futures.ThreadPoolExecutor(
578	        max_workers=convert_config.num_workers
579	    ) as executor:

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cli/convert_format_utils.py:615
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
614	  elif convert_config.num_workers > 1:
615	    with concurrent.futures.ThreadPoolExecutor(
616	        max_workers=convert_config.num_workers
617	    ) as executor:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cli/new.py:113
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
112	  print(
113	      'Dataset generated at {}\n'
114	      'You can start searching `{}` to complete the implementation.\n'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/cli/new.py:139
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
138	
139	  content = textwrap.dedent(f'''\
140	      """{info.name} dataset."""
141	
142	      from {info.ds_import} import {filename}
143	      import {info.tfds_api} as tfds
144	
145	      class {info.cls_name}Test(tfds.testing.DatasetBuilderTestCase):
146	        """Tests for {info.name} dataset."""
147	        # {info.todo}:
148	        DATASET_CLASS = {filename}.Builder
149	        SPLITS = {{

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/build_api_docs.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	    "code_url_prefix",
54	    "https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/",
55	    "The url prefix for links to code.",
56	)
57	
58	flags.DEFINE_bool(

--------------------------------------------------
>> Issue: [B836:rmtree] shutil.rmtree
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/build_api_docs_test.py:34
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b836_rmtree.html
33	    if os.path.exists(self.workdir):
34	      shutil.rmtree(self.workdir)
35	    os.makedirs(self.workdir)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/build_api_docs_test.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
46	    with open(os.path.join(self.workdir, "tfds.md")) as f:
47	      content = f.read()
48	    self.assertIn("__init__.py", content)

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/build_api_docs_test.py:52
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
51	    with open(os.path.join(self.workdir, "tfds/testing.md")) as f:
52	      content = f.read()
53	    self.assertIn("__init__.py", content)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/build_community_catalog.py:320
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
319	  def huggingface_link(self) -> str:
320	    return f'[Huggingface](https://huggingface.co/datasets/{self.name})'
321	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/build_community_catalog.py:323
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
322	  def huggingface_raw_info_url(self) -> str:
323	    return f'https://huggingface.co/datasets/{self.name}/raw/main/dataset_infos.json'
324	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/build_community_catalog.py:469
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
468	    )
469	    template = textwrap.dedent("""\
470	      # Huggingface datasets

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/build_community_catalog_test.py:117
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
116	  assert dataset_doc.extra_links() == [
117	      '[Code](https://github.com/huggingface/datasets/blob/master/datasets/ds1)',
118	      '[Huggingface](https://huggingface.co/datasets/ds1)',
119	  ]
120	  assert dataset_doc.huggingface_raw_info_url() == (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/build_community_catalog_test.py:118
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
117	      '[Code](https://github.com/huggingface/datasets/blob/master/datasets/ds1)',
118	      '[Huggingface](https://huggingface.co/datasets/ds1)',
119	  ]
120	  assert dataset_doc.huggingface_raw_info_url() == (
121	      'https://huggingface.co/datasets/ds1/raw/main/dataset_infos.json'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/build_community_catalog_test.py:121
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
120	  assert dataset_doc.huggingface_raw_info_url() == (
121	      'https://huggingface.co/datasets/ds1/raw/main/dataset_infos.json'
122	  )
123	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/build_community_catalog_test.py:156
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
155	      formatter.to_namespace_overview()
156	      == """\
157	# Huggingface datasets
158	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/collection_markdown_builder.py:148
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
147	    """Returns the link to a dataset documentation in TFDS catalog."""
148	    prefix = 'https://www.tensorflow.org/datasets/catalog/'
149	    # Links to TFDS catalog's entries don't include versions.
150	    ds_link = prefix + reference.dataset_name

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/collection_markdown_builder_test.py:87
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
86	      (
87	          '    * `a`:'
88	          ' [`a/c:1.3.5`](https://www.tensorflow.org/datasets/catalog/a#c)'
89	      ),
90	      (
91	          '    * `b`:'
92	          ' [`b/d:2.4.8`](https://www.tensorflow.org/datasets/catalog/b#d)'
93	      ),
94	      (
95	          '    * `c`:'
96	          ' [`c/e:3.5.7`](https://www.tensorflow.org/datasets/catalog/c#e)'
97	      ),
98	  ]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/collection_markdown_builder_test.py:91
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
90	      (
91	          '    * `b`:'
92	          ' [`b/d:2.4.8`](https://www.tensorflow.org/datasets/catalog/b#d)'
93	      ),
94	      (
95	          '    * `c`:'
96	          ' [`c/e:3.5.7`](https://www.tensorflow.org/datasets/catalog/c#e)'
97	      ),
98	  ]
99	  assert datasets_section.content(loader=dummy_dc_loader) == '\n'.join(
100	      expected_lines
101	  )
102	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/collection_markdown_builder_test.py:95
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
94	      (
95	          '    * `c`:'
96	          ' [`c/e:3.5.7`](https://www.tensorflow.org/datasets/catalog/c#e)'
97	      ),
98	  ]
99	  assert datasets_section.content(loader=dummy_dc_loader) == '\n'.join(
100	      expected_lines
101	  )
102	
103	
104	def test_get_collection_markdown_string(
105	    dummy_dc_loader: tfds.core.DatasetCollectionLoader,
106	):  # pylint: disable=redefined-outer-name

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/collection_markdown_builder_test.py:115
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
114	  assert (
115	      '[`c/e:3.5.7`](https://www.tensorflow.org/datasets/catalog/c#e)' in doc
116	  )  # Collection dataset.

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/dataset_markdown_builder.py:174
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
173	    return (
174	        'Missing (dataset generated before '
175	        '[#2813](https://github.com/tensorflow/datasets/issues/2813))'
176	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/dataset_markdown_builder.py:178
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
177	  url_title = builder.__module__
178	  module_url = 'https://github.com/tensorflow/datasets'
179	  # TODO(epot): For datasets built with `tfds build` __module__ correspond
180	  # to the relative path, not absolute so can't be recover.
181	  return f'[{url_title}]({module_url})'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/dataset_markdown_builder.py:289
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
288	  EXTRA_DOC = (
289	      ' ([documentation]'
290	      '(https://www.tensorflow.org/datasets/performances#auto-caching))'
291	  )
292	
293	  def _build_autocached_info(self, builder: tfds.core.DatasetBuilder):
294	    """Returns the auto-cache information string."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/dataset_markdown_builder.py:458
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
457	  EXTRA_DOC = (
458	      ' (See [`as_supervised` doc]'
459	      '(https://www.tensorflow.org/datasets/api_docs/python/tfds/load#args))'
460	  )
461	
462	  def get_key(self, builder: tfds.core.DatasetBuilder) -> Key:
463	    # get_key() must return a Key (str/int), but supervised_keys can be a tuple

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/dataset_markdown_builder.py:519
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
518	  EXTRA_DOC = (
519	      ' ([tfds.show_examples](https://www.tensorflow.org/datasets/api_docs/python/tfds/visualization/show_examples))'
520	  )
521	
522	  def __init__(self, visualization_util):
523	    self._visualization_util = visualization_util

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/dataset_markdown_builder.py:539
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
538	  EXTRA_DOC = (
539	      ' ([tfds.as_dataframe](https://www.tensorflow.org/datasets/api_docs/python/tfds/as_dataframe))'
540	  )
541	  # Artificially limit the amount of dataframe blocks to optimize page loading.
542	  _MAX_DATAFRAME_BLOCKS = 100
543	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/dataset_markdown_builder.py:707
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
706	  """
707	  description = f"""
708	
709	  To use this dataset:
710	
711	  ```python
712	  import tensorflow_datasets as tfds
713	
714	  ds = tfds.load('{builder.info.name}', split='train')
715	  for ex in ds.take(4):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/dataset_markdown_builder.py:727
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
726	
727	  text = f"""\
728	  <div itemscope itemtype="http://schema.org/Dataset">
729	    <div itemscope itemprop="includedInDataCatalog" itemtype="http://schema.org/DataCatalog">
730	      <meta itemprop="name" content="TensorFlow Datasets" />
731	    </div>
732	    <meta itemprop="name" content="{builder.info.name}" />
733	    <meta itemprop="description" content="{_escape(description)}" />
734	    <meta itemprop="url" content="https://www.tensorflow.org/datasets/catalog/{builder.info.name}" />
735	    <meta itemprop="sameAs" content="{_escape(builder.info.homepage)}" />
736	    <meta itemprop="citation" content="{_escape(builder.info.citation)}" />
737	  </div>

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/dataset_markdown_builder.py:727
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
726	
727	  text = f"""\
728	  <div itemscope itemtype="http://schema.org/Dataset">
729	    <div itemscope itemprop="includedInDataCatalog" itemtype="http://schema.org/DataCatalog">
730	      <meta itemprop="name" content="TensorFlow Datasets" />
731	    </div>
732	    <meta itemprop="name" content="{builder.info.name}" />
733	    <meta itemprop="description" content="{_escape(description)}" />
734	    <meta itemprop="url" content="https://www.tensorflow.org/datasets/catalog/{builder.info.name}" />
735	    <meta itemprop="sameAs" content="{_escape(builder.info.homepage)}" />
736	    <meta itemprop="citation" content="{_escape(builder.info.citation)}" />
737	  </div>

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/dataset_markdown_builder_test.py:96
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
95	def test_paperswithcode_section():
96	  tfds_to_pwc_links = {'dummy_dataset': 'https://paperswithcode/dummy_dataset'}
97	  pwc_section = dataset_markdown_builder.PapersWithCodeSection(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/doc_utils.py:57
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
56	  fig_base_url: str = (
57	      'https://storage.googleapis.com/tfds-data/visualization/fig/'
58	  )
59	  # DataframeDocUtil
60	  df_base_path: tfds.typing.PathLike | None = tfds.core.gcs_path(
61	      'visualization/dataframe'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/doc_utils.py:64
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
63	  df_base_url: str = (
64	      'https://storage.googleapis.com/tfds-data/visualization/dataframe/'
65	  )
66	  # NightlyDocUtil
67	  nightly_path: tfds.typing.PathLike | None = tfds.core.utils.tfds_path(
68	      'stable_versions.txt'

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/doc_utils.py:230
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
229	  with tf.io.gfile.GFile(os.fspath(version_path), 'r') as f:
230	    stable_versions = f.read().splitlines()
231	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/doc_utils_test.py:166
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
165	def test_format_hompepage_url():
166	  url = 'https://a/b'
167	  assert doc_utils.format_homepage_url(url) == f'[{url}]({url})'

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/document_datasets.py:201
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
200	
201	  with futures.ThreadPoolExecutor(max_workers=_WORKER_COUNT_CONFIGS) as tpool:
202	    config_names = sorted(common_dir.iterdir())

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/document_datasets.py:224
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
223	
224	    with futures.ThreadPoolExecutor(max_workers=_WORKER_COUNT_CONFIGS) as tpool:
225	      config_builders = list(

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/document_datasets.py:402
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
401	  print(f'Document {len(datasets)} builders...')
402	  with futures.ThreadPoolExecutor(max_workers=_WORKER_COUNT_DATASETS) as tpool:
403	    tasks = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/document_datasets_test.py:100
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
99	  assert (
100	      '<meta itemprop="url" content="'
101	      f'https://www.tensorflow.org/datasets/catalog/{DummyDatasetConfigs.name}"'
102	      ' />'

--------------------------------------------------
>> Issue: [B839:pool] multiprocessing.Pool
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/script_utils.py:141
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b839_pool.html
140	  logging.info(f'Generate figures for {len(full_names)} builders')
141	  with multiprocessing.Pool(_WORKER_COUNT_DATASETS) as tpool:
142	    list(tpool.map(worker_fn, full_names))

--------------------------------------------------
>> Issue: [B323:blacklist] multiprocessing_Pool
   Severity: Medium   Confidence: High
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/script_utils.py:141
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b323-multiprocessing-pool
140	  logging.info(f'Generate figures for {len(full_names)} builders')
141	  with multiprocessing.Pool(_WORKER_COUNT_DATASETS) as tpool:
142	    list(tpool.map(worker_fn, full_names))

--------------------------------------------------
>> Issue: [B840:executor] concurrent.futures.Executor
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/documentation/script_utils.py:157
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b840_executor.html
156	  full_names = _get_full_names(datasets)
157	  with concurrent.futures.ThreadPoolExecutor(
158	      max_workers=_WORKER_COUNT_DATASETS,
159	  ) as executor:

--------------------------------------------------
>> Issue: [B834:open] tarfile.open
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/replace_fake_images.py:57
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b834_open.html
56	  """
57	  image_content = PIL.Image.open(filepath)
58	  image = np.array(image_content)

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/replace_fake_images.py:99
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
98	          file_path = os.path.join(file_dir, file)
99	          zip_file.write(
100	              file_path, arcname=os.path.relpath(file_path, temp_dir)
101	          )

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/replace_fake_images.py:99
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
98	          file_path = os.path.join(file_dir, file)
99	          zip_file.write(
100	              file_path, arcname=os.path.relpath(file_path, temp_dir)
101	          )

--------------------------------------------------
>> Issue: [B834:open] tarfile.open
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/replace_fake_images.py:127
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b834_open.html
126	    # Extraction of .tar file
127	    with tarfile.open(tar_filepath, 'r' + extension) as tar:
128	      tar.extractall(path=temp_dir)

--------------------------------------------------
>> Issue: [B834:open] tarfile.open
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/scripts/replace_fake_images.py:133
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b834_open.html
132	    # Convert back into tar file
133	    with tarfile.open(tar_filepath, 'w' + extension) as tar:
134	      tar.add(temp_dir, arcname='', recursive=True)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/cherry_blossoms/cherry_blossoms.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	
25	URL = 'https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/cherry_blossoms.csv'
26	
27	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/cherry_blossoms/cherry_blossoms.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	
50	_CITATION = """
51	@ONLINE {
52	    author = "Aono, Yasuyuki",
53	    title  = "Historical Series of Phenological data for Cherry Tree Flowering at Kyoto City (and March Mean Temperature Reconstructions)",
54	    year   = "2012",
55	    url    = "http://atmenv.envi.osakafu-u.ac.jp/aono/kyophenotemp4/"
56	}
57	"""
58	
59	
60	class CherryBlossoms(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/cherry_blossoms/cherry_blossoms.py:82
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
81	        supervised_keys=None,  # Set to `None` to disable
82	        homepage='http://atmenv.envi.osakafu-u.ac.jp/aono/kyophenotemp4/',
83	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/covid19/covid19.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	
37	_CITATION = """
38	@article{Wahltinez2020,
39	  author = "O. Wahltinez and others",
40	  year = 2020,
41	  title = "COVID-19 Open-Data: curating a fine-grained, global-scale data repository for SARS-CoV-2",
42	  note = "Work in progress",
43	  url = {https://goo.gle/covid-19-open-data},
44	}
45	"""
46	
47	_N_RECORDS = 11266423  # It should be an upper bound

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/covid19/covid19.py:772
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
771	        supervised_keys=None,
772	        homepage='https://github.com/GoogleCloudPlatform/covid-19-open-data',
773	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/covid19/covid19.py:779
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
778	    archive_path = dl_manager.download(
779	        'https://storage.googleapis.com/covid19-open-data/v3/aggregated.csv.gz?generation=1620814656792419'
780	    )
781	    return {tfds.Split.TRAIN: self._generate_examples(dl_manager, archive_path)}

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/cs_restaurants/cs_restaurants.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_CITATION = r"""
27	
28	@inproceedings{dusek_neural_2019,
29	        author = {Duek, Ondej and Jurek, Filip},
30	        title = {Neural {Generation} for {Czech}: {Data} and {Baselines}},
31	        shorttitle = {Neural {Generation} for {Czech}},
32	        url = {https://www.aclweb.org/anthology/W19-8670/},
33	        urldate = {2019-10-18},
34	        booktitle = {Proceedings of the 12th {International} {Conference} on {Natural} {Language} {Generation} ({INLG} 2019)},
35	        month = oct,
36	        address = {Tokyo, Japan},
37	        year = {2019},
38	        pages = {563--574},
39	        abstract = {We present the first dataset targeted at end-to-end NLG in Czech in the restaurant domain, along with several strong baseline models using the sequence-to-sequence approach. While non-English NLG is under-explored in general, Czech, as a morphologically rich language, makes the task even harder: Since Czech requires inflecting named entities, delexicalization or copy mechanisms do not work out-of-the-box and lexicalizing the generated outputs is non-trivial. In our experiments, we present two different approaches to this this problem: (1) using a neural language model to select the correct inflected form while lexicalizing, (2) a two-step generation setup: our sequence-to-sequence model generates an interleaved sequence of lemmas and morphological tags, which are then inflected by a morphological generator.},
40	}
41	"""
42	
43	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/cs_restaurants/cs_restaurants.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
48	
49	_HOMEPAGE_URL = 'https://github.com/UFAL-DSG/cs_restaurant_dataset'
50	
51	_TRAIN_URL = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/cs_restaurants/cs_restaurants.py:52
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
51	_TRAIN_URL = (
52	    'https://github.com/UFAL-DSG/cs_restaurant_dataset/raw/master/train.json'
53	)
54	_DEV_URL = (
55	    'https://github.com/UFAL-DSG/cs_restaurant_dataset/raw/master/devel.json'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/cs_restaurants/cs_restaurants.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	_DEV_URL = (
55	    'https://github.com/UFAL-DSG/cs_restaurant_dataset/raw/master/devel.json'
56	)
57	_TEST_URL = (
58	    'https://github.com/UFAL-DSG/cs_restaurant_dataset/raw/master/test.json'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/cs_restaurants/cs_restaurants.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
57	_TEST_URL = (
58	    'https://github.com/UFAL-DSG/cs_restaurant_dataset/raw/master/test.json'
59	)
60	
61	
62	def _get_table_from_da(da):
63	  """Converts a dialogue act from the cs_restaurant dataset into a table row."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/dart/dart.py:52
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
51	
52	_URL = 'https://github.com/Yale-LILY/dart/archive/master.zip'
53	
54	
55	class Dart(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/dart/dart.py:79
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
78	        # Homepage of the dataset for documentation
79	        homepage='https://github.com/Yale-LILY/dart',
80	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/diamonds/diamonds.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	
44	_CITATION = """
45	@Book{,
46	  author = {Hadley Wickham},
47	  title = {ggplot2: Elegant Graphics for Data Analysis},
48	  publisher = {Springer-Verlag New York},
49	  year = {2016},
50	  isbn = {978-3-319-24277-4},
51	  url = {https://ggplot2.tidyverse.org},
52	}
53	"""
54	
55	_CUTS = ('Fair', 'Good', 'Very Good', 'Premium', 'Ideal')

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/diamonds/diamonds.py:74
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
73	
74	_URL = 'https://raw.githubusercontent.com/tidyverse/ggplot2/main/data-raw/diamonds.csv'
75	
76	
77	class Diamonds(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/diamonds/diamonds.py:95
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
94	        supervised_keys=('features', 'price'),
95	        homepage='https://ggplot2.tidyverse.org/reference/diamonds.html',
96	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/forest_fires.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_CITATION = r"""
29	@misc{Dua:2019 ,
30	author = "Dua, Dheeru and Graff, Casey",
31	year = "2017",
32	title = "{UCI} Machine Learning Repository",
33	url = "http://archive.ics.uci.edu/ml",
34	institution = "University of California, Irvine, School of Information and Computer Sciences" }
35	
36	@article{cortez2007data,
37	  title={A data mining approach to predict forest fires using meteorological data},
38	  author={Cortez, Paulo and Morais, Anibal de Jesus Raimundo},
39	  year={2007},
40	  publisher={Associa{\c{c}}{\~a}o Portuguesa para a Intelig{\^e}ncia Artificial (APPIA)}
41	}
42	"""
43	
44	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/forest_fires.py:112
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
111	
112	_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.csv'
113	
114	
115	def features():

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/forest_fires.py:146
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
145	        supervised_keys=('area', 'features'),
146	        homepage='https://archive.ics.uci.edu/ml/datasets/Forest+Fires',
147	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/genomics_ood.py:237
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
236	        # Homepage of the dataset for documentation
237	        homepage='https://github.com/google-research/google-research/tree/master/genomics_ood',
238	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/german_credit_numeric.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	
22	URL = "https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data-numeric"
23	
24	_CITATION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/german_credit_numeric.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_CITATION = """\
25	@misc{Dua:2019 ,
26	author = "Dua, Dheeru and Graff, Casey",
27	year = "2017",
28	title = "{UCI} Machine Learning Repository",
29	url = "http://archive.ics.uci.edu/ml",
30	institution = "University of California, Irvine, School of Information and Computer Sciences"
31	}
32	"""
33	
34	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/german_credit_numeric.py:56
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
55	        supervised_keys=("features", "label"),
56	        homepage="https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)",
57	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/higgs.py:57
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
56	
57	_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz'
58	
59	
60	class Higgs(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/higgs.py:65
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
64	  RELEASE_NOTES = {
65	      '2.0.0': 'New split API (https://tensorflow.org/datasets/splits)',
66	  }
67	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/higgs.py:106
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
105	        supervised_keys=None,
106	        homepage='https://archive.ics.uci.edu/ml/datasets/HIGGS',
107	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/howell/howell.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	
25	URL = "https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/Howell1.csv"
26	
27	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/howell/howell.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	
50	_CITATION = """
51	@ONLINE {
52	    author = "Howell, Nancy",
53	    title  = "Dobe !Kung Census of All Population.",
54	    year   = "2009",
55	    url    = "https://tspace.library.utoronto.ca/handle/1807/17973"
56	}
57	"""
58	
59	
60	class Howell(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/howell/howell.py:81
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
80	        supervised_keys=None,  # Set to `None` to disable
81	        homepage="https://tspace.library.utoronto.ca/handle/1807/10395",
82	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/iris.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	
22	IRIS_URL = "https://archive.ics.uci.edu/static/public/53/iris.zip"
23	
24	_CITATION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/iris.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_CITATION = """\
25	@misc{Dua:2019 ,
26	author = "Dua, Dheeru and Graff, Casey",
27	year = "2017",
28	title = "{UCI} Machine Learning Repository",
29	url = "http://archive.ics.uci.edu/ml",
30	institution = "University of California, Irvine, School of Information and Computer Sciences"
31	}
32	"""
33	
34	_DESCRIPTION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/iris.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	  RELEASE_NOTES = {
50	      "2.0.0": "New split API (https://tensorflow.org/datasets/splits)",
51	      "2.1.0": "Updated broken link",
52	  }
53	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/iris.py:67
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
66	        supervised_keys=("features", "label"),
67	        homepage="https://archive.ics.uci.edu/ml/datasets/iris",
68	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/movielens.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	
29	_CITATION = """
30	@article{10.1145/2827872,
31	author = {Harper, F. Maxwell and Konstan, Joseph A.},
32	title = {The MovieLens Datasets: History and Context},
33	year = {2015},
34	issue_date = {January 2016},
35	publisher = {Association for Computing Machinery},
36	address = {New York, NY, USA},
37	volume = {5},
38	number = {4},
39	issn = {2160-6455},
40	url = {https://doi.org/10.1145/2827872},
41	doi = {10.1145/2827872},
42	journal = {ACM Trans. Interact. Intell. Syst.},
43	month = dec,
44	articleno = {19},
45	numpages = {19},
46	keywords = {Datasets, recommendations, ratings, MovieLens}
47	}
48	"""
49	
50	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/movielens.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	
50	_DESCRIPTION = """
51	This dataset contains a set of movie ratings from the MovieLens website, a movie
52	recommendation service. This dataset was collected and maintained by [GroupLens]
53	(https://grouplens.org/), a research group at the University of Minnesota. There
54	are 5 versions included: "25m", "latest-small", "100k", "1m", "20m". In all
55	datasets, the movies data and ratings data are joined on "movieId". The 25m
56	dataset, latest-small dataset, and 20m dataset contain only movie data and
57	rating data. The 1m dataset and 100k dataset contain demographic data in
58	addition to movie and rating data.
59	
60	- "25m": This is the latest stable version of the MovieLens dataset. It is
61	recommended for research purposes.
62	- "latest-small": This is a small subset of the latest version of the MovieLens
63	dataset. It is changed and updated over time by GroupLens.
64	- "100k": This is the oldest version of the MovieLens datasets. It is a small
65	dataset with demographic data.
66	- "1m": This is the largest MovieLens dataset that contains demographic data.
67	- "20m": This is one of the most used MovieLens datasets in academic papers
68	along with the 1m dataset.
69	
70	For each version, users can view either only the movies data by adding the
71	"-movies" suffix (e.g. "25m-movies") or the ratings data joined with the movies
72	data (and users data in the 1m and 100k datasets) by adding the "-ratings"
73	suffix (e.g. "25m-ratings").
74	
75	The features below are included in all versions with the "-ratings" suffix.
76	
77	- "movie_id": a unique identifier of the rated movie
78	- "movie_title": the title of the rated movie with the release year in
79	parentheses
80	- "movie_genres": a sequence of genres to which the rated movie belongs
81	- "user_id": a unique identifier of the user who made the rating
82	- "user_rating": the score of the rating on a five-star scale
83	- "timestamp": the timestamp of the ratings, represented in seconds since
84	midnight Coordinated Universal Time (UTC) of January 1, 1970
85	
86	The "100k-ratings" and "1m-ratings" versions in addition include the following
87	demographic features.
88	
89	- "user_gender": gender of the user who made the rating; a true value
90	corresponds to male
91	- "bucketized_user_age": bucketized age values of the user who made the rating,
92	the values and the corresponding ranges are:
93	  - 1: "Under 18"
94	  - 18: "18-24"
95	  - 25: "25-34"
96	  - 35: "35-44"
97	  - 45: "45-49"
98	  - 50: "50-55"
99	  - 56: "56+"
100	- "user_occupation_label": the occupation of the user who made the rating
101	represented by an integer-encoded label; labels are preprocessed to be
102	consistent across different versions
103	- "user_occupation_text": the occupation of the user who made the rating in
104	the original string; different versions can have different set of raw text
105	labels
106	- "user_zip_code": the zip code of the user who made the rating
107	
108	In addition, the "100k-ratings" dataset would also have a feature "raw_user_age"
109	which is the exact ages of the users who made the rating
110	
111	Datasets with the "-movies" suffix contain only "movie_id", "movie_title", and
112	"movie_genres" features.
113	"""
114	
115	_FORMAT_VERSIONS = ['25m', 'latest-small', '20m', '100k', '1m']

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/movielens.py:200
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
199	          download_url=(
200	              'https://files.grouplens.org/datasets/movielens/ml-25m.zip'
201	          ),
202	          parsing_fn=movielens_parsing.parse_current_ratings_data,
203	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/movielens.py:215
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
214	          download_url=(
215	              'https://files.grouplens.org/datasets/movielens/ml-25m.zip'
216	          ),
217	          parsing_fn=movielens_parsing.parse_current_movies_data,
218	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/movielens.py:239
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
238	          download_url=(
239	              'https://files.grouplens.org/datasets/movielens/'
240	              'ml-latest-small.zip'
241	          ),
242	          parsing_fn=movielens_parsing.parse_current_ratings_data,
243	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/movielens.py:255
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
254	          download_url=(
255	              'https://files.grouplens.org/datasets/movielens/'
256	              'ml-latest-small.zip'
257	          ),
258	          parsing_fn=movielens_parsing.parse_current_movies_data,
259	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/movielens.py:276
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
275	          download_url=(
276	              'https://files.grouplens.org/datasets/movielens/ml-100k.zip'
277	          ),
278	          parsing_fn=movielens_parsing.parse_100k_ratings_data,
279	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/movielens.py:291
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
290	          download_url=(
291	              'https://files.grouplens.org/datasets/movielens/ml-100k.zip'
292	          ),
293	          parsing_fn=movielens_parsing.parse_100k_movies_data,
294	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/movielens.py:313
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
312	          download_url=(
313	              'https://files.grouplens.org/datasets/movielens/ml-1m.zip'
314	          ),
315	          parsing_fn=movielens_parsing.parse_1m_ratings_data,
316	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/movielens.py:328
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
327	          download_url=(
328	              'https://files.grouplens.org/datasets/movielens/ml-1m.zip'
329	          ),
330	          parsing_fn=movielens_parsing.parse_1m_movies_data,
331	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/movielens.py:347
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
346	          download_url=(
347	              'https://files.grouplens.org/datasets/movielens/ml-20m.zip'
348	          ),
349	          parsing_fn=movielens_parsing.parse_current_ratings_data,
350	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/movielens.py:362
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
361	          download_url=(
362	              'https://files.grouplens.org/datasets/movielens/ml-20m.zip'
363	          ),
364	          parsing_fn=movielens_parsing.parse_current_movies_data,
365	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/movielens.py:466
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
465	        supervised_keys=None,
466	        homepage='https://grouplens.org/datasets/movielens/',
467	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/web_graph/web_graph.py:189
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
188	        supervised_keys=None,  # Set to `None` to disable
189	        homepage='https://arxiv.org/abs/2112.02194',
190	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/web_nlg/web_nlg.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_CITATION = """
28	@inproceedings{gardent2017creating,
29	    title = ""Creating Training Corpora for {NLG} Micro-Planners"",
30	    author = ""Gardent, Claire  and
31	      Shimorina, Anastasia  and
32	      Narayan, Shashi  and
33	      Perez-Beltrachini, Laura"",
34	    booktitle = ""Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
35	    month = jul,
36	    year = ""2017"",
37	    address = ""Vancouver, Canada"",
38	    publisher = ""Association for Computational Linguistics"",
39	    doi = ""10.18653/v1/P17-1017"",
40	    pages = ""179--188"",
41	    url = ""https://www.aclweb.org/anthology/P17-1017.pdf""
42	}
43	"""
44	
45	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/web_nlg/web_nlg.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	
45	_DESCRIPTION = """
46	The data contains sets of 1 to 7 triples of the form subject-predicate-object
47	extracted from (DBpedia)[https://wiki.dbpedia.org/] and natural language text
48	that's a verbalisation of these triples.
49	The test data spans 15 different domains where only 10 appear in the training
50	data.
51	The dataset follows a standarized table format.
52	"""
53	
54	_URL = 'https://drive.google.com/uc?export=download&id=1C3d0a1wPkJqI3SVyYNtl99J1lErYYhsc'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/web_nlg/web_nlg.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	
54	_URL = 'https://drive.google.com/uc?export=download&id=1C3d0a1wPkJqI3SVyYNtl99J1lErYYhsc'
55	
56	
57	class UnexpectedFormatError(Exception):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/web_nlg/web_nlg.py:88
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
87	        # Homepage of the dataset for documentation
88	        homepage='https://webnlg-challenge.loria.fr/challenge_2017/',
89	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/wiki_bio.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	
25	_CITATION = """
26	@inproceedings{lebret-etal-2016-neural,
27	    title = "Neural Text Generation from Structured Data with Application to the Biography Domain",
28	    author = "Lebret, R{\'e}mi  and
29	      Grangier, David  and
30	      Auli, Michael",
31	    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
32	    month = nov,
33	    year = "2016",
34	    address = "Austin, Texas",
35	    publisher = "Association for Computational Linguistics",
36	    url = "https://www.aclweb.org/anthology/D16-1128",
37	    doi = "10.18653/v1/D16-1128",
38	    pages = "1203--1213",
39	}
40	"""
41	
42	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/wiki_bio.py:48
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
47	
48	_URL = 'https://huggingface.co/datasets/wiki_bio/resolve/main/data/wikipedia-biography-dataset.zip'
49	
50	
51	def _get_table(infobox_line):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/wiki_bio.py:101
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
100	        supervised_keys=('input_text', 'target_text'),
101	        homepage='https://github.com/DavidGrangier/wikipedia-biography-dataset',
102	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/wiki_table_questions/wiki_table_questions.py:42
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
41	
42	_CITATION = """
43	@inproceedings{pasupat-liang-2015-compositional,
44	    title = "Compositional Semantic Parsing on Semi-Structured Tables",
45	    author = "Pasupat, Panupong  and
46	      Liang, Percy",
47	    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
48	    month = jul,
49	    year = "2015",
50	    address = "Beijing, China",
51	    publisher = "Association for Computational Linguistics",
52	    url = "https://www.aclweb.org/anthology/P15-1142",
53	    doi = "10.3115/v1/P15-1142",
54	    pages = "1470--1480",
55	}
56	"""
57	
58	_DOWNLOAD_URL = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/wiki_table_questions/wiki_table_questions.py:59
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
58	_DOWNLOAD_URL = (
59	    'https://github.com/ppasupat/WikiTableQuestions/archive/master.zip'
60	)
61	
62	
63	class WikiTableQuestions(tfds.core.GeneratorBasedBuilder):
64	  """DatasetBuilder for wiki_table_questions dataset."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/wiki_table_questions/wiki_table_questions.py:89
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
88	        supervised_keys=('input_text', 'target_text'),
89	        homepage='https://ppasupat.github.io/WikiTableQuestions/#usage-notes',
90	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/wiki_table_text/wiki_table_text.py:33
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
32	
33	_CITATION = """
34	@inproceedings{bao2018table,
35	  title={Table-to-Text: Describing Table Region with Natural Language},
36	  author={Junwei Bao and Duyu Tang and Nan Duan and Zhao Yan and Yuanhua Lv and Ming Zhou and Tiejun Zhao},
37	  booktitle={AAAI},
38	  url={https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16138/16782},
39	  year={2018}
40	}
41	"""
42	
43	_TRAIN_URL = 'https://raw.githubusercontent.com/msra-nlc/Table2Text/master/MSRA_NLC.Table2Text.train'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/wiki_table_text/wiki_table_text.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	
43	_TRAIN_URL = 'https://raw.githubusercontent.com/msra-nlc/Table2Text/master/MSRA_NLC.Table2Text.train'
44	_DEV_URL = 'https://raw.githubusercontent.com/msra-nlc/Table2Text/master/MSRA_NLC.Table2Text.dev'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/wiki_table_text/wiki_table_text.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	_TRAIN_URL = 'https://raw.githubusercontent.com/msra-nlc/Table2Text/master/MSRA_NLC.Table2Text.train'
44	_DEV_URL = 'https://raw.githubusercontent.com/msra-nlc/Table2Text/master/MSRA_NLC.Table2Text.dev'
45	_TEST_URL = 'https://raw.githubusercontent.com/msra-nlc/Table2Text/master/MSRA_NLC.Table2Text.test'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/wiki_table_text/wiki_table_text.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	_DEV_URL = 'https://raw.githubusercontent.com/msra-nlc/Table2Text/master/MSRA_NLC.Table2Text.dev'
45	_TEST_URL = 'https://raw.githubusercontent.com/msra-nlc/Table2Text/master/MSRA_NLC.Table2Text.test'
46	
47	
48	class WikiTableText(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/wiki_table_text/wiki_table_text.py:72
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
71	        supervised_keys=('input_text', 'target_text'),
72	        homepage='https://github.com/msra-nlc/Table2Text',
73	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/wine_quality/wine_quality.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_CITATION = """
28	@ONLINE {cortezpaulo;cerdeiraantonio;almeidafernando;matostelmo;reisjose1999,
29	    author = "Cortez, Paulo; Cerdeira, Antonio; Almeida,Fernando;  Matos, Telmo;  Reis, Jose",
30	    title  = "Modeling wine preferences by data mining from physicochemical properties.",
31	    year   = "2009",
32	    url    = "https://archive.ics.uci.edu/ml/datasets/wine+quality"
33	}
34	"""
35	
36	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/wine_quality/wine_quality.py:36
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
35	
36	_DESCRIPTION = """
37	Two datasets were created, using red and white wine samples.
38	The inputs include objective tests (e.g. PH values) and the output is based on sensory data
39	(median of at least 3 evaluations made by wine experts).
40	Each expert graded the wine quality
41	between 0 (very bad) and 10 (very excellent).
42	Several data mining methods were applied to model
43	these datasets under a regression approach. The support vector machine model achieved the
44	best results. Several metrics were computed: MAD, confusion matrix for a fixed error tolerance (T),
45	etc. Also, we plot the relative importances of the input variables (as measured by a sensitivity
46	analysis procedure).
47	
48	The two datasets are related to red and white variants of the Portuguese "Vinho Verde" wine.
49	For more details, consult: http://www.vinhoverde.pt/en/ or the reference [Cortez et al., 2009].
50	Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables
51	are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).
52	
53	Number of Instances: red wine - 1599; white wine - 4898
54	
55	Input variables (based on physicochemical tests):
56	
57	1. fixed acidity
58	2. volatile acidity
59	3. citric acid
60	4. residual sugar
61	5. chlorides
62	6. free sulfur dioxide
63	7. total sulfur dioxide
64	8. density
65	9. pH
66	10. sulphates
67	11. alcohol
68	
69	Output variable (based on sensory data):
70	
71	12. quality (score between 0 and 10)
72	
73	"""
74	
75	_DOWNLOAD_URL_WHITE_WINES = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/wine_quality/wine_quality.py:75
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
74	
75	_DOWNLOAD_URL_WHITE_WINES = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
76	_DOWNLOAD_URL_RED_WINES = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/wine_quality/wine_quality.py:76
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
75	_DOWNLOAD_URL_WHITE_WINES = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
76	_DOWNLOAD_URL_RED_WINES = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
77	_HOMEPAGE_URL = "https://archive.ics.uci.edu/ml/datasets/wine+quality"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/structured/wine_quality/wine_quality.py:77
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
76	_DOWNLOAD_URL_RED_WINES = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
77	_HOMEPAGE_URL = "https://archive.ics.uci.edu/ml/datasets/wine+quality"
78	
79	
80	class WineQualityConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/cnn_dailymail.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	# introduces the specific form (non-anonymized) we use here.
37	_CITATION = """\
38	@article{DBLP:journals/corr/SeeLM17,
39	  author    = {Abigail See and
40	               Peter J. Liu and
41	               Christopher D. Manning},
42	  title     = {Get To The Point: Summarization with Pointer-Generator Networks},
43	  journal   = {CoRR},
44	  volume    = {abs/1704.04368},
45	  year      = {2017},
46	  url       = {http://arxiv.org/abs/1704.04368},
47	  archivePrefix = {arXiv},
48	  eprint    = {1704.04368},
49	  timestamp = {Mon, 13 Aug 2018 16:46:08 +0200},
50	  biburl    = {https://dblp.org/rec/bib/journals/corr/SeeLM17},
51	  bibsource = {dblp computer science bibliography, https://dblp.org}
52	}
53	
54	@inproceedings{hermann2015teaching,
55	  title={Teaching machines to read and comprehend},
56	  author={Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
57	  booktitle={Advances in neural information processing systems},
58	  pages={1693--1701},
59	  year={2015}
60	}
61	"""
62	
63	_DL_URLS = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/cnn_dailymail.py:65
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
64	    # pylint: disable=line-too-long
65	    'cnn_stories': 'https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ',
66	    'dm_stories': 'https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfM1BxdkxVaTY2bWs',
67	    'test_urls': 'https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_test.txt',
68	    'train_urls': 'https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_train.txt',
69	    'val_urls': 'https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_val.txt',
70	    # pylint: enable=line-too-long
71	}
72	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/cnn_dailymail.py:66
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
65	    'cnn_stories': 'https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ',
66	    'dm_stories': 'https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfM1BxdkxVaTY2bWs',
67	    'test_urls': 'https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_test.txt',
68	    'train_urls': 'https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_train.txt',
69	    'val_urls': 'https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_val.txt',
70	    # pylint: enable=line-too-long
71	}
72	
73	_HIGHLIGHTS = 'highlights'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/cnn_dailymail.py:67
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
66	    'dm_stories': 'https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfM1BxdkxVaTY2bWs',
67	    'test_urls': 'https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_test.txt',
68	    'train_urls': 'https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_train.txt',
69	    'val_urls': 'https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_val.txt',
70	    # pylint: enable=line-too-long
71	}
72	
73	_HIGHLIGHTS = 'highlights'
74	_ARTICLE = 'article'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/cnn_dailymail.py:68
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
67	    'test_urls': 'https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_test.txt',
68	    'train_urls': 'https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_train.txt',
69	    'val_urls': 'https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_val.txt',
70	    # pylint: enable=line-too-long
71	}
72	
73	_HIGHLIGHTS = 'highlights'
74	_ARTICLE = 'article'
75	_PUBLISHER = 'publisher'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/cnn_dailymail.py:69
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
68	    'train_urls': 'https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_train.txt',
69	    'val_urls': 'https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_val.txt',
70	    # pylint: enable=line-too-long
71	}
72	
73	_HIGHLIGHTS = 'highlights'
74	_ARTICLE = 'article'
75	_PUBLISHER = 'publisher'
76	_ID = 'id'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/cnn_dailymail.py:206
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
205	  RELEASE_NOTES = {
206	      '1.0.0': 'New split API (https://tensorflow.org/datasets/splits)',
207	      '2.0.0': """
208	      Separate target sentences with newline. (Having the model predict newline
209	      separators makes it easier to evaluate using summary-level ROUGE.)
210	      """,
211	      '3.0.0': 'Using cased version.',
212	      '3.1.0': 'Removed BuilderConfig',
213	      '3.2.0': """
214	      Remove extra space before added sentence period.
215	      This shouldn't affect ROUGE scores because punctuation is removed.
216	      """,
217	      '3.3.0': 'Add publisher feature.',
218	      '3.4.0': 'Add ID feature.',
219	  }
220	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/cnn_dailymail.py:233
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
232	        supervised_keys=(_ARTICLE, _HIGHLIGHTS),
233	        homepage='https://github.com/abisee/cnn-dailymail',
234	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/cnn_dailymail_test.py:53
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
52	    with tempfile.NamedTemporaryFile(delete=True) as f:
53	      f.write(_STORY_FILE)
54	      f.flush()

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/covid19sum.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_CITATION = """
29	@ONLINE {CORD-19-research-challenge,
30	    author = "An AI challenge with AI2, CZI, MSR, Georgetown, NIH & The White House",
31	    title  = "COVID-19 Open Research Dataset Challenge (CORD-19)",
32	    month  = "april",
33	    year   = "2020",
34	    url    = "https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge"
35	}
36	"""
37	
38	_HOMEPAGE = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/covid19sum.py:39
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
38	_HOMEPAGE = (
39	    "https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge"
40	)
41	
42	_DESCRIPTION = """
43	CORD-19 is a resource of over 45,000 scholarly articles, including over 33,000

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/gigaword.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_CITATION = """
24	@article{graff2003english,
25	  title={English gigaword},
26	  author={Graff, David and Kong, Junbo and Chen, Ke and Maeda, Kazuaki},
27	  journal={Linguistic Data Consortium, Philadelphia},
28	  volume={4},
29	  number={1},
30	  pages={34},
31	  year={2003}
32	}
33	
34	@article{Rush_2015,
35	   title={A Neural Attention Model for Abstractive Sentence Summarization},
36	   url={http://dx.doi.org/10.18653/v1/D15-1044},
37	   DOI={10.18653/v1/d15-1044},
38	   journal={Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
39	   publisher={Association for Computational Linguistics},
40	   author={Rush, Alexander M. and Chopra, Sumit and Weston, Jason},
41	   year={2015}
42	}
43	"""
44	
45	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/gigaword.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	
45	_DESCRIPTION = """
46	Headline-generation on a corpus of article pairs from Gigaword consisting of
47	around 4 million articles. Use the 'org_data' provided by
48	https://github.com/microsoft/unilm/ which is identical to
49	https://github.com/harvardnlp/sent-summary but with better format.
50	
51	There are two features:
52	  - document: article.
53	  - summary: headline.
54	
55	"""
56	
57	_URL = "https://drive.google.com/uc?export=download&id=1USoQ8lJgN8kAWnUnRrupMGrPMLlDVqlV"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/gigaword.py:57
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
56	
57	_URL = "https://drive.google.com/uc?export=download&id=1USoQ8lJgN8kAWnUnRrupMGrPMLlDVqlV"
58	
59	_DOCUMENT = "document"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/gigaword.py:79
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
78	        supervised_keys=(_DOCUMENT, _SUMMARY),
79	        homepage="https://github.com/harvardnlp/sent-summary",
80	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/gov_report/gov_report.py:32
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
31	
32	_CITATION = """
33	@inproceedings{
34	anonymous2022efficiently,
35	title={Efficiently Modeling Long Sequences with Structured State Spaces},
36	author={Anonymous},
37	booktitle={Submitted to The Tenth International Conference on Learning Representations },
38	year={2022},
39	url={https://openreview.net/forum?id=uYLFoz1vlAC},
40	note={under review}
41	}
42	"""
43	
44	_HOMEPAGE = "https://gov-report-data.github.io/"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/gov_report/gov_report.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	
44	_HOMEPAGE = "https://gov-report-data.github.io/"
45	_URL = "https://drive.google.com/uc?export=download&id=1ik8uUVeIU-ky63vlnvxtfN2ZN-TUeov2"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/gov_report/gov_report.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	_HOMEPAGE = "https://gov-report-data.github.io/"
45	_URL = "https://drive.google.com/uc?export=download&id=1ik8uUVeIU-ky63vlnvxtfN2ZN-TUeov2"
46	
47	_SUBSET_CRS = "crs"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/multi_news.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	
45	_URL = "https://drive.google.com/uc?export=download&id=1vRY2wM6rlOZrf9exGTm5pXj5ExlVwJ0C"
46	
47	_DOCUMENT = "document"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/multi_news.py:64
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
63	        supervised_keys=(_DOCUMENT, _SUMMARY),
64	        homepage="https://github.com/Alex-Fabbri/Multi-News",
65	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/wikihow.py:36
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
35	
36	_DESCRIPTION = """
37	WikiHow is a new large-scale dataset using the online WikiHow
38	(http://www.wikihow.com/) knowledge base.
39	
40	There are two features:
41	  - text: wikihow answers texts.
42	  - headline: bold lines as summary.
43	
44	There are two separate versions:
45	  - all: consisting of the concatenation of all paragraphs as the articles and
46	         the bold lines as the reference summaries.
47	  - sep: consisting of each paragraph and its summary.
48	
49	Download "wikihowAll.csv" and "wikihowSep.csv" from
50	https://github.com/mahnazkoupaee/WikiHow-Dataset and place them in manual folder
51	https://www.tensorflow.org/datasets/api_docs/python/tfds/download/DownloadConfig.
52	Train/validation/test splits are provided by the authors.
53	Preprocessing is applied to remove short articles
54	(abstract length < 0.75 article length) and clean up extra commas.
55	"""
56	
57	_DOCUMENT = "text"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/wikihow.py:61
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
60	_URLS = {
61	    "train": "https://raw.githubusercontent.com/mahnazkoupaee/WikiHow-Dataset/master/all_train.txt",
62	    "validation": "https://raw.githubusercontent.com/mahnazkoupaee/WikiHow-Dataset/master/all_val.txt",
63	    "test": "https://raw.githubusercontent.com/mahnazkoupaee/WikiHow-Dataset/master/all_test.txt",
64	}
65	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/wikihow.py:62
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
61	    "train": "https://raw.githubusercontent.com/mahnazkoupaee/WikiHow-Dataset/master/all_train.txt",
62	    "validation": "https://raw.githubusercontent.com/mahnazkoupaee/WikiHow-Dataset/master/all_val.txt",
63	    "test": "https://raw.githubusercontent.com/mahnazkoupaee/WikiHow-Dataset/master/all_test.txt",
64	}
65	
66	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/wikihow.py:63
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
62	    "validation": "https://raw.githubusercontent.com/mahnazkoupaee/WikiHow-Dataset/master/all_val.txt",
63	    "test": "https://raw.githubusercontent.com/mahnazkoupaee/WikiHow-Dataset/master/all_test.txt",
64	}
65	
66	
67	class WikihowConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/wikihow.py:88
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
87	
88	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
89	  Links to files can be found on https://github.com/mahnazkoupaee/WikiHow-Dataset
90	  Please download both wikihowAll.csv and wikihowSep.csv.
91	  """
92	
93	  BUILDER_CONFIGS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/wikihow.py:120
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
119	        supervised_keys=(_DOCUMENT, _SUMMARY),
120	        homepage="https://github.com/mahnazkoupaee/WikiHow-Dataset",
121	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/xsum.py:36
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
35	
36	_DESCRIPTION = """
37	Extreme Summarization (XSum) Dataset.
38	
39	There are two features:
40	  - document: Input news article.
41	  - summary: One sentence summary of the article.
42	
43	This data need to manaully downloaded and extracted as described in
44	https://github.com/EdinburghNLP/XSum/blob/master/XSum-Dataset/README.md.
45	The folder 'xsum-extracts-from-downloads' need to be compressed as
46	'xsum-extracts-from-downloads.tar.gz' and put in manually downloaded folder.
47	"""
48	
49	_URL = "https://raw.githubusercontent.com/EdinburghNLP/XSum/master/XSum-Dataset/XSum-TRAINING-DEV-TEST-SPLIT-90-5-5.json"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/xsum.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
48	
49	_URL = "https://raw.githubusercontent.com/EdinburghNLP/XSum/master/XSum-Dataset/XSum-TRAINING-DEV-TEST-SPLIT-90-5-5.json"
50	
51	_DOCUMENT = "document"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/xsum.py:79
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
78	
79	  MANUAL_DOWNLOAD_INSTRUCTIONS = """\
80	  Detailed download instructions (which require running a custom script) are
81	  here:
82	  https://github.com/EdinburghNLP/XSum/blob/master/XSum-Dataset/README.md#running-the-download-and-extraction-scripts
83	  Afterwards, please put xsum-extracts-from-downloads.tar.gz file in the manual_dir.
84	  """
85	
86	  def _info(self):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/summarization/xsum.py:96
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
95	        homepage=(
96	            "https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset"
97	        ),
98	        citation=_CITATION,
99	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/testing/dataset_builder_testing.py:399
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
398	    err_msg = (
399	        "Did you forget to record checksums with `--register_checksums` ? See"
400	        " instructions at:"
401	        " https://www.tensorflow.org/datasets/add_dataset#run_the_generation_codeIf"
402	        " want to opt-out of checksums validation, please add `SKIP_CHECKSUMS ="
403	        " True` to the `DatasetBuilderTestCase`.\n"
404	    )
405	    url_infos = self.dataset_class.url_infos
406	    filepath = self.dataset_class._checksums_path  # pylint: disable=protected-access

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/testing/fake_data_utils.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
46	  fobj = tempfile.NamedTemporaryFile(delete=False, mode='wb', suffix='.JPEG')
47	  fobj.write(res)
48	  fobj.close()

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/testing/fake_data_utils.py:65
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
64	  fobj = tempfile.NamedTemporaryFile(delete=False, mode='wb', suffix='.PNG')
65	  fobj.write(res)
66	  fobj.close()

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/testing/fake_data_utils.py:89
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
88	  with tempfile.NamedTemporaryFile(delete=False, mode='wb', suffix='.wav') as f:
89	    f.write(res)
90	  return f.name

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/testing/fake_data_utils.py:102
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
101	  with tempfile.NamedTemporaryFile(delete=False, mode='wb', suffix='.wav') as f:
102	    f.write(res)
103	  return f.name

--------------------------------------------------
>> Issue: [B815:write] os.write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/testing/test_utils.py:605
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b815_write.html
604	    with tf.io.gfile.GFile(fpath, 'w') as f:
605	      f.write(competition_or_dataset)
606	    return 'Downloading {} to {}'.format(competition_or_dataset, fpath)

--------------------------------------------------
>> Issue: [B832:write] tempfile_NamedTemporaryFile_write
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/testing/test_utils.py:605
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b832_write.html
604	    with tf.io.gfile.GFile(fpath, 'w') as f:
605	      f.write(competition_or_dataset)
606	    return 'Downloading {} to {}'.format(competition_or_dataset, fpath)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/testing/test_utils.py:874
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
873	        ),
874	        url='https://dummy_url',
875	        distribution=distribution,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/testing/test_utils_test.py:275
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
274	    assert dataset.metadata.description == 'Dummy description.'
275	    assert dataset.metadata.url == 'https://dummy_url'
276	    assert dataset.metadata.version == '1.2.0'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/c4.py:33
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
32	
33	_DESCRIPTION = """\
34	A colossal, cleaned version of Common Crawl's web crawl corpus.
35	
36	Based on Common Crawl dataset: https://commoncrawl.org
37	
38	To generate this dataset, please follow
39	[the instructions from t5](https://github.com/google-research/text-to-text-transfer-transformer#c4).
40	
41	Due to the overhead of cleaning the dataset, it is recommend you prepare it with
42	a distributed service like Cloud Dataflow. More info at
43	https://www.tensorflow.org/datasets/beam_datasets.
44	"""
45	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/c4.py:84
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
83	
84	_DOWNLOAD_HOST = "https://data.commoncrawl.org"
85	_WET_PATH_URL = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/c4.py:86
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
85	_WET_PATH_URL = (
86	    "https://data.commoncrawl.org/crawl-data/CC-MAIN-{cc_version}/wet.paths.gz"
87	)
88	_REALNEWS_DOMAINS_URL = "https://raw.githubusercontent.com/rowanz/grover/38f7184bd87237ae2d3bc330b99f1e2e246f6d51/realnews/domain_to_allowed_subdomains.json"
89	_OPENWEBTEXT_URLS_ZIP = "OpenWebText.zip"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/c4.py:88
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
87	)
88	_REALNEWS_DOMAINS_URL = "https://raw.githubusercontent.com/rowanz/grover/38f7184bd87237ae2d3bc330b99f1e2e246f6d51/realnews/domain_to_allowed_subdomains.json"
89	_OPENWEBTEXT_URLS_ZIP = "OpenWebText.zip"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/c4.py:90
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
89	_OPENWEBTEXT_URLS_ZIP = "OpenWebText.zip"
90	_OPENWEBTEXT_URLS_URL = "https://mega.nz/#F!EZZD0YwJ!9_PlEQzdMVLaNdKv_ICNVQ"
91	_OPENWEBTEXT_URLS_FILE_PATTERN = "OpenWebText/Version 1/URLs/*.txt"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/c4.py:92
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
91	_OPENWEBTEXT_URLS_FILE_PATTERN = "OpenWebText/Version 1/URLs/*.txt"
92	_EN_BADWORDS_URL = "https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/25e679f03d96baa721cde20db9944649e8d0a844/en"
93	_BADWORDS_URL = "https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/5faf2ba42d7b1c0977169ec3611df25a3c08eb13/{lang}"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/c4.py:93
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
92	_EN_BADWORDS_URL = "https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/25e679f03d96baa721cde20db9944649e8d0a844/en"
93	_BADWORDS_URL = "https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/5faf2ba42d7b1c0977169ec3611df25a3c08eb13/{lang}"
94	_BADWORDS_LANGS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/c4.py:465
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
464	          description=(
465	              "Filters from the default config to only include content from the"
466	              " URLs in OpenWebText"
467	              " (https://github.com/jcpeterson/openwebtext)."

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/c4.py:498
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
497	        citation=_CITATION,
498	        homepage="https://github.com/google-research/text-to-text-transfer-transformer#datasets",
499	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/c4_wsrs/c4_wsrs.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	
30	_DESCRIPTION = """\
31	A medical abbreviation expansion dataset which applies web-scale reverse
32	substitution (wsrs) to the C4 dataset, which is a colossal, cleaned version of
33	Common Crawl's web crawl corpus.
34	
35	The original source is the Common Crawl dataset: https://commoncrawl.org
36	"""
37	
38	
39	class C4WSRSConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/c4_wsrs/c4_wsrs.py:123
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
122	        features=tfds.features.FeaturesDict(features),
123	        homepage='https://github.com/google-research/google-research/tree/master/deciphering_clinical_abbreviations',
124	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/c4_wsrs/c4_wsrs_test.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	                'timestamp': 'this is a timestamp',
55	                'url': 'http://google.com/1',
56	            },
57	        ),
58	        tf.nest.map_structure(
59	            tf.convert_to_tensor,
60	            {
61	                'content-length': 'abc',
62	                'content-type': 'text/plain',
63	                'text': (
64	                    'magnetic resonance imaging test in patient with '

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/c4_wsrs/c4_wsrs_test.py:68
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
67	                'timestamp': 'this is a timestamp',
68	                'url': 'http://google.com/2',
69	            },
70	        ),
71	    ]
72	
73	    cls._exit_stack = contextlib.ExitStack().__enter__()
74	    cls._exit_stack.enter_context(
75	        mock.patch.object(tfds, 'builder', autospec=True)
76	    )
77	    cls._exit_stack.enter_context(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/cfq.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_CITATION = """
27	@inproceedings{Keysers2020,
28	  title={Measuring Compositional Generalization: A Comprehensive Method on
29	         Realistic Data},
30	  author={Daniel Keysers and Nathanael Sch\"{a}rli and Nathan Scales and
31	          Hylke Buisman and Daniel Furrer and Sergii Kashubin and
32	          Nikola Momchev and Danila Sinopalnikov and Lukasz Stafiniak and
33	          Tibor Tihon and Dmitry Tsarkov and Xiao Wang and Marc van Zee and
34	          Olivier Bousquet},
35	  booktitle={ICLR},
36	  year={2020},
37	  url={https://arxiv.org/abs/1912.09713.pdf},
38	}
39	"""
40	
41	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/cfq.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	
41	_DESCRIPTION = """
42	The CFQ dataset (and it's splits) for measuring compositional generalization.
43	
44	See https://arxiv.org/abs/1912.09713.pdf for background.
45	
46	A note about the validation set: Since it has the same distribution as the test
47	set and we are interested in measuring the compositional generalization of a
48	*model* with respect to an *unknown* test distribution we suggest that any
49	tuning should be done on a subset of the train set only (see section 5.1 of the
50	paper).
51	
52	Example usage:
53	
54	```
55	data = tfds.load('cfq/mcd1')
56	```
57	"""
58	
59	_DATA_URL = 'https://storage.googleapis.com/cfq_dataset/cfq.tar.gz'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/cfq.py:59
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
58	
59	_DATA_URL = 'https://storage.googleapis.com/cfq_dataset/cfq.tar.gz'
60	
61	_RANDOM_SEEDS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/cfq.py:169
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
168	        homepage=(
169	            'https://github.com/google-research/google-research/tree/master/cfq'
170	        ),
171	        citation=_CITATION,
172	    )

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/cfq.py:236
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
235	        logging.info('Reading json from %s into memory...', samples_path)
236	        samples = json.loads(self._scrub_json(samples_file.read()))
237	        logging.info('%d samples loaded', len(samples))

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/civil_comments.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	# General (main) citation for CivilComments and CivilCommentsIdentities.
29	_CITATION = """
30	@article{DBLP:journals/corr/abs-1903-04561,
31	  author    = {Daniel Borkan and
32	               Lucas Dixon and
33	               Jeffrey Sorensen and
34	               Nithum Thain and
35	               Lucy Vasserman},
36	  title     = {Nuanced Metrics for Measuring Unintended Bias with Real Data for Text
37	               Classification},
38	  journal   = {CoRR},
39	  volume    = {abs/1903.04561},
40	  year      = {2019},
41	  url       = {http://arxiv.org/abs/1903.04561},
42	  archivePrefix = {arXiv},
43	  eprint    = {1903.04561},
44	  timestamp = {Sun, 31 Mar 2019 19:01:24 +0200},
45	  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-04561},
46	  bibsource = {dblp computer science bibliography, https://dblp.org}
47	}
48	"""
49	
50	# Citation for CivilCommentsCovert.
51	_COVERT_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/civil_comments.py:51
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
50	# Citation for CivilCommentsCovert.
51	_COVERT_CITATION = """
52	@inproceedings{lees-etal-2021-capturing,
53	    title = "Capturing Covertly Toxic Speech via Crowdsourcing",
54	    author = "Lees, Alyssa  and
55	      Borkan, Daniel  and
56	      Kivlichan, Ian  and
57	      Nario, Jorge  and
58	      Goyal, Tesh",
59	    booktitle = "Proceedings of the First Workshop on Bridging Human{--}Computer Interaction and Natural Language Processing",
60	    month = apr,
61	    year = "2021",
62	    address = "Online",
63	    publisher = "Association for Computational Linguistics",
64	    url = "https://www.aclweb.org/anthology/2021.hcinlp-1.3",
65	    pages = "14--20"
66	}
67	"""
68	
69	# Citation for CivilComments Toxic Spans.
70	_SPANS_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/civil_comments.py:70
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
69	# Citation for CivilComments Toxic Spans.
70	_SPANS_CITATION = """
71	@inproceedings{pavlopoulos-etal-2021-semeval,
72	    title = "{S}em{E}val-2021 Task 5: Toxic Spans Detection",
73	    author = "Pavlopoulos, John  and Sorensen, Jeffrey  and Laugier, L{\'e}o and Androutsopoulos, Ion",
74	    booktitle = "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
75	    month = aug,
76	    year = "2021",
77	    address = "Online",
78	    publisher = "Association for Computational Linguistics",
79	    url = "https://aclanthology.org/2021.semeval-1.6",
80	    doi = "10.18653/v1/2021.semeval-1.6",
81	    pages = "59--69",
82	}
83	"""
84	
85	# Citation for CivilComments Context.
86	_CONTEXT_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/civil_comments.py:141
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
140	
141	_CC_COVERT_DESCRIPTION = """
142	WARNING: there's a potential data quality issue with CivilCommentsCovert that
143	we're actively working on fixing (06/28/22); the underlying data may change!
144	
145	The CivilCommentsCovert set is a subset of CivilCommentsIdentities with ~20% of
146	the train and test splits further annotated for covert offensiveness, in
147	addition to the toxicity and identity labels. Raters were asked to categorize
148	comments as one of explicitly, implicitly, not, or not sure if offensive, as
149	well as whether it contained different types of covert offensiveness. The full
150	annotation procedure is detailed in a forthcoming paper at
151	https://sites.google.com/corp/view/hciandnlp/accepted-papers.
152	"""
153	
154	_CC_SPANS_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/civil_comments.py:167
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
166	
167	_DOWNLOAD_URL = 'https://storage.googleapis.com/jigsaw-unintended-bias-in-toxicity-classification/civil_comments_v1.2.zip'
168	
169	IDENTITY_LABELS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/civil_comments.py:363
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
362	        supervised_keys=('text', supervised_value),
363	        homepage='https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data',
364	        citation=citation,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/clinc_oos.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_DOWNLOAD_URL = 'https://github.com/jereliu/datasets/raw/master/clinc_oos.zip'
28	
29	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/clinc_oos.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	
29	_CITATION = """
30	@inproceedings{larson-etal-2019-evaluation,
31	    title = "An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction",
32	    author = "Larson, Stefan  and
33	      Mahendran, Anish  and
34	      Peper, Joseph J.  and
35	      Clarke, Christopher  and
36	      Lee, Andrew  and
37	      Hill, Parker  and
38	      Kummerfeld, Jonathan K.  and
39	      Leach, Kevin  and
40	      Laurenzano, Michael A.  and
41	      Tang, Lingjia  and
42	      Mars, Jason",
43	    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
44	    month = nov,
45	    year = "2019",
46	    address = "Hong Kong, China",
47	    publisher = "Association for Computational Linguistics",
48	    url = "https://www.aclweb.org/anthology/D19-1131",
49	    doi = "10.18653/v1/D19-1131",
50	    pages = "1311--1316",
51	}
52	"""
53	
54	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/clinc_oos.py:90
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
89	        supervised_keys=('text', 'intent'),
90	        homepage='https://github.com/clinc/oos-eval/',
91	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/conll2002/conll2002.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	
35	_CITATION = """
36	@inproceedings{tjong-kim-sang-2002-introduction,
37	    title = "Introduction to the {C}o{NLL}-2002 Shared Task: Language-Independent Named Entity Recognition",
38	    author = "Tjong Kim Sang, Erik F.",
39	    booktitle = "{COLING}-02: The 6th Conference on Natural Language Learning 2002 ({C}o{NLL}-2002)",
40	    year = "2002",
41	    url = "https://aclanthology.org/W02-2024",
42	}
43	"""
44	
45	_URL = "https://raw.githubusercontent.com/teropa/nlp/master/resources/corpora/conll2002/"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/conll2002/conll2002.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	
45	_URL = "https://raw.githubusercontent.com/teropa/nlp/master/resources/corpora/conll2002/"
46	
47	ES_POS_TAGS = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/conll2002/conll2002.py:159
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
158	        description=_DESCRIPTION,
159	        homepage="https://aclanthology.org/W02-2024/",
160	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/conll2003/conll2003.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_CITATION = """
29	@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,
30	    title = "Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition",
31	    author = "Tjong Kim Sang, Erik F.  and
32	      De Meulder, Fien",
33	    booktitle = "Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003",
34	    year = "2003",
35	    url = "https://www.aclweb.org/anthology/W03-0419",
36	    pages = "142--147",
37	}
38	"""
39	
40	
41	class Conll2003(tfds.dataset_builders.ConllDatasetBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/conll2003/conll2003.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	        description=_DESCRIPTION,
54	        homepage='https://www.aclweb.org/anthology/W03-0419/',
55	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/conll2003/conll2003.py:61
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
60	    path = dl_manager.download_and_extract(
61	        'https://data.deepai.org/conll2003.zip'
62	    )
63	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/cos_e.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	
25	_CITATION = """
26	@inproceedings{rajani2019explain,
27	     title = "Explain Yourself! Leveraging Language models for Commonsense Reasoning",
28	    author = "Rajani, Nazneen Fatema  and
29	      McCann, Bryan  and
30	      Xiong, Caiming  and
31	      Socher, Richard",
32	      year="2019",
33	    booktitle = "Proceedings of the 2019 Conference of the Association for Computational Linguistics (ACL2019)",
34	    url ="https://arxiv.org/abs/1906.02361"
35	}
36	"""
37	
38	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/cos_e.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	
44	_COS_E_URL = "https://raw.githubusercontent.com/salesforce/cos-e/master/data/"
45	
46	# COS E has explanations for the CQA dataset, which is joined by ID.
47	_CQA_URL_TRAIN = "https://s3.amazonaws.com/commensenseqa/train_rand_split.jsonl"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/cos_e.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	# COS E has explanations for the CQA dataset, which is joined by ID.
47	_CQA_URL_TRAIN = "https://s3.amazonaws.com/commensenseqa/train_rand_split.jsonl"
48	_CQA_URL_DEV = "https://s3.amazonaws.com/commensenseqa/dev_rand_split.jsonl"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/cos_e.py:48
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
47	_CQA_URL_TRAIN = "https://s3.amazonaws.com/commensenseqa/train_rand_split.jsonl"
48	_CQA_URL_DEV = "https://s3.amazonaws.com/commensenseqa/dev_rand_split.jsonl"
49	_CQA_URL_TEST = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/cos_e.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	_CQA_URL_TEST = (
50	    "https://s3.amazonaws.com/commensenseqa/test_rand_split_no_answers.jsonl"
51	)
52	
53	
54	def _download_and_index_cqa(dl_manager):
55	  """Downloads CQA and returns it, indexed by id, for joining with Cos-E."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/cos_e.py:109
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
108	        supervised_keys=None,
109	        homepage="https://github.com/salesforce/cos-e",
110	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/definite_pronoun_resolution.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	
43	_DATA_URL_PATTERN = 'https://s3.amazonaws.com/datasets.huggingface.co/definite_pronoun_resolution/{}.c.txt'
44	
45	
46	class DefinitePronounResolution(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/definite_pronoun_resolution.py:64
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
63	        supervised_keys=('sentence', 'label'),
64	        homepage='https://www.hlt.utdallas.edu/~vince/data/emnlp12/',
65	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/docnli/docnli.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	_DOCNLI_URL = (
47	    'https://drive.google.com/uc?export=download&id='
48	    '16TZBTZcb9laNKxIvgbs5nOBgq3MhND5s'
49	)
50	
51	_EXTRACT_PATH_TOKEN = 'DocNLI_dataset'
52	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/docnli/docnli.py:81
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
80	        supervised_keys=None,  # Set to `None` to disable
81	        homepage='https://github.com/salesforce/DocNLI/',
82	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/dolphin_number_word/dolphin_number_word.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	
25	_DS_PATH = 'https://www.microsoft.com/en-us/research/uploads/prod/2016/02//dolphin-number_word_std.zip'
26	
27	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/dolphin_number_word/dolphin_number_word.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_DESCRIPTION = """
28	Dolphin Math Word Problem dataset (2015), as presented in https://www.microsoft.com/en-us/research/uploads/prod/2016/02//dolphin-sigmadolphin.datasets.pdf
29	"""
30	
31	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/dolphin_number_word/dolphin_number_word.py:77
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
76	        homepage=(
77	            'https://www.microsoft.com/en-us/research/project/sigmadolphin-2/'
78	        ),
79	        citation=_CITATION,
80	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/drop/drop.py:42
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
41	_DATA_LINK = (
42	    "https://s3-us-west-2.amazonaws.com/allennlp/datasets/drop/drop_dataset.zip"
43	)
44	
45	
46	def _get_answer(answer_dict):
47	  if answer_dict.get("number", ""):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/drop/drop.py:81
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
80	        }),
81	        homepage="https://allennlp.org/drop",
82	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/eraser_multi_rc.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	
47	_DOWNLOAD_URL = 'http://www.eraserbenchmark.com/zipped/multirc.tar.gz'
48	
49	
50	class EraserMultiRc(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/eraser_multi_rc.py:66
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
65	        supervised_keys=None,
66	        homepage='https://cogcomp.seas.upenn.edu/multirc/',
67	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/eraser_multi_rc.py:117
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
116	        with epath.Path(passage_file).open() as f1:
117	          passage_text = f1.read()
118	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/esnli.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_CITATION = """
25	@incollection{NIPS2018_8163,
26	title = {e-SNLI: Natural Language Inference with Natural Language Explanations},
27	author = {Camburu, Oana-Maria and Rockt\"{a}schel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},
28	booktitle = {Advances in Neural Information Processing Systems 31},
29	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
30	pages = {9539--9549},
31	year = {2018},
32	publisher = {Curran Associates, Inc.},
33	url = {http://papers.nips.cc/paper/8163-e-snli-natural-language-inference-with-natural-language-explanations.pdf}
34	}
35	"""
36	
37	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/esnli.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	_URL = (
43	    'https://raw.githubusercontent.com/OanaMariaCamburu/e-SNLI/master/dataset/'
44	)
45	
46	
47	class Esnli(tfds.core.GeneratorBasedBuilder):
48	  """e-SNLI: Natural Language Inference with Natural Language Explanations corpus."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/esnli.py:72
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
71	        supervised_keys=None,
72	        homepage='https://github.com/OanaMariaCamburu/e-SNLI',
73	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gap.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_CITATION = """
28	@article{DBLP:journals/corr/abs-1810-05201,
29	  author    = {Kellie Webster and
30	               Marta Recasens and
31	               Vera Axelrod and
32	               Jason Baldridge},
33	  title     = {Mind the {GAP:} {A} Balanced Corpus of Gendered Ambiguous Pronouns},
34	  journal   = {CoRR},
35	  volume    = {abs/1810.05201},
36	  year      = {2018},
37	  url       = {http://arxiv.org/abs/1810.05201},
38	  archivePrefix = {arXiv},
39	  eprint    = {1810.05201},
40	  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
41	  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-05201},
42	  bibsource = {dblp computer science bibliography, https://dblp.org}
43	}
44	"""
45	
46	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gap.py:53
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
52	
53	_TRAINURL = 'https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv'
54	_VALIDATIONURL = 'https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gap.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	_TRAINURL = 'https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv'
54	_VALIDATIONURL = 'https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv'
55	_TESTURL = 'https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gap.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	_VALIDATIONURL = 'https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv'
55	_TESTURL = 'https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv'
56	
57	
58	class Gap(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gap.py:89
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
88	        supervised_keys=None,
89	        homepage='https://github.com/google-research-datasets/gap-coreference',
90	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	
30	_DESCRIPTION = """
31	**GEM** is a benchmark environment for Natural Language Generation with a focus
32	on its Evaluation, both through human annotations and automated Metrics.
33	
34	GEM aims to: (1) measure NLG progress across 13 datasets spanning many NLG
35	tasks and languages. (2) provide an in-depth analysis of data and models
36	presented via data statements and challenge sets. (3) develop standards for
37	evaluation of generated text using both automated and human metrics.
38	
39	More information can be found at
40	[https://gem-benchmark.com](https://gem-benchmark.com).
41	"""
42	
43	_URL = "https://gem-benchmark.com"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	
43	_URL = "https://gem-benchmark.com"
44	
45	_CITATION = r"""@article{gehrmann2021gem,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	
45	_CITATION = r"""@article{gehrmann2021gem,
46	  author    = {Sebastian Gehrmann and
47	               Tosin P. Adewumi and
48	               Karmanya Aggarwal and
49	               Pawan Sasanka Ammanamanchi and
50	               Aremu Anuoluwapo and
51	               Antoine Bosselut and
52	               Khyathi Raghavi Chandu and
53	               Miruna{-}Adriana Clinciu and
54	               Dipanjan Das and
55	               Kaustubh D. Dhole and
56	               Wanyu Du and
57	               Esin Durmus and
58	               Ondrej Dusek and
59	               Chris Emezue and
60	               Varun Gangal and
61	               Cristina Garbacea and
62	               Tatsunori Hashimoto and
63	               Yufang Hou and
64	               Yacine Jernite and
65	               Harsh Jhamtani and
66	               Yangfeng Ji and
67	               Shailza Jolly and
68	               Dhruv Kumar and
69	               Faisal Ladhak and
70	               Aman Madaan and
71	               Mounica Maddela and
72	               Khyati Mahajan and
73	               Saad Mahamood and
74	               Bodhisattwa Prasad Majumder and
75	               Pedro Henrique Martins and
76	               Angelina McMillan{-}Major and
77	               Simon Mille and
78	               Emiel van Miltenburg and
79	               Moin Nadeem and
80	               Shashi Narayan and
81	               Vitaly Nikolaev and
82	               Rubungo Andre Niyongabo and
83	               Salomey Osei and
84	               Ankur P. Parikh and
85	               Laura Perez{-}Beltrachini and
86	               Niranjan Ramesh Rao and
87	               Vikas Raunak and
88	               Juan Diego Rodriguez and
89	               Sashank Santhanam and
90	               Jo{\~{a}}o Sedoc and
91	               Thibault Sellam and
92	               Samira Shaikh and
93	               Anastasia Shimorina and
94	               Marco Antonio Sobrevilla Cabezudo and
95	               Hendrik Strobelt and
96	               Nishant Subramani and
97	               Wei Xu and
98	               Diyi Yang and
99	               Akhila Yerukola and
100	               Jiawei Zhou},
101	  title     = {The {GEM} Benchmark: Natural Language Generation, its Evaluation and
102	               Metrics},
103	  journal   = {CoRR},
104	  volume    = {abs/2102.01672},
105	  year      = {2021},
106	  url       = {https://arxiv.org/abs/2102.01672},
107	  archivePrefix = {arXiv},
108	  eprint    = {2102.01672}
109	}
110	
111	Note that each GEM dataset has its own citation. Please see the source to see
112	the correct citation for each contained dataset."
113	"""
114	
115	# Used to convert IDs into dialog act names in schema-guided dialog.
116	_SGD_ACTS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:153
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
152	_WIKI_LINGUA_LANGS = {
153	    "wiki_lingua_arabic_ar": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/arabic.zip",
154	    "wiki_lingua_chinese_zh": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/chinese.zip",
155	    "wiki_lingua_czech_cs": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/czech.zip",
156	    "wiki_lingua_dutch_nl": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/dutch.zip",
157	    "wiki_lingua_english_en": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/english.zip",
158	    "wiki_lingua_french_fr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/french.zip",
159	    "wiki_lingua_german_de": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/german.zip",
160	    "wiki_lingua_hindi_hi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/hindi.zip",
161	    "wiki_lingua_indonesian_id": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/indonesian.zip",
162	    "wiki_lingua_italian_it": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/italian.zip",
163	    "wiki_lingua_japanese_ja": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/japanese.zip",
164	    "wiki_lingua_korean_ko": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/korean.zip",
165	    "wiki_lingua_portuguese_pt": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/portuguese.zip",
166	    "wiki_lingua_russian_ru": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/russian.zip",
167	    "wiki_lingua_spanish_es": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/spanish.zip",
168	    "wiki_lingua_thai_th": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/thai.zip",
169	    "wiki_lingua_turkish_tr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/turkish.zip",
170	    "wiki_lingua_vietnamese_vi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/vietnamese.zip",
171	}
172	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:154
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
153	    "wiki_lingua_arabic_ar": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/arabic.zip",
154	    "wiki_lingua_chinese_zh": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/chinese.zip",
155	    "wiki_lingua_czech_cs": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/czech.zip",
156	    "wiki_lingua_dutch_nl": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/dutch.zip",
157	    "wiki_lingua_english_en": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/english.zip",
158	    "wiki_lingua_french_fr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/french.zip",
159	    "wiki_lingua_german_de": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/german.zip",
160	    "wiki_lingua_hindi_hi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/hindi.zip",
161	    "wiki_lingua_indonesian_id": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/indonesian.zip",
162	    "wiki_lingua_italian_it": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/italian.zip",
163	    "wiki_lingua_japanese_ja": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/japanese.zip",
164	    "wiki_lingua_korean_ko": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/korean.zip",
165	    "wiki_lingua_portuguese_pt": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/portuguese.zip",
166	    "wiki_lingua_russian_ru": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/russian.zip",
167	    "wiki_lingua_spanish_es": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/spanish.zip",
168	    "wiki_lingua_thai_th": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/thai.zip",
169	    "wiki_lingua_turkish_tr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/turkish.zip",
170	    "wiki_lingua_vietnamese_vi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/vietnamese.zip",
171	}
172	
173	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:155
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
154	    "wiki_lingua_chinese_zh": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/chinese.zip",
155	    "wiki_lingua_czech_cs": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/czech.zip",
156	    "wiki_lingua_dutch_nl": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/dutch.zip",
157	    "wiki_lingua_english_en": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/english.zip",
158	    "wiki_lingua_french_fr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/french.zip",
159	    "wiki_lingua_german_de": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/german.zip",
160	    "wiki_lingua_hindi_hi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/hindi.zip",
161	    "wiki_lingua_indonesian_id": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/indonesian.zip",
162	    "wiki_lingua_italian_it": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/italian.zip",
163	    "wiki_lingua_japanese_ja": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/japanese.zip",
164	    "wiki_lingua_korean_ko": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/korean.zip",
165	    "wiki_lingua_portuguese_pt": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/portuguese.zip",
166	    "wiki_lingua_russian_ru": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/russian.zip",
167	    "wiki_lingua_spanish_es": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/spanish.zip",
168	    "wiki_lingua_thai_th": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/thai.zip",
169	    "wiki_lingua_turkish_tr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/turkish.zip",
170	    "wiki_lingua_vietnamese_vi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/vietnamese.zip",
171	}
172	
173	
174	class GemConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:156
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
155	    "wiki_lingua_czech_cs": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/czech.zip",
156	    "wiki_lingua_dutch_nl": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/dutch.zip",
157	    "wiki_lingua_english_en": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/english.zip",
158	    "wiki_lingua_french_fr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/french.zip",
159	    "wiki_lingua_german_de": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/german.zip",
160	    "wiki_lingua_hindi_hi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/hindi.zip",
161	    "wiki_lingua_indonesian_id": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/indonesian.zip",
162	    "wiki_lingua_italian_it": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/italian.zip",
163	    "wiki_lingua_japanese_ja": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/japanese.zip",
164	    "wiki_lingua_korean_ko": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/korean.zip",
165	    "wiki_lingua_portuguese_pt": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/portuguese.zip",
166	    "wiki_lingua_russian_ru": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/russian.zip",
167	    "wiki_lingua_spanish_es": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/spanish.zip",
168	    "wiki_lingua_thai_th": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/thai.zip",
169	    "wiki_lingua_turkish_tr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/turkish.zip",
170	    "wiki_lingua_vietnamese_vi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/vietnamese.zip",
171	}
172	
173	
174	class GemConfig(tfds.core.BuilderConfig):
175	  """BuilderConfig for GEM."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:157
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
156	    "wiki_lingua_dutch_nl": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/dutch.zip",
157	    "wiki_lingua_english_en": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/english.zip",
158	    "wiki_lingua_french_fr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/french.zip",
159	    "wiki_lingua_german_de": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/german.zip",
160	    "wiki_lingua_hindi_hi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/hindi.zip",
161	    "wiki_lingua_indonesian_id": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/indonesian.zip",
162	    "wiki_lingua_italian_it": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/italian.zip",
163	    "wiki_lingua_japanese_ja": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/japanese.zip",
164	    "wiki_lingua_korean_ko": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/korean.zip",
165	    "wiki_lingua_portuguese_pt": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/portuguese.zip",
166	    "wiki_lingua_russian_ru": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/russian.zip",
167	    "wiki_lingua_spanish_es": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/spanish.zip",
168	    "wiki_lingua_thai_th": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/thai.zip",
169	    "wiki_lingua_turkish_tr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/turkish.zip",
170	    "wiki_lingua_vietnamese_vi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/vietnamese.zip",
171	}
172	
173	
174	class GemConfig(tfds.core.BuilderConfig):
175	  """BuilderConfig for GEM."""
176	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:158
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
157	    "wiki_lingua_english_en": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/english.zip",
158	    "wiki_lingua_french_fr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/french.zip",
159	    "wiki_lingua_german_de": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/german.zip",
160	    "wiki_lingua_hindi_hi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/hindi.zip",
161	    "wiki_lingua_indonesian_id": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/indonesian.zip",
162	    "wiki_lingua_italian_it": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/italian.zip",
163	    "wiki_lingua_japanese_ja": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/japanese.zip",
164	    "wiki_lingua_korean_ko": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/korean.zip",
165	    "wiki_lingua_portuguese_pt": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/portuguese.zip",
166	    "wiki_lingua_russian_ru": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/russian.zip",
167	    "wiki_lingua_spanish_es": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/spanish.zip",
168	    "wiki_lingua_thai_th": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/thai.zip",
169	    "wiki_lingua_turkish_tr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/turkish.zip",
170	    "wiki_lingua_vietnamese_vi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/vietnamese.zip",
171	}
172	
173	
174	class GemConfig(tfds.core.BuilderConfig):
175	  """BuilderConfig for GEM."""
176	
177	  def __init__(self, *, features, data_urls, citation, **kwargs):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:159
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
158	    "wiki_lingua_french_fr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/french.zip",
159	    "wiki_lingua_german_de": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/german.zip",
160	    "wiki_lingua_hindi_hi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/hindi.zip",
161	    "wiki_lingua_indonesian_id": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/indonesian.zip",
162	    "wiki_lingua_italian_it": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/italian.zip",
163	    "wiki_lingua_japanese_ja": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/japanese.zip",
164	    "wiki_lingua_korean_ko": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/korean.zip",
165	    "wiki_lingua_portuguese_pt": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/portuguese.zip",
166	    "wiki_lingua_russian_ru": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/russian.zip",
167	    "wiki_lingua_spanish_es": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/spanish.zip",
168	    "wiki_lingua_thai_th": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/thai.zip",
169	    "wiki_lingua_turkish_tr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/turkish.zip",
170	    "wiki_lingua_vietnamese_vi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/vietnamese.zip",
171	}
172	
173	
174	class GemConfig(tfds.core.BuilderConfig):
175	  """BuilderConfig for GEM."""
176	
177	  def __init__(self, *, features, data_urls, citation, **kwargs):
178	    """BuilderConfig for GEM.

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:160
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
159	    "wiki_lingua_german_de": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/german.zip",
160	    "wiki_lingua_hindi_hi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/hindi.zip",
161	    "wiki_lingua_indonesian_id": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/indonesian.zip",
162	    "wiki_lingua_italian_it": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/italian.zip",
163	    "wiki_lingua_japanese_ja": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/japanese.zip",
164	    "wiki_lingua_korean_ko": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/korean.zip",
165	    "wiki_lingua_portuguese_pt": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/portuguese.zip",
166	    "wiki_lingua_russian_ru": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/russian.zip",
167	    "wiki_lingua_spanish_es": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/spanish.zip",
168	    "wiki_lingua_thai_th": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/thai.zip",
169	    "wiki_lingua_turkish_tr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/turkish.zip",
170	    "wiki_lingua_vietnamese_vi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/vietnamese.zip",
171	}
172	
173	
174	class GemConfig(tfds.core.BuilderConfig):
175	  """BuilderConfig for GEM."""
176	
177	  def __init__(self, *, features, data_urls, citation, **kwargs):
178	    """BuilderConfig for GEM.
179	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:161
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
160	    "wiki_lingua_hindi_hi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/hindi.zip",
161	    "wiki_lingua_indonesian_id": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/indonesian.zip",
162	    "wiki_lingua_italian_it": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/italian.zip",
163	    "wiki_lingua_japanese_ja": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/japanese.zip",
164	    "wiki_lingua_korean_ko": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/korean.zip",
165	    "wiki_lingua_portuguese_pt": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/portuguese.zip",
166	    "wiki_lingua_russian_ru": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/russian.zip",
167	    "wiki_lingua_spanish_es": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/spanish.zip",
168	    "wiki_lingua_thai_th": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/thai.zip",
169	    "wiki_lingua_turkish_tr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/turkish.zip",
170	    "wiki_lingua_vietnamese_vi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/vietnamese.zip",
171	}
172	
173	
174	class GemConfig(tfds.core.BuilderConfig):
175	  """BuilderConfig for GEM."""
176	
177	  def __init__(self, *, features, data_urls, citation, **kwargs):
178	    """BuilderConfig for GEM.
179	
180	    Args:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:162
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
161	    "wiki_lingua_indonesian_id": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/indonesian.zip",
162	    "wiki_lingua_italian_it": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/italian.zip",
163	    "wiki_lingua_japanese_ja": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/japanese.zip",
164	    "wiki_lingua_korean_ko": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/korean.zip",
165	    "wiki_lingua_portuguese_pt": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/portuguese.zip",
166	    "wiki_lingua_russian_ru": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/russian.zip",
167	    "wiki_lingua_spanish_es": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/spanish.zip",
168	    "wiki_lingua_thai_th": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/thai.zip",
169	    "wiki_lingua_turkish_tr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/turkish.zip",
170	    "wiki_lingua_vietnamese_vi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/vietnamese.zip",
171	}
172	
173	
174	class GemConfig(tfds.core.BuilderConfig):
175	  """BuilderConfig for GEM."""
176	
177	  def __init__(self, *, features, data_urls, citation, **kwargs):
178	    """BuilderConfig for GEM.
179	
180	    Args:
181	      features: `tfds.features.FeaturesDict`, specific feature dict for the

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:163
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
162	    "wiki_lingua_italian_it": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/italian.zip",
163	    "wiki_lingua_japanese_ja": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/japanese.zip",
164	    "wiki_lingua_korean_ko": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/korean.zip",
165	    "wiki_lingua_portuguese_pt": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/portuguese.zip",
166	    "wiki_lingua_russian_ru": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/russian.zip",
167	    "wiki_lingua_spanish_es": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/spanish.zip",
168	    "wiki_lingua_thai_th": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/thai.zip",
169	    "wiki_lingua_turkish_tr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/turkish.zip",
170	    "wiki_lingua_vietnamese_vi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/vietnamese.zip",
171	}
172	
173	
174	class GemConfig(tfds.core.BuilderConfig):
175	  """BuilderConfig for GEM."""
176	
177	  def __init__(self, *, features, data_urls, citation, **kwargs):
178	    """BuilderConfig for GEM.
179	
180	    Args:
181	      features: `tfds.features.FeaturesDict`, specific feature dict for the
182	        dataset.

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:164
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
163	    "wiki_lingua_japanese_ja": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/japanese.zip",
164	    "wiki_lingua_korean_ko": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/korean.zip",
165	    "wiki_lingua_portuguese_pt": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/portuguese.zip",
166	    "wiki_lingua_russian_ru": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/russian.zip",
167	    "wiki_lingua_spanish_es": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/spanish.zip",
168	    "wiki_lingua_thai_th": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/thai.zip",
169	    "wiki_lingua_turkish_tr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/turkish.zip",
170	    "wiki_lingua_vietnamese_vi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/vietnamese.zip",
171	}
172	
173	
174	class GemConfig(tfds.core.BuilderConfig):
175	  """BuilderConfig for GEM."""
176	
177	  def __init__(self, *, features, data_urls, citation, **kwargs):
178	    """BuilderConfig for GEM.
179	
180	    Args:
181	      features: `tfds.features.FeaturesDict`, specific feature dict for the
182	        dataset.
183	      data_urls: `dict`, urls to download the files from

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:165
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
164	    "wiki_lingua_korean_ko": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/korean.zip",
165	    "wiki_lingua_portuguese_pt": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/portuguese.zip",
166	    "wiki_lingua_russian_ru": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/russian.zip",
167	    "wiki_lingua_spanish_es": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/spanish.zip",
168	    "wiki_lingua_thai_th": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/thai.zip",
169	    "wiki_lingua_turkish_tr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/turkish.zip",
170	    "wiki_lingua_vietnamese_vi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/vietnamese.zip",
171	}
172	
173	
174	class GemConfig(tfds.core.BuilderConfig):
175	  """BuilderConfig for GEM."""
176	
177	  def __init__(self, *, features, data_urls, citation, **kwargs):
178	    """BuilderConfig for GEM.
179	
180	    Args:
181	      features: `tfds.features.FeaturesDict`, specific feature dict for the
182	        dataset.
183	      data_urls: `dict`, urls to download the files from
184	      citation: `string`, citation for the data set

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:166
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
165	    "wiki_lingua_portuguese_pt": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/portuguese.zip",
166	    "wiki_lingua_russian_ru": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/russian.zip",
167	    "wiki_lingua_spanish_es": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/spanish.zip",
168	    "wiki_lingua_thai_th": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/thai.zip",
169	    "wiki_lingua_turkish_tr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/turkish.zip",
170	    "wiki_lingua_vietnamese_vi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/vietnamese.zip",
171	}
172	
173	
174	class GemConfig(tfds.core.BuilderConfig):
175	  """BuilderConfig for GEM."""
176	
177	  def __init__(self, *, features, data_urls, citation, **kwargs):
178	    """BuilderConfig for GEM.
179	
180	    Args:
181	      features: `tfds.features.FeaturesDict`, specific feature dict for the
182	        dataset.
183	      data_urls: `dict`, urls to download the files from
184	      citation: `string`, citation for the data set
185	      **kwargs: keyword arguments forwarded to super.

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:167
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
166	    "wiki_lingua_russian_ru": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/russian.zip",
167	    "wiki_lingua_spanish_es": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/spanish.zip",
168	    "wiki_lingua_thai_th": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/thai.zip",
169	    "wiki_lingua_turkish_tr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/turkish.zip",
170	    "wiki_lingua_vietnamese_vi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/vietnamese.zip",
171	}
172	
173	
174	class GemConfig(tfds.core.BuilderConfig):
175	  """BuilderConfig for GEM."""
176	
177	  def __init__(self, *, features, data_urls, citation, **kwargs):
178	    """BuilderConfig for GEM.
179	
180	    Args:
181	      features: `tfds.features.FeaturesDict`, specific feature dict for the
182	        dataset.
183	      data_urls: `dict`, urls to download the files from
184	      citation: `string`, citation for the data set
185	      **kwargs: keyword arguments forwarded to super.
186	    """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:168
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
167	    "wiki_lingua_spanish_es": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/spanish.zip",
168	    "wiki_lingua_thai_th": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/thai.zip",
169	    "wiki_lingua_turkish_tr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/turkish.zip",
170	    "wiki_lingua_vietnamese_vi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/vietnamese.zip",
171	}
172	
173	
174	class GemConfig(tfds.core.BuilderConfig):
175	  """BuilderConfig for GEM."""
176	
177	  def __init__(self, *, features, data_urls, citation, **kwargs):
178	    """BuilderConfig for GEM.
179	
180	    Args:
181	      features: `tfds.features.FeaturesDict`, specific feature dict for the
182	        dataset.
183	      data_urls: `dict`, urls to download the files from
184	      citation: `string`, citation for the data set
185	      **kwargs: keyword arguments forwarded to super.
186	    """
187	    super(GemConfig, self).__init__(**kwargs)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:169
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
168	    "wiki_lingua_thai_th": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/thai.zip",
169	    "wiki_lingua_turkish_tr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/turkish.zip",
170	    "wiki_lingua_vietnamese_vi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/vietnamese.zip",
171	}
172	
173	
174	class GemConfig(tfds.core.BuilderConfig):
175	  """BuilderConfig for GEM."""
176	
177	  def __init__(self, *, features, data_urls, citation, **kwargs):
178	    """BuilderConfig for GEM.
179	
180	    Args:
181	      features: `tfds.features.FeaturesDict`, specific feature dict for the
182	        dataset.
183	      data_urls: `dict`, urls to download the files from
184	      citation: `string`, citation for the data set
185	      **kwargs: keyword arguments forwarded to super.
186	    """
187	    super(GemConfig, self).__init__(**kwargs)
188	    self.features = features

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:170
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
169	    "wiki_lingua_turkish_tr": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/turkish.zip",
170	    "wiki_lingua_vietnamese_vi": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_wikilingua_full/vietnamese.zip",
171	}
172	
173	
174	class GemConfig(tfds.core.BuilderConfig):
175	  """BuilderConfig for GEM."""
176	
177	  def __init__(self, *, features, data_urls, citation, **kwargs):
178	    """BuilderConfig for GEM.
179	
180	    Args:
181	      features: `tfds.features.FeaturesDict`, specific feature dict for the
182	        dataset.
183	      data_urls: `dict`, urls to download the files from
184	      citation: `string`, citation for the data set
185	      **kwargs: keyword arguments forwarded to super.
186	    """
187	    super(GemConfig, self).__init__(**kwargs)
188	    self.features = features
189	    self.data_urls = data_urls

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:225
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
224	          data_urls={
225	              "data": "https://storage.googleapis.com/huggingface-nlp/datasets/common_gen/commongen_data.zip",
226	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/common_gen.zip",
227	          },
228	          citation=textwrap.dedent(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:226
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
225	              "data": "https://storage.googleapis.com/huggingface-nlp/datasets/common_gen/commongen_data.zip",
226	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/common_gen.zip",
227	          },
228	          citation=textwrap.dedent(
229	              """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:229
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
228	          citation=textwrap.dedent(
229	              """\
230	          @inproceedings{lin2020commongen,
231	            title = "CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:271
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
270	          data_urls={
271	              "train": "https://raw.githubusercontent.com/UFAL-DSG/cs_restaurant_dataset/master/train.json",
272	              "validation": "https://raw.githubusercontent.com/UFAL-DSG/cs_restaurant_dataset/master/devel.json",
273	              "test": "https://raw.githubusercontent.com/UFAL-DSG/cs_restaurant_dataset/master/test.json",
274	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/cs_restaurants.zip",
275	          },
276	          citation=textwrap.dedent(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:272
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
271	              "train": "https://raw.githubusercontent.com/UFAL-DSG/cs_restaurant_dataset/master/train.json",
272	              "validation": "https://raw.githubusercontent.com/UFAL-DSG/cs_restaurant_dataset/master/devel.json",
273	              "test": "https://raw.githubusercontent.com/UFAL-DSG/cs_restaurant_dataset/master/test.json",
274	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/cs_restaurants.zip",
275	          },
276	          citation=textwrap.dedent(
277	              """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:273
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
272	              "validation": "https://raw.githubusercontent.com/UFAL-DSG/cs_restaurant_dataset/master/devel.json",
273	              "test": "https://raw.githubusercontent.com/UFAL-DSG/cs_restaurant_dataset/master/test.json",
274	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/cs_restaurants.zip",
275	          },
276	          citation=textwrap.dedent(
277	              """\
278	          @inproceedings{cs_restaurants,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:274
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
273	              "test": "https://raw.githubusercontent.com/UFAL-DSG/cs_restaurant_dataset/master/test.json",
274	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/cs_restaurants.zip",
275	          },
276	          citation=textwrap.dedent(
277	              """\
278	          @inproceedings{cs_restaurants,
279	            address = {Tokyo, Japan},

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:277
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
276	          citation=textwrap.dedent(
277	              """\
278	          @inproceedings{cs_restaurants,
279	            address = {Tokyo, Japan},

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:314
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
313	          data_urls={
314	              "train": "https://raw.githubusercontent.com/Yale-LILY/dart/master/data/v1.1.1/dart-v1.1.1-full-train.json",
315	              "validation": "https://raw.githubusercontent.com/Yale-LILY/dart/master/data/v1.1.1/dart-v1.1.1-full-dev.json",
316	              "test": "https://raw.githubusercontent.com/Yale-LILY/dart/master/data/v1.1.1/dart-v1.1.1-full-test.json",
317	          },
318	          citation=textwrap.dedent(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:315
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
314	              "train": "https://raw.githubusercontent.com/Yale-LILY/dart/master/data/v1.1.1/dart-v1.1.1-full-train.json",
315	              "validation": "https://raw.githubusercontent.com/Yale-LILY/dart/master/data/v1.1.1/dart-v1.1.1-full-dev.json",
316	              "test": "https://raw.githubusercontent.com/Yale-LILY/dart/master/data/v1.1.1/dart-v1.1.1-full-test.json",
317	          },
318	          citation=textwrap.dedent(
319	              """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:316
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
315	              "validation": "https://raw.githubusercontent.com/Yale-LILY/dart/master/data/v1.1.1/dart-v1.1.1-full-dev.json",
316	              "test": "https://raw.githubusercontent.com/Yale-LILY/dart/master/data/v1.1.1/dart-v1.1.1-full-test.json",
317	          },
318	          citation=textwrap.dedent(
319	              """\
320	            @article{radev2020dart,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:347
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
346	          data_urls={
347	              "train": "https://github.com/tuetschek/e2e-cleaning/raw/master/cleaned-data/train-fixed.no-ol.csv",
348	              "validation": "https://github.com/tuetschek/e2e-cleaning/raw/master/cleaned-data/devel-fixed.no-ol.csv",
349	              "test": "https://github.com/tuetschek/e2e-cleaning/raw/master/cleaned-data/test-fixed.csv",
350	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/e2e_nlg.zip",
351	          },
352	          citation=textwrap.dedent(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:348
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
347	              "train": "https://github.com/tuetschek/e2e-cleaning/raw/master/cleaned-data/train-fixed.no-ol.csv",
348	              "validation": "https://github.com/tuetschek/e2e-cleaning/raw/master/cleaned-data/devel-fixed.no-ol.csv",
349	              "test": "https://github.com/tuetschek/e2e-cleaning/raw/master/cleaned-data/test-fixed.csv",
350	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/e2e_nlg.zip",
351	          },
352	          citation=textwrap.dedent(
353	              """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:349
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
348	              "validation": "https://github.com/tuetschek/e2e-cleaning/raw/master/cleaned-data/devel-fixed.no-ol.csv",
349	              "test": "https://github.com/tuetschek/e2e-cleaning/raw/master/cleaned-data/test-fixed.csv",
350	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/e2e_nlg.zip",
351	          },
352	          citation=textwrap.dedent(
353	              """\
354	            @inproceedings{e2e_cleaned,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:350
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
349	              "test": "https://github.com/tuetschek/e2e-cleaning/raw/master/cleaned-data/test-fixed.csv",
350	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/e2e_nlg.zip",
351	          },
352	          citation=textwrap.dedent(
353	              """\
354	            @inproceedings{e2e_cleaned,
355	              address = {Tokyo, Japan},

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:353
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
352	          citation=textwrap.dedent(
353	              """\
354	            @inproceedings{e2e_cleaned,
355	              address = {Tokyo, Japan},

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:386
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
385	          data_urls={
386	              "train": "https://gitlab.lip6.fr/scialom/mlsum_data/-/raw/master/MLSUM/de_train.zip",
387	              "validation": "https://gitlab.lip6.fr/scialom/mlsum_data/-/raw/master/MLSUM/de_val.zip",
388	              "test": "https://gitlab.lip6.fr/scialom/mlsum_data/-/raw/master/MLSUM/de_test.zip",
389	              "bad_ids": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_mlsum_bad_ids_fixed.json",
390	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/mlsum_de.zip",
391	          },
392	          citation=textwrap.dedent(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:387
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
386	              "train": "https://gitlab.lip6.fr/scialom/mlsum_data/-/raw/master/MLSUM/de_train.zip",
387	              "validation": "https://gitlab.lip6.fr/scialom/mlsum_data/-/raw/master/MLSUM/de_val.zip",
388	              "test": "https://gitlab.lip6.fr/scialom/mlsum_data/-/raw/master/MLSUM/de_test.zip",
389	              "bad_ids": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_mlsum_bad_ids_fixed.json",
390	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/mlsum_de.zip",
391	          },
392	          citation=textwrap.dedent(
393	              """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:388
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
387	              "validation": "https://gitlab.lip6.fr/scialom/mlsum_data/-/raw/master/MLSUM/de_val.zip",
388	              "test": "https://gitlab.lip6.fr/scialom/mlsum_data/-/raw/master/MLSUM/de_test.zip",
389	              "bad_ids": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_mlsum_bad_ids_fixed.json",
390	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/mlsum_de.zip",
391	          },
392	          citation=textwrap.dedent(
393	              """\
394	            @inproceedings{scialom-etal-2020-mlsum,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:389
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
388	              "test": "https://gitlab.lip6.fr/scialom/mlsum_data/-/raw/master/MLSUM/de_test.zip",
389	              "bad_ids": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_mlsum_bad_ids_fixed.json",
390	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/mlsum_de.zip",
391	          },
392	          citation=textwrap.dedent(
393	              """\
394	            @inproceedings{scialom-etal-2020-mlsum,
395	                title = "{MLSUM}: The Multilingual Summarization Corpus",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:390
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
389	              "bad_ids": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_mlsum_bad_ids_fixed.json",
390	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/mlsum_de.zip",
391	          },
392	          citation=textwrap.dedent(
393	              """\
394	            @inproceedings{scialom-etal-2020-mlsum,
395	                title = "{MLSUM}: The Multilingual Summarization Corpus",
396	                author = {Scialom, Thomas  and Dray, Paul-Alexis  and Lamprier, Sylvain  and Piwowarski, Benjamin  and Staiano, Jacopo},

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:424
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
423	          data_urls={
424	              "train": "https://gitlab.lip6.fr/scialom/mlsum_data/-/raw/master/MLSUM/es_train.zip",
425	              "validation": "https://gitlab.lip6.fr/scialom/mlsum_data/-/raw/master/MLSUM/es_val.zip",
426	              "test": "https://gitlab.lip6.fr/scialom/mlsum_data/-/raw/master/MLSUM/es_test.zip",
427	              "bad_ids": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_mlsum_bad_ids_fixed.json",
428	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/mlsum_es.zip",
429	          },
430	          citation=textwrap.dedent(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:425
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
424	              "train": "https://gitlab.lip6.fr/scialom/mlsum_data/-/raw/master/MLSUM/es_train.zip",
425	              "validation": "https://gitlab.lip6.fr/scialom/mlsum_data/-/raw/master/MLSUM/es_val.zip",
426	              "test": "https://gitlab.lip6.fr/scialom/mlsum_data/-/raw/master/MLSUM/es_test.zip",
427	              "bad_ids": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_mlsum_bad_ids_fixed.json",
428	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/mlsum_es.zip",
429	          },
430	          citation=textwrap.dedent(
431	              """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:426
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
425	              "validation": "https://gitlab.lip6.fr/scialom/mlsum_data/-/raw/master/MLSUM/es_val.zip",
426	              "test": "https://gitlab.lip6.fr/scialom/mlsum_data/-/raw/master/MLSUM/es_test.zip",
427	              "bad_ids": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_mlsum_bad_ids_fixed.json",
428	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/mlsum_es.zip",
429	          },
430	          citation=textwrap.dedent(
431	              """\
432	            @inproceedings{scialom-etal-2020-mlsum,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:427
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
426	              "test": "https://gitlab.lip6.fr/scialom/mlsum_data/-/raw/master/MLSUM/es_test.zip",
427	              "bad_ids": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_mlsum_bad_ids_fixed.json",
428	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/mlsum_es.zip",
429	          },
430	          citation=textwrap.dedent(
431	              """\
432	            @inproceedings{scialom-etal-2020-mlsum,
433	                title = "{MLSUM}: The Multilingual Summarization Corpus",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:428
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
427	              "bad_ids": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_mlsum_bad_ids_fixed.json",
428	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/mlsum_es.zip",
429	          },
430	          citation=textwrap.dedent(
431	              """\
432	            @inproceedings{scialom-etal-2020-mlsum,
433	                title = "{MLSUM}: The Multilingual Summarization Corpus",
434	                author = {Scialom, Thomas  and Dray, Paul-Alexis  and Lamprier, Sylvain  and Piwowarski, Benjamin  and Staiano, Jacopo},

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:470
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
469	          data_urls={
470	              "data": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_sgd_context.zip",
471	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/schema_guided_dialog.zip",
472	          },
473	          citation=textwrap.dedent(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:471
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
470	              "data": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_sgd_context.zip",
471	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/schema_guided_dialog.zip",
472	          },
473	          citation=textwrap.dedent(
474	              """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:525
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
524	          data_urls={
525	              "data": "https://storage.googleapis.com/totto/totto_data.zip",
526	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/totto.zip",
527	          },
528	          citation=textwrap.dedent(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:526
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
525	              "data": "https://storage.googleapis.com/totto/totto_data.zip",
526	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/totto.zip",
527	          },
528	          citation=textwrap.dedent(
529	              """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:561
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
560	          data_urls={
561	              "train": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_web_nlg/webnlg_en_train.json",
562	              "validation": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_web_nlg/webnlg_en_val.json",
563	              "test": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_web_nlg/webnlg_en_test.json",
564	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/web_nlg_en.zip",
565	          },
566	          citation=textwrap.dedent(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:562
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
561	              "train": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_web_nlg/webnlg_en_train.json",
562	              "validation": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_web_nlg/webnlg_en_val.json",
563	              "test": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_web_nlg/webnlg_en_test.json",
564	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/web_nlg_en.zip",
565	          },
566	          citation=textwrap.dedent(
567	              """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:563
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
562	              "validation": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_web_nlg/webnlg_en_val.json",
563	              "test": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_web_nlg/webnlg_en_test.json",
564	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/web_nlg_en.zip",
565	          },
566	          citation=textwrap.dedent(
567	              """\
568	            @inproceedings{gardent2017creating,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:564
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
563	              "test": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_web_nlg/webnlg_en_test.json",
564	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/web_nlg_en.zip",
565	          },
566	          citation=textwrap.dedent(
567	              """\
568	            @inproceedings{gardent2017creating,
569	              author = "Gardent, Claire

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:567
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
566	          citation=textwrap.dedent(
567	              """\
568	            @inproceedings{gardent2017creating,
569	              author = "Gardent, Claire

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:606
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
605	          data_urls={
606	              "train": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_web_nlg/webnlg_ru_train.json",
607	              "validation": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_web_nlg/webnlg_ru_val.json",
608	              "test": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_web_nlg/webnlg_ru_test.json",
609	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/web_nlg_ru.zip",
610	          },
611	          citation=textwrap.dedent(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:607
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
606	              "train": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_web_nlg/webnlg_ru_train.json",
607	              "validation": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_web_nlg/webnlg_ru_val.json",
608	              "test": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_web_nlg/webnlg_ru_test.json",
609	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/web_nlg_ru.zip",
610	          },
611	          citation=textwrap.dedent(
612	              """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:608
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
607	              "validation": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_web_nlg/webnlg_ru_val.json",
608	              "test": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_web_nlg/webnlg_ru_test.json",
609	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/web_nlg_ru.zip",
610	          },
611	          citation=textwrap.dedent(
612	              """\
613	            @inproceedings{gardent2017creating,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:609
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
608	              "test": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_web_nlg/webnlg_ru_test.json",
609	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/web_nlg_ru.zip",
610	          },
611	          citation=textwrap.dedent(
612	              """\
613	            @inproceedings{gardent2017creating,
614	              author = "Gardent, Claire

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:612
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
611	          citation=textwrap.dedent(
612	              """\
613	            @inproceedings{gardent2017creating,
614	              author = "Gardent, Claire

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:648
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
647	          data_urls={
648	              "train": "https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_with_split/train.tsv",
649	              "validation": "https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_with_split/valid.tsv",
650	              "test_turk": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_turk_detokenized.json",
651	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/wiki_auto_asset_turk_train_valid.zip",
652	              "test_asset_0": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.0",
653	              "test_asset_1": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.1",
654	              "test_asset_2": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.2",
655	              "test_asset_3": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.3",
656	              "test_asset_4": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.4",
657	              "test_asset_5": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.5",
658	              "test_asset_6": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.6",
659	              "test_asset_7": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.7",
660	              "test_asset_8": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.8",
661	              "test_asset_9": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.9",
662	          },
663	          citation=textwrap.dedent(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:649
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
648	              "train": "https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_with_split/train.tsv",
649	              "validation": "https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_with_split/valid.tsv",
650	              "test_turk": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_turk_detokenized.json",
651	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/wiki_auto_asset_turk_train_valid.zip",
652	              "test_asset_0": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.0",
653	              "test_asset_1": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.1",
654	              "test_asset_2": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.2",
655	              "test_asset_3": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.3",
656	              "test_asset_4": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.4",
657	              "test_asset_5": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.5",
658	              "test_asset_6": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.6",
659	              "test_asset_7": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.7",
660	              "test_asset_8": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.8",
661	              "test_asset_9": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.9",
662	          },
663	          citation=textwrap.dedent(
664	              """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:650
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
649	              "validation": "https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_with_split/valid.tsv",
650	              "test_turk": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_turk_detokenized.json",
651	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/wiki_auto_asset_turk_train_valid.zip",
652	              "test_asset_0": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.0",
653	              "test_asset_1": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.1",
654	              "test_asset_2": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.2",
655	              "test_asset_3": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.3",
656	              "test_asset_4": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.4",
657	              "test_asset_5": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.5",
658	              "test_asset_6": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.6",
659	              "test_asset_7": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.7",
660	              "test_asset_8": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.8",
661	              "test_asset_9": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.9",
662	          },
663	          citation=textwrap.dedent(
664	              """\
665	            @inproceedings{jiang-etal-2020-neural,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:651
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
650	              "test_turk": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_turk_detokenized.json",
651	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/wiki_auto_asset_turk_train_valid.zip",
652	              "test_asset_0": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.0",
653	              "test_asset_1": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.1",
654	              "test_asset_2": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.2",
655	              "test_asset_3": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.3",
656	              "test_asset_4": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.4",
657	              "test_asset_5": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.5",
658	              "test_asset_6": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.6",
659	              "test_asset_7": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.7",
660	              "test_asset_8": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.8",
661	              "test_asset_9": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.9",
662	          },
663	          citation=textwrap.dedent(
664	              """\
665	            @inproceedings{jiang-etal-2020-neural,
666	              title = "Neural {CRF} Model for Sentence Alignment in Text Simplification",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:652
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
651	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/wiki_auto_asset_turk_train_valid.zip",
652	              "test_asset_0": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.0",
653	              "test_asset_1": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.1",
654	              "test_asset_2": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.2",
655	              "test_asset_3": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.3",
656	              "test_asset_4": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.4",
657	              "test_asset_5": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.5",
658	              "test_asset_6": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.6",
659	              "test_asset_7": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.7",
660	              "test_asset_8": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.8",
661	              "test_asset_9": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.9",
662	          },
663	          citation=textwrap.dedent(
664	              """\
665	            @inproceedings{jiang-etal-2020-neural,
666	              title = "Neural {CRF} Model for Sentence Alignment in Text Simplification",
667	              author = "Jiang, Chao  and

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:653
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
652	              "test_asset_0": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.0",
653	              "test_asset_1": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.1",
654	              "test_asset_2": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.2",
655	              "test_asset_3": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.3",
656	              "test_asset_4": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.4",
657	              "test_asset_5": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.5",
658	              "test_asset_6": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.6",
659	              "test_asset_7": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.7",
660	              "test_asset_8": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.8",
661	              "test_asset_9": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.9",
662	          },
663	          citation=textwrap.dedent(
664	              """\
665	            @inproceedings{jiang-etal-2020-neural,
666	              title = "Neural {CRF} Model for Sentence Alignment in Text Simplification",
667	              author = "Jiang, Chao  and
668	                Maddela, Mounica  and

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:654
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
653	              "test_asset_1": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.1",
654	              "test_asset_2": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.2",
655	              "test_asset_3": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.3",
656	              "test_asset_4": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.4",
657	              "test_asset_5": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.5",
658	              "test_asset_6": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.6",
659	              "test_asset_7": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.7",
660	              "test_asset_8": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.8",
661	              "test_asset_9": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.9",
662	          },
663	          citation=textwrap.dedent(
664	              """\
665	            @inproceedings{jiang-etal-2020-neural,
666	              title = "Neural {CRF} Model for Sentence Alignment in Text Simplification",
667	              author = "Jiang, Chao  and
668	                Maddela, Mounica  and
669	                Lan, Wuwei  and

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:655
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
654	              "test_asset_2": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.2",
655	              "test_asset_3": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.3",
656	              "test_asset_4": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.4",
657	              "test_asset_5": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.5",
658	              "test_asset_6": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.6",
659	              "test_asset_7": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.7",
660	              "test_asset_8": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.8",
661	              "test_asset_9": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.9",
662	          },
663	          citation=textwrap.dedent(
664	              """\
665	            @inproceedings{jiang-etal-2020-neural,
666	              title = "Neural {CRF} Model for Sentence Alignment in Text Simplification",
667	              author = "Jiang, Chao  and
668	                Maddela, Mounica  and
669	                Lan, Wuwei  and
670	                Zhong, Yang  and

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:656
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
655	              "test_asset_3": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.3",
656	              "test_asset_4": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.4",
657	              "test_asset_5": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.5",
658	              "test_asset_6": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.6",
659	              "test_asset_7": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.7",
660	              "test_asset_8": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.8",
661	              "test_asset_9": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.9",
662	          },
663	          citation=textwrap.dedent(
664	              """\
665	            @inproceedings{jiang-etal-2020-neural,
666	              title = "Neural {CRF} Model for Sentence Alignment in Text Simplification",
667	              author = "Jiang, Chao  and
668	                Maddela, Mounica  and
669	                Lan, Wuwei  and
670	                Zhong, Yang  and
671	                Xu, Wei",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:657
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
656	              "test_asset_4": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.4",
657	              "test_asset_5": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.5",
658	              "test_asset_6": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.6",
659	              "test_asset_7": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.7",
660	              "test_asset_8": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.8",
661	              "test_asset_9": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.9",
662	          },
663	          citation=textwrap.dedent(
664	              """\
665	            @inproceedings{jiang-etal-2020-neural,
666	              title = "Neural {CRF} Model for Sentence Alignment in Text Simplification",
667	              author = "Jiang, Chao  and
668	                Maddela, Mounica  and
669	                Lan, Wuwei  and
670	                Zhong, Yang  and
671	                Xu, Wei",
672	              booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:658
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
657	              "test_asset_5": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.5",
658	              "test_asset_6": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.6",
659	              "test_asset_7": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.7",
660	              "test_asset_8": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.8",
661	              "test_asset_9": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.9",
662	          },
663	          citation=textwrap.dedent(
664	              """\
665	            @inproceedings{jiang-etal-2020-neural,
666	              title = "Neural {CRF} Model for Sentence Alignment in Text Simplification",
667	              author = "Jiang, Chao  and
668	                Maddela, Mounica  and
669	                Lan, Wuwei  and
670	                Zhong, Yang  and
671	                Xu, Wei",
672	              booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
673	              month = jul,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:659
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
658	              "test_asset_6": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.6",
659	              "test_asset_7": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.7",
660	              "test_asset_8": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.8",
661	              "test_asset_9": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.9",
662	          },
663	          citation=textwrap.dedent(
664	              """\
665	            @inproceedings{jiang-etal-2020-neural,
666	              title = "Neural {CRF} Model for Sentence Alignment in Text Simplification",
667	              author = "Jiang, Chao  and
668	                Maddela, Mounica  and
669	                Lan, Wuwei  and
670	                Zhong, Yang  and
671	                Xu, Wei",
672	              booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
673	              month = jul,
674	              year = "2020",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:660
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
659	              "test_asset_7": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.7",
660	              "test_asset_8": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.8",
661	              "test_asset_9": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.9",
662	          },
663	          citation=textwrap.dedent(
664	              """\
665	            @inproceedings{jiang-etal-2020-neural,
666	              title = "Neural {CRF} Model for Sentence Alignment in Text Simplification",
667	              author = "Jiang, Chao  and
668	                Maddela, Mounica  and
669	                Lan, Wuwei  and
670	                Zhong, Yang  and
671	                Xu, Wei",
672	              booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
673	              month = jul,
674	              year = "2020",
675	              address = "Online",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:661
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
660	              "test_asset_8": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.8",
661	              "test_asset_9": "https://github.com/facebookresearch/asset/raw/main/dataset/asset.test.simp.9",
662	          },
663	          citation=textwrap.dedent(
664	              """\
665	            @inproceedings{jiang-etal-2020-neural,
666	              title = "Neural {CRF} Model for Sentence Alignment in Text Simplification",
667	              author = "Jiang, Chao  and
668	                Maddela, Mounica  and
669	                Lan, Wuwei  and
670	                Zhong, Yang  and
671	                Xu, Wei",
672	              booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
673	              month = jul,
674	              year = "2020",
675	              address = "Online",
676	              publisher = "Association for Computational Linguistics",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:664
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
663	          citation=textwrap.dedent(
664	              """\
665	            @inproceedings{jiang-etal-2020-neural,
666	              title = "Neural {CRF} Model for Sentence Alignment in Text Simplification",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:702
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
701	          data_urls={
702	              "data": "http://bollin.inf.ed.ac.uk/public/direct/XSUM-EMNLP18-Summary-Data-Original.tar.gz",
703	              "splits": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_xsum_confidence_0.8.json",
704	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/xsum.zip",
705	          },
706	          citation=textwrap.dedent(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:703
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
702	              "data": "http://bollin.inf.ed.ac.uk/public/direct/XSUM-EMNLP18-Summary-Data-Original.tar.gz",
703	              "splits": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_xsum_confidence_0.8.json",
704	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/xsum.zip",
705	          },
706	          citation=textwrap.dedent(
707	              """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:704
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
703	              "splits": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_xsum_confidence_0.8.json",
704	              "challenge_set": "https://storage.googleapis.com/huggingface-nlp/datasets/gem/gem_challenge_sets/xsum.zip",
705	          },
706	          citation=textwrap.dedent(
707	              """\
708	            @inproceedings{Narayan2018dont,

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gem/gem.py:1452
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
1451	      with epath.Path(filepath).open() as f:
1452	        data = json.loads(f.read())
1453	        id_ = -1

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:38
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
37	
38	_GLUE_DESCRIPTION = """\
39	GLUE, the General Language Understanding Evaluation benchmark
40	(https://gluebenchmark.com/) is a collection of resources for training,
41	evaluating, and analyzing natural language understanding systems.
42	
43	"""
44	
45	_MRPC_DEV_IDS = "https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	
45	_MRPC_DEV_IDS = "https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc"
46	_MRPC_TRAIN = "https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:46
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
45	_MRPC_DEV_IDS = "https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc"
46	_MRPC_TRAIN = "https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt"
47	_MRPC_TEST = "https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	_MRPC_TRAIN = "https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt"
47	_MRPC_TEST = "https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt"
48	
49	_MNLI_BASE_KWARGS = dict(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:56
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
55	    label_column="gold_label",
56	    data_url="https://dl.fbaipublicfiles.com/glue/data/MNLI.zip",
57	    data_dir="MNLI",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:59
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
58	    citation=textwrap.dedent(
59	        """\
60	      @InProceedings{N18-1101,
61	        author = "Williams, Adina

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:84
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
83	    ),
84	    url="http://www.nyu.edu/projects/bowman/multinli/",
85	)

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:130
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
129	        release_notes={
130	            "1.0.0": "New split API (https://tensorflow.org/datasets/splits)",
131	            "1.0.1": "Update dead URL links.",
132	            "2.0.0": "Update data source for glue/qqp.",
133	        },
134	        **kwargs,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:162
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
161	          label_column="is_acceptable",
162	          data_url="https://dl.fbaipublicfiles.com/glue/data/CoLA.zip",
163	          data_dir="CoLA",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:173
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
172	          ),
173	          url="https://nyu-mll.github.io/CoLA/",
174	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:187
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
186	          label_column="label",
187	          data_url="https://dl.fbaipublicfiles.com/glue/data/SST-2.zip",
188	          data_dir="SST-2",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:199
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
198	          ),
199	          url="https://nlp.stanford.edu/sentiment/index.html",
200	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:223
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
222	          ),
223	          url="https://www.microsoft.com/en-us/download/details.aspx?id=52398",
224	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:239
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
238	          label_column="is_duplicate",
239	          data_url="https://dl.fbaipublicfiles.com/glue/data/QQP.zip",
240	          data_dir="QQP",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:242
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
241	          citation=textwrap.dedent(
242	              """\
243	          @online{WinNT,
244	            author = {Iyer, Shankar and Dandekar, Nikhil and Csernai, Kornel},

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:251
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
250	          ),
251	          url="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs",
252	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:267
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
266	          label_column="score",
267	          data_url="https://dl.fbaipublicfiles.com/glue/data/STS-B.zip",
268	          data_dir="STS-B",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:278
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
277	          ),
278	          url="http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark",
279	          process_label=np.float32,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:334
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
333	          label_column="label",
334	          data_url="https://dl.fbaipublicfiles.com/glue/data/QNLIv2.zip",
335	          data_dir="QNLI",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:345
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
344	          ),
345	          url="https://rajpurkar.github.io/SQuAD-explorer/",
346	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:363
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
362	          label_column="label",
363	          data_url="https://dl.fbaipublicfiles.com/glue/data/RTE.zip",
364	          data_dir="RTE",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:400
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
399	          ),
400	          url="https://aclweb.org/aclwiki/Recognizing_Textual_Entailment",
401	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:428
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
427	          label_column="label",
428	          data_url="https://dl.fbaipublicfiles.com/glue/data/WNLI.zip",
429	          data_dir="WNLI",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:440
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
439	          url=(
440	              "https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html"
441	          ),
442	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:461
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
460	          # causes issues in TFDS.
461	          data_url="https://bit.ly/2BOtOJ7",
462	          data_dir="",  # We are downloading a tsv.

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/glue.py:464
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
463	          citation="",  # The GLUE citation is sufficient.
464	          url="https://gluebenchmark.com/diagnostics",
465	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/goemotions.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	_CITATION = """
25	@inproceedings{demszky-2020-goemotions,
26	    title = "{G}o{E}motions: A Dataset of Fine-Grained Emotions",
27	    author = "Demszky, Dorottya  and
28	      Movshovitz-Attias, Dana  and
29	      Ko, Jeongwoo  and
30	      Cowen, Alan  and
31	      Nemade, Gaurav  and
32	      Ravi, Sujith",
33	    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
34	    month = jul,
35	    year = "2020",
36	    address = "Online",
37	    publisher = "Association for Computational Linguistics",
38	    url = "https://www.aclweb.org/anthology/2020.acl-main.372",
39	    pages = "4040--4054",
40	}
41	"""
42	
43	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/goemotions.py:52
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
51	
52	_URL_TRAIN = 'https://github.com/google-research/google-research/raw/master/goemotions/data/train.tsv'
53	_URL_DEV = 'https://github.com/google-research/google-research/raw/master/goemotions/data/dev.tsv'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/goemotions.py:53
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
52	_URL_TRAIN = 'https://github.com/google-research/google-research/raw/master/goemotions/data/train.tsv'
53	_URL_DEV = 'https://github.com/google-research/google-research/raw/master/goemotions/data/dev.tsv'
54	_URL_TEST = 'https://github.com/google-research/google-research/raw/master/goemotions/data/test.tsv'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/goemotions.py:54
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
53	_URL_DEV = 'https://github.com/google-research/google-research/raw/master/goemotions/data/dev.tsv'
54	_URL_TEST = 'https://github.com/google-research/google-research/raw/master/goemotions/data/test.tsv'
55	
56	_TEXT_LABEL = 'comment_text'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/goemotions.py:107
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
106	        supervised_keys=None,
107	        homepage='https://github.com/google-research/google-research/tree/master/goemotions',
108	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gpt3.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	
41	_DATA_URL = "https://github.com/openai/gpt-3/archive/master.zip"
42	
43	_MODULES = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gpt3.py:90
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
89	        }),
90	        homepage="https://github.com/openai/gpt-3",
91	        supervised_keys=("context", "completion"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gsm8k/gsm8k.py:38
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
37	
38	_URL_PREFIX = 'https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/'
39	_URLS = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/gsm8k/gsm8k.py:64
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
63	        supervised_keys=None,
64	        homepage='https://github.com/openai/grade-school-math',
65	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/hellaswag.py:42
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
41	_HELLASWAG_URL = (
42	    'https://raw.githubusercontent.com/rowanz/hellaswag/master/data/'
43	)
44	
45	
46	class Hellaswag(tfds.core.GeneratorBasedBuilder):
47	  """HellaSwag Dataset."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/hellaswag.py:76
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
75	        supervised_keys=None,
76	        homepage='https://rowanzellers.com/hellaswag/',
77	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/movie_rationales.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	
43	_DOWNLOAD_URL = 'http://www.eraserbenchmark.com/zipped/movies.tar.gz'
44	
45	
46	class MovieRationales(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/movie_rationales.py:61
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
60	        supervised_keys=None,
61	        homepage='http://www.cs.jhu.edu/~ozaidan/rationales/',
62	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/movie_rationales.py:104
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
103	        with epath.Path(review_file).open() as f1:
104	          review_text = f1.read()
105	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/mrqa/mrqa.py:29
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
28	
29	_DESCRIPTION = """
30	The MRQA 2019 Shared Task focuses on generalization in question answering. An
31	effective question answering system should do more than merely interpolate from
32	the training set to answer test examples drawn from the same distribution: it
33	should also be able to extrapolate to out-of-distribution examples  a
34	significantly harder challenge.
35	
36	MRQA adapts and unifies multiple distinct question answering datasets (carefully
37	selected subsets of existing datasets) into the same format (SQuAD format).
38	Among them, six datasets were made available for training, and six datasets were
39	made available for testing. Small portions of the training datasets
40	were held-out as in-domain data that may be used for development. The testing
41	datasets only contain out-of-domain data. This benchmark is released as part of
42	the MRQA 2019 Shared Task.
43	
44	More information can be found at: `https://mrqa.github.io/2019/shared.html`.
45	"""
46	
47	_HOMEPAGE = 'https://mrqa.github.io/2019/shared.html'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/mrqa/mrqa.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	
47	_HOMEPAGE = 'https://mrqa.github.io/2019/shared.html'
48	
49	_CITATION = """@inproceedings{fisch-etal-2019-mrqa,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/mrqa/mrqa.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
48	
49	_CITATION = """@inproceedings{fisch-etal-2019-mrqa,
50	    title = "{MRQA} 2019 Shared Task: Evaluating Generalization in Reading Comprehension",
51	    author = "Fisch, Adam  and
52	      Talmor, Alon  and
53	      Jia, Robin  and
54	      Seo, Minjoon  and
55	      Choi, Eunsol  and
56	      Chen, Danqi",
57	    booktitle = "Proceedings of the 2nd Workshop on Machine Reading for Question Answering",
58	    month = nov,
59	    year = "2019",
60	    address = "Hong Kong, China",
61	    publisher = "Association for Computational Linguistics",
62	    url = "https://aclanthology.org/D19-5801",
63	    doi = "10.18653/v1/D19-5801",
64	    pages = "1--13",
65	}
66	
67	Note that each MRQA dataset has its own citation. Please see the source to see
68	the correct citation for each contained dataset."
69	"""
70	
71	_URL_BASE = 'https://s3.us-east-2.amazonaws.com/mrqa/release/v2'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/mrqa/mrqa.py:71
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
70	
71	_URL_BASE = 'https://s3.us-east-2.amazonaws.com/mrqa/release/v2'
72	_URLs = {

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/mrqa/mrqa.py:93
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
92	    'test+BioASQ': (
93	        'http://participants-area.bioasq.org/MRQA2019/'
94	    ),  # BioASQ.jsonl.gz
95	    'test+DROP': f'{_URL_BASE}/dev/DROP.jsonl.gz',
96	    'test+DuoRC': f'{_URL_BASE}/dev/DuoRC.ParaphraseRC.jsonl.gz',
97	    'test+RACE': f'{_URL_BASE}/dev/RACE.jsonl.gz',
98	    'test+RelationExtraction': f'{_URL_BASE}/dev/RelationExtraction.jsonl.gz',
99	    'test+TextbookQA': f'{_URL_BASE}/dev/TextbookQA.jsonl.gz',
100	}
101	
102	
103	class MRQAConfig(tfds.core.BuilderConfig):
104	  """BuilderConfig for MRQA."""
105	
106	  def __init__(self, *, features, data_urls, citation, **kwargs):
107	    """BuilderConfig for MRQA.
108	
109	    Args:
110	      features: `tfds.features.FeaturesDict`, specific feature dict for the
111	        dataset.
112	      data_urls: `dict`, urls to download the files from
113	      citation: `string`, citation for the data set
114	      **kwargs: keyword arguments forwarded to super.
115	    """
116	    super(MRQAConfig, self).__init__(**kwargs)
117	    self.features = features
118	    self.data_urls = data_urls
119	    self.citation = citation
120	
121	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/mrqa/mrqa.py:172
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
171	          citation=textwrap.dedent(
172	              """\
173	          @inproceedings{rajpurkar-etal-2016-squad,
174	              title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/mrqa/mrqa.py:208
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
207	          citation=textwrap.dedent(
208	              """\
209	          @inproceedings{trischler-etal-2017-newsqa,
210	              title = "{N}ews{QA}: A Machine Comprehension Dataset",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/mrqa/mrqa.py:245
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
244	          citation=textwrap.dedent(
245	              """\
246	          @inproceedings{joshi-etal-2017-triviaqa,
247	              title = "{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/mrqa/mrqa.py:306
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
305	          citation=textwrap.dedent(
306	              """\
307	          @inproceedings{yang-etal-2018-hotpotqa,
308	              title = "{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/mrqa/mrqa.py:349
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
348	          citation=textwrap.dedent(
349	              """\
350	          @article{kwiatkowski-etal-2019-natural,
351	              title = "Natural Questions: A Benchmark for Question Answering Research",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/mrqa/mrqa.py:432
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
431	          citation=textwrap.dedent(
432	              """\
433	          @inproceedings{dua-etal-2019-drop,
434	              title = "{DROP}: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/mrqa/mrqa.py:471
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
470	          citation=textwrap.dedent(
471	              """\
472	          @inproceedings{saha-etal-2018-duorc,
473	              title = "{D}uo{RC}: Towards Complex Language Understanding with Paraphrased Reading Comprehension",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/mrqa/mrqa.py:506
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
505	          citation=textwrap.dedent(
506	              """\
507	          @inproceedings{lai-etal-2017-race,
508	              title = "{RACE}: Large-scale {R}e{A}ding Comprehension Dataset From Examinations",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/mrqa/mrqa.py:545
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
544	          citation=textwrap.dedent(
545	              """\
546	          @inproceedings{levy-etal-2017-zero,
547	              title = "Zero-Shot Relation Extraction via Reading Comprehension",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/multi_nli.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_CITATION = """\
24	@InProceedings{N18-1101,
25	  author = "Williams, Adina
26	            and Nangia, Nikita
27	            and Bowman, Samuel",
28	  title = "A Broad-Coverage Challenge Corpus for
29	           Sentence Understanding through Inference",
30	  booktitle = "Proceedings of the 2018 Conference of
31	               the North American Chapter of the
32	               Association for Computational Linguistics:
33	               Human Language Technologies, Volume 1 (Long
34	               Papers)",
35	  year = "2018",
36	  publisher = "Association for Computational Linguistics",
37	  pages = "1112--1122",
38	  location = "New Orleans, Louisiana",
39	  url = "http://aclweb.org/anthology/N18-1101"
40	}
41	"""
42	
43	_DESCRIPTION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/multi_nli.py:72
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
71	        supervised_keys=None,
72	        homepage="https://www.nyu.edu/projects/bowman/multinli/",
73	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/multi_nli.py:78
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
77	    downloaded_dir = dl_manager.download_and_extract(
78	        "https://cims.nyu.edu/~sbowman/multinli/multinli_1.0.zip"
79	    )
80	    mnli_path = os.path.join(downloaded_dir, "multinli_1.0")

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/multi_nli_mismatch.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_CITATION = """\
24	@InProceedings{N18-1101,
25	  author = "Williams, Adina
26	            and Nangia, Nikita
27	            and Bowman, Samuel",
28	  title = "A Broad-Coverage Challenge Corpus for
29	           Sentence Understanding through Inference",
30	  booktitle = "Proceedings of the 2018 Conference of
31	               the North American Chapter of the
32	               Association for Computational Linguistics:
33	               Human Language Technologies, Volume 1 (Long
34	               Papers)",
35	  year = "2018",
36	  publisher = "Association for Computational Linguistics",
37	  pages = "1112--1122",
38	  location = "New Orleans, Louisiana",
39	  url = "http://aclweb.org/anthology/N18-1101"
40	}
41	"""
42	
43	_DESCRIPTION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/multi_nli_mismatch.py:52
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
51	
52	ROOT_URL = "https://cims.nyu.edu/~sbowman/multinli/multinli_1.0.zip"
53	
54	
55	class MultiNLIMismatch(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/multi_nli_mismatch.py:72
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
71	        supervised_keys=None,
72	        homepage="https://www.nyu.edu/projects/bowman/multinli/",
73	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/qrecc/qrecc.py:36
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
35	
36	_DATASET_URL = "https://github.com/apple/ml-qrecc/blob/main/dataset/qrecc_data.zip?raw=true"
37	
38	
39	class QReCC(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/qrecc/qrecc.py:73
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
72	        supervised_keys=None,
73	        homepage="https://github.com/apple/ml-qrecc",
74	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/scrolls/scrolls.py:39
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
38	
39	_SCROLLS_DESCRIPTION = """
40	SCROLLS: Standardized CompaRison Over Long Language Sequences.
41	A suite of natural language tfds.core.that require reasoning over long texts.
42	https://scrolls-benchmark.com/
43	"""
44	
45	_SUMM_SCREEN_CITATION = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/scrolls/scrolls.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	
55	_QASPER_CITATION = r"""
56	@inproceedings{dasigi-etal-2021-dataset,
57	    title = "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers",
58	    author = "Dasigi, Pradeep  and
59	      Lo, Kyle  and
60	      Beltagy, Iz  and
61	      Cohan, Arman  and
62	      Smith, Noah A.  and
63	      Gardner, Matt",
64	    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
65	    month = jun,
66	    year = "2021",
67	    address = "Online",
68	    publisher = "Association for Computational Linguistics",
69	    url = "https://aclanthology.org/2021.naacl-main.365",
70	    doi = "10.18653/v1/2021.naacl-main.365",
71	    pages = "4599--4610",
72	    abstract = "Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering tfds.core.usually contain questions about generic factoid-type information. We therefore present Qasper, a dataset of 5049 questions over 1585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate.",
73	}"""
74	
75	_QMSUM_CITATION = r"""@inproceedings{zhong-etal-2021-qmsum,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/scrolls/scrolls.py:75
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
74	
75	_QMSUM_CITATION = r"""@inproceedings{zhong-etal-2021-qmsum,
76	    title = "{QMS}um: A New Benchmark for Query-based Multi-domain Meeting Summarization",
77	    author = "Zhong, Ming  and
78	      Yin, Da  and
79	      Yu, Tao  and
80	      Zaidi, Ahmad  and
81	      Mutuma, Mutethia  and
82	      Jha, Rahul  and
83	      Awadallah, Ahmed Hassan  and
84	      Celikyilmaz, Asli  and
85	      Liu, Yang  and
86	      Qiu, Xipeng  and
87	      Radev, Dragomir",
88	    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
89	    month = jun,
90	    year = "2021",
91	    address = "Online",
92	    publisher = "Association for Computational Linguistics",
93	    url = "https://aclanthology.org/2021.naacl-main.472",
94	    doi = "10.18653/v1/2021.naacl-main.472",
95	    pages = "5905--5921",
96	    abstract = "Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summarization for future research. Dataset is available at \url{https://github.com/Yale-LILY/QMSum}.",
97	}"""
98	
99	_NARRATIVE_QA_CITATION = r"""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/scrolls/scrolls.py:110
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
109	
110	_GOV_REPORT_CITATION = r"""
111	@inproceedings{huang-etal-2021-efficient,
112	    title = "Efficient Attentions for Long Document Summarization",
113	    author = "Huang, Luyang  and
114	      Cao, Shuyang  and
115	      Parulian, Nikolaus  and
116	      Ji, Heng  and
117	      Wang, Lu",
118	    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
119	    month = jun,
120	    year = "2021",
121	    address = "Online",
122	    publisher = "Association for Computational Linguistics",
123	    url = "https://aclanthology.org/2021.naacl-main.112",
124	    doi = "10.18653/v1/2021.naacl-main.112",
125	    pages = "1419--1436",
126	    abstract = "The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper, we propose Hepos, a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient self-attentions. Combined with Hepos, we are able to process ten times more tokens than existing models that use full attentions. For evaluation, we present a new dataset, GovReport, with significantly longer documents and summaries. Results show that our models produce significantly higher ROUGE scores than competitive comparisons, including new state-of-the-art results on PubMed. Human evaluation also shows that our models generate more informative summaries with fewer unfaithful errors.",
127	}"""
128	
129	_CONTRACT_NLI_CITATION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/scrolls/scrolls.py:175
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
174	          description="summ_screen_fd subset",
175	          data_url="https://scrolls-tau.s3.us-east-2.amazonaws.com/summ_screen_fd.zip",
176	          citation=_SUMM_SCREEN_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/scrolls/scrolls.py:177
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
176	          citation=_SUMM_SCREEN_CITATION,
177	          url="https://github.com/mingdachen/SummScreen",
178	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/scrolls/scrolls.py:182
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
181	          description="qasper subset",
182	          data_url="https://scrolls-tau.s3.us-east-2.amazonaws.com/qasper.zip",
183	          citation=_QASPER_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/scrolls/scrolls.py:184
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
183	          citation=_QASPER_CITATION,
184	          url="https://allenai.org/project/qasper",
185	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/scrolls/scrolls.py:189
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
188	          description="qmsum subset",
189	          data_url="https://scrolls-tau.s3.us-east-2.amazonaws.com/qmsum.zip",
190	          citation=_QMSUM_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/scrolls/scrolls.py:191
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
190	          citation=_QMSUM_CITATION,
191	          url="https://github.com/Yale-LILY/QMSum",
192	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/scrolls/scrolls.py:197
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
196	          data_url=(
197	              "https://scrolls-tau.s3.us-east-2.amazonaws.com/narrative_qa.zip"
198	          ),
199	          citation=_NARRATIVE_QA_CITATION,
200	          url="https://deepmind.com/research/publications/narrativeqa-reading-comprehension-challenge",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/scrolls/scrolls.py:200
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
199	          citation=_NARRATIVE_QA_CITATION,
200	          url="https://deepmind.com/research/publications/narrativeqa-reading-comprehension-challenge",
201	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/scrolls/scrolls.py:206
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
205	          data_url=(
206	              "https://scrolls-tau.s3.us-east-2.amazonaws.com/gov_report.zip"
207	          ),
208	          citation=_GOV_REPORT_CITATION,
209	          url="https://gov-report-data.github.io/",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/scrolls/scrolls.py:209
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
208	          citation=_GOV_REPORT_CITATION,
209	          url="https://gov-report-data.github.io/",
210	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/scrolls/scrolls.py:215
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
214	          data_url=(
215	              "https://scrolls-tau.s3.us-east-2.amazonaws.com/contract_nli.zip"
216	          ),
217	          citation=_CONTRACT_NLI_CITATION,
218	          url="https://stanfordnlp.github.io/contract-nli/",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/scrolls/scrolls.py:218
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
217	          citation=_CONTRACT_NLI_CITATION,
218	          url="https://stanfordnlp.github.io/contract-nli/",
219	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/scrolls/scrolls.py:223
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
222	          description="quality subset",
223	          data_url="https://scrolls-tau.s3.us-east-2.amazonaws.com/quality.zip",
224	          citation=_QUALITY_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/scrolls/scrolls.py:225
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
224	          citation=_QUALITY_CITATION,
225	          url="https://github.com/nyu-mll/quality",
226	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/squad_question_generation/squad_question_generation.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	_CITATION_DU_ET_AL = textwrap.dedent(
28	    """\
29	@inproceedings{du-etal-2017-learning,
30	    title = "Learning to Ask: Neural Question Generation for Reading Comprehension",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/squad_question_generation/squad_question_generation.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	_DATA_URLS_DU_ET_AL = {
45	    "train": "https://raw.githubusercontent.com/xinyadu/nqg/master/data/raw/train.json",
46	    "dev": (
47	        "https://raw.githubusercontent.com/xinyadu/nqg/master/data/raw/dev.json"
48	    ),
49	    "test": "https://raw.githubusercontent.com/xinyadu/nqg/master/data/raw/test.json",
50	}
51	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/squad_question_generation/squad_question_generation.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	    "dev": (
47	        "https://raw.githubusercontent.com/xinyadu/nqg/master/data/raw/dev.json"
48	    ),
49	    "test": "https://raw.githubusercontent.com/xinyadu/nqg/master/data/raw/test.json",
50	}
51	
52	_HOMEPAGE_URL_DU_ET_AL = "https://github.com/xinyadu/nqg"
53	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/squad_question_generation/squad_question_generation.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
48	    ),
49	    "test": "https://raw.githubusercontent.com/xinyadu/nqg/master/data/raw/test.json",
50	}
51	
52	_HOMEPAGE_URL_DU_ET_AL = "https://github.com/xinyadu/nqg"
53	
54	_CITATION_ZHOU_ET_AL = textwrap.dedent(
55	    """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/squad_question_generation/squad_question_generation.py:52
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
51	
52	_HOMEPAGE_URL_DU_ET_AL = "https://github.com/xinyadu/nqg"
53	
54	_CITATION_ZHOU_ET_AL = textwrap.dedent(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/squad_question_generation/squad_question_generation.py:55
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
54	_CITATION_ZHOU_ET_AL = textwrap.dedent(
55	    """\
56	@inproceedings{du-etal-2017-learning,
57	    title = "Learning to Ask: Neural Question Generation for Reading Comprehension",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/squad_question_generation/squad_question_generation.py:73
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
72	    "train": (
73	        "https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json"
74	    ),
75	    "dev": "https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json",
76	    "mapping": "https://res.qyzhou.me/qas_id_in_squad.zip",
77	    "redistribute": "https://res.qyzhou.me/redistribute.zip",
78	}
79	
80	_HOMEPAGE_URL_ZHOU_ET_AL = "https://github.com/magic282/NQG"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/squad_question_generation/squad_question_generation.py:75
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
74	    ),
75	    "dev": "https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json",
76	    "mapping": "https://res.qyzhou.me/qas_id_in_squad.zip",
77	    "redistribute": "https://res.qyzhou.me/redistribute.zip",
78	}
79	
80	_HOMEPAGE_URL_ZHOU_ET_AL = "https://github.com/magic282/NQG"
81	
82	_CITATION_SQUAD = textwrap.dedent(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/squad_question_generation/squad_question_generation.py:76
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
75	    "dev": "https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json",
76	    "mapping": "https://res.qyzhou.me/qas_id_in_squad.zip",
77	    "redistribute": "https://res.qyzhou.me/redistribute.zip",
78	}
79	
80	_HOMEPAGE_URL_ZHOU_ET_AL = "https://github.com/magic282/NQG"
81	
82	_CITATION_SQUAD = textwrap.dedent(
83	    """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/squad_question_generation/squad_question_generation.py:77
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
76	    "mapping": "https://res.qyzhou.me/qas_id_in_squad.zip",
77	    "redistribute": "https://res.qyzhou.me/redistribute.zip",
78	}
79	
80	_HOMEPAGE_URL_ZHOU_ET_AL = "https://github.com/magic282/NQG"
81	
82	_CITATION_SQUAD = textwrap.dedent(
83	    """\
84	@inproceedings{rajpurkar-etal-2016-squad,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/squad_question_generation/squad_question_generation.py:80
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
79	
80	_HOMEPAGE_URL_ZHOU_ET_AL = "https://github.com/magic282/NQG"
81	
82	_CITATION_SQUAD = textwrap.dedent(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/squad_question_generation/squad_question_generation.py:83
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
82	_CITATION_SQUAD = textwrap.dedent(
83	    """\
84	@inproceedings{rajpurkar-etal-2016-squad,
85	    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/squad_question_generation/squad_question_generation.py:268
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
267	              zip(
268	                  mapping_file.read().splitlines(),
269	                  qgen_data_file.read().splitlines(),

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/squad_question_generation/squad_question_generation.py:269
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
268	                  mapping_file.read().splitlines(),
269	                  qgen_data_file.read().splitlines(),
270	              )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:40
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
39	
40	_GLUE_DESCRIPTION = """\
41	SuperGLUE (https://super.gluebenchmark.com/) is a new benchmark styled after
42	GLUE with a new set of more difficult language understanding tasks, improved
43	resources, and a new public leaderboard.
44	
45	"""
46	
47	_BOOLQ_DESCRIPTION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:247
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
246	
247	_WIC_CITATION = """\
248	@article{DBLP:journals/corr/abs-1808-09121,
249	  author={Mohammad Taher Pilehvar and os{\'{e}} Camacho{-}Collados},
250	  title={WiC: 10, 000 Example Pairs for Evaluating Context-Sensitive Representations},
251	  journal={CoRR},
252	  volume={abs/1808.09121},
253	  year={2018},
254	  url={http://arxiv.org/abs/1808.09121},
255	  archivePrefix={arXiv},
256	  eprint={1808.09121},
257	  timestamp={Mon, 03 Sep 2018 13:36:40 +0200},
258	  biburl={https://dblp.org/rec/bib/journals/corr/abs-1808-09121},
259	  bibsource={dblp computer science bibliography, https://dblp.org}
260	}"""
261	
262	_WSC_CITATION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:334
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
333	          data_url=(
334	              "https://dl.fbaipublicfiles.com/glue/superglue/data/v2/BoolQ.zip"
335	          ),
336	          citation=_BOOLQ_CITATION,
337	          url="https://github.com/google-research-datasets/boolean-questions",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:337
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
336	          citation=_BOOLQ_CITATION,
337	          url="https://github.com/google-research-datasets/boolean-questions",
338	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:345
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
344	          data_url=(
345	              "https://dl.fbaipublicfiles.com/glue/superglue/data/v2/CB.zip"
346	          ),
347	          citation=_CB_CITATION,
348	          url="https://github.com/mcdm/CommitmentBank",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:348
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
347	          citation=_CB_CITATION,
348	          url="https://github.com/mcdm/CommitmentBank",
349	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:358
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
357	          data_url=(
358	              "https://dl.fbaipublicfiles.com/glue/superglue/data/v2/COPA.zip"
359	          ),
360	          citation=_COPA_CITATION,
361	          url="http://people.ict.usc.edu/~gordon/copa.html",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:361
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
360	          citation=_COPA_CITATION,
361	          url="http://people.ict.usc.edu/~gordon/copa.html",
362	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:367
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
366	          features=["paragraph", "question", "answer"],
367	          data_url="https://dl.fbaipublicfiles.com/glue/superglue/data/v2/MultiRC.zip",
368	          citation=_MULTIRC_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:369
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
368	          citation=_MULTIRC_CITATION,
369	          url="https://cogcomp.org/multirc/",
370	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:379
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
378	          data_url=(
379	              "https://dl.fbaipublicfiles.com/glue/superglue/data/v2/ReCoRD.zip"
380	          ),
381	          citation=_RECORD_CITATION,
382	          url="https://sheng-z.github.io/ReCoRD-explorer/",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:382
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
381	          citation=_RECORD_CITATION,
382	          url="https://sheng-z.github.io/ReCoRD-explorer/",
383	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:390
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
389	          data_url=(
390	              "https://dl.fbaipublicfiles.com/glue/superglue/data/v2/RTE.zip"
391	          ),
392	          citation=_RTE_CITATION,
393	          url="https://aclweb.org/aclwiki/Recognizing_Textual_Entailment",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:393
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
392	          citation=_RTE_CITATION,
393	          url="https://aclweb.org/aclwiki/Recognizing_Textual_Entailment",
394	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:410
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
409	          data_url=(
410	              "https://dl.fbaipublicfiles.com/glue/superglue/data/v2/WiC.zip"
411	          ),
412	          citation=_WIC_CITATION,
413	          url="https://pilehvar.github.io/wic/",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:413
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
412	          citation=_WIC_CITATION,
413	          url="https://pilehvar.github.io/wic/",
414	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:428
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
427	          data_url=(
428	              "https://dl.fbaipublicfiles.com/glue/superglue/data/v2/WSC.zip"
429	          ),
430	          citation=_WSC_CITATION,
431	          url=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:432
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
431	          url=(
432	              "https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html"
433	          ),
434	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:453
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
452	          data_url=(
453	              "https://dl.fbaipublicfiles.com/glue/superglue/data/v2/WSC.zip"
454	          ),
455	          citation=_WSC_CITATION,
456	          url=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:457
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
456	          url=(
457	              "https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html"
458	          ),
459	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:466
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
465	          data_url=(
466	              "https://dl.fbaipublicfiles.com/glue/superglue/data/v2/AX-b.zip"
467	          ),
468	          citation="",  # The GLUE citation is sufficient.
469	          url="https://gluebenchmark.com/diagnostics",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:469
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
468	          citation="",  # The GLUE citation is sufficient.
469	          url="https://gluebenchmark.com/diagnostics",
470	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:477
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
476	          data_url=(
477	              "https://dl.fbaipublicfiles.com/glue/superglue/data/v2/AX-g.zip"
478	          ),
479	          citation=_AXG_CITATION,
480	          url="https://github.com/rudinger/winogender-schemas",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/super_glue.py:480
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
479	          citation=_AXG_CITATION,
480	          url="https://github.com/rudinger/winogender-schemas",
481	      ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_DESCRIPTION = """
28	The UnifiedQA benchmark consists of 20 main question answering (QA) datasets
29	(each may have multiple versions) that target different formats as well as
30	various complex linguistic phenomena. These datasets are grouped into several
31	formats/categories, including: extractive QA, abstractive QA, multiple-choice
32	QA, and yes/no QA. Additionally, contrast sets are used for several datasets
33	(denoted with "contrast_sets_"). These evaluation sets are expert-generated
34	perturbations that deviate from the patterns common in the original dataset. For
35	several datasets that do not come with evidence paragraphs, two variants are
36	included: one where the datasets are used as-is and another that uses paragraphs
37	fetched via an information retrieval system as additional evidence, indicated
38	with "_ir" tags.
39	
40	More information can be found at: https://github.com/allenai/unifiedqa.
41	"""
42	
43	_HOMEPAGE = 'https://github.com/allenai/unifiedqa'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	
43	_HOMEPAGE = 'https://github.com/allenai/unifiedqa'
44	
45	_CITATION = """@inproceedings{khashabi-etal-2020-unifiedqa,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	
45	_CITATION = """@inproceedings{khashabi-etal-2020-unifiedqa,
46	    title = "{UNIFIEDQA}: Crossing Format Boundaries with a Single {QA} System",
47	    author = "Khashabi, Daniel  and
48	      Min, Sewon  and
49	      Khot, Tushar  and
50	      Sabharwal, Ashish  and
51	      Tafjord, Oyvind  and
52	      Clark, Peter  and
53	      Hajishirzi, Hannaneh",
54	    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
55	    month = nov,
56	    year = "2020",
57	    address = "Online",
58	    publisher = "Association for Computational Linguistics",
59	    url = "https://aclanthology.org/2020.findings-emnlp.171",
60	    doi = "10.18653/v1/2020.findings-emnlp.171",
61	    pages = "1896--1907",
62	}
63	
64	Note that each UnifiedQA dataset has its own citation. Please see the source to
65	see the correct citation for each contained dataset."
66	"""
67	
68	_URL_BASE = 'https://storage.googleapis.com/unifiedqa/data'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:68
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
67	
68	_URL_BASE = 'https://storage.googleapis.com/unifiedqa/data'
69	
70	
71	class UnifiedQAConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:122
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
121	          citation=textwrap.dedent(
122	              """\
123	          http://data.allenai.org/ai2-science-questions
124	          """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:145
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
144	          citation=textwrap.dedent(
145	              """\
146	          http://data.allenai.org/ai2-science-questions
147	          """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:165
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
164	          citation=textwrap.dedent(
165	              """\
166	          @inproceedings{min-etal-2020-ambigqa,
167	              title = "{A}mbig{QA}: Answering Ambiguous Open-domain Questions",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:442
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
441	          citation=textwrap.dedent(
442	              """\
443	          @inproceedings{clark-etal-2019-boolq,
444	              title = "{B}ool{Q}: Exploring the Surprising Difficulty of Natural Yes/No Questions",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:482
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
481	          citation=textwrap.dedent(
482	              """\
483	          @inproceedings{khashabi-etal-2020-bang,
484	              title = "More Bang for Your Buck: Natural Perturbation for Robust Question Answering",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:517
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
516	          citation=textwrap.dedent(
517	              """\
518	          @inproceedings{talmor-etal-2019-commonsenseqa,
519	              title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:553
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
552	          citation=textwrap.dedent(
553	              """\
554	          @inproceedings{talmor-etal-2019-commonsenseqa,
555	              title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:592
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
591	          citation=textwrap.dedent(
592	              """\
593	          @inproceedings{clark-etal-2019-boolq,
594	              title = "{B}ool{Q}: Exploring the Surprising Difficulty of Natural Yes/No Questions",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:634
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
633	          citation=textwrap.dedent(
634	              """\
635	          @inproceedings{dua-etal-2019-drop,
636	              title = "{DROP}: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:674
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
673	          citation=textwrap.dedent(
674	              """\
675	          @inproceedings{dasigi-etal-2019-quoref,
676	              title = "{Q}uoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:715
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
714	          citation=textwrap.dedent(
715	              """\
716	          @inproceedings{lin-etal-2019-reasoning,
717	              title = "Reasoning Over Paragraph Effects in Situations",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:752
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
751	          citation=textwrap.dedent(
752	              """\
753	          @inproceedings{dua-etal-2019-drop,
754	              title = "{DROP}: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:794
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
793	          citation=textwrap.dedent(
794	              """\
795	          @inproceedings{richardson-etal-2013-mctest,
796	              title = "{MCT}est: A Challenge Dataset for the Open-Domain Machine Comprehension of Text",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:834
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
833	          citation=textwrap.dedent(
834	              """\
835	          @inproceedings{richardson-etal-2013-mctest,
836	              title = "{MCT}est: A Challenge Dataset for the Open-Domain Machine Comprehension of Text",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:871
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
870	          citation=textwrap.dedent(
871	              """\
872	          @inproceedings{khashabi-etal-2018-looking,
873	              title = "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:907
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
906	          citation=textwrap.dedent(
907	              """\
908	          @article{kocisky-etal-2018-narrativeqa,
909	              title = "The {N}arrative{QA} Reading Comprehension Challenge",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:945
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
944	          citation=textwrap.dedent(
945	              """\
946	          @article{kocisky-etal-2018-narrativeqa,
947	              title = "The {N}arrative{QA} Reading Comprehension Challenge",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:985
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
984	          citation=textwrap.dedent(
985	              """\
986	          @article{kwiatkowski-etal-2019-natural,
987	              title = "Natural Questions: A Benchmark for Question Answering Research",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:1038
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1037	          citation=textwrap.dedent(
1038	              """\
1039	          @article{kwiatkowski-etal-2019-natural,
1040	              title = "Natural Questions: A Benchmark for Question Answering Research",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:1095
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1094	          citation=textwrap.dedent(
1095	              """\
1096	          @article{kwiatkowski-etal-2019-natural,
1097	              title = "Natural Questions: A Benchmark for Question Answering Research",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:1150
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1149	          citation=textwrap.dedent(
1150	              """\
1151	          @article{kwiatkowski-etal-2019-natural,
1152	              title = "Natural Questions: A Benchmark for Question Answering Research",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:1207
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1206	          citation=textwrap.dedent(
1207	              """\
1208	          @article{kwiatkowski-etal-2019-natural,
1209	              title = "Natural Questions: A Benchmark for Question Answering Research",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:1256
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1255	          citation=textwrap.dedent(
1256	              """\
1257	          @inproceedings{trischler-etal-2017-newsqa,
1258	              title = "{N}ews{QA}: A Machine Comprehension Dataset",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:1299
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1298	          citation=textwrap.dedent(
1299	              """\
1300	          @inproceedings{mihaylov-etal-2018-suit,
1301	              title = "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:1339
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1338	          citation=textwrap.dedent(
1339	              """\
1340	          @inproceedings{mihaylov-etal-2018-suit,
1341	              title = "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:1380
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1379	          citation=textwrap.dedent(
1380	              """\
1381	          @inproceedings{mihaylov-etal-2018-suit,
1382	              title = "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:1421
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1420	          citation=textwrap.dedent(
1421	              """\
1422	          @inproceedings{mihaylov-etal-2018-suit,
1423	              title = "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:1614
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1613	          citation=textwrap.dedent(
1614	              """\
1615	          @inproceedings{dasigi-etal-2019-quoref,
1616	              title = "{Q}uoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:1651
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1650	          citation=textwrap.dedent(
1651	              """\
1652	          @inproceedings{lai-etal-2017-race,
1653	              title = "{RACE}: Large-scale {R}e{A}ding Comprehension Dataset From Examinations",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:1688
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1687	          citation=textwrap.dedent(
1688	              """\
1689	          @inproceedings{lai-etal-2017-race,
1690	              title = "{RACE}: Large-scale {R}e{A}ding Comprehension Dataset From Examinations",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:1727
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1726	          citation=textwrap.dedent(
1727	              """\
1728	          @inproceedings{lin-etal-2019-reasoning,
1729	              title = "Reasoning Over Paragraph Effects in Situations",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:1766
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1765	          citation=textwrap.dedent(
1766	              """\
1767	          @inproceedings{sap-etal-2019-social,
1768	              title = "Social {IQ}a: Commonsense Reasoning about Social Interactions",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:1802
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1801	          citation=textwrap.dedent(
1802	              """\
1803	          @inproceedings{rajpurkar-etal-2016-squad,
1804	              title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unifiedqa/unifiedqa.py:1836
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
1835	          citation=textwrap.dedent(
1836	              """\
1837	          @inproceedings{rajpurkar-etal-2018-know,
1838	              title = "Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unnatural_instructions/unnatural_instructions.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_DS_PATH = 'https://github.com/orhonovich/unnatural-instructions/raw/main/data/full_data.zip'
24	
25	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unnatural_instructions/unnatural_instructions.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	
30	_CITATION = """
31	@misc{honovich2022unnatural,
32	      title = {Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor},
33	      author = {Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo},
34	      url = {https://arxiv.org/abs/2212.09689},
35	      publisher = {arXiv},
36	      year={2022}
37	}
38	"""
39	
40	
41	class UnnaturalInstructions(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/unnatural_instructions/unnatural_instructions.py:101
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
100	        }),
101	        homepage='https://github.com/orhonovich/unnatural-instructions',
102	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/userlibri_lm_data/userlibri_lm_data.py:35
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
34	
35	_DESCRIPTION = """\
36	UserLibri is a dataset containing paired audio-transcripts and additional text
37	only data for each of 107 users. It is a reformatting of the LibriSpeech dataset
38	found at http://www.openslr.org/12, reorganizing the data into users with an
39	average of 52 LibriSpeech utterances and about 6,700 text example sentences per
40	user. The UserLibriAudio class provides access to the audio-transcript pairs.
41	See UserLibriText for the additional text data.
42	"""
43	
44	_URL = "https://www.kaggle.com/datasets/google/userlibri"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/userlibri_lm_data/userlibri_lm_data.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	
44	_URL = "https://www.kaggle.com/datasets/google/userlibri"
45	_DL_URL = tfds.download.Resource(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/userlibri_lm_data/userlibri_lm_data.py:46
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
45	_DL_URL = tfds.download.Resource(
46	    url="https://www.kaggle.com/datasets/google/userlibri/download",
47	    extract_method=tfds.download.ExtractMethod.ZIP,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wiki40b.py:33
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
32	
33	_DESCRIPTION = """
34	Clean-up text for 40+ Wikipedia languages editions of pages
35	correspond to entities. The datasets have train/dev/test splits per language.
36	The dataset is cleaned up by page filtering to remove disambiguation pages,
37	redirect pages, deleted pages, and non-entity pages. Each example contains the
38	wikidata id of the entity, and the full Wikipedia article after page processing
39	that removes non-content sections and structured objects. The language models
40	trained on this corpus - including 41 monolingual models, and 2 multilingual
41	models - can be found at https://tfhub.dev/google/collections/wiki40b-lm/1.
42	"""
43	
44	_LICENSE = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wiki40b.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	
44	_LICENSE = """
45	This work is licensed under the Creative Commons Attribution-ShareAlike
46	3.0 Unported License. To view a copy of this license, visit
47	http://creativecommons.org/licenses/by-sa/3.0/ or send a letter to
48	Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.
49	"""
50	
51	_URL = "https://research.google/pubs/pub49029/"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wiki40b.py:51
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
50	
51	_URL = "https://research.google/pubs/pub49029/"
52	
53	_DATA_DIRECTORY = tfds.core.gcs_path("downloads/wiki40b/tfrecord_prod")

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wiki_dialog/wiki_dialog.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	_BASE_DOWNLOAD_URL = (
44	    'https://storage.googleapis.com/gresearch/dialog-inpainting/'
45	)
46	
47	
48	def _parse_json(text: str) -> Tuple[int, Dict[str, Any]]:
49	  """Parses query json object."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wiki_dialog/wiki_dialog.py:107
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
106	        homepage=(
107	            'https://github.com/google-research/dialog-inpainting#wikidialog-oq'
108	        ),
109	    )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wikiann/wikiann.py:32
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
31	
32	_CITATION = """
33	@inproceedings{rahimi-etal-2019-massively,
34	    title = "Massively Multilingual Transfer for {NER}",
35	    author = "Rahimi, Afshin  and
36	      Li, Yuan  and
37	      Cohn, Trevor",
38	    booktitle = "Proceedings of the 57th Annual Meeting of the Association \
39	    for Computational Linguistics",
40	    month = jul,
41	    year = "2019",
42	    address = "Florence, Italy",
43	    publisher = "Association for Computational Linguistics",
44	    url = "https://www.aclweb.org/anthology/P19-1015",
45	    pages = "151--164",
46	}
47	"""
48	
49	URL = "https://www.dropbox.com/s/12h3qqog6q4bjve/panx_dataset.tar?dl=1"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wikiann/wikiann.py:49
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
48	
49	URL = "https://www.dropbox.com/s/12h3qqog6q4bjve/panx_dataset.tar?dl=1"
50	
51	LANGS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wikiann/wikiann.py:345
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
344	        supervised_keys=None,
345	        homepage="https://github.com/afshinrahimi/mmner",
346	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wikipedia.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	
30	_CITATION = """\
31	@ONLINE {wikidump,
32	    author = "Wikimedia Foundation",
33	    title  = "Wikimedia Downloads",
34	    url    = "https://dumps.wikimedia.org"
35	}
36	"""
37	
38	_DESCRIPTION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wikipedia.py:38
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
37	
38	_DESCRIPTION = """\
39	Wikipedia dataset containing cleaned articles of all languages.
40	The datasets are built from the Wikipedia dump
41	(https://dumps.wikimedia.org/) with one split per language. Each example
42	contains the content of one full Wikipedia article with cleaning to strip
43	markdown and unwanted sections (references, etc.).
44	"""
45	
46	_LICENSE = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wikipedia.py:47
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
46	_LICENSE = (
47	    "This work is licensed under the Creative Commons Attribution-ShareAlike "
48	    "3.0 Unported License. To view a copy of this license, visit "
49	    "http://creativecommons.org/licenses/by-sa/3.0/ or send a letter to "
50	    "Creative Commons, PO Box 1866, Mountain View, CA 94042, USA."
51	)
52	
53	# Source: https://en.wikipedia.org/wiki/List_of_Wikipedias (accessed 3/1/2019)
54	# Removed because no articles: hz.
55	WIKIPEDIA_LANGUAGES = [
56	    "aa",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wikipedia.py:364
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
363	_BASE_URL_TMPL = (
364	    "https://mirror.accum.se/mirror/wikimedia.org/dumps/{lang}wiki/{date}/"
365	)
366	_BASE_URL_TMPL_OLD = "https://dumps.wikimedia.your.org/{lang}wiki/{date}/"
367	_INFO_FILE = "dumpstatus.json"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wikipedia.py:366
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
365	)
366	_BASE_URL_TMPL_OLD = "https://dumps.wikimedia.your.org/{lang}wiki/{date}/"
367	_INFO_FILE = "dumpstatus.json"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wikipedia.py:417
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
416	  RELEASE_NOTES = {
417	      "1.0.0": "New split API (https://tensorflow.org/datasets/splits)",
418	  }
419	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wikipedia.py:441
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
440	        supervised_keys=None,
441	        homepage="https://dumps.wikimedia.org",
442	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wikipedia_toxicity_subtypes.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	
25	_CITATION = """
26	@inproceedings{10.1145/3038912.3052591,
27	  author = {Wulczyn, Ellery and Thain, Nithum and Dixon, Lucas},
28	  title = {Ex Machina: Personal Attacks Seen at Scale},
29	  year = {2017},
30	  isbn = {9781450349130},
31	  publisher = {International World Wide Web Conferences Steering Committee},
32	  address = {Republic and Canton of Geneva, CHE},
33	  url = {https://doi.org/10.1145/3038912.3052591},
34	  doi = {10.1145/3038912.3052591},
35	  booktitle = {Proceedings of the 26th International Conference on World Wide Web},
36	  pages = {1391-1399},
37	  numpages = {9},
38	  keywords = {online discussions, wikipedia, online harassment},
39	  location = {Perth, Australia},
40	  series = {WWW '17}
41	}
42	"""
43	
44	_SUBTYPES_HOMEPAGE = 'https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wikipedia_toxicity_subtypes.py:44
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
43	
44	_SUBTYPES_HOMEPAGE = 'https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data'
45	_MULTILINGUAL_HOMEPAGE = 'https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/data'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wikipedia_toxicity_subtypes.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	_SUBTYPES_HOMEPAGE = 'https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data'
45	_MULTILINGUAL_HOMEPAGE = 'https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/data'
46	
47	_COMMON_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wikipedia_toxicity_subtypes.py:60
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
59	
60	_SUBTYPES_DESCRIPTION = """
61	The comments in the WikipediaToxicitySubtypes config are from an archive of
62	English Wikipedia talk page comments which have been annotated by Jigsaw for
63	toxicity, as well as five toxicity subtype labels (severe toxicity, obscene,
64	threat, insult, identity_attack). The toxicity and toxicity subtype labels are
65	binary values (0 or 1) indicating whether the majority of annotators assigned
66	that attribute to the comment text. This config is a replica of the data
67	released for the Jigsaw Toxic Comment Classification Challenge on Kaggle, with
68	the test dataset joined with the test_labels released after the competition, and
69	test data not used for scoring dropped.
70	
71	See the Kaggle documentation
72	https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data
73	or https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973 for more
74	details.
75	"""
76	
77	_MULTILINGUAL_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wikipedia_toxicity_subtypes.py:77
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
76	
77	_MULTILINGUAL_DESCRIPTION = """
78	The comments in the WikipediaToxicityMultilingual config here are from an
79	archive of non-English Wikipedia talk page comments annotated by Jigsaw for
80	toxicity, with a binary value (0 or 1) indicating whether the majority of
81	annotators rated the comment text as toxic. The comments in this config are in
82	multiple different languages (Turkish, Italian, Spanish, Portuguese, Russian,
83	and French). This config is a replica of the data released for the Jigsaw
84	Multilingual Toxic Comment Classification on Kaggle, with the test dataset
85	joined with the test_labels released after the competition.
86	
87	See the Kaggle documentation
88	https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/data
89	for more details.
90	"""
91	
92	_DOWNLOAD_URL = 'https://storage.googleapis.com/jigsaw-unintended-bias-in-toxicity-classification/wikipedia_toxicity_subtypes_v0.3.zip'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wikipedia_toxicity_subtypes.py:92
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
91	
92	_DOWNLOAD_URL = 'https://storage.googleapis.com/jigsaw-unintended-bias-in-toxicity-classification/wikipedia_toxicity_subtypes_v0.3.zip'
93	
94	TOXICITY_SUBTYPES = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/winogrande.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	
43	_DATA_URL = 'https://storage.googleapis.com/ai2-mosaic/public/winogrande/winogrande_1.1.zip'
44	
45	
46	class Winogrande(tfds.core.GeneratorBasedBuilder):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/winogrande.py:67
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
66	        supervised_keys=None,
67	        homepage='http://winogrande.allenai.org/',
68	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wordnet.py:45
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
44	
45	_CITATION = """@article{10.1145/219717.219748,
46	author = {Miller, George A.},
47	title = {WordNet: A Lexical Database for English},
48	year = {1995},
49	issue_date = {Nov. 1995},
50	publisher = {Association for Computing Machinery},
51	address = {New York, NY, USA},
52	volume = {38},
53	number = {11},
54	issn = {0001-0782},
55	url = {https://doi.org/10.1145/219717.219748},
56	doi = {10.1145/219717.219748},
57	journal = {Commun. ACM},
58	month = nov,
59	pages = {39--41},
60	numpages = {3}
61	}
62	"""
63	
64	_DESCRIPTION = """WordNet is a large lexical database of English. Nouns, verbs,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wordnet.py:70
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
69	
70	_WN18_DESCRIPTION = """This WORDNET TENSOR DATA consists of a collection of
71	triplets (synset, relation_type, triplet) extracted from WordNet 3.0
72	(http://wordnet.princeton.edu). This data set can be seen as a 3-mode tensor
73	depicting ternary relationships between synsets. See
74	https://everest.hds.utc.fr/doku.php?id=en:transe.
75	"""
76	
77	_WN18_CITATION = """@incollection{NIPS2013_5071,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wordnet.py:77
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
76	
77	_WN18_CITATION = """@incollection{NIPS2013_5071,
78	title = {Translating Embeddings for Modeling Multi-relational Data},
79	author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
80	booktitle = {Advances in Neural Information Processing Systems 26},
81	editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
82	pages = {2787--2795},
83	year = {2013},
84	publisher = {Curran Associates, Inc.},
85	url = {http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf}
86	}
87	"""
88	
89	_WN18RR_DESCRIPTION = """Same as WN18 but fixes test leakage through inverse

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wordnet.py:89
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
88	
89	_WN18RR_DESCRIPTION = """Same as WN18 but fixes test leakage through inverse
90	relations. See https://github.com/TimDettmers/ConvE.
91	"""
92	
93	_WN18RR_CITATION = """@inproceedings{dettmers2018conve,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wordnet.py:93
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
92	
93	_WN18RR_CITATION = """@inproceedings{dettmers2018conve,
94		Author = {Dettmers, Tim and Pasquale, Minervini and Pontus, Stenetorp and Riedel, Sebastian},
95		Booktitle = {Proceedings of the 32th AAAI Conference on Artificial Intelligence},
96		Title = {Convolutional 2D Knowledge Graph Embeddings},
97		Url = {https://arxiv.org/abs/1707.01476},
98		Year = {2018},
99	        pages  = {1811--1818},
100	  	Month = {February}
101	}
102	"""
103	
104	_RELATIONS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wordnet.py:180
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
179	        }),
180	        homepage='https://wordnet.princeton.edu/',
181	        citation=self.builder_config.citation,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wordnet.py:189
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
188	    dl_paths = dl_manager.download_and_extract({
189	        'WN18': 'https://everest.hds.utc.fr/lib/exe/fetch.php?media=en:wordnet-mlj12.tar.gz',
190	        'WN18RR': (
191	            'https://github.com/TimDettmers/ConvE/raw/master/WN18RR.tar.gz'
192	        ),
193	    })

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wordnet.py:191
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
190	        'WN18RR': (
191	            'https://github.com/TimDettmers/ConvE/raw/master/WN18RR.tar.gz'
192	        ),
193	    })
194	    # Metadata is at the configuration level and is the same for all splits.
195	    synset_definitions_path = os.path.join(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wsc273/wsc273.py:42
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
41	_HOMEPAGE_URL = (
42	    "https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html"
43	)
44	
45	_DOWNLOAD_URL = (
46	    "https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WSCollection.xml"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/wsc273/wsc273.py:46
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
45	_DOWNLOAD_URL = (
46	    "https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WSCollection.xml"
47	)
48	
49	
50	class Wsc273(tfds.core.GeneratorBasedBuilder):
51	  """The WSC273 dataset."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/xnli.py:50
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
49	
50	_DATA_URL = 'https://cims.nyu.edu/~sbowman/xnli/XNLI-1.0.zip'
51	
52	_LANGUAGES = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/xnli.py:94
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
93	        supervised_keys=None,
94	        homepage='https://www.nyu.edu/projects/bowman/xnli/',
95	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/xtreme_pawsx/xtreme_pawsx.py:34
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
33	
34	_DESCRIPTION = """
35	This dataset contains machine translations of the English PAWS training
36	data. The translations are provided by the XTREME benchmark and cover the following
37	languages:
38	
39	* French
40	* Spanish
41	* German
42	* Chinese
43	* Japanese
44	* Korean
45	
46	For further details on PAWS, see the  papers:
47	PAWS: Paraphrase Adversaries from Word Scrambling
48	at https://arxiv.org/abs/1904.01130
49	and
50	PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification
51	at  https://arxiv.org/abs/1908.11828
52	
53	For details related to XTREME, please refer to:
54	XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization
55	at https://arxiv.org/abs/2003.11080
56	"""
57	
58	_XTREME_TRANSLATIONS_FORMAT = "https://storage.googleapis.com/xtreme_translations/PAWSX/translate-train/en-{0}-translated.tsv"

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/xtreme_pawsx/xtreme_pawsx.py:58
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
57	
58	_XTREME_TRANSLATIONS_FORMAT = "https://storage.googleapis.com/xtreme_translations/PAWSX/translate-train/en-{0}-translated.tsv"
59	
60	_CLASS_LABELS = ["different_meaning", "paraphrase"]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/xtreme_pawsx/xtreme_pawsx.py:101
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
100	        }),
101	        homepage="https://github.com/google-research/xtreme",
102	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/xtreme_xnli/xtreme_xnli.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	
41	_XTREME_TRANSLATIONS_FORMAT = 'https://storage.googleapis.com/xtreme_translations/XNLI/translate-train/en-{0}-translated.tsv'
42	
43	_LANGUAGES = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/xtreme_xnli/xtreme_xnli.py:85
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
84	        supervised_keys=None,
85	        homepage='https://www.nyu.edu/projects/bowman/xnli/',
86	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/yelp_polarity.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_DESCRIPTION = """\
24	Large Yelp Review Dataset.
25	This is a dataset for binary sentiment classification. \
26	We provide a set of 560,000 highly polar yelp reviews for training, and 38,000 for testing. \
27	
28	ORIGIN
29	The Yelp reviews dataset consists of reviews from Yelp. It is extracted
30	from the Yelp Dataset Challenge 2015 data. For more information, please
31	refer to http://www.yelp.com/dataset
32	
33	The Yelp reviews polarity dataset is constructed by
34	Xiang Zhang (xiang.zhang@nyu.edu) from the above dataset.
35	It is first used as a text classification benchmark in the following paper:
36	Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks
37	for Text Classification. Advances in Neural Information Processing Systems 28
38	(NIPS 2015).
39	
40	
41	DESCRIPTION
42	
43	The Yelp reviews polarity dataset is constructed by considering stars 1 and 2
44	negative, and 3 and 4 positive. For each polarity 280,000 training samples and
45	19,000 testing samples are take randomly. In total there are 560,000 trainig
46	samples and 38,000 testing samples. Negative polarity is class 1,
47	and positive class 2.
48	
49	The files train.csv and test.csv contain all the training samples as
50	comma-sparated values. There are 2 columns in them, corresponding to class
51	index (1 and 2) and review text. The review texts are escaped using double
52	quotes ("), and any internal double quote is escaped by 2 double quotes ("").
53	New lines are escaped by a backslash followed with an "n" character,
54	that is "\n".
55	"""
56	
57	_CITATION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/yelp_polarity.py:74
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
73	_DOWNLOAD_URL = (
74	    "https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz"
75	)
76	
77	
78	class YelpPolarityReviews(tfds.core.GeneratorBasedBuilder):
79	  """Yelp Polarity reviews dataset."""

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text/yelp_polarity.py:92
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
91	        supervised_keys=("text", "label"),
92	        homepage="https://course.fast.ai/datasets",
93	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text_simplification/wiki_auto/wiki_auto.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	
23	_CITATION = """\
24	@inproceedings{acl/JiangMLZX20,
25	  author    = {Chao Jiang and
26	               Mounica Maddela and
27	               Wuwei Lan and
28	               Yang Zhong and
29	               Wei Xu},
30	  editor    = {Dan Jurafsky and
31	               Joyce Chai and
32	               Natalie Schluter and
33	               Joel R. Tetreault},
34	  title     = {Neural {CRF} Model for Sentence Alignment in Text Simplification},
35	  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational
36	               Linguistics, {ACL} 2020, Online, July 5-10, 2020},
37	  pages     = {7943--7960},
38	  publisher = {Association for Computational Linguistics},
39	  year      = {2020},
40	  url       = {https://www.aclweb.org/anthology/2020.acl-main.709/}
41	}
42	"""
43	
44	_DESCRIPTION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text_simplification/wiki_auto/wiki_auto.py:59
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
58	    'manual': {
59	        'dev': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-manual/dev.tsv',
60	        'test': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-manual/test.tsv',
61	    },
62	    'auto_acl': {
63	        'normal': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/ACL2020/train.src',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text_simplification/wiki_auto/wiki_auto.py:60
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
59	        'dev': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-manual/dev.tsv',
60	        'test': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-manual/test.tsv',
61	    },
62	    'auto_acl': {
63	        'normal': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/ACL2020/train.src',
64	        'simple': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/ACL2020/train.dst',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text_simplification/wiki_auto/wiki_auto.py:63
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
62	    'auto_acl': {
63	        'normal': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/ACL2020/train.src',
64	        'simple': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/ACL2020/train.dst',
65	    },
66	    'auto_full_no_split': {
67	        'normal': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_no_split/train.src',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text_simplification/wiki_auto/wiki_auto.py:64
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
63	        'normal': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/ACL2020/train.src',
64	        'simple': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/ACL2020/train.dst',
65	    },
66	    'auto_full_no_split': {
67	        'normal': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_no_split/train.src',
68	        'simple': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_no_split/train.dst',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text_simplification/wiki_auto/wiki_auto.py:67
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
66	    'auto_full_no_split': {
67	        'normal': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_no_split/train.src',
68	        'simple': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_no_split/train.dst',
69	    },
70	    'auto_full_with_split': {
71	        'normal': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_with_split/train.src',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text_simplification/wiki_auto/wiki_auto.py:68
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
67	        'normal': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_no_split/train.src',
68	        'simple': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_no_split/train.dst',
69	    },
70	    'auto_full_with_split': {
71	        'normal': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_with_split/train.src',
72	        'simple': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_with_split/train.dst',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text_simplification/wiki_auto/wiki_auto.py:71
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
70	    'auto_full_with_split': {
71	        'normal': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_with_split/train.src',
72	        'simple': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_with_split/train.dst',
73	    },
74	    'auto': {
75	        'part_1': 'https://dl.dropboxusercontent.com/sh/ohqaw41v48c7e5p/AAATBDhU1zpdcT5x5WgO8DMaa/wiki-auto-all-data/wiki-auto-part-1-data.json',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text_simplification/wiki_auto/wiki_auto.py:72
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
71	        'normal': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_with_split/train.src',
72	        'simple': 'https://github.com/chaojiang06/wiki-auto/raw/master/wiki-auto/GEM2021/full_with_split/train.dst',
73	    },
74	    'auto': {
75	        'part_1': 'https://dl.dropboxusercontent.com/sh/ohqaw41v48c7e5p/AAATBDhU1zpdcT5x5WgO8DMaa/wiki-auto-all-data/wiki-auto-part-1-data.json',
76	        'part_2': 'https://dl.dropboxusercontent.com/sh/ohqaw41v48c7e5p/AAATgPkjo_tPt9z12vZxJ3MRa/wiki-auto-all-data/wiki-auto-part-2-data.json',

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text_simplification/wiki_auto/wiki_auto.py:75
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
74	    'auto': {
75	        'part_1': 'https://dl.dropboxusercontent.com/sh/ohqaw41v48c7e5p/AAATBDhU1zpdcT5x5WgO8DMaa/wiki-auto-all-data/wiki-auto-part-1-data.json',
76	        'part_2': 'https://dl.dropboxusercontent.com/sh/ohqaw41v48c7e5p/AAATgPkjo_tPt9z12vZxJ3MRa/wiki-auto-all-data/wiki-auto-part-2-data.json',
77	    },
78	}

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text_simplification/wiki_auto/wiki_auto.py:76
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
75	        'part_1': 'https://dl.dropboxusercontent.com/sh/ohqaw41v48c7e5p/AAATBDhU1zpdcT5x5WgO8DMaa/wiki-auto-all-data/wiki-auto-part-1-data.json',
76	        'part_2': 'https://dl.dropboxusercontent.com/sh/ohqaw41v48c7e5p/AAATgPkjo_tPt9z12vZxJ3MRa/wiki-auto-all-data/wiki-auto-part-2-data.json',
77	    },
78	}
79	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/text_simplification/wiki_auto/wiki_auto.py:178
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
177	        supervised_keys=None,
178	        homepage='https://github.com/chaojiang06/wiki-auto',
179	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/flores.py:38
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
37	
38	_DATA_URL = "https://github.com/facebookresearch/flores/raw/master/data/wikipedia_en_ne_si_test_sets.tgz"
39	
40	# Tuple that describes a single pair of files with matching translations.
41	# language_to_file is the map from language (2 letter string: example 'en')
42	# to the file path in the extracted directory.
43	TranslateData = collections.namedtuple(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/flores.py:108
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
107	        supervised_keys=(source, target),
108	        homepage="https://github.com/facebookresearch/flores/",
109	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/mtnt/mtnt.py:37
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
36	
37	URL_1_1 = 'https://github.com/pmichel31415/mtnt/releases/download/v1.1/MTNT.1.1.tar.gz'
38	
39	
40	@dataclasses.dataclass
41	class MtntConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/mtnt/mtnt.py:75
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
74	        supervised_keys=('src', 'dst'),  # Set to `None` to disable
75	        homepage='https://pmichel31415.github.io/mtnt/index.html',
76	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:132
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
131	        sources={"cs", "de", "es", "fr", "ru"},
132	        url="http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz",
133	        path=("commoncrawl.{src}-en.{src}", "commoncrawl.{src}-en.en"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:140
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
139	        url=(
140	            "http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/commoncrawl.fr.gz",
141	            "http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/commoncrawl.de.gz",
142	        ),
143	        path=("", ""),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:141
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
140	            "http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/commoncrawl.fr.gz",
141	            "http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/commoncrawl.de.gz",
142	        ),
143	        path=("", ""),
144	    ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:149
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
148	        sources={"cs"},
149	        url="http://ufal.mff.cuni.cz/czeng/czeng10",
150	        manual_dl_files=["data-plaintext-format.%d.tar" % i for i in range(10)],

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:159
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
158	        sources={"cs"},
159	        url="http://ufal.mff.cuni.cz/czeng/czeng16pre",
160	        manual_dl_files=["czeng16pre.deduped-ignoring-sections.txt.gz"],

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:167
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
166	        sources={"cs"},
167	        url="http://ufal.mff.cuni.cz/czeng",
168	        manual_dl_files=["data-plaintext-format.%d.tar" % i for i in range(10)],

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:179
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
178	        sources={"cs"},
179	        url="http://ufal.mff.cuni.cz/czeng",
180	        manual_dl_files=["data-plaintext-format.%d.tar" % i for i in range(10)],

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:189
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
188	        sources={"lv"},
189	        url="http://data.statmt.org/wmt17/translation-task/dcep.lv-en.v1.tgz",
190	        path=("dcep.en-lv/dcep.lv", "dcep.en-lv/dcep.en"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:196
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
195	        sources={"cs", "de", "es", "fr"},
196	        url="http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz",
197	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:207
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
206	        url=(
207	            "http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/europarl-v7.fr.gz",
208	            "http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/europarl-v7.de.gz",
209	        ),
210	        path=("", ""),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:208
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
207	            "http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/europarl-v7.fr.gz",
208	            "http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/europarl-v7.de.gz",
209	        ),
210	        path=("", ""),
211	    ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:216
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
215	        sources={"et", "fi"},
216	        url="http://data.statmt.org/wmt18/translation-task/training-parallel-ep-v8.tgz",
217	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:226
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
225	        sources={"fi", "ro"},
226	        url="http://data.statmt.org/wmt16/translation-task/training-parallel-ep-v8.tgz",
227	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:236
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
235	        sources={"cs", "de", "fi", "lt"},
236	        url="http://www.statmt.org/europarl/v9/training/europarl-v9.{src}-en.tsv.gz",
237	        path="",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:243
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
242	        sources={"fr"},
243	        url="http://www.statmt.org/wmt10/training-giga-fren.tar",
244	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:253
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
252	        sources={"hi"},
253	        url="http://ufallab.ms.mff.cuni.cz/~bojar/hindencorp",
254	        manual_dl_files=["hindencorp0.1.gz"],

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:261
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
260	        sources={"lv"},
261	        url="http://data.statmt.org/wmt17/translation-task/leta.v1.tgz",
262	        path=("LETA-lv-en/leta.lv", "LETA-lv-en/leta.en"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:268
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
267	        sources={"es", "fr"},
268	        url="http://www.statmt.org/wmt13/training-parallel-un.tgz",
269	        path=("un/undoc.2000.{src}-en.{src}", "un/undoc.2000.{src}-en.en"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:275
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
274	        sources={"cs", "de", "fr", "es", "ru"},
275	        url="http://www.statmt.org/wmt13/training-parallel-nc-v8.tgz",
276	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:285
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
284	        sources={"cs", "de", "fr", "ru"},
285	        url="http://www.statmt.org/wmt14/training-parallel-nc-v9.tgz",
286	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:295
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
294	        sources={"cs", "de", "fr", "ru"},
295	        url="http://www.statmt.org/wmt15/training-parallel-nc-v10.tgz",
296	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:305
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
304	        sources={"cs", "de", "ru"},
305	        url="http://data.statmt.org/wmt16/translation-task/training-parallel-nc-v11.tgz",
306	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:315
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
314	        sources={"cs", "de", "ru", "zh"},
315	        url="http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz",
316	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:325
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
324	        sources={"cs", "de", "ru", "zh"},
325	        url="http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz",
326	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:335
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
334	        sources={"cs", "de", "kk", "ru", "zh"},
335	        url="http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.{0}-{1}.tsv.gz",
336	        path="",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:342
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
341	        sources={"fr"},
342	        url="http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.de-fr.tsv.gz",
343	        path="",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:349
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
348	        sources={"lv"},
349	        url="http://data.statmt.org/wmt17/translation-task/books.lv-en.v1.tgz",
350	        path=("farewell/farewell.lv", "farewell/farewell.en"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:356
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
355	        sources={"cs", "de", "et", "fi", "ru"},
356	        url="https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-{src}.zipporah0-dedup-clean.tgz",
357	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:366
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
365	        sources={"ru"},
366	        url="https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-ru.zipporah0-dedup-clean.tgz",
367	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:376
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
375	        sources={"cs", "de", "fi", "lt"},
376	        url="https://s3.amazonaws.com/web-language-models/paracrawl/release3/en-{src}.bicleaner07.tmx.gz",
377	        path="",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:384
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
383	        url=(
384	            "http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/de-fr.bicleaner07.de.gz",
385	            "http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/de-fr.bicleaner07.fr.gz",
386	        ),
387	        path=("", ""),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:385
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
384	            "http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/de-fr.bicleaner07.de.gz",
385	            "http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/de-fr.bicleaner07.fr.gz",
386	        ),
387	        path=("", ""),
388	    ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:393
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
392	        sources={"de", "et", "fi"},
393	        url="http://data.statmt.org/wmt18/translation-task/rapid2016.tgz",
394	        path=("rapid2016.{0}-{1}.{src}", "rapid2016.{0}-{1}.en"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:400
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
399	        sources={"fi", "lt"},
400	        url="https://tilde-model.s3-eu-west-1.amazonaws.com/rapid2016.en-{src}.tmx.zip",
401	        path="rapid2016.en-{src}.tmx",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:408
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
407	        url=(
408	            "https://s3-eu-west-1.amazonaws.com/tilde-model/rapid2019.de-en.zip"
409	        ),
410	        path=("rapid2019.de-en.de", "rapid2019.de-en.en"),
411	    ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:416
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
415	        sources={"ro", "tr"},
416	        url="http://opus.nlpl.eu/download.php?f=SETIMES/v2/tmx/en-{src}.tmx.gz",
417	        path="",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:423
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
422	        sources={"ru", "zh"},
423	        url="https://storage.googleapis.com/tfds-data/downloads/uncorpus/UNv1.0.en-{src}.tar.gz",
424	        path=("en-{src}/UNv1.0.en-{src}.{src}", "en-{src}/UNv1.0.en-{src}.en"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:430
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
429	        sources={"fi"},
430	        url="http://www.statmt.org/wmt15/wiki-titles.tgz",
431	        path="wiki/fi-en/titles.fi-en",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:437
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
436	        sources={"hi"},
437	        url="http://www.statmt.org/wmt14/wiki-titles.tgz",
438	        path="wiki/hi-en/wiki-titles.hi-en",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:445
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
444	        sources={"ru"},
445	        url="http://www.statmt.org/wmt15/wiki-titles.tgz",
446	        path="wiki/ru-en/wiki.ru-en",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:453
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
452	        url=(
453	            "http://data.statmt.org/wikititles/v1/wikititles-v1.{src}-en.tsv.gz"
454	        ),
455	        path="",
456	    ),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:461
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
460	        sources={"ru"},
461	        url="https://translate.yandex.ru/corpus?lang=en",
462	        manual_dl_files=["1mcorpus.zip"],

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:482
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
481	        sources={"fr"},
482	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
483	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:492
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
491	        sources={"hi"},
492	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
493	        path=("dev/newsdev2014.hi", "dev/newsdev2014.en"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:499
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
498	        sources={"fi"},
499	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
500	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:509
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
508	        sources={"ro", "tr"},
509	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
510	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:519
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
518	        sources={"ro", "tr"},
519	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
520	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:529
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
528	        sources={"lv", "zh"},
529	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
530	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:539
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
538	        sources={"et"},
539	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
540	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:549
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
548	        sources={"gu", "kk", "lt"},
549	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
550	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:559
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
558	        sources={"fr"},
559	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
560	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:569
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
568	        sources={"fr"},
569	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
570	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:579
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
578	        sources={"cs", "de", "es", "fr"},
579	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
580	        path=("dev/newssyscomb2009.{src}", "dev/newssyscomb2009.en"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:586
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
585	        sources={"cs", "de", "es", "fr", "hu"},
586	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
587	        path=("dev/news-test2008.{src}", "dev/news-test2008.en"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:593
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
592	        sources={"cs", "de", "es", "fr"},
593	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
594	        path=("dev/newstest2009.{src}", "dev/newstest2009.en"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:600
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
599	        sources={"cs", "de", "es", "fr"},
600	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
601	        path=("dev/newstest2010.{src}", "dev/newstest2010.en"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:607
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
606	        sources={"cs", "de", "es", "fr"},
607	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
608	        path=("dev/newstest2011.{src}", "dev/newstest2011.en"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:614
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
613	        sources={"cs", "de", "es", "fr", "ru"},
614	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
615	        path=("dev/newstest2012.{src}", "dev/newstest2012.en"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:621
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
620	        sources={"cs", "de", "es", "fr", "ru"},
621	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
622	        path=("dev/newstest2013.{src}", "dev/newstest2013.en"),

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:628
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
627	        sources={"cs", "de", "es", "fr", "hi", "ru"},
628	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
629	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:638
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
637	        sources={"cs", "de", "fi", "ru"},
638	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
639	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:648
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
647	        sources={"fr"},
648	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
649	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:658
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
657	        sources={"cs", "de", "fi", "ro", "ru", "tr"},
658	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
659	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:668
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
667	        sources={"fi"},
668	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
669	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:678
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
677	        sources={"cs", "de", "fi", "lv", "ru", "tr", "zh"},
678	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
679	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:688
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
687	        sources={"fi"},
688	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
689	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:698
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
697	        sources={"cs", "de", "et", "fi", "ru", "tr", "zh"},
698	        url="http://data.statmt.org/wmt19/translation-task/dev.tgz",
699	        path=(

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:712
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
711	    sources={"cs"},
712	    url="http://ufal.mff.cuni.cz/czeng/download.php?f=convert_czeng16_to_17.pl.zip",
713	    path="convert_czeng16_to_17.pl",

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:750
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
749	
750	    self.url = url or "http://www.statmt.org"
751	    self.citation = citation

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:967
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
966	      with epath.Path(path).open("rb") as f, gzip.GzipFile(fileobj=f) as g:
967	        return g.read().decode("utf-8").split("\n"), lang
968	

--------------------------------------------------
>> Issue: [B814:read] os.read
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt.py:1109
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b814_read.html
1108	      bad_blocks = set(
1109	          re.search(r"qw{([\s\d]*)}", f.read()).groups()[0].split()  # pytype: disable=attribute-error
1110	      )

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt13.py:21
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
20	
21	_URL = "http://www.statmt.org/wmt13/translation-task.html"
22	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt13.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	_URL = "http://www.statmt.org/wmt13/translation-task.html"
22	_CITATION = """
23	@InProceedings{bojar-EtAl:2013:WMT,
24	  author    = {Bojar, Ondrej  and  Buck, Christian  and  Callison-Burch, Chris  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Monz, Christof  and  Post, Matt  and  Soricut, Radu  and  Specia, Lucia},
25	  title     = {Findings of the 2013 {Workshop on Statistical Machine Translation}},
26	  booktitle = {Proceedings of the Eighth Workshop on Statistical Machine Translation},
27	  month     = {August},
28	  year      = {2013},
29	  address   = {Sofia, Bulgaria},
30	  publisher = {Association for Computational Linguistics},
31	  pages     = {1--44},
32	  url       = {http://www.aclweb.org/anthology/W13-2201}
33	}
34	
35	"""
36	
37	_LANGUAGE_PAIRS = [(lang, "en") for lang in ["cs", "de", "fr", "es", "ru"]]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt14.py:21
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
20	
21	_URL = "http://www.statmt.org/wmt14/translation-task.html"
22	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt14.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	_URL = "http://www.statmt.org/wmt14/translation-task.html"
22	_CITATION = """
23	@InProceedings{bojar-EtAl:2014:W14-33,
24	  author    = {Bojar, Ondrej  and  Buck, Christian  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Leveling, Johannes  and  Monz, Christof  and  Pecina, Pavel  and  Post, Matt  and  Saint-Amand, Herve  and  Soricut, Radu  and  Specia, Lucia  and  Tamchyna, Ale\v{s}},
25	  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},
26	  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
27	  month     = {June},
28	  year      = {2014},
29	  address   = {Baltimore, Maryland, USA},
30	  publisher = {Association for Computational Linguistics},
31	  pages     = {12--58},
32	  url       = {http://www.aclweb.org/anthology/W/W14/W14-3302}
33	}
34	"""
35	
36	_LANGUAGE_PAIRS = [(lang, "en") for lang in ["cs", "de", "fr", "hi", "ru"]]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt15.py:21
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
20	
21	_URL = "http://www.statmt.org/wmt15/translation-task.html"
22	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt15.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	_URL = "http://www.statmt.org/wmt15/translation-task.html"
22	_CITATION = """
23	@InProceedings{bojar-EtAl:2015:WMT,
24	  author    = {Bojar, Ond\v{r}ej  and  Chatterjee, Rajen  and  Federmann, Christian  and  Haddow, Barry  and  Huck, Matthias  and  Hokamp, Chris  and  Koehn, Philipp  and  Logacheva, Varvara  and  Monz, Christof  and  Negri, Matteo  and  Post, Matt  and  Scarton, Carolina  and  Specia, Lucia  and  Turchi, Marco},
25	  title     = {Findings of the 2015 Workshop on Statistical Machine Translation},
26	  booktitle = {Proceedings of the Tenth Workshop on Statistical Machine Translation},
27	  month     = {September},
28	  year      = {2015},
29	  address   = {Lisbon, Portugal},
30	  publisher = {Association for Computational Linguistics},
31	  pages     = {1--46},
32	  url       = {http://aclweb.org/anthology/W15-3001}
33	}
34	"""
35	
36	_LANGUAGE_PAIRS = [(lang, "en") for lang in ["cs", "de", "fi", "fr", "ru"]]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt16.py:21
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
20	
21	_URL = "http://www.statmt.org/wmt16/translation-task.html"
22	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt16.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	_URL = "http://www.statmt.org/wmt16/translation-task.html"
22	_CITATION = """
23	@InProceedings{bojar-EtAl:2016:WMT1,
24	  author    = {Bojar, Ond\v{r}ej  and  Chatterjee, Rajen  and  Federmann, Christian  and  Graham, Yvette  and  Haddow, Barry  and  Huck, Matthias  and  Jimeno Yepes, Antonio  and  Koehn, Philipp  and  Logacheva, Varvara  and  Monz, Christof  and  Negri, Matteo  and  Neveol, Aurelie  and  Neves, Mariana  and  Popel, Martin  and  Post, Matt  and  Rubino, Raphael  and  Scarton, Carolina  and  Specia, Lucia  and  Turchi, Marco  and  Verspoor, Karin  and  Zampieri, Marcos},
25	  title     = {Findings of the 2016 Conference on Machine Translation},
26	  booktitle = {Proceedings of the First Conference on Machine Translation},
27	  month     = {August},
28	  year      = {2016},
29	  address   = {Berlin, Germany},
30	  publisher = {Association for Computational Linguistics},
31	  pages     = {131--198},
32	  url       = {http://www.aclweb.org/anthology/W/W16/W16-2301}
33	}
34	"""
35	
36	_LANGUAGE_PAIRS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt17.py:21
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
20	
21	_URL = "http://www.statmt.org/wmt17/translation-task.html"
22	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt17.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	_URL = "http://www.statmt.org/wmt17/translation-task.html"
22	_CITATION = """
23	@InProceedings{bojar-EtAl:2017:WMT1,
24	  author    = {Bojar, Ond\v{r}ej  and  Chatterjee, Rajen  and  Federmann, Christian  and  Graham, Yvette  and  Haddow, Barry  and  Huang, Shujian  and  Huck, Matthias  and  Koehn, Philipp  and  Liu, Qun  and  Logacheva, Varvara  and  Monz, Christof  and  Negri, Matteo  and  Post, Matt  and  Rubino, Raphael  and  Specia, Lucia  and  Turchi, Marco},
25	  title     = {Findings of the 2017 Conference on Machine Translation (WMT17)},
26	  booktitle = {Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Task Papers},
27	  month     = {September},
28	  year      = {2017},
29	  address   = {Copenhagen, Denmark},
30	  publisher = {Association for Computational Linguistics},
31	  pages     = {169--214},
32	  url       = {http://www.aclweb.org/anthology/W17-4717}
33	}
34	"""
35	
36	_LANGUAGE_PAIRS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt18.py:21
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
20	
21	_URL = "http://www.statmt.org/wmt18/translation-task.html"
22	_CITATION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt18.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	_URL = "http://www.statmt.org/wmt18/translation-task.html"
22	_CITATION = """\
23	@InProceedings{bojar-EtAl:2018:WMT1,
24	  author    = {Bojar, Ond\v{r}ej  and  Federmann, Christian  and  Fishel, Mark
25	    and Graham, Yvette  and  Haddow, Barry  and  Huck, Matthias  and
26	    Koehn, Philipp  and  Monz, Christof},
27	  title     = {Findings of the 2018 Conference on Machine Translation (WMT18)},
28	  booktitle = {Proceedings of the Third Conference on Machine Translation,
29	    Volume 2: Shared Task Papers},
30	  month     = {October},
31	  year      = {2018},
32	  address   = {Belgium, Brussels},
33	  publisher = {Association for Computational Linguistics},
34	  pages     = {272--307},
35	  url       = {http://www.aclweb.org/anthology/W18-6401}
36	}
37	"""
38	
39	_LANGUAGE_PAIRS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt19.py:21
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
20	
21	_URL = "http://www.statmt.org/wmt19/translation-task.html"
22	# TODO(adarob): Update with citation of overview paper once it is published.
23	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt19.py:23
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
22	# TODO(adarob): Update with citation of overview paper once it is published.
23	_CITATION = """
24	@ONLINE {wmt19translate,
25	    author = "Wikimedia Foundation",
26	    title  = "ACL 2019 Fourth Conference on Machine Translation (WMT19), Shared Task: Machine Translation of News",
27	    url    = "http://www.statmt.org/wmt19/translation-task.html"
28	}
29	"""
30	
31	_LANGUAGE_PAIRS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt19_test.py:36
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
35	  SEQUENTUAL_DL_EXTRACT_RESULT = {
36	      "http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz": (
37	          "commoncrawl"
38	      ),
39	      "http://www.statmt.org/europarl/v9/training/europarl-v9.de-en.tsv.gz": (
40	          "sentences.de-en.tsv"
41	      ),
42	  }

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt19_test.py:39
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
38	      ),
39	      "http://www.statmt.org/europarl/v9/training/europarl-v9.de-en.tsv.gz": (
40	          "sentences.de-en.tsv"
41	      ),
42	  }
43	
44	  def _get_dl_extract_result(self, url):
45	    if not url:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt19_test.py:71
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
70	  SEQUENTUAL_DL_EXTRACT_RESULT = {
71	      "http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz": (
72	          "commoncrawl"
73	      ),
74	      "http://www.statmt.org/europarl/v9/training/europarl-v9.cs-en.tsv.gz": (
75	          "sentences.cs-en.tsv"
76	      ),
77	  }

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt19_test.py:74
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
73	      ),
74	      "http://www.statmt.org/europarl/v9/training/europarl-v9.cs-en.tsv.gz": (
75	          "sentences.cs-en.tsv"
76	      ),
77	  }
78	
79	  def _get_dl_extract_result(self, url):
80	    if not url:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt_t2t.py:21
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
20	
21	_URL = "https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/translate_ende.py"
22	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/translate/wmt_t2t.py:22
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
21	_URL = "https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/translate_ende.py"
22	_CITATION = """
23	@InProceedings{bojar-EtAl:2014:W14-33,
24	  author    = {Bojar, Ondrej  and  Buck, Christian  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Leveling, Johannes  and  Monz, Christof  and  Pecina, Pavel  and  Post, Matt  and  Saint-Amand, Herve  and  Soricut, Radu  and  Specia, Lucia  and  Tamchyna, Ale\v{s}},
25	  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},
26	  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
27	  month     = {June},
28	  year      = {2014},
29	  address   = {Baltimore, Maryland, USA},
30	  publisher = {Association for Computational Linguistics},
31	  pages     = {12--58},
32	  url       = {http://www.aclweb.org/anthology/W/W14/W14-3302}
33	}
34	"""
35	
36	
37	class WmtT2tTranslate(wmt.WmtTranslate):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/davis/davis.py:28
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
27	
28	_URL = 'https://data.vision.ee.ethz.ch/csergi/share/davis/'
29	
30	_CITATION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/davis/davis.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	
30	_CITATION = """\
31	@article{DBLP:journals/corr/Pont-TusetPCASG17,
32	  author    = {Jordi Pont{-}Tuset and
33	               Federico Perazzi and
34	               Sergi Caelles and
35	               Pablo Arbelaez and
36	               Alexander Sorkine{-}Hornung and
37	               Luc Van Gool},
38	  title     = {The 2017 {DAVIS} Challenge on Video Object Segmentation},
39	  journal   = {CoRR},
40	  volume    = {abs/1704.00675},
41	  year      = {2017},
42	  url       = {http://arxiv.org/abs/1704.00675},
43	  archivePrefix = {arXiv},
44	  eprint    = {1704.00675},
45	  timestamp = {Mon, 13 Aug 2018 16:48:55 +0200},
46	  biburl    = {https://dblp.org/rec/journals/corr/Pont-TusetPCASG17.bib},
47	  bibsource = {dblp computer science bibliography, https://dblp.org}
48	}
49	"""
50	
51	
52	def _read_annotation(annotation_path):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/davis/davis.py:122
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
121	        supervised_keys=None,
122	        homepage='https://davischallenge.org/',
123	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/moving_mnist.py:25
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
24	_SEQUENCE_LENGTH = 20
25	_URL = "http://www.cs.toronto.edu/~nitish/unsupervised_video/"
26	_CITATION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/moving_mnist.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	_URL = "http://www.cs.toronto.edu/~nitish/unsupervised_video/"
26	_CITATION = """\
27	@article{DBLP:journals/corr/SrivastavaMS15,
28	  author    = {Nitish Srivastava and
29	               Elman Mansimov and
30	               Ruslan Salakhutdinov},
31	  title     = {Unsupervised Learning of Video Representations using LSTMs},
32	  journal   = {CoRR},
33	  volume    = {abs/1502.04681},
34	  year      = {2015},
35	  url       = {http://arxiv.org/abs/1502.04681},
36	  archivePrefix = {arXiv},
37	  eprint    = {1502.04681},
38	  timestamp = {Mon, 13 Aug 2018 16:47:05 +0200},
39	  biburl    = {https://dblp.org/rec/bib/journals/corr/SrivastavaMS15},
40	  bibsource = {dblp computer science bibliography, https://dblp.org}
41	}
42	"""
43	_DESCRIPTION = """\

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/moving_mnist.py:56
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
55	  RELEASE_NOTES = {
56	      "1.0.0": "New split API (https://tensorflow.org/datasets/splits)",
57	  }
58	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/tao/tao.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	
30	_VIDEO_URL = 'https://motchallenge.net/data/'
31	_ANNOTATIONS_URL = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/tao/tao.py:32
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
31	_ANNOTATIONS_URL = (
32	    'https://github.com/TAO-Dataset/annotations/archive/v1.2.tar.gz'
33	)
34	
35	_DESCRIPTION = """
36	The TAO dataset is a large video object detection dataset consisting of

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/tao/tao.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	
41	_CITATION = """
42	@article{Dave_2020,
43	   title={TAO: A Large-Scale Benchmark for Tracking Any Object},
44	   ISBN={9783030585587},
45	   ISSN={1611-3349},
46	   url={http://dx.doi.org/10.1007/978-3-030-58558-7_26},
47	   DOI={10.1007/978-3-030-58558-7_26},
48	   journal={Lecture Notes in Computer Science},
49	   publisher={Springer International Publishing},
50	   author={Dave, Achal and Khurana, Tarasha and Tokmakov, Pavel and Schmid, Cordelia and Ramanan, Deva},
51	   year={2020},
52	   pages={436-454}
53	}
54	"""
55	
56	NestedDict = Mapping[str, Any]

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/tao/tao.py:259
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
258	
259	  MANUAL_DOWNLOAD_INSTRUCTIONS = """
260	  Some TAO files (HVACS and AVA videos) must be manually downloaded because
261	  a login to MOT is required. Please download and those data following
262	  the instructions at https://motchallenge.net/tao_download.php
263	
264	  Download this data and move the resulting .zip files to
265	  ~/tensorflow_datasets/downloads/manual/
266	
267	  If the data requiring manual download is not present, it will be skipped over
268	  and only the data not requiring manual download will be used.
269	  """
270	  BUILDER_CONFIGS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/tao/tao.py:330
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
329	        supervised_keys=None,
330	        homepage='https://taodataset.org/',
331	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/ucf101.py:24
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
23	
24	UCF_101_URL = 'https://storage.googleapis.com/thumos14_files/UCF101_videos.zip'
25	SPLITS_URL = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/ucf101.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	SPLITS_URL = (
26	    'https://www.crcv.ucf.edu/data/UCF101/'
27	    'UCF101TrainTestSplits-RecognitionTask.zip'
28	)
29	
30	_CITATION = """\
31	@article{DBLP:journals/corr/abs-1212-0402,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/ucf101.py:30
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
29	
30	_CITATION = """\
31	@article{DBLP:journals/corr/abs-1212-0402,
32	  author    = {Khurram Soomro and
33	               Amir Roshan Zamir and
34	               Mubarak Shah},
35	  title     = {{UCF101:} {A} Dataset of 101 Human Actions Classes From Videos in
36	               The Wild},
37	  journal   = {CoRR},
38	  volume    = {abs/1212.0402},
39	  year      = {2012},
40	  url       = {http://arxiv.org/abs/1212.0402},
41	  archivePrefix = {arXiv},
42	  eprint    = {1212.0402},
43	  timestamp = {Mon, 13 Aug 2018 16:47:45 +0200},
44	  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1212-0402},
45	  bibsource = {dblp computer science bibliography, https://dblp.org}
46	}
47	"""
48	
49	_LABELS_FNAME = 'video/ucf101_labels.txt'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/ucf101.py:71
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
70	        release_notes={
71	            '2.0.0': 'New split API (https://tensorflow.org/datasets/splits)',
72	        },
73	        **kwargs,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/ucf101.py:156
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
155	        features=features,
156	        homepage='https://www.crcv.ucf.edu/data-sets/ucf101/',
157	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/youtube_vis/youtube_vis.py:41
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
40	
41	_CITATION = """
42	@article{DBLP:journals/corr/abs-1905-04804,
43	  author    = {Linjie Yang and
44	               Yuchen Fan and
45	               Ning Xu},
46	  title     = {Video Instance Segmentation},
47	  journal   = {CoRR},
48	  volume    = {abs/1905.04804},
49	  year      = {2019},
50	  url       = {http://arxiv.org/abs/1905.04804},
51	  archivePrefix = {arXiv},
52	  eprint    = {1905.04804},
53	  timestamp = {Tue, 28 May 2019 12:48:08 +0200},
54	  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-04804.bib},
55	  bibsource = {dblp computer science bibliography, https://dblp.org}
56	}
57	"""
58	
59	NUM_TRAIN_EXAMPLES = 2238

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/youtube_vis/youtube_vis.py:315
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
314	
315	  MANUAL_DOWNLOAD_INSTRUCTIONS = """
316	  Please download all files for the 2019 version of the dataset
317	  (test_all_frames.zip, test.json, train_all_frames.zip, train.json,
318	  valid_all_frames.zip, valid.json) from the youtube-vis website
319	  and move them to ~/tensorflow_datasets/downloads/manual/.
320	
321	  Note that the dataset landing page is located at
322	  https://youtube-vos.org/dataset/vis/, and it will then redirect you to a page
323	  on https://competitions.codalab.org where you can download the 2019 version
324	  of the dataset. You will need to make an account on codalab to download the
325	  data. Note that at the time of writing this, you will need to bypass a
326	  "Connection not secure" warning when accessing codalab.
327	  """
328	  BUILDER_CONFIGS = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/video/youtube_vis/youtube_vis.py:402
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
401	        supervised_keys=None,
402	        homepage='https://youtube-vos.org/dataset/vis/',
403	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/gref/gref.py:104
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
103	  }
104	  MANUAL_DOWNLOAD_INSTRUCTIONS = """
105	    Follow instructions at https://github.com/mjhucla/Google_Refexp_toolbox
106	    to download and pre-process the data into aligned format with COCO.
107	    The directory contains 2 files and one folder:
108	    * google_refexp_train_201511_coco_aligned_catg.json
109	    * google_refexp_val_201511_coco_aligned_catg.json
110	    * coco_train2014/
111	
112	    The coco_train2014 folder contains all of COCO 2014 training images.
113	    """
114	
115	  def _info(self) -> tfds.core.DatasetInfo:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/gref/gref.py:121
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
120	        description=_DESCRIPTION,
121	        homepage='https://github.com/mjhucla/Google_Refexp_toolbox',
122	        features=tfds.features.FeaturesDict({

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/grounded_scan/grounded_scan.py:27
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
26	
27	_DESCRIPTION = """
28	Grounded SCAN (gSCAN) is a synthetic dataset for evaluating compositional
29	generalization in situated language understanding. gSCAN pairs natural language
30	instructions with action sequences, and requires the agent to interpret
31	instructions within the context of a grid-based visual navigation environment.
32	
33	More information can be found at:
34	
35	* For the `compositional_splits` and the `target_length_split`:
36	https://github.com/LauraRuis/groundedSCAN
37	
38	* For the `spatial_relation_splits`:
39	https://github.com/google-research/language/tree/master/language/gscan/data
40	"""
41	
42	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/grounded_scan/grounded_scan.py:42
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
41	
42	_CITATION = """
43	@inproceedings{NEURIPS2020_e5a90182,
44	 author = {Ruis, Laura and Andreas, Jacob and Baroni, Marco and Bouchacourt, Diane and Lake, Brenden M},
45	 booktitle = {Advances in Neural Information Processing Systems},
46	 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
47	 pages = {19861--19872},
48	 publisher = {Curran Associates, Inc.},
49	 title = {A Benchmark for Systematic Generalization in Grounded Language Understanding},
50	 url = {https://proceedings.neurips.cc/paper/2020/file/e5a90182cc81e12ab5e72d66e0b46fe3-Paper.pdf},
51	 volume = {33},
52	 year = {2020}
53	}
54	
55	@inproceedings{qiu-etal-2021-systematic,
56	    title = "Systematic Generalization on g{SCAN}: {W}hat is Nearly Solved and What is Next?",
57	    author = "Qiu, Linlu  and
58	      Hu, Hexiang  and
59	      Zhang, Bowen  and
60	      Shaw, Peter  and
61	      Sha, Fei",
62	    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
63	    month = nov,
64	    year = "2021",
65	    address = "Online and Punta Cana, Dominican Republic",
66	    publisher = "Association for Computational Linguistics",
67	    url = "https://aclanthology.org/2021.emnlp-main.166",
68	    doi = "10.18653/v1/2021.emnlp-main.166",
69	    pages = "2180--2188",
70	}
71	"""
72	
73	_GSCAN_DATA_PATH = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/grounded_scan/grounded_scan.py:74
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
73	_GSCAN_DATA_PATH = (
74	    'https://raw.githubusercontent.com/LauraRuis/groundedSCAN/master/data/'
75	)
76	_SPATIAL_DATA_PATH = 'https://storage.googleapis.com/gresearch/gscan/'
77	

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/grounded_scan/grounded_scan.py:76
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
75	)
76	_SPATIAL_DATA_PATH = 'https://storage.googleapis.com/gresearch/gscan/'
77	
78	
79	class GroundedScanConfig(tfds.core.BuilderConfig):

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/grounded_scan/grounded_scan.py:183
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
182	        supervised_keys=None,
183	        homepage='https://github.com/LauraRuis/groundedSCAN',
184	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/laion400m/laion400m.py:26
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
25	
26	_DESCRIPTION = """
27	The LAION-400M dataset is completely openly, freely accessible.
28	
29	Check
30	[https://laion.ai/laion-400-open-dataset/](https://laion.ai/laion-400-open-dataset/)
31	for the full description of this dataset.
32	
33	All images and texts in the LAION-400M dataset have been filtered with OpenAIs
34	CLIP by calculating the cosine similarity between the text and image embeddings
35	and dropping those with a similarity below 0.3. The threshold of 0.3 had been
36	determined through human evaluations and seemed to be a good heuristic for
37	estimating semantic image-text-content matching.
38	
39	The image-text-pairs have been extracted from the Common Crawl web data dump and
40	are from random web pages crawled between 2014 and 2021.
41	"""
42	
43	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/laion400m/laion400m.py:43
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
42	
43	_CITATION = """
44	@article{DBLP:journals/corr/abs-2111-02114,
45	  author    = {Christoph Schuhmann and
46	               Richard Vencu and
47	               Romain Beaumont and
48	               Robert Kaczmarczyk and
49	               Clayton Mullis and
50	               Aarush Katta and
51	               Theo Coombes and
52	               Jenia Jitsev and
53	               Aran Komatsuzaki},
54	  title     = {{LAION-400M:} Open Dataset of CLIP-Filtered 400 Million Image-Text
55	               Pairs},
56	  journal   = {CoRR},
57	  volume    = {abs/2111.02114},
58	  year      = {2021},
59	  url       = {https://arxiv.org/abs/2111.02114},
60	  eprinttype = {arXiv},
61	  eprint    = {2111.02114},
62	  timestamp = {Fri, 05 Nov 2021 15:25:54 +0100},
63	  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-02114.bib},
64	  bibsource = {dblp computer science bibliography, https://dblp.org}
65	}
66	"""
67	
68	_HOMEPAGE = 'https://laion.ai/blog/laion-400-open-dataset/'

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/laion400m/laion400m.py:68
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
67	
68	_HOMEPAGE = 'https://laion.ai/blog/laion-400-open-dataset/'
69	_EMBEDDINGS_URL = (

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/laion400m/laion400m.py:70
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
69	_EMBEDDINGS_URL = (
70	    'https://deploy.laion.ai/8f83b608504d46bb81708ec86e912220/embeddings/'
71	)
72	
73	_CLIP_EMBEDDING_SHAPE = (512,)
74	_MISSING_SIMILARITY_VALUE = -1.0

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/wit/wit.py:52
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
51	      "1.0.0": (
52	          "Initial release. It loads the WIT dataset from "
53	          "https://storage.googleapis.com/gresearch/wit/"
54	      ),
55	      "1.1.0": "Added `val` and `test` splits.",
56	  }
57	
58	  def _info(self) -> tfds.core.DatasetInfo:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/wit/wit.py:83
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
82	        supervised_keys=None,
83	        homepage="https://github.com/google-research-datasets/wit/",
84	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/wit/wit.py:89
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
88	    """Returns SplitGenerators."""
89	    wit_homepage = "https://storage.googleapis.com/gresearch/wit/"
90	    wit_train_urls_to_download = [

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/wit_kaggle/wit_kaggle.py:32
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
31	
32	_DESCRIPTION = """
33	Wikipedia - Image/Caption Matching Kaggle Competition.
34	
35	This competition is organized by the
36	[Research team](https://research.wikimedia.org/) at the
37	[Wikimedia Foundation](https://wikimediafoundation.org/) in collaboration with
38	Google Research and a few external collaborators.
39	This competition is based on the
40	[WIT dataset](https://github.com/google-research-datasets/wit) published by
41	Google Research as detailed in this\
42	[SIGIR paper](https://dl.acm.org/doi/abs/10.1145/3404835.3463257).
43	
44	In this competition, youll build a model that automatically retrieves the text
45	closest to an image. Specifically, you'll train your model to associate given
46	images with article titles or complex captions, in multiple languages.
47	The best models will account for the semantic granularity of Wikipedia images.
48	If successful, you'll be contributing to the accessibility of the largest
49	online encyclopedia. The millions of Wikipedia readers and edietors will be able
50	to more easily understand, search, and describe media at scale. As a result,
51	youll contribute to an open model to improve learning for all.
52	"""
53	
54	_CITATION = """

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/wit_kaggle/wit_kaggle.py:130
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
129	      ),
130	      "1.0.0": """Initial release. It provides the train and test datasets from the
131	      Wikipedia - Image/Caption Matching Kaggle competition
132	      (https://www.kaggle.com/c/wikipedia-image-caption/data).
133	
134	      The goal of the competition is to build a model that automatically
135	      retrieves the text closest to an image. Specifically, the model shuld be
136	      trained to associate given images with article titles or complex captions,
137	      in multiple languages. The best models will account for the semantic

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/wit_kaggle/wit_kaggle.py:209
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
208	
209	  MANUAL_DOWNLOAD_INSTRUCTIONS = """
210	  Depending on the config called, manual_dir should contain some of the
211	  following subdirectories:
212	    * train
213	      - train-{0000x}-of-00005.tsv.zip
214	      - image_data_train/
215	        * image_pixels/
216	          - train_image_pixels_part-00{000-199}.csv.gz
217	        * resnet_embeddings/
218	          - train_resnet_embeddings_part-00{000-214}.csv.gz
219	    * test
220	      - test.tsv.zip
221	      - image_data_test/
222	        * image_pixels/
223	          - test_image_pixels_part-0000{0-4}.csv
224	        * resnet_embeddings/
225	          - test_resnet_embeddings_part-0000{0-9}.csv
226	
227	  Registration at https://www.kaggle.com/c/wikipedia-image-caption/data
228	  is needed to get the links to download the dataset.
229	  """
230	
231	  def _info(self) -> tfds.core.DatasetInfo:

--------------------------------------------------
>> Issue: [B824:url_found] url_found
   Severity: Medium   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/wit_kaggle/wit_kaggle.py:241
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b824_url_found.html
240	        ),
241	        homepage="https://www.kaggle.com/c/wikipedia-image-caption/code",
242	        citation=_CITATION,

--------------------------------------------------
>> Issue: [B801:b64decode] base64.b64decode
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/wit_kaggle/wit_kaggle.py:369
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b801_b64decode.html
368	          sample_image, sample_metadata = sample_fields["image_pixels"][0]
369	          sample["image"] = io.BytesIO(base64.b64decode(sample_image))
370	          sample["metadata_url"] = sample_metadata

--------------------------------------------------
>> Issue: [B300:blacklist] base64.b64decode
   Severity: Medium   Confidence: High
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/wit_kaggle/wit_kaggle.py:369
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b300-base64-b64decode
368	          sample_image, sample_metadata = sample_fields["image_pixels"][0]
369	          sample["image"] = io.BytesIO(base64.b64decode(sample_image))
370	          sample["metadata_url"] = sample_metadata

--------------------------------------------------
>> Issue: [B801:b64decode] base64.b64decode
   Severity: High   Confidence: Medium
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/wit_kaggle/wit_kaggle.py:376
   More Info: https://bandit.readthedocs.io/en/latest/plugins/b801_b64decode.html
375	            counter("image_pixels_missing").inc()
376	            sample["image"] = io.BytesIO(base64.b64decode(_EMPTY_IMAGE_BYTES))
377	          sample["metadata_url"] = ""

--------------------------------------------------
>> Issue: [B300:blacklist] base64.b64decode
   Severity: Medium   Confidence: High
   Location: /home2/blue/Documents/PyPIAgent/Dataset/evaluation/unzip_benign/tensorflow_datasets-4.9.8/tensorflow_datasets-4.9.8/tensorflow_datasets/vision_language/wit_kaggle/wit_kaggle.py:376
   More Info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b300-base64-b64decode
375	            counter("image_pixels_missing").inc()
376	            sample["image"] = io.BytesIO(base64.b64decode(_EMPTY_IMAGE_BYTES))
377	          sample["metadata_url"] = ""

--------------------------------------------------

Code scanned:
	Total lines of code: 131313
	Total lines skipped (#nosec): 0

Run metrics:
	Total issues (by severity):
		Undefined: 0.0
		Low: 0.0
		Medium: 1524.0
		High: 257.0
	Total issues (by confidence):
		Undefined: 0.0
		Low: 0.0
		Medium: 1775.0
		High: 6.0
Files skipped (0):
