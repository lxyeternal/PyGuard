import os
import sys
import json
import glob
import time
import logging
from pathlib import Path
from typing import Dict, List, Any

PYGUARD_ROOT = Path(__file__).parent.parent.parent.parent
sys.path.append(str(PYGUARD_ROOT))
from Core.utils.llmquery import LLMAgent

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("MissedMalwareAnalyzer")


class MissedMalwareAnalyzer:
    """Analyze false negative packages missed by guarddog using LLM."""

    def __init__(self, dataset_name: str = "Evaluation"):
        self.dataset_name = dataset_name
        self.guarddog_results_dir = PYGUARD_ROOT / "Experiment" / "Results" / "PyPI" / dataset_name / "guarddog" / "malware"
        self.malware_path = PYGUARD_ROOT / "Dataset" / "PyPI" / "Experiment" / dataset_name / "unzip_malware"
        self.output_dir = Path(__file__).parent / "llm_analysis" / dataset_name
        os.makedirs(self.output_dir, exist_ok=True)
        self.prompt_dir = PYGUARD_ROOT / "Resources" / "Prompts" / "false_negative_analysis"
        self.extract_prompt = self._load_prompt("malicious_code_extract.txt")
        self.confirm_prompt = self._load_prompt("malicious_code_confirm.txt")
        self.llm_agent = LLMAgent()

    def _load_prompt(self, prompt_file: str) -> str:
        prompt_path = self.prompt_dir / prompt_file
        try:
            with open(prompt_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            logger.error(f"Cannot load prompt file {prompt_path}: {e}")
            raise

    def is_false_negative(self, file_path: str) -> bool:
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                return "Found 0 potentially malicious indicators" in content or content.strip() == "benign"
        except Exception as e:
            logger.error(f"Error reading file {file_path}: {e}")
            return False

    def find_false_negatives(self) -> List[str]:
        missed_packages = []
        for file_path in glob.glob(str(self.guarddog_results_dir / "*.txt")):
            if self.is_false_negative(file_path):
                missed_packages.append(os.path.basename(file_path)[:-4])
        logger.info(f"Found {len(missed_packages)} false negative packages")
        return missed_packages

    def confirm_malicious_code(self, code_snippet: str) -> bool:
        if not code_snippet.strip():
            return False
        try:
            prompt = self.confirm_prompt.replace("{CODE}", code_snippet)
            messages = [
                {"role": "system", "content": "You are a malicious code expert. Your task is to confirm whether the given code snippet contains malicious behavior."},
                {"role": "user", "content": prompt}
            ]
            response = self.llm_agent.perform_query(messages)
            result = json.loads(response)
            return result.get("is_malicious", False)
        except Exception as e:
            logger.error(f"Error confirming malicious code: {str(e)}")
            raise

    def analyze_file_with_llm(self, file_path: str) -> Dict[str, Any]:
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                code_content = f.read()
            if not code_content.strip():
                return {"malicious_code": "", "llm_success": True}
            prompt = self.extract_prompt.replace("{CODE}", code_content)
            messages = [
                {"role": "system", "content": "You are a malicious code expert. You need to extract the malicious code from the given code."},
                {"role": "user", "content": prompt}
            ]
            try:
                response = self.llm_agent.perform_query(messages)
            except Exception as e:
                logger.error(f"LLM query failed: {str(e)}")
                return {"malicious_code": "", "llm_success": False}
            try:
                result = json.loads(response)
                malicious_code = result.get("malicious_code", "")
                if malicious_code:
                    logger.info(f"Found potential malicious code, confirming...")
                    try:
                        if self.confirm_malicious_code(malicious_code):
                            logger.info(f"Confirmed as malicious")
                            return {"malicious_code": malicious_code, "llm_success": True}
                        else:
                            logger.info(f"Confirmed as non-malicious")
                            return {"malicious_code": "", "llm_success": True}
                    except Exception as e:
                        logger.error(f"Confirmation failed: {str(e)}")
                        return {"malicious_code": "", "llm_success": False}
                return {"malicious_code": "", "llm_success": True}
            except json.JSONDecodeError:
                logger.warning(f"LLM response is not valid JSON: {response[:100]}...")
                return {"malicious_code": "", "llm_success": False}
        except Exception as e:
            logger.error(f"Error analyzing file {file_path}: {str(e)}")
            return {"malicious_code": "", "llm_success": False}

    def analyze_package(self, package_name: str) -> None:
        package_path = self.malware_path / package_name
        output_file = self.output_dir / f"{package_name}.json"
        if not os.path.exists(package_path):
            logger.warning(f"Package path does not exist: {package_path}")
            return
        if os.path.exists(output_file) and os.path.getsize(output_file) > 0:
            logger.info(f"Skipping already analyzed package: {package_name}")
            return
        logger.info(f"Analyzing package: {package_name}")
        start_time = time.time()
        python_files = glob.glob(f"{package_path}/**/*.py", recursive=True)
        if not python_files:
            logger.warning(f"No Python files found in package {package_name}")
            return
        malicious_results = []
        llm_failed = False
        for py_file in python_files:
            logger.info(f"Analyzing file: {py_file}")
            result = self.analyze_file_with_llm(py_file)
            if not result.get("llm_success", False):
                llm_failed = True
                continue
            if result.get("malicious_code"):
                malicious_results.append({
                    "file_path": os.path.abspath(py_file),
                    "malicious_code": result["malicious_code"]
                })
        if llm_failed:
            logger.warning(f"LLM failed for {package_name}, not saving results")
            return
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(malicious_results, f, ensure_ascii=False, indent=2)
        logger.info(f"Completed {package_name}, found {len(malicious_results)} malicious files, time: {time.time() - start_time:.2f}s")

    def analyze_all_missed_packages(self):
        missed_packages = self.find_false_negatives()
        if not missed_packages:
            logger.error("No false negative packages found")
            return
        total = len(missed_packages)
        malicious_packages = []
        for i, package_name in enumerate(missed_packages, 1):
            self.analyze_package(package_name)
            output_file = self.output_dir / f"{package_name}.json"
            if os.path.exists(output_file):
                try:
                    with open(output_file, 'r', encoding='utf-8') as f:
                        if json.load(f):
                            malicious_packages.append(str(output_file))
                except Exception:
                    pass
            logger.info(f"Progress: {i}/{total} ({i/total*100:.2f}%)")
        logger.info(f"Completed! Processed {total} packages, found {len(malicious_packages)} with malicious code")


def main():
    import argparse
    parser = argparse.ArgumentParser(description="Analyze false negative malware packages using LLM")
    parser.add_argument("--dataset", type=str, default="Evaluation",
                       choices=["Evaluation", "Latest", "Obfuscation"],
                       help="Dataset name to analyze")
    args = parser.parse_args()
    logger.info(f"Starting analysis for dataset: {args.dataset}")
    analyzer = MissedMalwareAnalyzer(dataset_name=args.dataset)
    analyzer.analyze_all_missed_packages()
    logger.info("Analysis completed!")


if __name__ == "__main__":
    main()
